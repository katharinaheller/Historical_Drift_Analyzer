{
  "timestamp": "2025-11-16T23-02-07",
  "query_refined": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
  "intent": "comparative",
  "prompt_final_to_llm": "You are a comparative analyst. Compare major frameworks or schools of thought, specifying explicit historical information only when stated in the provided snippets. Never infer missing event years. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nCompare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.\n\nContext snippets:\n[1] 2201.05273v4.pdf (2022)\nSimilarly, Liu et al. proposed two topic-aware contrastive learning objectives, among which the coherence detection objective identifies topics of a dialogue by detecting the coherence change among topics and the sub-summary generation objective forces the model to capture the most salient information and generate a sub-summary for each topic. Representation Learning Efficiency. . Efficiency is a crucial aspect for modeling long documents, especially when generating long text. . Since the self-a\n\n[1] 2201.05273v4.pdf (2022)\n. We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo\n\n[1] 2201.05273v4.pdf (2022)\n. mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa\n\n[1] 2201.05273v4.pdf (2022)\n. A representative example is Manakul et al., which proposed local self-attention, allowing longer input spans during training; and explicit content selection, reducing memory and compute requirements.. Furthermore, several researchers adopted divide-and-conquer encoding methods. By splitting the long document into short sentences, it is easier to summarize each short part of the document separately, reducing the computational complexity. . 3.1.3 Multi-lingual Representation Learning. . Existing\n\n[1] 2201.05273v4.pdf (2022)\n. 3.1 Unstructured Input In text generation, most studies focus on modeling unstructured text input (e.g., sentence, paragraph, and document), which requires to accurately understand the input information and derive meaningful text representations. The aim of text representation learning is to condense the input text into low-dimensional vectors that can preserve the core semantic meanings. . In what follows, we will discuss how to derive effective semantic representations for three kinds of uns\n\n[2] D14-1162.pdf (2014)\nThe result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.. Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.. 2014. Don't count, predict! . A\nsystematic comparison of context-counting vs. context-predicting semantic vectors. . In. . Extracting semantic representations from word cooccurrence statistics: A computational study. . Behavi\n\n[1] 2201.05273v4.pdf (2022)\n. Besides, in conversational machine reading, Ouyang et al. formulated the input text as two complementary graphs, i.e., explicit and implicit discourse graphs, to fully capture the discourse relations and latent vector interactions among all the elementary discourse units. 3.1.2 Document Representation Learning. . In many text generation tasks such as document translation and document summarization, the input text might be a long document consisting of multiple paragraphs. . When encoding the d\n\n[3] NatureDeepReview.pdf (2025)\nDistributed representations and language processing Deep-learning theory shows that deep nets have two different expo nential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.\n\n[4] 2303.18223v16.pdf (2025)\n. As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s\n\n[5] 1910.10683v4.pdf (2023)\n. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. . As such, our work primarily comprises a survey, exploration, and empirical compari\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 2201.05273v4.pdf (2022)\n[2] D14-1162.pdf (2014)\n[3] NatureDeepReview.pdf (2025)\n[4] 2303.18223v16.pdf (2025)\n[5] 1910.10683v4.pdf (2023)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "chunks_final_to_llm": [
    {
      "score": 0.42787978053092957,
      "text": "Similarly, Liu et al. proposed two topic-aware contrastive learning objectives, among which the coherence detection objective identifies topics of a dialogue by detecting the coherence change among topics and the sub-summary generation objective forces the model to capture the most salient information and generate a sub-summary for each topic. Representation Learning Efficiency. . Efficiency is a crucial aspect for modeling long documents, especially when generating long text. . Since the self-a",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -2.7244242504239082,
      "relevance": 3,
      "rank": 1,
      "id": "2201.05273v4.pdf::2022::4feb806a3096"
    },
    {
      "score": 0.5028806924819946,
      "text": ". We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -3.658405750989914,
      "relevance": 3,
      "rank": 2,
      "id": "2201.05273v4.pdf::2022::47a1308f9d9e"
    },
    {
      "score": 0.452836275100708,
      "text": ". mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -3.716995418071747,
      "relevance": 3,
      "rank": 3,
      "id": "2201.05273v4.pdf::2022::9316daacef76"
    },
    {
      "score": 0.44914859533309937,
      "text": ". A representative example is Manakul et al., which proposed local self-attention, allowing longer input spans during training; and explicit content selection, reducing memory and compute requirements.. Furthermore, several researchers adopted divide-and-conquer encoding methods. By splitting the long document into short sentences, it is easier to summarize each short part of the document separately, reducing the computational complexity. . 3.1.3 Multi-lingual Representation Learning. . Existing",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -4.207024231553078,
      "relevance": 3,
      "rank": 4,
      "id": "2201.05273v4.pdf::2022::2ab4a11cfa70"
    },
    {
      "score": 0.42111125588417053,
      "text": ". 3.1 Unstructured Input In text generation, most studies focus on modeling unstructured text input (e.g., sentence, paragraph, and document), which requires to accurately understand the input information and derive meaningful text representations. The aim of text representation learning is to condense the input text into low-dimensional vectors that can preserve the core semantic meanings. . In what follows, we will discuss how to derive effective semantic representations for three kinds of uns",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -4.64045687764883,
      "relevance": 3,
      "rank": 5,
      "id": "2201.05273v4.pdf::2022::1914353417f9"
    },
    {
      "score": 0.4619614779949188,
      "text": "The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.. Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.. 2014. Don't count, predict! . A\nsystematic comparison of context-counting vs. context-predicting semantic vectors. . In. . Extracting semantic representations from word cooccurrence statistics: A computational study. . Behavi",
      "metadata": {
        "source_file": "D14-1162.pdf",
        "title": null,
        "authors": null,
        "year": "2014",
        "detected_language": null,
        "page_count": 12,
        "origin_chunk_file": "D14-1162.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.001065440475941,
      "relevance": 3,
      "rank": 6,
      "id": "D14-1162.pdf::2014::a47cb0a05267"
    },
    {
      "score": 0.5097607374191284,
      "text": ". Besides, in conversational machine reading, Ouyang et al. formulated the input text as two complementary graphs, i.e., explicit and implicit discourse graphs, to fully capture the discourse relations and latent vector interactions among all the elementary discourse units. 3.1.2 Document Representation Learning. . In many text generation tasks such as document translation and document summarization, the input text might be a long document consisting of multiple paragraphs. . When encoding the d",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.156658560037613,
      "relevance": 3,
      "rank": 7,
      "id": "2201.05273v4.pdf::2022::0d34ae9fc589"
    },
    {
      "score": 0.4544440507888794,
      "text": "Distributed representations and language processing Deep-learning theory shows that deep nets have two different expo nential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.200768440961838,
      "relevance": 3,
      "rank": 8,
      "id": "NatureDeepReview.pdf::2025::83f25ef42380"
    },
    {
      "score": 0.4400355815887451,
      "text": ". As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.237280905246735,
      "relevance": 3,
      "rank": 9,
      "id": "2303.18223v16.pdf::2025::53cf2c81bbe2"
    },
    {
      "score": 0.49583008885383606,
      "text": ". With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. . As such, our work primarily comprises a survey, exploration, and empirical compari",
      "metadata": {
        "source_file": "1910.10683v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 67,
        "origin_chunk_file": "1910.10683v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.322084419429302,
      "relevance": 3,
      "rank": 10,
      "id": "1910.10683v4.pdf::2023::665a1633ba5c"
    }
  ]
}