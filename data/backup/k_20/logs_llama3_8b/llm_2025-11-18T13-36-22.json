{
  "timestamp": "2025-11-18T13-36-22",
  "query_id": "Trace_the_historical_development_and_evolution_of_trace_key_ai_milestones_across",
  "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
  "query_refined": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
  "intent": "chronological",
  "prompt_final_to_llm": "You are an analytical historian of Artificial Intelligence. Describe how the concept evolved across time, highlighting paradigm shifts, milestones, and key theoretical transformations. Present findings in a coherent historical narrative ordered strictly by explicit *event years* found in the snippets. If a snippet provides no explicit event year, you MUST write '(event year not stated; described in YEAR PDF [n])'. Never guess or estimate historical periods under any circumstances. Avoid enumeration; emphasize causal relations and conceptual transitions. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nTrace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.\n\nContext snippets:\n[1] 1304.1083v1.pdf (1989)\n. Although it is widely acknowledged that much of human knowledge is uncertain, it is in the field of artificial intelligence that research on the representation and management of uncertainty in rule based reasoning has been focused (Kanal & Lemmer, 1986; Hink & Woods, 1987). Most of the work on uncertainty in artificial intelligence has so far been normative, stressing issues of mathematical correctness and effectiveness. . The approach taken in this paper is not normative, but descriptive. . I\n\n[2] 1304.1106v1.pdf (1990)\n. Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime\n\n[3] 1301.2254v1.pdf (2001)\n. The ancestor sets A logic program P together with a goal G, defines an SLD tree each branch of which is a refutation of G using P. no ENs with s, l and b as nodes\") we (essentially) get BNT REE as an SLD-tree.. Each successful branch re (logs of) the probabilities added be>.. = (>.1, >.2,..., An). For any goal G, S has an associated SLD-tree: the one for of the well-known 'Asia' network given in Fig 3. . This is BN19 in Fig 5. . We then used a uniform prior over the set ing a cyclic transition\n\n[4] 1887-Article Text-1883-1-10-20080129.pdf (2006)\n. Opened just last week, with the help of National Museums of Scotland funds and a gaggle of sponsors, the gallery at the Royal Museum offers a wealth of interactive, visually-stunning and unique displays.... The exhibition space has been divided into five main subject areas covering transport (Move It!), artificial intelligence (Robots), cloning (Me2), space travel (Blast Off!) and energy (Power Up). . Each subject area is designed around a number of significant museum objects, complemented by\n\n[5] 0712.3329v1.pdf (2007)\n. We then examine some of the properties of universal intelligence, such as its ability to sensibly order simple learning algorithms and connections to the theory of universal optimal learning agents.. Section 4 overviews other definitions and tests of machine intelligence that have been proposed. Although surveys of the Turing test and its many variants exist, for example [SCA00], as far as we know this section is the first general survey of definitions and tests of machine intelligence. . Give\n\n[6] 1301.3781v3.pdf (2013)\n. [%] Training time Dimensionality words [days] Semantic Syntactic Total 3 epoch CBOW 783M 15.5 53.1 36.1 3 epoch Skip-gram 783M 50.0 55.9 53.3 1 epoch CBOW 783M 13.8 49.9 33.6 0.3 1 epoch CBOW 1.6B 16.1 52.6 36.1 0.6 1 epoch CBOW 783M 15.4 53.3 36.2 0.7 1 epoch Skip-gram 783\n\n[7] D14-1162.pdf (2014)\nWe tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices below. . In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. . This is on\n\n[8] Expert_Systems.pdf (2016)\n. Knowledge-based expert systems will continue to increase individual and social potential by preserving know-how, distributing knowledge more effectively, and improving performance of tasks that require expertise. 24. I. M. Dokas and A. Alapetite, A Development Process Meta-Model for Web Based Expert Systems: The Web Engineering Point of View, Risø National Laboratory, Denmark, 2006. . 25. . R. Islam and K. Andersson, Mohammad Shahadat Hossain, A Web Based Belief Rule Based Expert System to Pre\n\n[9] 0311031v1.pdf (2018)\nIt provides a framework for processing that knowledge that integrates and simplifies a range of artificial intelligence functions including probabilistic and exact forms of reasoning, unsupervised learning, fuzzy pattern recognition, best-match information retrieval, planning, problem solving and others. Prototypes of the SP system have been developed as software simulations running on an ordinary computer. . These prototypes serve to demonstrate what can be done with the system and they provide\n\n[10] N18-1202.pdf (2018)\n. Our baseline, the ESIM sequence model from Chen et al., uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. . A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9%. . Semantic\n\n[11] 1810.04805v2.pdf (2019)\nThese include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). word based only on its con\n\n[12] 2005.14165v4.pdf (2020)\nThis last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19].\n\n[13] 2201.05273v4.pdf (2022)\n. Gu et al. represented the dialogue context using DialogBERT, a hierarchical framework that utilizes sentence- and discourselevel Transformer encoders to encode each dialogue utterance and the sequence of utterance vectors, respectively. However, when encoding each individual utterance, it does not consider the history information, which is essential for understanding dialogue utterances. . Thus, Li et al. employed a Transformer to encode each utterance into a dense vector, upon which a left-to\n\n[14] 2205.01068v4.pdf (2022)\nWe also include a gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks (splitting the division by the world size of N into two division operations by √\nThe pre-training corpus contains a concatenation of datasets used in RoBERTa, the Pile, and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021). All corpora were previously collected or filtered to contain predominantly English text, but a small amount of non-English data is\n\n[15] 2210.07321v4.pdf (2023)\n. To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),\n\n[16] 1910.10683v4.pdf (2023)\nThe rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.\n\n[17] 3641289.pdf (2024)\n. Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and\n\n[18] 2303.18223v16.pdf (2025)\nRecently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance\n\n[19] NatureDeepReview.pdf (2025)\nIn addition to beating records in image recognition1–4 and speech recognition5–7, it has beaten other machine-learning techniques at predicting the activ ity of potential drug molecules8, analysing particle accelerator data9,10, reconstructing brain circuits11, and predicting the effects of mutations in non-coding DNA on gene expression and disease12,13.\n\n[20] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. For example, DS-1000 (Lai et al. 2023) provides a large-scale benchmark of 1000 realistic problems spanning seven core Python data science libraries, with execution-based multi-criteria evaluation and mechanisms to reduce memorization bias. MLAgentBench (Huang et al. 2024a) introduces a benchmark focused on machine learning research workflows by constructing an LLM-agent pipeline.\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 1304.1083v1.pdf (1989)\n[2] 1304.1106v1.pdf (1990)\n[3] 1301.2254v1.pdf (2001)\n[4] 1887-Article Text-1883-1-10-20080129.pdf (2006)\n[5] 0712.3329v1.pdf (2007)\n[6] 1301.3781v3.pdf (2013)\n[7] D14-1162.pdf (2014)\n[8] Expert_Systems.pdf (2016)\n[9] 0311031v1.pdf (2018)\n[10] N18-1202.pdf (2018)\n[11] 1810.04805v2.pdf (2019)\n[12] 2005.14165v4.pdf (2020)\n[13] 2201.05273v4.pdf (2022)\n[14] 2205.01068v4.pdf (2022)\n[15] 2210.07321v4.pdf (2023)\n[16] 1910.10683v4.pdf (2023)\n[17] 3641289.pdf (2024)\n[18] 2303.18223v16.pdf (2025)\n[19] NatureDeepReview.pdf (2025)\n[20] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "retrieved_chunks": [
    {
      "score": 0.5301447510719299,
      "text": "Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2025,
      "final_score": 0.5301447510719299,
      "relevance": 3,
      "rank": 1,
      "id": "2303.18223v16.pdf::2025::ea3b381a808a"
    },
    {
      "score": 0.527250349521637,
      "text": ". Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2024,
      "final_score": 0.527250349521637,
      "relevance": 3,
      "rank": 2,
      "id": "3641289.pdf::2024::85dff6bd2fb4"
    },
    {
      "score": 0.5198386907577515,
      "text": ". To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2023,
      "final_score": 0.5198386907577515,
      "relevance": 3,
      "rank": 3,
      "id": "2210.07321v4.pdf::2023::f2298e5175a1"
    },
    {
      "score": 0.5026209354400635,
      "text": "In addition to beating records in image recognition1–4 and speech recognition5–7, it has beaten other machine-learning techniques at predicting the activ ity of potential drug molecules8, analysing particle accelerator data9,10, reconstructing brain circuits11, and predicting the effects of mutations in non-coding DNA on gene expression and disease12,13.",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2025,
      "final_score": 0.5026209354400635,
      "relevance": 3,
      "rank": 4,
      "id": "NatureDeepReview.pdf::2025::b3d9208bbfb5"
    },
    {
      "score": 0.4746171832084656,
      "text": "The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning.",
      "metadata": {
        "source_file": "1910.10683v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 67,
        "origin_chunk_file": "1910.10683v4.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2023,
      "final_score": 0.4746171832084656,
      "relevance": 3,
      "rank": 5,
      "id": "1910.10683v4.pdf::2023::a7d0f7cc72f9"
    },
    {
      "score": 0.4715920090675354,
      "text": "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19].",
      "metadata": {
        "source_file": "2005.14165v4.pdf",
        "title": null,
        "authors": null,
        "year": "2020",
        "detected_language": null,
        "page_count": 75,
        "origin_chunk_file": "2005.14165v4.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2020,
      "final_score": 0.4715920090675354,
      "relevance": 3,
      "rank": 6,
      "id": "2005.14165v4.pdf::2020::18e242ac88d3"
    },
    {
      "score": 0.46784642338752747,
      "text": ". The ancestor sets A logic program P together with a goal G, defines an SLD tree each branch of which is a refutation of G using P. no ENs with s, l and b as nodes\") we (essentially) get BNT REE as an SLD-tree.. Each successful branch re (logs of) the probabilities added be>.. = (>.1, >.2,..., An). For any goal G, S has an associated SLD-tree: the one for of the well-known 'Asia' network given in Fig 3. . This is BN19 in Fig 5. . We then used a uniform prior over the set ing a cyclic transition",
      "metadata": {
        "source_file": "1301.2254v1.pdf",
        "title": null,
        "authors": null,
        "year": "2001",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "1301.2254v1.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2001,
      "final_score": 0.46784642338752747,
      "relevance": 3,
      "rank": 7,
      "id": "1301.2254v1.pdf::2001::02719e1eecb2"
    },
    {
      "score": 0.46071603894233704,
      "text": ". For example, DS-1000 (Lai et al. 2023) provides a large-scale benchmark of 1000 realistic problems spanning seven core Python data science libraries, with execution-based multi-criteria evaluation and mechanisms to reduce memorization bias. MLAgentBench (Huang et al. 2024a) introduces a benchmark focused on machine learning research workflows by constructing an LLM-agent pipeline.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2025,
      "final_score": 0.46071603894233704,
      "relevance": 2,
      "rank": 8,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::846d78b62e9a"
    },
    {
      "score": 0.4597541093826294,
      "text": ". Gu et al. represented the dialogue context using DialogBERT, a hierarchical framework that utilizes sentence- and discourselevel Transformer encoders to encode each dialogue utterance and the sequence of utterance vectors, respectively. However, when encoding each individual utterance, it does not consider the history information, which is essential for understanding dialogue utterances. . Thus, Li et al. employed a Transformer to encode each utterance into a dense vector, upon which a left-to",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2022,
      "final_score": 0.4597541093826294,
      "relevance": 2,
      "rank": 9,
      "id": "2201.05273v4.pdf::2022::0b5b95a2776c"
    },
    {
      "score": 0.4556267559528351,
      "text": ". Opened just last week, with the help of National Museums of Scotland funds and a gaggle of sponsors, the gallery at the Royal Museum offers a wealth of interactive, visually-stunning and unique displays.... The exhibition space has been divided into five main subject areas covering transport (Move It!), artificial intelligence (Robots), cloning (Me2), space travel (Blast Off!) and energy (Power Up). . Each subject area is designed around a number of significant museum objects, complemented by ",
      "metadata": {
        "source_file": "1887-Article Text-1883-1-10-20080129.pdf",
        "title": null,
        "authors": null,
        "year": "2006",
        "detected_language": null,
        "page_count": 1,
        "origin_chunk_file": "1887-Article Text-1883-1-10-20080129.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2006,
      "final_score": 0.4556267559528351,
      "relevance": 2,
      "rank": 10,
      "id": "1887-Article Text-1883-1-10-20080129.pdf::2006::fa88a06ed47a"
    },
    {
      "score": 0.4552728533744812,
      "text": "We also include a gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks (splitting the division by the world size of N into two division operations by √\nThe pre-training corpus contains a concatenation of datasets used in RoBERTa, the Pile, and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021). All corpora were previously collected or filtered to contain predominantly English text, but a small amount of non-English data is ",
      "metadata": {
        "source_file": "2205.01068v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 30,
        "origin_chunk_file": "2205.01068v4.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2022,
      "final_score": 0.4552728533744812,
      "relevance": 2,
      "rank": 11,
      "id": "2205.01068v4.pdf::2022::f0cb1e4128fa"
    },
    {
      "score": 0.45283398032188416,
      "text": "These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). word based only on its con",
      "metadata": {
        "source_file": "1810.04805v2.pdf",
        "title": null,
        "authors": null,
        "year": "2019",
        "detected_language": null,
        "page_count": 16,
        "origin_chunk_file": "1810.04805v2.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2019,
      "final_score": 0.45283398032188416,
      "relevance": 2,
      "rank": 12,
      "id": "1810.04805v2.pdf::2019::2dfd474a5b5f"
    },
    {
      "score": 0.4509235620498657,
      "text": "It provides a framework for processing that knowledge that integrates and simplifies a range of artificial intelligence functions including probabilistic and exact forms of reasoning, unsupervised learning, fuzzy pattern recognition, best-match information retrieval, planning, problem solving and others. Prototypes of the SP system have been developed as software simulations running on an ordinary computer. . These prototypes serve to demonstrate what can be done with the system and they provide",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2018,
      "final_score": 0.4509235620498657,
      "relevance": 2,
      "rank": 13,
      "id": "0311031v1.pdf::2018::1d8bb9d6b450"
    },
    {
      "score": 0.4452262222766876,
      "text": ". Knowledge-based expert systems will continue to increase individual and social potential by preserving know-how, distributing knowledge more effectively, and improving performance of tasks that require expertise. 24. I. M. Dokas and A. Alapetite, A Development Process Meta-Model for Web Based Expert Systems: The Web Engineering Point of View, Risø National Laboratory, Denmark, 2006. . 25. . R. Islam and K. Andersson, Mohammad Shahadat Hossain, A Web Based Belief Rule Based Expert System to Pre",
      "metadata": {
        "source_file": "Expert_Systems.pdf",
        "title": null,
        "authors": null,
        "year": "2016",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "Expert_Systems.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2016,
      "final_score": 0.4452262222766876,
      "relevance": 2,
      "rank": 14,
      "id": "Expert_Systems.pdf::2016::3f868e4f3792"
    },
    {
      "score": 0.439639151096344,
      "text": ". Although it is widely acknowledged that much of human knowledge is uncertain, it is in the field of artificial intelligence that research on the representation and management of uncertainty in rule based reasoning has been focused (Kanal & Lemmer, 1986; Hink & Woods, 1987). Most of the work on uncertainty in artificial intelligence has so far been normative, stressing issues of mathematical correctness and effectiveness. . The approach taken in this paper is not normative, but descriptive. . I",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 1989,
      "final_score": 0.439639151096344,
      "relevance": 1,
      "rank": 15,
      "id": "1304.1083v1.pdf::1989::e330b10ef834"
    },
    {
      "score": 0.4390386939048767,
      "text": ". Our baseline, the ESIM sequence model from Chen et al., uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer. Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds. . A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9%. . Semantic ",
      "metadata": {
        "source_file": "N18-1202.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 11,
        "origin_chunk_file": "N18-1202.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2018,
      "final_score": 0.4390386939048767,
      "relevance": 1,
      "rank": 16,
      "id": "N18-1202.pdf::2018::b65019079157"
    },
    {
      "score": 0.4345223903656006,
      "text": ". We then examine some of the properties of universal intelligence, such as its ability to sensibly order simple learning algorithms and connections to the theory of universal optimal learning agents.. Section 4 overviews other definitions and tests of machine intelligence that have been proposed. Although surveys of the Turing test and its many variants exist, for example [SCA00], as far as we know this section is the first general survey of definitions and tests of machine intelligence. . Give",
      "metadata": {
        "source_file": "0712.3329v1.pdf",
        "title": null,
        "authors": null,
        "year": "2007",
        "detected_language": null,
        "page_count": 49,
        "origin_chunk_file": "0712.3329v1.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2007,
      "final_score": 0.4345223903656006,
      "relevance": 1,
      "rank": 17,
      "id": "0712.3329v1.pdf::2007::d93703f6d548"
    },
    {
      "score": 0.43341854214668274,
      "text": "We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices below. . In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. . This is on",
      "metadata": {
        "source_file": "D14-1162.pdf",
        "title": null,
        "authors": null,
        "year": "2014",
        "detected_language": null,
        "page_count": 12,
        "origin_chunk_file": "D14-1162.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2014,
      "final_score": 0.43341854214668274,
      "relevance": 1,
      "rank": 18,
      "id": "D14-1162.pdf::2014::3e6b1d89b8a2"
    },
    {
      "score": 0.4285779595375061,
      "text": ". Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime",
      "metadata": {
        "source_file": "1304.1106v1.pdf",
        "title": null,
        "authors": null,
        "year": "1990",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "1304.1106v1.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 1990,
      "final_score": 0.4285779595375061,
      "relevance": 1,
      "rank": 19,
      "id": "1304.1106v1.pdf::1990::6ce03695c520"
    },
    {
      "score": 0.4230341911315918,
      "text": ". [%] Training time Dimensionality words [days] Semantic Syntactic Total 3 epoch CBOW 783M 15.5 53.1 36.1 3 epoch Skip-gram 783M 50.0 55.9 53.3 1 epoch CBOW 783M 13.8 49.9 33.6 0.3 1 epoch CBOW 1.6B 16.1 52.6 36.1 0.6 1 epoch CBOW 783M 15.4 53.3 36.2 0.7 1 epoch Skip-gram 783",
      "metadata": {
        "source_file": "1301.3781v3.pdf",
        "title": null,
        "authors": null,
        "year": "2013",
        "detected_language": null,
        "page_count": 12,
        "origin_chunk_file": "1301.3781v3.chunks.json"
      },
      "query": "Trace the historical development and evolution of trace key ai milestones across decades referenced in the corpus. strictly through the explicit event years present in the snippets. If no explicit event year is present for a point, note that the event year is not stated.",
      "year": 2013,
      "final_score": 0.4230341911315918,
      "relevance": 1,
      "rank": 20,
      "id": "1301.3781v3.pdf::2013::69e58b36cd05"
    }
  ],
  "model_output": "The development and evolution of Artificial Intelligence (AI) across decades, as referenced in the provided corpus, can be outlined as follows:\n\n1. **Early AI Research (1980s - 1990s)**\n   - Focus on representing and managing uncertainty in rule-based reasoning was prevalent during this period [1]. - Bayesian networks were introduced as a formalism for modeling uncertainty, promoting transparency and reproducibility in AI [2]. 2. **Probabilistic Reasoning (2001)**\n   - The BN Learner (BNT) system was developed to learn Bayesian network structures from data using the K2 score [3]. 3. **Advancements in Natural Language Processing (NLP) (2013 – Present)**\n   - Significant progress in NLP, particularly in areas like semantic understanding and text generation, began with publications like Word2Vec [6] and DeepMind's neural Turing machine [9]. 4. **Transformer Models (2017 – Present)**\n   - Transformer models, such as BERT (Bidirectional Encoder Representations from Transformers), revolutionized NLP by providing state-of-the-art performance in various tasks [11, 18]. - Large Language Models (LLMs) have become a focus of research, with advancements like RoBERTa and ChatGPT leading the way [13, 18]. 5. **Benchmarking AI Systems (2023 – Present)**\n   - Efforts have been made to create benchmarks for evaluating AI systems effectively, reducing memorization bias, and comparing different algorithms [20]. References:\n[1] 1304.1083v1.pdf (1989)\n[2] 1304.1106v1.pdf (1990)\n[3] 1301.2254v1.pdf (2001)\n[6] 1301.3781v3.pdf (2013)\n[11] 1810.04805v2.pdf (2019)\n[18] 2303.18223v16.pdf (2025)\n[20] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)"
}