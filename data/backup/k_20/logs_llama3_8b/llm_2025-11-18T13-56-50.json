{
  "timestamp": "2025-11-18T13-56-50",
  "query_id": "Define_summarize_the_discussion_of_cognitive_plausibility_in_ai_modeling___descr",
  "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
  "query_refined": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
  "intent": "conceptual",
  "prompt_final_to_llm": "You are a domain expert in Artificial Intelligence. Provide a precise definition, clarify theoretical foundations, and explain how interpretations evolved across time and publications. Use event years ONLY if explicitly stated in the snippets. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nDefine summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.\n\nContext snippets:\n[1] 3641289.pdf (2024)\n. Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and\n\n[2] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.\n\n[3] 0311031v1.pdf (2018)\n. The SP theory is a new theory of computing and cognition developed with the aim of integrating and simplifying a range of concepts in computing and cognitive science, with a particular emphasis on concepts in artificial intelligence. An overview of the theory is presented in Wolff and more detail may be found in earlier publications cited there. . Amongst other things, the SP theory provides an attractive model for database applications, especially those requiring a measure of human-like 'inte\n\n[4] 2210.07321v4.pdf (2023)\n. To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),\n\n[1] 3641289.pdf (2024)\n. A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse\n\n[5] 2303.18223v16.pdf (2025)\n. Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4.\n\n[6] 2201.05273v4.pdf (2022)\n. mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa\n\n[1] 3641289.pdf (2024)\nThis statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.. 3.1.2. Reasoning. The task of reasoning poses significant challenges for an intelligent AI model. . To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inferenc\n\n[3] 0311031v1.pdf (2018)\n. Chief amongst these is the remarkable simplicity of the system combined with its very wide scope, much wider than the great majority of artificial intelligence systems, with the possible exception of unified theories of cognition such as Soar (Laird et al., 1987; Rosenbloom et al., 1993) and ACT-R (Anderson and Lebiere, 1998). . Like those two systems, the development of the SP system was inspired by the writings of Allen Newell, putting the case for greater breadth and depth in theories of co\n\n[6] 2201.05273v4.pdf (2022)\n. Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.\n\n[7] 0712.3329v1.pdf (2007)\n. In this paper we approach the problem of defining machine intelligence as follows: Section 2 overviews well known theories, definitions and tests of intelligence that have been developed by psychologists. Our objective in this section is to gain an understanding of the essence of intelligence in the broadest possible terms. . In particular we are interested in commonly expressed ideas that could be applied to arbitrary systems and contexts, not just humans. . Section 3 takes these key ideas an\n\n[8] 1304.1082v1.pdf (1990)\nThere is a vast literature on human judgment under uncertainty for very simple inference problems, typically with a single hypothesis variable and a single observation (e.g. see Kahneman et al., 1982 and Morgan & Henrion, 1990 for reviews), but relatively little is known about cognitive processes in more complex situations. To improve our insights into this and to seek inspiration for alternative approaches to explanation, we conducted a series of cognitive process-tracing studies. . We recorded\n\n[5] 2303.18223v16.pdf (2025)\n. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models.\n\n[4] 2210.07321v4.pdf (2023)\nAdditional Key Words and Phrases: machine learning, artificial intelligence, neural networks, trustworthy AI, machine generated text, transformer, text generation, threat modeling, cybersecurity, disinformation, generative AI Since the release of GPT-2 and subsequent explosion of high-quality Transformer-based NLG models, there has been only one general survey on detection of machine generated text. The scope of this previous survey is constrained to detection methods specifically targeting the\n\n[5] 2303.18223v16.pdf (2025)\nRecently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance\n\n[5] 2303.18223v16.pdf (2025)\n. Abstract—Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language.\n\n[9] 1304.1083v1.pdf (1989)\nThis was what I call the mean model since it summarizes antecedent evidence within a rule by taking the mean certainty factor, scales down the maximum certainty factor in the conclusion by taking the mean of the maximum certainty factor and the summarized antecedent certainty factor, and combines certainties across rules by taking the mean. Mean models were included in our previous experiments, but were not found to be among the best fitting. . Nonetheless, taking the mean is not an unreasonable\n\n[1] 3641289.pdf (2024)\n. We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed\n\n[10] 2205.01068v4.pdf (2022)\n. While this is a significant achievement, the energy cost of creating such a model is still nontrivial, and repeated efforts to replicate a model of this size will only amplify the growing compute footprint of these LLMs. We believe the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear 125M 6.0e−4 0.5M 350M 3.0e−4 0.5M 1.3B . 2.0e−4 1M 2.7B 1.6e−4 1M 6.7B . 1.2e−4 2M 13B 1.0e−4 4M 30B 1.0e−4 4M 66B 0.8e−4 2M 175B 1.2e−4 2\n\n[9] 1304.1083v1.pdf (1989)\n. A realistic way to simulate the data of human subjects in rule based propagation of uncertainty is to use maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules. All of the relation between the mean model and subject data can be attributed to the fact that both were highly correlated with the predictions of the MMH model. . With the MMH model predictions partialled out, the correlation between the mean model and subject\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 3641289.pdf (2024)\n[2] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[3] 0311031v1.pdf (2018)\n[4] 2210.07321v4.pdf (2023)\n[5] 2303.18223v16.pdf (2025)\n[6] 2201.05273v4.pdf (2022)\n[7] 0712.3329v1.pdf (2007)\n[8] 1304.1082v1.pdf (1990)\n[9] 1304.1083v1.pdf (1989)\n[10] 2205.01068v4.pdf (2022)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "retrieved_chunks": [
    {
      "score": 0.5402907729148865,
      "text": ". Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.6510086208581924,
      "relevance": 3,
      "rank": 1,
      "id": "3641289.pdf::2024::85dff6bd2fb4"
    },
    {
      "score": 0.43302464485168457,
      "text": ". The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.153492748737335,
      "relevance": 3,
      "rank": 2,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::53ec537880b8"
    },
    {
      "score": 0.5186558961868286,
      "text": ". The SP theory is a new theory of computing and cognition developed with the aim of integrating and simplifying a range of concepts in computing and cognitive science, with a particular emphasis on concepts in artificial intelligence. An overview of the theory is presented in Wolff and more detail may be found in earlier publications cited there. . Amongst other things, the SP theory provides an attractive model for database applications, especially those requiring a measure of human-like 'inte",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.3905822932720184,
      "relevance": 3,
      "rank": 3,
      "id": "0311031v1.pdf::2018::e0c690d03c51"
    },
    {
      "score": 0.4984820485115051,
      "text": ". To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.4967514723539352,
      "relevance": 3,
      "rank": 4,
      "id": "2210.07321v4.pdf::2023::f2298e5175a1"
    },
    {
      "score": 0.4669511914253235,
      "text": ". A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.7371527403593063,
      "relevance": 3,
      "rank": 5,
      "id": "3641289.pdf::2024::f744bf595495"
    },
    {
      "score": 0.4437774419784546,
      "text": ". Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.169234663248062,
      "relevance": 3,
      "rank": 6,
      "id": "2303.18223v16.pdf::2025::ba66543280b5"
    },
    {
      "score": 0.45019567012786865,
      "text": ". mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.291978746652603,
      "relevance": 3,
      "rank": 7,
      "id": "2201.05273v4.pdf::2022::9316daacef76"
    },
    {
      "score": 0.494204044342041,
      "text": "This statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.. 3.1.2. Reasoning. The task of reasoning poses significant challenges for an intelligent AI model. . To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inferenc",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.40830934047699,
      "relevance": 3,
      "rank": 8,
      "id": "3641289.pdf::2024::ecc662314003"
    },
    {
      "score": 0.40922534465789795,
      "text": ". Chief amongst these is the remarkable simplicity of the system combined with its very wide scope, much wider than the great majority of artificial intelligence systems, with the possible exception of unified theories of cognition such as Soar (Laird et al., 1987; Rosenbloom et al., 1993) and ACT-R (Anderson and Lebiere, 1998). . Like those two systems, the development of the SP system was inspired by the writings of Allen Newell, putting the case for greater breadth and depth in theories of co",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.625529795885086,
      "relevance": 3,
      "rank": 9,
      "id": "0311031v1.pdf::2018::d8a1ca3b655a"
    },
    {
      "score": 0.46280351281166077,
      "text": ". Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.669963680207729,
      "relevance": 3,
      "rank": 10,
      "id": "2201.05273v4.pdf::2022::bcb2357ff742"
    },
    {
      "score": 0.5896293520927429,
      "text": ". In this paper we approach the problem of defining machine intelligence as follows: Section 2 overviews well known theories, definitions and tests of intelligence that have been developed by psychologists. Our objective in this section is to gain an understanding of the essence of intelligence in the broadest possible terms. . In particular we are interested in commonly expressed ideas that could be applied to arbitrary systems and contexts, not just humans. . Section 3 takes these key ideas an",
      "metadata": {
        "source_file": "0712.3329v1.pdf",
        "title": null,
        "authors": null,
        "year": "2007",
        "detected_language": null,
        "page_count": 49,
        "origin_chunk_file": "0712.3329v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.865910246968269,
      "relevance": 3,
      "rank": 11,
      "id": "0712.3329v1.pdf::2007::b551e995a739"
    },
    {
      "score": 0.417244017124176,
      "text": "There is a vast literature on human judgment under uncertainty for very simple inference problems, typically with a single hypothesis variable and a single observation (e.g. see Kahneman et al., 1982 and Morgan & Henrion, 1990 for reviews), but relatively little is known about cognitive processes in more complex situations. To improve our insights into this and to seek inspiration for alternative approaches to explanation, we conducted a series of cognitive process-tracing studies. . We recorded",
      "metadata": {
        "source_file": "1304.1082v1.pdf",
        "title": null,
        "authors": null,
        "year": "1990",
        "detected_language": null,
        "page_count": 11,
        "origin_chunk_file": "1304.1082v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.907515272498131,
      "relevance": 3,
      "rank": 12,
      "id": "1304.1082v1.pdf::1990::1c5905d87f4b"
    },
    {
      "score": 0.4299169182777405,
      "text": ". As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.03788922727108,
      "relevance": 3,
      "rank": 13,
      "id": "2303.18223v16.pdf::2025::2814364959af"
    },
    {
      "score": 0.4276212751865387,
      "text": "Additional Key Words and Phrases: machine learning, artificial intelligence, neural networks, trustworthy AI, machine generated text, transformer, text generation, threat modeling, cybersecurity, disinformation, generative AI Since the release of GPT-2 and subsequent explosion of high-quality Transformer-based NLG models, there has been only one general survey on detection of machine generated text. The scope of this previous survey is constrained to detection methods specifically targeting the ",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.07630767673254,
      "relevance": 3,
      "rank": 14,
      "id": "2210.07321v4.pdf::2023::de479b9a4253"
    },
    {
      "score": 0.44765979051589966,
      "text": "Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.082578346133232,
      "relevance": 3,
      "rank": 15,
      "id": "2303.18223v16.pdf::2025::ea3b381a808a"
    },
    {
      "score": 0.5270822644233704,
      "text": ". Abstract—Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.093442961573601,
      "relevance": 3,
      "rank": 16,
      "id": "2303.18223v16.pdf::2025::63f7ecf49d70"
    },
    {
      "score": 0.3750554025173187,
      "text": "This was what I call the mean model since it summarizes antecedent evidence within a rule by taking the mean certainty factor, scales down the maximum certainty factor in the conclusion by taking the mean of the maximum certainty factor and the summarized antecedent certainty factor, and combines certainties across rules by taking the mean. Mean models were included in our previous experiments, but were not found to be among the best fitting. . Nonetheless, taking the mean is not an unreasonable",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.103758193552494,
      "relevance": 3,
      "rank": 17,
      "id": "1304.1083v1.pdf::1989::b317535e3e3b"
    },
    {
      "score": 0.5284572839736938,
      "text": ". We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.11808380484581,
      "relevance": 3,
      "rank": 18,
      "id": "3641289.pdf::2024::e1d85cdbb466"
    },
    {
      "score": 0.40549522638320923,
      "text": ". While this is a significant achievement, the energy cost of creating such a model is still nontrivial, and repeated efforts to replicate a model of this size will only amplify the growing compute footprint of these LLMs. We believe the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear 125M 6.0e−4 0.5M 350M 3.0e−4 0.5M 1.3B . 2.0e−4 1M 2.7B 1.6e−4 1M 6.7B . 1.2e−4 2M 13B 1.0e−4 4M 30B 1.0e−4 4M 66B 0.8e−4 2M 175B 1.2e−4 2",
      "metadata": {
        "source_file": "2205.01068v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 30,
        "origin_chunk_file": "2205.01068v4.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.137070521712303,
      "relevance": 3,
      "rank": 19,
      "id": "2205.01068v4.pdf::2022::9943acd00d7a"
    },
    {
      "score": 0.3666343688964844,
      "text": ". A realistic way to simulate the data of human subjects in rule based propagation of uncertainty is to use maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules. All of the relation between the mean model and subject data can be attributed to the fact that both were highly correlated with the predictions of the MMH model. . With the MMH model predictions partialled out, the correlation between the mean model and subject ",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define summarize the discussion of cognitive plausibility in ai modeling., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.158201217651367,
      "relevance": 3,
      "rank": 20,
      "id": "1304.1083v1.pdf::1989::8a4aedfcecad"
    }
  ],
  "model_output": "Artificial Intelligence (AI) models have historically been evaluated based on their ability to pass the Turing Test, which aims to discern if responses are of human or machine origin [1]. This test is considered a benchmark for assessing intelligence in AI, and a machine that successfully passes it is often seen as intelligent [1]. The theoretical foundations of cognitive plausibility in AI modeling stem from the concept of integrating and simplifying various concepts in computing and cognitive science, particularly those related to artificial intelligence. One example is the SP theory, which aims to provide an attractive model for database applications with a measure of human-like 'intelligence' [3]. The discussion on cognitive plausibility in AI modeling evolved across publications as follows:\n\n1. (event year not stated; described in 2007 PDF [7]) The problem of defining machine intelligence was approached by overviewing well-known theories, definitions, and tests of intelligence developed by psychologists, aiming to gain an understanding of the essence of intelligence in the broadest possible terms for arbitrary systems and contexts. 2. (event year not stated; described in 1989 PDF [9]) Research focused on simulating data of human subjects in rule-based propagation of uncertainty using methods such as maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules. 3. (2018; described in 2018 PDF [3]) The development of the SP theory aimed to provide a remarkable simplicity combined with wide scope in artificial intelligence, comparable to unified theories such as Soar and ACT-R. 4. (2022; described in 2022 PDF [6]) Text summarization approaches for AI focused on either extractive or abstractive methods to condense text into a brief summary that retains key information from the source text, with an emphasis on controlling output text for various applications. 5. (2023; described in 2023 PDF [4]) The potential of large language models (LLMs) and their impact on data science, analysis, trustworthy AI, cybersecurity, disinformation, and generative AI have gained increasing recognition in recent years. 6. (2024; described in 2024 PDF [1]) The importance of evaluating AI, particularly large language models, was emphasized as a critical tool to identify current system limitations and inform the design of more powerful models. Recent advancements in LLMs have raised promise that they could be considered artificial general intelligence (AGI) in this era. References:\n[1] 3641289.pdf (2024)\n[2] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[3] 0311031v1.pdf (2018)\n[4] 2210.07321v4.pdf (2023)\n[5] 2303.18223v16.pdf (2025)\n[6] 2201.05273v4.pdf (2022)\n[7] 0712.3329v1.pdf (2007)\n[8] 1304.1082v1.pdf (1990)\n[9] 1304.1083v1.pdf (1989)\n[10] 2205.01068v4.pdf (2022)"
}