{
  "timestamp": "2025-11-18T13-44-46",
  "query_refined": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
  "intent": "comparative",
  "prompt_final_to_llm": "You are a comparative analyst. Compare major frameworks or schools of thought, specifying explicit historical information only when stated in the provided snippets. Never infer missing event years. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nCompare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.\n\nContext snippets:\n[1] 2201.05273v4.pdf (2022)\nSimilarly, Liu et al. proposed two topic-aware contrastive learning objectives, among which the coherence detection objective identifies topics of a dialogue by detecting the coherence change among topics and the sub-summary generation objective forces the model to capture the most salient information and generate a sub-summary for each topic. Representation Learning Efficiency. . Efficiency is a crucial aspect for modeling long documents, especially when generating long text. . Since the self-a\n\n[2] NatureDeepReview.pdf (2025)\n. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification.\n\n[1] 2201.05273v4.pdf (2022)\n. We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo\n\n[1] 2201.05273v4.pdf (2022)\n. mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa\n\n[1] 2201.05273v4.pdf (2022)\n. A representative example is Manakul et al., which proposed local self-attention, allowing longer input spans during training; and explicit content selection, reducing memory and compute requirements.. Furthermore, several researchers adopted divide-and-conquer encoding methods. By splitting the long document into short sentences, it is easier to summarize each short part of the document separately, reducing the computational complexity. . 3.1.3 Multi-lingual Representation Learning. . Existing\n\n[1] 2201.05273v4.pdf (2022)\n. 3.1 Unstructured Input In text generation, most studies focus on modeling unstructured text input (e.g., sentence, paragraph, and document), which requires to accurately understand the input information and derive meaningful text representations. The aim of text representation learning is to condense the input text into low-dimensional vectors that can preserve the core semantic meanings. . In what follows, we will discuss how to derive effective semantic representations for three kinds of uns\n\n[3] D14-1162.pdf (2014)\nThe result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.. Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.. 2014. Don't count, predict! . A\nsystematic comparison of context-counting vs. context-predicting semantic vectors. . In. . Extracting semantic representations from word cooccurrence statistics: A computational study. . Behavi\n\n[1] 2201.05273v4.pdf (2022)\n. Besides, in conversational machine reading, Ouyang et al. formulated the input text as two complementary graphs, i.e., explicit and implicit discourse graphs, to fully capture the discourse relations and latent vector interactions among all the elementary discourse units. 3.1.2 Document Representation Learning. . In many text generation tasks such as document translation and document summarization, the input text might be a long document consisting of multiple paragraphs. . When encoding the d\n\n[2] NatureDeepReview.pdf (2025)\nDistributed representations and language processing Deep-learning theory shows that deep nets have two different expo nential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.\n\n[2] NatureDeepReview.pdf (2025)\n. Such representations are called distributed representations because their elements (the features) are not mutually exclusive and their many configurations correspond to the variations seen in the observed data. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. . Vector representations of words learned from text are now very widely used in natural language applications14,17,72–76. . The issu\n\n[4] 2303.18223v16.pdf (2025)\n. As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s\n\n[4] 2303.18223v16.pdf (2025)\nFurthermore, word2vec was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks. These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP. . • Pre-trained language models (PLM).\n\n[5] 1910.10683v4.pdf (2023)\n. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. . As such, our work primarily comprises a survey, exploration, and empirical compari\n\n[6] N18-1202.pdf (2018)\n. We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. . For tasks where direct comparisons are possible, ELMo outperforms CoVe, which computes contextualized representations using a neural machine trans\n\n[2] NatureDeepReview.pdf (2025)\n. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. . 4 3 6 | N . A T U R E | V O L 5 2 1 |\n\n[7] 1301.3781v3.pdf (2013)\n. In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words; LDA moreover becomes computationally very expensive on large data sets. Similar to, to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. . Next, we will try to\n\n[1] 2201.05273v4.pdf (2022)\n. To this end, Gunel et al. combined the cross-entropy loss with a supervised contrastive learning loss that pushes the words from the same class close and the words from different classes further apart.. EVALUATION AND RESOURCES In this section, we will discuss several commonly used evaluation metrics and resources with respect to PLMs for text generation.\n\n[8] 1810.04805v2.pdf (2019)\n. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. . Similar to ELMo, their model is feature-based and not deeply bidirectional. .\n\n[6] N18-1202.pdf (2018)\n. This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags.\n\n[8] 1810.04805v2.pdf (2019)\n. Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch.\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 2201.05273v4.pdf (2022)\n[2] NatureDeepReview.pdf (2025)\n[3] D14-1162.pdf (2014)\n[4] 2303.18223v16.pdf (2025)\n[5] 1910.10683v4.pdf (2023)\n[6] N18-1202.pdf (2018)\n[7] 1301.3781v3.pdf (2013)\n[8] 1810.04805v2.pdf (2019)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "chunks_final_to_llm": [
    {
      "score": 0.42787978053092957,
      "text": "Similarly, Liu et al. proposed two topic-aware contrastive learning objectives, among which the coherence detection objective identifies topics of a dialogue by detecting the coherence change among topics and the sub-summary generation objective forces the model to capture the most salient information and generate a sub-summary for each topic. Representation Learning Efficiency. . Efficiency is a crucial aspect for modeling long documents, especially when generating long text. . Since the self-a",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -2.7244242504239082,
      "relevance": 3,
      "rank": 1,
      "id": "2201.05273v4.pdf::2022::4feb806a3096"
    },
    {
      "score": 0.3866032361984253,
      "text": ". Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification.",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -3.1966287195682526,
      "relevance": 3,
      "rank": 2,
      "id": "NatureDeepReview.pdf::2025::18f38311376f"
    },
    {
      "score": 0.5028806924819946,
      "text": ". We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -3.6584050357341766,
      "relevance": 3,
      "rank": 3,
      "id": "2201.05273v4.pdf::2022::47a1308f9d9e"
    },
    {
      "score": 0.452836275100708,
      "text": ". mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -3.716995418071747,
      "relevance": 3,
      "rank": 4,
      "id": "2201.05273v4.pdf::2022::9316daacef76"
    },
    {
      "score": 0.44914859533309937,
      "text": ". A representative example is Manakul et al., which proposed local self-attention, allowing longer input spans during training; and explicit content selection, reducing memory and compute requirements.. Furthermore, several researchers adopted divide-and-conquer encoding methods. By splitting the long document into short sentences, it is easier to summarize each short part of the document separately, reducing the computational complexity. . 3.1.3 Multi-lingual Representation Learning. . Existing",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -4.207022443413734,
      "relevance": 3,
      "rank": 5,
      "id": "2201.05273v4.pdf::2022::2ab4a11cfa70"
    },
    {
      "score": 0.42111125588417053,
      "text": ". 3.1 Unstructured Input In text generation, most studies focus on modeling unstructured text input (e.g., sentence, paragraph, and document), which requires to accurately understand the input information and derive meaningful text representations. The aim of text representation learning is to condense the input text into low-dimensional vectors that can preserve the core semantic meanings. . In what follows, we will discuss how to derive effective semantic representations for three kinds of uns",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -4.64045687764883,
      "relevance": 3,
      "rank": 6,
      "id": "2201.05273v4.pdf::2022::1914353417f9"
    },
    {
      "score": 0.4619614779949188,
      "text": "The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.. Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.. 2014. Don't count, predict! . A\nsystematic comparison of context-counting vs. context-predicting semantic vectors. . In. . Extracting semantic representations from word cooccurrence statistics: A computational study. . Behavi",
      "metadata": {
        "source_file": "D14-1162.pdf",
        "title": null,
        "authors": null,
        "year": "2014",
        "detected_language": null,
        "page_count": 12,
        "origin_chunk_file": "D14-1162.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.001065440475941,
      "relevance": 3,
      "rank": 7,
      "id": "D14-1162.pdf::2014::a47cb0a05267"
    },
    {
      "score": 0.5097607374191284,
      "text": ". Besides, in conversational machine reading, Ouyang et al. formulated the input text as two complementary graphs, i.e., explicit and implicit discourse graphs, to fully capture the discourse relations and latent vector interactions among all the elementary discourse units. 3.1.2 Document Representation Learning. . In many text generation tasks such as document translation and document summarization, the input text might be a long document consisting of multiple paragraphs. . When encoding the d",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.15665927529335,
      "relevance": 3,
      "rank": 8,
      "id": "2201.05273v4.pdf::2022::0d34ae9fc589"
    },
    {
      "score": 0.4544440507888794,
      "text": "Distributed representations and language processing Deep-learning theory shows that deep nets have two different expo nential advantages over classic learning algorithms that do not use distributed representations21. Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.200768440961838,
      "relevance": 3,
      "rank": 9,
      "id": "NatureDeepReview.pdf::2025::83f25ef42380"
    },
    {
      "score": 0.39225703477859497,
      "text": ". Such representations are called distributed representations because their elements (the features) are not mutually exclusive and their many configurations correspond to the variations seen in the observed data. These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network. . Vector representations of words learned from text are now very widely used in natural language applications14,17,72–76. . The issu",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.205624982714653,
      "relevance": 3,
      "rank": 10,
      "id": "NatureDeepReview.pdf::2025::6e07f063491e"
    },
    {
      "score": 0.4400355815887451,
      "text": ". As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.237280905246735,
      "relevance": 3,
      "rank": 11,
      "id": "2303.18223v16.pdf::2025::53cf2c81bbe2"
    },
    {
      "score": 0.4033030867576599,
      "text": "Furthermore, word2vec was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks. These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP. . • Pre-trained language models (PLM).",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.2417261749506,
      "relevance": 3,
      "rank": 12,
      "id": "2303.18223v16.pdf::2025::6eb710a600c0"
    },
    {
      "score": 0.49583008885383606,
      "text": ". With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. . As such, our work primarily comprises a survey, exploration, and empirical compari",
      "metadata": {
        "source_file": "1910.10683v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 67,
        "origin_chunk_file": "1910.10683v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.322084419429302,
      "relevance": 3,
      "rank": 13,
      "id": "1910.10683v4.pdf::2023::665a1633ba5c"
    },
    {
      "score": 0.41124746203422546,
      "text": ". We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis. The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions. . For tasks where direct comparisons are possible, ELMo outperforms CoVe, which computes contextualized representations using a neural machine trans",
      "metadata": {
        "source_file": "N18-1202.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 11,
        "origin_chunk_file": "N18-1202.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.511314816772938,
      "relevance": 3,
      "rank": 14,
      "id": "N18-1202.pdf::2018::91d8e5f5e7a2"
    },
    {
      "score": 0.41410326957702637,
      "text": ". Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. . 4 3 6 | N . A T U R E | V O L 5 2 1 |",
      "metadata": {
        "source_file": "NatureDeepReview.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 9,
        "origin_chunk_file": "NatureDeepReview.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.621962606906891,
      "relevance": 3,
      "rank": 15,
      "id": "NatureDeepReview.pdf::2025::1b7bb54a2b2e"
    },
    {
      "score": 0.41355466842651367,
      "text": ". In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words; LDA moreover becomes computationally very expensive on large data sets. Similar to, to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. . Next, we will try to ",
      "metadata": {
        "source_file": "1301.3781v3.pdf",
        "title": null,
        "authors": null,
        "year": "2013",
        "detected_language": null,
        "page_count": 12,
        "origin_chunk_file": "1301.3781v3.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.651970267295837,
      "relevance": 3,
      "rank": 16,
      "id": "1301.3781v3.pdf::2013::baadc05c3b47"
    },
    {
      "score": 0.43468064069747925,
      "text": ". To this end, Gunel et al. combined the cross-entropy loss with a supervised contrastive learning loss that pushes the words from the same class close and the words from different classes further apart.. EVALUATION AND RESOURCES In this section, we will discuss several commonly used evaluation metrics and resources with respect to PLMs for text generation.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.662658646702766,
      "relevance": 3,
      "rank": 17,
      "id": "2201.05273v4.pdf::2022::6f62650bd6aa"
    },
    {
      "score": 0.4052239656448364,
      "text": ". When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks including question answering, sentiment analysis, and named entity recognition (Tjong Kim Sang and De Meulder, 2003). Melamud et al. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. . Similar to ELMo, their model is feature-based and not deeply bidirectional. .",
      "metadata": {
        "source_file": "1810.04805v2.pdf",
        "title": null,
        "authors": null,
        "year": "2019",
        "detected_language": null,
        "page_count": 16,
        "origin_chunk_file": "1810.04805v2.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.739633291959763,
      "relevance": 3,
      "rank": 18,
      "id": "1810.04805v2.pdf::2019::d0e232ebeacc"
    },
    {
      "score": 0.45480990409851074,
      "text": ". This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags.",
      "metadata": {
        "source_file": "N18-1202.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 11,
        "origin_chunk_file": "N18-1202.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.762575447559357,
      "relevance": 3,
      "rank": 19,
      "id": "N18-1202.pdf::2018::6207b1dd8f2a"
    },
    {
      "score": 0.4178858995437622,
      "text": ". Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch.",
      "metadata": {
        "source_file": "1810.04805v2.pdf",
        "title": null,
        "authors": null,
        "year": "2019",
        "detected_language": null,
        "page_count": 16,
        "origin_chunk_file": "1810.04805v2.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on discuss the transition from handcrafted features to representation learning., grounding historical claims only in explicit snippet content.",
      "final_score": -5.767177313566208,
      "relevance": 3,
      "rank": 20,
      "id": "1810.04805v2.pdf::2019::d870d3ee930a"
    }
  ]
}