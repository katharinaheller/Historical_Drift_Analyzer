{
  "timestamp": "2025-11-18T13-52-25",
  "query_refined": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
  "intent": "conceptual",
  "prompt_final_to_llm": "You are a domain expert in Artificial Intelligence. Provide a precise definition, clarify theoretical foundations, and explain how interpretations evolved across time and publications. Use event years ONLY if explicitly stated in the snippets. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nDefine summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.\n\nContext snippets:\n[1] 1304.1083v1.pdf (1989)\n. A realistic way to simulate the data of human subjects in rule based propagation of uncertainty is to use maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules. All of the relation between the mean model and subject data can be attributed to the fact that both were highly correlated with the predictions of the MMH model. . With the MMH model predictions partialled out, the correlation between the mean model and subject\n\n[2] 3641289.pdf (2024)\nIn addition, LLMs like GPT-3 were found to have moral biases in terms of the Moral Foundation theory; The study conducted by reveals that existing LMs have potential in ethical judgment, but still need improvement. proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods. Moreover, in the assessment of GPT-4 alignment, discovered a systematic bias. . ChatGPT is also observed to exhibit somewhat bias on cultural v\n\n[3] 2201.05273v4.pdf (2022)\n. It has been shown that the generated texts from PLMs are likely to be biased towards some attributes, i.e., favoring a particular race, gender or aged people, which is not desired for the text generation tasks. These undesirable biases are unexpectedly hidden in model components such as word embeddings and attention heads. . A simple approach to mitigating the gender bias in word embeddings is to \"swap\" gendered terms in training data when generating word embeddings. . Furthermore, simply mask\n\n[2] 3641289.pdf (2024)\nMore recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated\n\n[1] 1304.1083v1.pdf (1989)\nThis was what I call the mean model since it summarizes antecedent evidence within a rule by taking the mean certainty factor, scales down the maximum certainty factor in the conclusion by taking the mean of the maximum certainty factor and the summarized antecedent certainty factor, and combines certainties across rules by taking the mean. Mean models were included in our previous experiments, but were not found to be among the best fitting. . Nonetheless, taking the mean is not an unreasonable\n\n[3] 2201.05273v4.pdf (2022)\n. Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.\n\n[2] 3641289.pdf (2024)\n. As for new adversarial datasets, Wang et al. introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.. 3.2.2 Ethics and Bias.\n\n[4] 2303.18223v16.pdf (2025)\nThe three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B nonembedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors. . • . Chinchilla scaling law. . As another representative study, Hoffmann et al. (the Google DeepMind team) proposed a\n\n[2] 3641289.pdf (2024)\nAt the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.. 3.1.5 Factuality. Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.\n\n[4] 2303.18223v16.pdf (2025)\n. As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s\n\n[3] 2201.05273v4.pdf (2022)\n. mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa\n\n[5] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\nFor example, when prompted to perform regression diagnostics, different phrasings such as \"analyze residuals\" versus \"check model assumptions\" resulted in the same core analysis but with different statistical tests or plotting choices. Similarly, in visualization tasks, one prompt might produce a bar chart while another yields a pie chart, depending on how the goal is described. . Even for model training, default hyperparameters, such as learning rate or number of iterations, could vary slightly\n\n[6] reluICML.pdf (2010)\n. In many applications, it is much easier to first normalise each component of the data to have zero mean and unit variance and then to use noise-free reconstructions, with the variance in equation 6 set to 1. The reconstructed value of a Gaussian visible unit is then equal to its top-down input from the binary hidden units plus its bias. . We use this type of noise-free visible unit for the models of object and face images described later. . To allow each unit to express more information, intro\n\n[5] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. ABSTRACT In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm.\n\n[7] 0311031v1.pdf (2018)\n. In the development of the SP theory, computer models have been created as a way of reducing vagueness and inconsistencies in the theory, as a way of verifying that the system really does work according to expectations, and as a means of demonstrating what the system can do. Two main models have been developed to date: • SP61 which is a partial model of the system that builds multiple alignments from New and Old patterns (Wolff, 2000). . This model does not attempt any learning and it does not\n\n[6] reluICML.pdf (2010)\nThen we repeatedly update each weight, wij, using the difference between two measured, pairwise correlations where ǫ is a learning rate, data is the frequency with which visible unit i and hidden unit j are on together when the feature detectors are being driven by images from the training set and recon is the corresponding frequency when the hidden units are being driven by reconstructed images. A similar learning rule can be used for the biases. . where bj is the bias of j and vi is the binary\n\n[3] 2201.05273v4.pdf (2022)\n. In recent literature, data augmentation has emerged as a critical method for increasing the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. One line of research is to use retrieval models to obtain real data from external corpora as the augmented data.\n\n[8] N18-1202.pdf (2018)\n. We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.\n\n[4] 2303.18223v16.pdf (2025)\n. Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4.\n\n[1] 1304.1083v1.pdf (1989)\nSeven different quantitative models for summarizing the antecedent certainty and two models for using this sumarized antecedent certainty to scale down The results indicated that the best fitting model for summarizing antecedent cenainties was the so-called maximin model, which took the maximum cenainty factor from disjunctively connected antecedents and the minimum certainty factor from conjunctively connected antecedents. The better technique for scaling down the maximum certainty factor in th\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 1304.1083v1.pdf (1989)\n[2] 3641289.pdf (2024)\n[3] 2201.05273v4.pdf (2022)\n[4] 2303.18223v16.pdf (2025)\n[5] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[6] reluICML.pdf (2010)\n[7] 0311031v1.pdf (2018)\n[8] N18-1202.pdf (2018)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "chunks_final_to_llm": [
    {
      "score": 0.35488012433052063,
      "text": ". A realistic way to simulate the data of human subjects in rule based propagation of uncertainty is to use maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules. All of the relation between the mean model and subject data can be attributed to the fact that both were highly correlated with the predictions of the MMH model. . With the MMH model predictions partialled out, the correlation between the mean model and subject ",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.816007725894451,
      "relevance": 3,
      "rank": 1,
      "id": "1304.1083v1.pdf::1989::8a4aedfcecad"
    },
    {
      "score": 0.38528144359588623,
      "text": "In addition, LLMs like GPT-3 were found to have moral biases in terms of the Moral Foundation theory; The study conducted by reveals that existing LMs have potential in ethical judgment, but still need improvement. proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods. Moreover, in the assessment of GPT-4 alignment, discovered a systematic bias. . ChatGPT is also observed to exhibit somewhat bias on cultural v",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.863470107316971,
      "relevance": 3,
      "rank": 2,
      "id": "3641289.pdf::2024::afc702007d9c"
    },
    {
      "score": 0.42848291993141174,
      "text": ". It has been shown that the generated texts from PLMs are likely to be biased towards some attributes, i.e., favoring a particular race, gender or aged people, which is not desired for the text generation tasks. These undesirable biases are unexpectedly hidden in model components such as word embeddings and attention heads. . A simple approach to mitigating the gender bias in word embeddings is to \"swap\" gendered terms in training data when generating word embeddings. . Furthermore, simply mask",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.5523865446448326,
      "relevance": 3,
      "rank": 3,
      "id": "2201.05273v4.pdf::2022::006bd10b6475"
    },
    {
      "score": 0.37409889698028564,
      "text": "More recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.695372313261032,
      "relevance": 3,
      "rank": 4,
      "id": "3641289.pdf::2024::65b19d8d0cfe"
    },
    {
      "score": 0.36294347047805786,
      "text": "This was what I call the mean model since it summarizes antecedent evidence within a rule by taking the mean certainty factor, scales down the maximum certainty factor in the conclusion by taking the mean of the maximum certainty factor and the summarized antecedent certainty factor, and combines certainties across rules by taking the mean. Mean models were included in our previous experiments, but were not found to be among the best fitting. . Nonetheless, taking the mean is not an unreasonable",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.8649402111768723,
      "relevance": 3,
      "rank": 5,
      "id": "1304.1083v1.pdf::1989::b317535e3e3b"
    },
    {
      "score": 0.4370036721229553,
      "text": ". Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.042414531111717,
      "relevance": 3,
      "rank": 6,
      "id": "2201.05273v4.pdf::2022::bcb2357ff742"
    },
    {
      "score": 0.3833816349506378,
      "text": ". As for new adversarial datasets, Wang et al. introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.. 3.2.2 Ethics and Bias.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.049429230391979,
      "relevance": 3,
      "rank": 7,
      "id": "3641289.pdf::2024::42b119384bd6"
    },
    {
      "score": 0.33703380823135376,
      "text": "The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B nonembedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence relation on the three factors. . • . Chinchilla scaling law. . As another representative study, Hoffmann et al. (the Google DeepMind team) proposed a",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.078727707266808,
      "relevance": 3,
      "rank": 8,
      "id": "2303.18223v16.pdf::2025::f31b30f00ec4"
    },
    {
      "score": 0.4801763594150543,
      "text": "At the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.. 3.1.5 Factuality. Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.258099220693111,
      "relevance": 3,
      "rank": 9,
      "id": "3641289.pdf::2024::db8f6d33a287"
    },
    {
      "score": 0.35539817810058594,
      "text": ". As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig. . 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (s",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.452932119369507,
      "relevance": 3,
      "rank": 10,
      "id": "2303.18223v16.pdf::2025::53cf2c81bbe2"
    },
    {
      "score": 0.3514378070831299,
      "text": ". mRASP2 applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text. The mainstream approaches to text summarization based on PLMs are either extractive or abstractive. . Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary. . In contrast, abstractive summarization generates the summa",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.592668235301971,
      "relevance": 3,
      "rank": 11,
      "id": "2201.05273v4.pdf::2022::9316daacef76"
    },
    {
      "score": 0.3743593394756317,
      "text": "For example, when prompted to perform regression diagnostics, different phrasings such as \"analyze residuals\" versus \"check model assumptions\" resulted in the same core analysis but with different statistical tests or plotting choices. Similarly, in visualization tasks, one prompt might produce a bar chart while another yields a pie chart, depending on how the goal is described. . Even for model training, default hyperparameters, such as learning rate or number of iterations, could vary slightly",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.666429944336414,
      "relevance": 3,
      "rank": 12,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::868dc3d38171"
    },
    {
      "score": 0.34689947962760925,
      "text": ". In many applications, it is much easier to first normalise each component of the data to have zero mean and unit variance and then to use noise-free reconstructions, with the variance in equation 6 set to 1. The reconstructed value of a Gaussian visible unit is then equal to its top-down input from the binary hidden units plus its bias. . We use this type of noise-free visible unit for the models of object and face images described later. . To allow each unit to express more information, intro",
      "metadata": {
        "source_file": "reluICML.pdf",
        "title": null,
        "authors": null,
        "year": "2010",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "reluICML.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.781571753323078,
      "relevance": 3,
      "rank": 13,
      "id": "reluICML.pdf::2010::58da0c452446"
    },
    {
      "score": 0.336465060710907,
      "text": ". ABSTRACT In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.034705832600594,
      "relevance": 3,
      "rank": 14,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::9cf0f7df401d"
    },
    {
      "score": 0.39138704538345337,
      "text": ". In the development of the SP theory, computer models have been created as a way of reducing vagueness and inconsistencies in the theory, as a way of verifying that the system really does work according to expectations, and as a means of demonstrating what the system can do. Two main models have been developed to date: • SP61 which is a partial model of the system that builds multiple alignments from New and Old patterns (Wolff, 2000). . This model does not attempt any learning and it does not ",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.052318200469017,
      "relevance": 3,
      "rank": 15,
      "id": "0311031v1.pdf::2018::17b63504fb56"
    },
    {
      "score": 0.36865025758743286,
      "text": "Then we repeatedly update each weight, wij, using the difference between two measured, pairwise correlations where ǫ is a learning rate, data is the frequency with which visible unit i and hidden unit j are on together when the feature detectors are being driven by images from the training set and recon is the corresponding frequency when the hidden units are being driven by reconstructed images. A similar learning rule can be used for the biases. . where bj is the bias of j and vi is the binary",
      "metadata": {
        "source_file": "reluICML.pdf",
        "title": null,
        "authors": null,
        "year": "2010",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "reluICML.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.09977225959301,
      "relevance": 3,
      "rank": 16,
      "id": "reluICML.pdf::2010::0c566d44b02d"
    },
    {
      "score": 0.38621628284454346,
      "text": ". In recent literature, data augmentation has emerged as a critical method for increasing the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. One line of research is to use retrieval models to obtain real data from external corpora as the augmented data.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.112024039030075,
      "relevance": 3,
      "rank": 17,
      "id": "2201.05273v4.pdf::2022::1292775a2900"
    },
    {
      "score": 0.4004223048686981,
      "text": ". We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.",
      "metadata": {
        "source_file": "N18-1202.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 11,
        "origin_chunk_file": "N18-1202.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.169554300606251,
      "relevance": 3,
      "rank": 18,
      "id": "N18-1202.pdf::2018::77b5ba936913"
    },
    {
      "score": 0.35468482971191406,
      "text": ". Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years. Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.2181572914123535,
      "relevance": 3,
      "rank": 19,
      "id": "2303.18223v16.pdf::2025::ba66543280b5"
    },
    {
      "score": 0.3470211923122406,
      "text": "Seven different quantitative models for summarizing the antecedent certainty and two models for using this sumarized antecedent certainty to scale down The results indicated that the best fitting model for summarizing antecedent cenainties was the so-called maximin model, which took the maximum cenainty factor from disjunctively connected antecedents and the minimum certainty factor from conjunctively connected antecedents. The better technique for scaling down the maximum certainty factor in th",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define summarize how bias is introduced through data or model assumptions., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -5.266670681536198,
      "relevance": 3,
      "rank": 20,
      "id": "1304.1083v1.pdf::1989::fc68c99378df"
    }
  ]
}