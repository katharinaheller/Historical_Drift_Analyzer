{
  "timestamp": "2025-11-18T12-33-48",
  "query_id": "Define_describe_the_purpose_of_ontologies_in_knowledge-based_ai_approaches___des",
  "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
  "query_refined": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
  "intent": "conceptual",
  "prompt_final_to_llm": "You are a domain expert in Artificial Intelligence. Provide a precise definition, clarify theoretical foundations, and explain how interpretations evolved across time and publications. Use event years ONLY if explicitly stated in the snippets. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nDefine describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.\n\nContext snippets:\n[1] 0311031v1.pdf (2018)\n. In the context of knowledge-based systems, a benefit of this versatile 'universal' format for knowledge is the scope that it offers for the seamless integration of different kinds of knowledge, minimising the awkward incompatibilities that arise in many computing systems.. • Ontologies and 'semantic' retrieval of information.\n\n[2] Expert_Systems.pdf (2016)\n. Expert systems (ES) are knowledge-based systems that were one of the earlier research fields in Artificial Intelligence (AI) and can be defined as knowledgeintensive software that can perform some tasks normally requiring human expertise. Expert systems are used to solve specific domain problems and each step of reasoning for a specific problem is determined by the human expert professionally. . So, they behave as an artificial advisory system for a particular problem domain. . Although AI is\n\n[3] 3641289.pdf (2024)\n. Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and\n\n[3] 3641289.pdf (2024)\n. A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse\n\n[4] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.\n\n[2] Expert_Systems.pdf (2016)\n. Knowledge-based expert systems will continue to increase individual and social potential by preserving know-how, distributing knowledge more effectively, and improving performance of tasks that require expertise. 24. I. M. Dokas and A. Alapetite, A Development Process Meta-Model for Web Based Expert Systems: The Web Engineering Point of View, Risø National Laboratory, Denmark, 2006. . 25. . R. Islam and K. Andersson, Mohammad Shahadat Hossain, A Web Based Belief Rule Based Expert System to Pre\n\n[3] 3641289.pdf (2024)\n. We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed\n\n[5] 2303.18223v16.pdf (2025)\nRecently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance\n\n[1] 0311031v1.pdf (2018)\n. The SP system provides a powerful framework for the representation and processing of ontologies and for the retrieval of information by meanings rather than literal matching of patterns (Wolff, 2003a).. •. Analysis and production of natural languages. The syntax of natural languages may be represented with SP patterns and both the parsing and the production of sentences may be achieved by the formation of multiple alignments (Wolff, 2000). . Non-syntactic 'semantic' structures may also be repr\n\n[6] 1304.1106v1.pdf (1990)\n. Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime\n\n[7] 0712.3329v1.pdf (2007)\n. In this paper we approach the problem of defining machine intelligence as follows: Section 2 overviews well known theories, definitions and tests of intelligence that have been developed by psychologists. Our objective in this section is to gain an understanding of the essence of intelligence in the broadest possible terms. . In particular we are interested in commonly expressed ideas that could be applied to arbitrary systems and contexts, not just humans. . Section 3 takes these key ideas an\n\n[2] Expert_Systems.pdf (2016)\n. After expert systems have moved out of research laboratories during early 1980s, they became more popular and found several application fields such as engineering, chemistry, medicine, industry, and many others. The construction process of expert systems with specialized domain knowledge is defined as knowledge engineering. . Knowledge-based expert systems contain knowledge acquired from periodicals, books, or from domain interviews with human experts. . Expert systems are mostly preferred as\n\n[8] 1304.1083v1.pdf (1989)\n. Although it is widely acknowledged that much of human knowledge is uncertain, it is in the field of artificial intelligence that research on the representation and management of uncertainty in rule based reasoning has been focused (Kanal & Lemmer, 1986; Hink & Woods, 1987). Most of the work on uncertainty in artificial intelligence has so far been normative, stressing issues of mathematical correctness and effectiveness. . The approach taken in this paper is not normative, but descriptive. . I\n\n[9] 2210.07321v4.pdf (2023)\nThird, guided by the EU Ethics Guidelines for Trustworthy AI and research community efforts, we present our survey with sociotechnical and human-centric considerations integrated throughout, focusing not only on NLG systems and machine text detection technologies, but on the humans who will be exposed to both text generation and detection systems in daily life. The goal of trustworthy AI is to ensure that AI systems are developed in ways that are lawful, ethical, and robust both from a technical\n\n[1] 0311031v1.pdf (2018)\n. The SP theory is a new theory of computing and cognition developed with the aim of integrating and simplifying a range of concepts in computing and cognitive science, with a particular emphasis on concepts in artificial intelligence. An overview of the theory is presented in Wolff and more detail may be found in earlier publications cited there. . Amongst other things, the SP theory provides an attractive model for database applications, especially those requiring a measure of human-like 'inte\n\n[9] 2210.07321v4.pdf (2023)\n. It may be difficult to differentiate those who mean to exploit such systems (e.g., thoughtlessly spam submissions to as many avenues as possible), and those who are relying on AI writing tools to better express themselves.\n\n[5] 2303.18223v16.pdf (2025)\n. Abstract—Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language.\n\n[9] 2210.07321v4.pdf (2023)\n. To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),\n\n[4] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\nFor example, PSAAM (Steffensen, Dufault-Thompson, and Zhang 2016) is software designed for the curation and analysis of metabolic models, yet a biologist researching metabolism might find it challenging to integrate this analytical method into common data analysis tools like Excel or R. With the rise of generative AI, new opportunities have emerged in statistics and data science. LLM-based data agents are gradually addressing existing challenges while introducing a new paradigm for approaching d\n\n[4] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. Another method involves the Retrieval-Augmented Generation (RAG) technique (Lewis et al. 2020), where relevant code is first retrieved and then embedded within the context to facilitate incontext learning. LLM-based agents can also access and interact with external knowledge sources, such as databases or knowledge graphs, to augment their reasoning capabilities (Wang et al. 2024b). . Sun et al. proposes a Knowledge Integration method that builds on this concept. . In LAMBDA, analysis codes are\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 0311031v1.pdf (2018)\n[2] Expert_Systems.pdf (2016)\n[3] 3641289.pdf (2024)\n[4] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[5] 2303.18223v16.pdf (2025)\n[6] 1304.1106v1.pdf (1990)\n[7] 0712.3329v1.pdf (2007)\n[8] 1304.1083v1.pdf (1989)\n[9] 2210.07321v4.pdf (2023)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "retrieved_chunks": [
    {
      "score": 0.5609762668609619,
      "text": ". In the context of knowledge-based systems, a benefit of this versatile 'universal' format for knowledge is the scope that it offers for the seamless integration of different kinds of knowledge, minimising the awkward incompatibilities that arise in many computing systems.. • Ontologies and 'semantic' retrieval of information.",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": 0.30848710238933563,
      "relevance": 3,
      "rank": 1,
      "id": "0311031v1.pdf::2018::4e87dbc3eb7f"
    },
    {
      "score": 0.4316098093986511,
      "text": ". Expert systems (ES) are knowledge-based systems that were one of the earlier research fields in Artificial Intelligence (AI) and can be defined as knowledgeintensive software that can perform some tasks normally requiring human expertise. Expert systems are used to solve specific domain problems and each step of reasoning for a specific problem is determined by the human expert professionally. . So, they behave as an artificial advisory system for a particular problem domain. . Although AI is ",
      "metadata": {
        "source_file": "Expert_Systems.pdf",
        "title": null,
        "authors": null,
        "year": "2016",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "Expert_Systems.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -1.666735902428627,
      "relevance": 3,
      "rank": 2,
      "id": "Expert_Systems.pdf::2016::3ff2aebe9c8e"
    },
    {
      "score": 0.4570256471633911,
      "text": ". Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.32008895277977,
      "relevance": 3,
      "rank": 3,
      "id": "3641289.pdf::2024::85dff6bd2fb4"
    },
    {
      "score": 0.4042235016822815,
      "text": ". A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.5391263216733932,
      "relevance": 3,
      "rank": 4,
      "id": "3641289.pdf::2024::f744bf595495"
    },
    {
      "score": 0.3683391809463501,
      "text": ". The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -2.8933188021183014,
      "relevance": 3,
      "rank": 5,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::53ec537880b8"
    },
    {
      "score": 0.49512404203414917,
      "text": ". Knowledge-based expert systems will continue to increase individual and social potential by preserving know-how, distributing knowledge more effectively, and improving performance of tasks that require expertise. 24. I. M. Dokas and A. Alapetite, A Development Process Meta-Model for Web Based Expert Systems: The Web Engineering Point of View, Risø National Laboratory, Denmark, 2006. . 25. . R. Islam and K. Andersson, Mohammad Shahadat Hossain, A Web Based Belief Rule Based Expert System to Pre",
      "metadata": {
        "source_file": "Expert_Systems.pdf",
        "title": null,
        "authors": null,
        "year": "2016",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "Expert_Systems.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.3558169156312943,
      "relevance": 3,
      "rank": 6,
      "id": "Expert_Systems.pdf::2016::3f868e4f3792"
    },
    {
      "score": 0.530552327632904,
      "text": ". We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.483131304383278,
      "relevance": 3,
      "rank": 7,
      "id": "3641289.pdf::2024::e1d85cdbb466"
    },
    {
      "score": 0.42394718527793884,
      "text": "Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.522690810263157,
      "relevance": 3,
      "rank": 8,
      "id": "2303.18223v16.pdf::2025::ea3b381a808a"
    },
    {
      "score": 0.4353342652320862,
      "text": ". The SP system provides a powerful framework for the representation and processing of ontologies and for the retrieval of information by meanings rather than literal matching of patterns (Wolff, 2003a).. •. Analysis and production of natural languages. The syntax of natural languages may be represented with SP patterns and both the parsing and the production of sentences may be achieved by the formation of multiple alignments (Wolff, 2000). . Non-syntactic 'semantic' structures may also be repr",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.6623519510030746,
      "relevance": 3,
      "rank": 9,
      "id": "0311031v1.pdf::2018::93b27048e722"
    },
    {
      "score": 0.3779551684856415,
      "text": ". Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime",
      "metadata": {
        "source_file": "1304.1106v1.pdf",
        "title": null,
        "authors": null,
        "year": "1990",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "1304.1106v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -3.795066900551319,
      "relevance": 3,
      "rank": 10,
      "id": "1304.1106v1.pdf::1990::6ce03695c520"
    },
    {
      "score": 0.5186043977737427,
      "text": ". In this paper we approach the problem of defining machine intelligence as follows: Section 2 overviews well known theories, definitions and tests of intelligence that have been developed by psychologists. Our objective in this section is to gain an understanding of the essence of intelligence in the broadest possible terms. . In particular we are interested in commonly expressed ideas that could be applied to arbitrary systems and contexts, not just humans. . Section 3 takes these key ideas an",
      "metadata": {
        "source_file": "0712.3329v1.pdf",
        "title": null,
        "authors": null,
        "year": "2007",
        "detected_language": null,
        "page_count": 49,
        "origin_chunk_file": "0712.3329v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.061398476362228,
      "relevance": 3,
      "rank": 11,
      "id": "0712.3329v1.pdf::2007::b551e995a739"
    },
    {
      "score": 0.503067135810852,
      "text": ". After expert systems have moved out of research laboratories during early 1980s, they became more popular and found several application fields such as engineering, chemistry, medicine, industry, and many others. The construction process of expert systems with specialized domain knowledge is defined as knowledge engineering. . Knowledge-based expert systems contain knowledge acquired from periodicals, books, or from domain interviews with human experts. . Expert systems are mostly preferred as ",
      "metadata": {
        "source_file": "Expert_Systems.pdf",
        "title": null,
        "authors": null,
        "year": "2016",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "Expert_Systems.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.17081019282341,
      "relevance": 3,
      "rank": 12,
      "id": "Expert_Systems.pdf::2016::bab1f1e7ac80"
    },
    {
      "score": 0.4162050783634186,
      "text": ". Although it is widely acknowledged that much of human knowledge is uncertain, it is in the field of artificial intelligence that research on the representation and management of uncertainty in rule based reasoning has been focused (Kanal & Lemmer, 1986; Hink & Woods, 1987). Most of the work on uncertainty in artificial intelligence has so far been normative, stressing issues of mathematical correctness and effectiveness. . The approach taken in this paper is not normative, but descriptive. . I",
      "metadata": {
        "source_file": "1304.1083v1.pdf",
        "title": null,
        "authors": null,
        "year": "1989",
        "detected_language": null,
        "page_count": 6,
        "origin_chunk_file": "1304.1083v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.179148279130459,
      "relevance": 3,
      "rank": 13,
      "id": "1304.1083v1.pdf::1989::e330b10ef834"
    },
    {
      "score": 0.4363168478012085,
      "text": "Third, guided by the EU Ethics Guidelines for Trustworthy AI and research community efforts, we present our survey with sociotechnical and human-centric considerations integrated throughout, focusing not only on NLG systems and machine text detection technologies, but on the humans who will be exposed to both text generation and detection systems in daily life. The goal of trustworthy AI is to ensure that AI systems are developed in ways that are lawful, ethical, and robust both from a technical",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.184797674417496,
      "relevance": 3,
      "rank": 14,
      "id": "2210.07321v4.pdf::2023::341f89823db2"
    },
    {
      "score": 0.48756706714630127,
      "text": ". The SP theory is a new theory of computing and cognition developed with the aim of integrating and simplifying a range of concepts in computing and cognitive science, with a particular emphasis on concepts in artificial intelligence. An overview of the theory is presented in Wolff and more detail may be found in earlier publications cited there. . Amongst other things, the SP theory provides an attractive model for database applications, especially those requiring a measure of human-like 'inte",
      "metadata": {
        "source_file": "0311031v1.pdf",
        "title": null,
        "authors": null,
        "year": "2018",
        "detected_language": null,
        "page_count": 25,
        "origin_chunk_file": "0311031v1.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.187093108892441,
      "relevance": 3,
      "rank": 15,
      "id": "0311031v1.pdf::2018::e0c690d03c51"
    },
    {
      "score": 0.3658669888973236,
      "text": ". It may be difficult to differentiate those who mean to exploit such systems (e.g., thoughtlessly spam submissions to as many avenues as possible), and those who are relying on AI writing tools to better express themselves.",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.296153701841831,
      "relevance": 3,
      "rank": 16,
      "id": "2210.07321v4.pdf::2023::7b9fad77ea54"
    },
    {
      "score": 0.4226152300834656,
      "text": ". Abstract—Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine.. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.321799233555794,
      "relevance": 3,
      "rank": 17,
      "id": "2303.18223v16.pdf::2025::63f7ecf49d70"
    },
    {
      "score": 0.44107627868652344,
      "text": ". To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.. • The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating. . • . A meaningful exploration of both topics through the lens of Trustworthy AI (TAI),",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.329094409942627,
      "relevance": 3,
      "rank": 18,
      "id": "2210.07321v4.pdf::2023::f2298e5175a1"
    },
    {
      "score": 0.38765132427215576,
      "text": "For example, PSAAM (Steffensen, Dufault-Thompson, and Zhang 2016) is software designed for the curation and analysis of metabolic models, yet a biologist researching metabolism might find it challenging to integrate this analytical method into common data analysis tools like Excel or R. With the rise of generative AI, new opportunities have emerged in statistics and data science. LLM-based data agents are gradually addressing existing challenges while introducing a new paradigm for approaching d",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.380322366952896,
      "relevance": 3,
      "rank": 19,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::3656d6ba434e"
    },
    {
      "score": 0.3797033429145813,
      "text": ". Another method involves the Retrieval-Augmented Generation (RAG) technique (Lewis et al. 2020), where relevant code is first retrieved and then embedded within the context to facilitate incontext learning. LLM-based agents can also access and interact with external knowledge sources, such as databases or knowledge graphs, to augment their reasoning capabilities (Wang et al. 2024b). . Sun et al. proposes a Knowledge Integration method that builds on this concept. . In LAMBDA, analysis codes are",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Define describe the purpose of ontologies in knowledge-based ai approaches., describe its theoretical foundations, and explain how definitions evolved historically across publications.",
      "final_score": -4.705891415476799,
      "relevance": 3,
      "rank": 20,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::172c5daf9044"
    }
  ],
  "model_output": "Artificial Intelligence (AI) ontologies serve as a foundational structure for knowledge representation and management within Knowledge-Based AI approaches. These ontologies enable seamless integration of diverse forms of knowledge, minimizing compatibility issues [1]. Theoretical foundations of ontologies in AI encompass the frameworks used to represent and process ontologies, such as the Semantic Pattern (SP) theory [1]. Historically, the concept of AI ontologies evolved with the development of Expert Systems (ES), one of the earlier research fields in Artificial Intelligence. ES were defined as knowledge-intensive software that perform specific tasks requiring human expertise [2]. With the advent of the Turing Test in the 1950s [5], researchers began to focus on creating models capable of intelligent behavior, thereby emphasizing the importance of ontologies and AI evaluation for improving system capabilities [3]. Over time, the scope of AI ontologies has expanded beyond expert systems to include Large Language Models (LLMs) in various domains such as statistics and data science [4]. The rise of LLMs has brought new opportunities and challenges, necessitating advancements in ontology-based approaches for AI [4][5]. References:\n[1] 0311031v1.pdf (2018)\n[2] Expert_Systems.pdf (2016)\n[3] 3641289.pdf (2024)\n[4] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[5] 2303.18223v16.pdf (2025)"
}