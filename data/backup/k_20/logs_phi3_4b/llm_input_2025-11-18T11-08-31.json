{
  "timestamp": "2025-11-18T11-08-31",
  "query_refined": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
  "intent": "analytical",
  "prompt_final_to_llm": "You are a rigorous AI researcher. Analyze mechanisms, methodologies, and implications over time. Event years may only be used if explicitly present in the snippet text. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles ‚Äî only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  ‚Ä¢ If an explicit event year exists in the snippet ‚Üí include it.\n  ‚Ä¢ If no explicit event year exists ‚Üí write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  ‚Ä¢ (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nAnalyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.\n\nContext snippets:\n[1] 3641289.pdf (2024)\n. A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse\n\n[1] 3641289.pdf (2024)\n. Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and\n\n[2] 2303.18223v16.pdf (2025)\n. A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (e.g., the discontinuous evaluation metrics): when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way.\n\n[1] 3641289.pdf (2024)\nTo address this, they transformed existing fact consistency tasks into binary labels, specifically considering only whether there is a factual conflict with the input text, without factoring in external knowledge. The research discovered that fact evaluation methods founded on natural language inference and question generation answering exhibit superior performance and can complement each other. . Pezeshkpour proposed a novel metric, based on information theory, to assess the inclusion of specif\n\n[2] 2303.18223v16.pdf (2025)\n. For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. . Despite these efforts, more fundamental research (e.g., grokking10) about the working mechanism of LLMs is still in need to understand the emergenc\n\n[3] 2201.05273v4.pdf (2022)\n. To this end, Gunel et al. combined the cross-entropy loss with a supervised contrastive learning loss that pushes the words from the same class close and the words from different classes further apart.. EVALUATION AND RESOURCES In this section, we will discuss several commonly used evaluation metrics and resources with respect to PLMs for text generation.\n\n[4] 2005.14165v4.pdf (2020)\n. By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).\n\n[5] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.\n\n[3] 2201.05273v4.pdf (2022)\n. With the growing variety of text generation applications and datasets, there are several advantages of automatic evaluation: it is potentially much cheaper and quicker than human evaluation, and it is repeatable. Therefore, we mainly concentrate on automatic evaluation metrics for text generation in this part. . Following Celikyilmaz et al., we present four categories of metrics, i.e., ùëõ-gram overlap metrics, diversity metrics, semantic similarity metrics, and logit-based metrics. . We list th\n\n[3] 2201.05273v4.pdf (2022)\n. Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.\n\n[1] 3641289.pdf (2024)\n. Furthermore, the ongoing evolution of LLMs has also presented novel aspects for evaluation, thereby challenging existing evaluation protocols and reinforcing the need for thorough, multifaceted evaluation techniques. While existing research such as Bubeck et al. claimed that GPT-4 can be seen as sparks of AGI, others contest this claim due to the human-crafted nature of its evaluation approach. . This paper serves as the first comprehensive survey on the evaluation of large language models. .\n\n[3] 2201.05273v4.pdf (2022)\n. We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo\n\n[1] 3641289.pdf (2024)\n. (2) Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful conclusions on the success and failure case of LLMs (Section 6), providing experience for future research. (3) As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to provide a profound understanding of current LLMs evaluations. . In terms of how to evaluate, we explore current protocols and summarize novel evaluation approaches. . (4) We further discuss future chal\n\n[1] 3641289.pdf (2024)\n. Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics. . They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more\n\n[1] 3641289.pdf (2024)\n. Wang et al. assessed the internal knowledge capabilities of several large models, namely InstructGPT, ChatGPT-3.5, GPT-4, and BingChat, by examining their ability to answer open questions based on the Natural Questions and TriviaQA datasets.. The evaluation process involved human assessment.\n\n[1] 3641289.pdf (2024)\nComparison Traditional ML Deep Learning LLMs Training Data Size Large Large Very large Feature Engineering Manual Automatic Automatic Model Complexity Limited Complex Very Complex Interpretability Good Poor Poorer Performance Moderate High Highest Hardware Requirements Low High Very High AI model evaluation is an essential step in assessing the performance of a model. There are some standard model evaluation protocols, including k-fold cross-validation, holdout validation, leave one out cross-va\n\n[1] 3641289.pdf (2024)\nMore recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated\n\n[2] 2303.18223v16.pdf (2025)\nRecently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance\n\n[1] 3641289.pdf (2024)\nThis statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.. 3.1.2. Reasoning. The task of reasoning poses significant challenges for an intelligent AI model. . To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inferenc\n\n[6] 2205.01068v4.pdf (2022)\n. We compare primarily to GPT-3, having aimed to re-implement their evaluation settings, but include reported performance of other LLMs on a per-task basis when available (Lieber et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022) We report performance in accuracy (omitting F1 for MultiRC and ReCoRD for consistency in evaluation metrics). . For the Winograd Schema Challenge (WSC) task in the SuperGLUE benchmark, we follow and formulate the task as multiple choice questions\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 3641289.pdf (2024)\n[2] 2303.18223v16.pdf (2025)\n[3] 2201.05273v4.pdf (2022)\n[4] 2005.14165v4.pdf (2020)\n[5] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[6] 2205.01068v4.pdf (2022)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "chunks_final_to_llm": [
    {
      "score": 0.5149648189544678,
      "text": ". A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -0.3116990625858307,
      "relevance": 3,
      "rank": 1,
      "id": "3641289.pdf::2024::f744bf595495"
    },
    {
      "score": 0.5847499966621399,
      "text": ". Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -1.6921207755804062,
      "relevance": 3,
      "rank": 2,
      "id": "3641289.pdf::2024::85dff6bd2fb4"
    },
    {
      "score": 0.4165431261062622,
      "text": ". A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (e.g., the discontinuous evaluation metrics): when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way.",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -2.0741165578365326,
      "relevance": 3,
      "rank": 3,
      "id": "2303.18223v16.pdf::2025::3f0a227725ea"
    },
    {
      "score": 0.4294896125793457,
      "text": "To address this, they transformed existing fact consistency tasks into binary labels, specifically considering only whether there is a factual conflict with the input text, without factoring in external knowledge. The research discovered that fact evaluation methods founded on natural language inference and question generation answering exhibit superior performance and can complement each other. . Pezeshkpour proposed a novel metric, based on information theory, to assess the inclusion of specif",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -3.277479887008667,
      "relevance": 3,
      "rank": 4,
      "id": "3641289.pdf::2024::b678961257ae"
    },
    {
      "score": 0.40436041355133057,
      "text": ". For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. . Despite these efforts, more fundamental research (e.g., grokking10) about the working mechanism of LLMs is still in need to understand the emergenc",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -3.5190645158290863,
      "relevance": 3,
      "rank": 5,
      "id": "2303.18223v16.pdf::2025::1437b359f021"
    },
    {
      "score": 0.43347352743148804,
      "text": ". To this end, Gunel et al. combined the cross-entropy loss with a supervised contrastive learning loss that pushes the words from the same class close and the words from different classes further apart.. EVALUATION AND RESOURCES In this section, we will discuss several commonly used evaluation metrics and resources with respect to PLMs for text generation.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -3.7209824472665787,
      "relevance": 3,
      "rank": 6,
      "id": "2201.05273v4.pdf::2022::6f62650bd6aa"
    },
    {
      "score": 0.4621708393096924,
      "text": ". By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).",
      "metadata": {
        "source_file": "2005.14165v4.pdf",
        "title": null,
        "authors": null,
        "year": "2020",
        "detected_language": null,
        "page_count": 75,
        "origin_chunk_file": "2005.14165v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -3.9449777007102966,
      "relevance": 3,
      "rank": 7,
      "id": "2005.14165v4.pdf::2020::26b7dadfcc8c"
    },
    {
      "score": 0.3707677125930786,
      "text": ". The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.061676114797592,
      "relevance": 3,
      "rank": 8,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::53ec537880b8"
    },
    {
      "score": 0.5064309239387512,
      "text": ". With the growing variety of text generation applications and datasets, there are several advantages of automatic evaluation: it is potentially much cheaper and quicker than human evaluation, and it is repeatable. Therefore, we mainly concentrate on automatic evaluation metrics for text generation in this part. . Following Celikyilmaz et al., we present four categories of metrics, i.e., ùëõ-gram overlap metrics, diversity metrics, semantic similarity metrics, and logit-based metrics. . We list th",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.1130077093839645,
      "relevance": 3,
      "rank": 9,
      "id": "2201.05273v4.pdf::2022::304dbe278de5"
    },
    {
      "score": 0.43876567482948303,
      "text": ". Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality.. Controllability. In text generation, many applications need a good control over the output text. . For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.47008391469717,
      "relevance": 3,
      "rank": 10,
      "id": "2201.05273v4.pdf::2022::bcb2357ff742"
    },
    {
      "score": 0.5194209814071655,
      "text": ". Furthermore, the ongoing evolution of LLMs has also presented novel aspects for evaluation, thereby challenging existing evaluation protocols and reinforcing the need for thorough, multifaceted evaluation techniques. While existing research such as Bubeck et al. claimed that GPT-4 can be seen as sparks of AGI, others contest this claim due to the human-crafted nature of its evaluation approach. . This paper serves as the first comprehensive survey on the evaluation of large language models. . ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.589663177728653,
      "relevance": 3,
      "rank": 11,
      "id": "3641289.pdf::2024::1049e7043b91"
    },
    {
      "score": 0.45878157019615173,
      "text": ". We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization. Besides, we discussed several non-trivial challenges related to the above three aspects. . Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and emplo",
      "metadata": {
        "source_file": "2201.05273v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 35,
        "origin_chunk_file": "2201.05273v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.599731124937534,
      "relevance": 3,
      "rank": 12,
      "id": "2201.05273v4.pdf::2022::47a1308f9d9e"
    },
    {
      "score": 0.5650558471679688,
      "text": ". (2) Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful conclusions on the success and failure case of LLMs (Section 6), providing experience for future research. (3) As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to provide a profound understanding of current LLMs evaluations. . In terms of how to evaluate, we explore current protocols and summarize novel evaluation approaches. . (4) We further discuss future chal",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.719359397888184,
      "relevance": 3,
      "rank": 13,
      "id": "3641289.pdf::2024::0247478323b2"
    },
    {
      "score": 0.415657103061676,
      "text": ". Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics. . They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -4.982566103339195,
      "relevance": 3,
      "rank": 14,
      "id": "3641289.pdf::2024::fe77b56619e1"
    },
    {
      "score": 0.5035994052886963,
      "text": ". Wang et al. assessed the internal knowledge capabilities of several large models, namely InstructGPT, ChatGPT-3.5, GPT-4, and BingChat, by examining their ability to answer open questions based on the Natural Questions and TriviaQA datasets.. The evaluation process involved human assessment.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.078701436519623,
      "relevance": 3,
      "rank": 15,
      "id": "3641289.pdf::2024::ed4da0927aa2"
    },
    {
      "score": 0.4390881061553955,
      "text": "Comparison Traditional ML Deep Learning LLMs Training Data Size Large Large Very large Feature Engineering Manual Automatic Automatic Model Complexity Limited Complex Very Complex Interpretability Good Poor Poorer Performance Moderate High Highest Hardware Requirements Low High Very High AI model evaluation is an essential step in assessing the performance of a model. There are some standard model evaluation protocols, including k-fold cross-validation, holdout validation, leave one out cross-va",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.0864861607551575,
      "relevance": 3,
      "rank": 16,
      "id": "3641289.pdf::2024::fb8524993698"
    },
    {
      "score": 0.36318403482437134,
      "text": "More recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.11799381673336,
      "relevance": 3,
      "rank": 17,
      "id": "3641289.pdf::2024::65b19d8d0cfe"
    },
    {
      "score": 0.4676625430583954,
      "text": "Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. . Considering this rapid technical progress, in this survey, we review the recent advance",
      "metadata": {
        "source_file": "2303.18223v16.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 144,
        "origin_chunk_file": "2303.18223v16.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.158821411430836,
      "relevance": 3,
      "rank": 18,
      "id": "2303.18223v16.pdf::2025::ea3b381a808a"
    },
    {
      "score": 0.4932916462421417,
      "text": "This statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.. 3.1.2. Reasoning. The task of reasoning poses significant challenges for an intelligent AI model. . To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inferenc",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.160796217620373,
      "relevance": 3,
      "rank": 19,
      "id": "3641289.pdf::2024::ecc662314003"
    },
    {
      "score": 0.3717848062515259,
      "text": ". We compare primarily to GPT-3, having aimed to re-implement their evaluation settings, but include reported performance of other LLMs on a per-task basis when available (Lieber et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022) We report performance in accuracy (omitting F1 for MultiRC and ReCoRD for consistency in evaluation metrics). . For the Winograd Schema Challenge (WSC) task in the SuperGLUE benchmark, we follow and formulate the task as multiple choice questions",
      "metadata": {
        "source_file": "2205.01068v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 30,
        "origin_chunk_file": "2205.01068v4.chunks.json"
      },
      "query": "Analyze the mechanisms, strengths, and limitations of describe how ai evaluation metrics evolved over time., noting origins only when explicitly stated.",
      "final_score": -5.276998966932297,
      "relevance": 3,
      "rank": 20,
      "id": "2205.01068v4.pdf::2022::899da319122b"
    }
  ]
}