{
  "timestamp": "2025-11-18T11-22-54",
  "query_refined": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
  "intent": "comparative",
  "prompt_final_to_llm": "You are a comparative analyst. Compare major frameworks or schools of thought, specifying explicit historical information only when stated in the provided snippets. Never infer missing event years. Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. Never assign multiple citation numbers to the same source.\n\n**Your final answer MUST end with a separate section titled 'References'.**\nThis section MUST list all unique PDFs exactly once, in the following strict format:\n[n] FILENAME.pdf (YEAR)\n\nDo not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\nTemporal Attribution Rules:\n1. You may ONLY use event years that appear explicitly in the snippet text.\n2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n5. Never replace or override an explicit event year with a metadata year.\n6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\nOutput Structuring Guidelines:\n- For every key historical or conceptual point:\n  • If an explicit event year exists in the snippet → include it.\n  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n- Recommended dual-year structure:\n  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\nThis dual timestamping ensures full temporal grounding without hallucination.\n\nIMPORTANT:\n**Your output MUST end with a final section titled 'References'.**\nThis section must list all unique PDFs exactly once in IEEE numeric format.\n\nRefined query:\nCompare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.\n\nContext snippets:\n[1] 3641289.pdf (2024)\n. Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics. . They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more\n\n[1] 3641289.pdf (2024)\nIn addition, LLMs like GPT-3 were found to have moral biases in terms of the Moral Foundation theory; The study conducted by reveals that existing LMs have potential in ethical judgment, but still need improvement. proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods. Moreover, in the assessment of GPT-4 alignment, discovered a systematic bias. . ChatGPT is also observed to exhibit somewhat bias on cultural v\n\n[2] 2005.14165v4.pdf (2020)\nFinally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. . Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. . Section 4 addresses questions of data\n\n[1] 3641289.pdf (2024)\n. As for new adversarial datasets, Wang et al. introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.. 3.2.2 Ethics and Bias.\n\n[3] 2210.07321v4.pdf (2023)\nThe EU ethics guidelines for trustworthy AI emphasize that unintended or dual-use applications of AI systems should be taken into account, and that steps should be taken to prevent and mitigate abuse of AI systems to cause harm.\n\n[3] 2210.07321v4.pdf (2023)\nThird, guided by the EU Ethics Guidelines for Trustworthy AI and research community efforts, we present our survey with sociotechnical and human-centric considerations integrated throughout, focusing not only on NLG systems and machine text detection technologies, but on the humans who will be exposed to both text generation and detection systems in daily life. The goal of trustworthy AI is to ensure that AI systems are developed in ways that are lawful, ethical, and robust both from a technical\n\n[1] 3641289.pdf (2024)\n. Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and\n\n[1] 3641289.pdf (2024)\nAt the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.. 3.1.5 Factuality. Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.\n\n[4] 2205.01068v4.pdf (2022)\n2Exceptions include work by EleutherAI, who released dense models up to 20B in size, Salesforce, and Meta AI, who released dense models up to 13B and sparse models up to 1.1T. There is also ongoing work from the BigScience workshop ( huggingface.co/), which aims to open source very large multilingual language models and datasets. progress on improving known challenges in areas such as robustness, bias, and toxicity.\n\n[1] 3641289.pdf (2024)\n. A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse\n\n[1] 3641289.pdf (2024)\n. We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed\n\n[5] 1304.1106v1.pdf (1990)\n. Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime\n\n[1] 3641289.pdf (2024)\nMore recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated\n\n[3] 2210.07321v4.pdf (2023)\nAs such, trustworthy AI in the context of NLG necessitates understanding the areas where such models may be abused, and how these abuses may be prevented (either with detection technologies, moderation mechanisms, government legislation, or platform policies). When discussing attacks, we discuss not only the direct impact on targets, but also the broader impacts of both attacks and mitigation measures on trust.\n\n[6] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n. The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.\n\n[5] 1304.1106v1.pdf (1990)\n. The recent wave of work on Bayes nets, however, has suggested several diferent types of experiments: comparisons of different uncertainty formalisms, competi tions between Bayes nets and rule bases [14). [32), and several diferent approaches to (and motivations for) sensitivity analyses (29]. For the most part, these studies ad dress the behavior of a system; although they are al system-specific, they should have some general implications to the way in which we approach system design. . Our re\n\n[1] 3641289.pdf (2024)\n. Question answering is a crucial technology in the field of human-computer interaction, and it has found wide application in scenarios like search engines, intelligent customer service, and QA systems. The measurement of accuracy and efficiency in QA models will have significant implications for these applications. . According to Liang et al., among all the evaluated models, InstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness, and fairness across the\n\n[7] 0712.3329v1.pdf (2007)\n. The issues that continue to draw debate are the questions such as whether the tests test only a part or a particular type of intelligence, or whether they are somehow biased towards a particular group or set of mental skills. Great effort has gone into dealing with these issues, but they are difficult problems with no easy solutions. . Somewhat disconnected from this exists a parallel debate over the nature of intelligence in the context of machines. . While the debate is less politically char\n\n[3] 2210.07321v4.pdf (2023)\n. It may be difficult to differentiate those who mean to exploit such systems (e.g., thoughtlessly spam submissions to as many avenues as possible), and those who are relying on AI writing tools to better express themselves.\n\n[1] 3641289.pdf (2024)\n. Beyond social biases, LLMs have also been assessed by political tendency and personality traits based questionnaires like the Political Compass Test and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality type.\n\nAnswer the refined query using only the context above. Use numeric citations. If a claim lacks evidence write 'insufficient evidence'.\n\nReference index:\n[1] 3641289.pdf (2024)\n[2] 2005.14165v4.pdf (2020)\n[3] 2210.07321v4.pdf (2023)\n[4] 2205.01068v4.pdf (2022)\n[5] 1304.1106v1.pdf (1990)\n[6] A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf (2025)\n[7] 0712.3329v1.pdf (2007)\n\nIMPORTANT OUTPUT REQUIREMENTS:\nYour final answer must end with a section titled 'References'.\nList all unique PDFs exactly once in the format:\n[n] FILENAME.pdf (YEAR)\nThis section must be at the end of your output.",
  "chunks_final_to_llm": [
    {
      "score": 0.4419722557067871,
      "text": ". Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness. DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics. . They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -3.4919568300247192,
      "relevance": 3,
      "rank": 1,
      "id": "3641289.pdf::2024::fe77b56619e1"
    },
    {
      "score": 0.522107720375061,
      "text": "In addition, LLMs like GPT-3 were found to have moral biases in terms of the Moral Foundation theory; The study conducted by reveals that existing LMs have potential in ethical judgment, but still need improvement. proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods. Moreover, in the assessment of GPT-4 alignment, discovered a systematic bias. . ChatGPT is also observed to exhibit somewhat bias on cultural v",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -3.8397925794124603,
      "relevance": 3,
      "rank": 2,
      "id": "3641289.pdf::2024::afc702007d9c"
    },
    {
      "score": 0.5281139612197876,
      "text": "Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. . Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. . Section 4 addresses questions of data",
      "metadata": {
        "source_file": "2005.14165v4.pdf",
        "title": null,
        "authors": null,
        "year": "2020",
        "detected_language": null,
        "page_count": 75,
        "origin_chunk_file": "2005.14165v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -4.263972610235214,
      "relevance": 3,
      "rank": 3,
      "id": "2005.14165v4.pdf::2020::e2981918a7fc"
    },
    {
      "score": 0.4882218539714813,
      "text": ". As for new adversarial datasets, Wang et al. introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.. 3.2.2 Ethics and Bias.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -5.074866123497486,
      "relevance": 3,
      "rank": 4,
      "id": "3641289.pdf::2024::42b119384bd6"
    },
    {
      "score": 0.48924240469932556,
      "text": "The EU ethics guidelines for trustworthy AI emphasize that unintended or dual-use applications of AI systems should be taken into account, and that steps should be taken to prevent and mitigate abuse of AI systems to cause harm.",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -5.2516632452607155,
      "relevance": 3,
      "rank": 5,
      "id": "2210.07321v4.pdf::2023::44d5f016167a"
    },
    {
      "score": 0.4607967734336853,
      "text": "Third, guided by the EU Ethics Guidelines for Trustworthy AI and research community efforts, we present our survey with sociotechnical and human-centric considerations integrated throughout, focusing not only on NLG systems and machine text detection technologies, but on the humans who will be exposed to both text generation and detection systems in daily life. The goal of trustworthy AI is to ensure that AI systems are developed in ways that are lawful, ethical, and robust both from a technical",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -5.458239808678627,
      "relevance": 3,
      "rank": 6,
      "id": "2210.07321v4.pdf::2023::341f89823db2"
    },
    {
      "score": 0.4572978615760803,
      "text": ". Within the scope of AI, the Turing Test, a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution. It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent. . Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -5.713534936308861,
      "relevance": 3,
      "rank": 7,
      "id": "3641289.pdf::2024::85dff6bd2fb4"
    },
    {
      "score": 0.4579290747642517,
      "text": "At the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.. 3.1.5 Factuality. Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.174293652176857,
      "relevance": 3,
      "rank": 8,
      "id": "3641289.pdf::2024::db8f6d33a287"
    },
    {
      "score": 0.35891640186309814,
      "text": "2Exceptions include work by EleutherAI, who released dense models up to 20B in size, Salesforce, and Meta AI, who released dense models up to 13B and sparse models up to 1.1T. There is also ongoing work from the BigScience workshop ( huggingface.co/), which aims to open source very large multilingual language models and datasets. progress on improving known challenges in areas such as robustness, bias, and toxicity.",
      "metadata": {
        "source_file": "2205.01068v4.pdf",
        "title": null,
        "authors": null,
        "year": "2022",
        "detected_language": null,
        "page_count": 30,
        "origin_chunk_file": "2205.01068v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.213040798902512,
      "relevance": 3,
      "rank": 9,
      "id": "2205.01068v4.pdf::2022::daef66d2ddcc"
    },
    {
      "score": 0.3979127109050751,
      "text": ". A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models. Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains. . As demonstrated by existing work, the great performance of LLMs has raised promise that they could be AGI in this era. . LLMs possess the capabilities to solve diverse ",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.401253767311573,
      "relevance": 3,
      "rank": 10,
      "id": "3641289.pdf::2024::f744bf595495"
    },
    {
      "score": 0.4633009731769562,
      "text": ". We consistently maintain the related open-source materials at: INTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities. . In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.401552133262157,
      "relevance": 3,
      "rank": 11,
      "id": "3641289.pdf::2024::e1d85cdbb466"
    },
    {
      "score": 0.4870745837688446,
      "text": ". Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise. This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism. . Verifiable, reproducible, and controlled ex perime",
      "metadata": {
        "source_file": "1304.1106v1.pdf",
        "title": null,
        "authors": null,
        "year": "1990",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "1304.1106v1.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.456755943596363,
      "relevance": 3,
      "rank": 12,
      "id": "1304.1106v1.pdf::1990::6ce03695c520"
    },
    {
      "score": 0.4125816822052002,
      "text": "More recently, Zhuo et al. used conventional testing sets and metrics to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend. Taking a further step, Deshpande et al. introduced role-playing into the model and observed an increase in generated toxicity up to 6x. . Furthermore, such role-playing also caused biased toxicity towards specific entities. . Different from simply measuring social biases, Ferrara investigated",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.5059255957603455,
      "relevance": 3,
      "rank": 13,
      "id": "3641289.pdf::2024::65b19d8d0cfe"
    },
    {
      "score": 0.5002731084823608,
      "text": "As such, trustworthy AI in the context of NLG necessitates understanding the areas where such models may be abused, and how these abuses may be prevented (either with detection technologies, moderation mechanisms, government legislation, or platform policies). When discussing attacks, we discuss not only the direct impact on targets, but also the broader impacts of both attacks and mitigation measures on trust.",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.517982393503189,
      "relevance": 3,
      "rank": 14,
      "id": "2210.07321v4.pdf::2023::a3adc0e308cc"
    },
    {
      "score": 0.3792549669742584,
      "text": ". The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.",
      "metadata": {
        "source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf",
        "title": null,
        "authors": null,
        "year": "2025",
        "detected_language": null,
        "page_count": 15,
        "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.558914266526699,
      "relevance": 3,
      "rank": 15,
      "id": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf::2025::53ec537880b8"
    },
    {
      "score": 0.44446873664855957,
      "text": ". The recent wave of work on Bayes nets, however, has suggested several diferent types of experiments: comparisons of different uncertainty formalisms, competi tions between Bayes nets and rule bases [14). [32), and several diferent approaches to (and motivations for) sensitivity analyses (29]. For the most part, these studies ad dress the behavior of a system; although they are al system-specific, they should have some general implications to the way in which we approach system design. . Our re",
      "metadata": {
        "source_file": "1304.1106v1.pdf",
        "title": null,
        "authors": null,
        "year": "1990",
        "detected_language": null,
        "page_count": 8,
        "origin_chunk_file": "1304.1106v1.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.640511453151703,
      "relevance": 3,
      "rank": 16,
      "id": "1304.1106v1.pdf::1990::0b2161b2f928"
    },
    {
      "score": 0.38809722661972046,
      "text": ". Question answering is a crucial technology in the field of human-computer interaction, and it has found wide application in scenarios like search engines, intelligent customer service, and QA systems. The measurement of accuracy and efficiency in QA models will have significant implications for these applications. . According to Liang et al., among all the evaluated models, InstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness, and fairness across the",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.6591111570596695,
      "relevance": 3,
      "rank": 17,
      "id": "3641289.pdf::2024::7f3ab5d231e6"
    },
    {
      "score": 0.4907032549381256,
      "text": ". The issues that continue to draw debate are the questions such as whether the tests test only a part or a particular type of intelligence, or whether they are somehow biased towards a particular group or set of mental skills. Great effort has gone into dealing with these issues, but they are difficult problems with no easy solutions. . Somewhat disconnected from this exists a parallel debate over the nature of intelligence in the context of machines. . While the debate is less politically char",
      "metadata": {
        "source_file": "0712.3329v1.pdf",
        "title": null,
        "authors": null,
        "year": "2007",
        "detected_language": null,
        "page_count": 49,
        "origin_chunk_file": "0712.3329v1.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.67690285295248,
      "relevance": 3,
      "rank": 18,
      "id": "0712.3329v1.pdf::2007::7a353eff6735"
    },
    {
      "score": 0.4205602705478668,
      "text": ". It may be difficult to differentiate those who mean to exploit such systems (e.g., thoughtlessly spam submissions to as many avenues as possible), and those who are relying on AI writing tools to better express themselves.",
      "metadata": {
        "source_file": "2210.07321v4.pdf",
        "title": null,
        "authors": null,
        "year": "2023",
        "detected_language": null,
        "page_count": 36,
        "origin_chunk_file": "2210.07321v4.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.702013157308102,
      "relevance": 3,
      "rank": 19,
      "id": "2210.07321v4.pdf::2023::7b9fad77ea54"
    },
    {
      "score": 0.4055396318435669,
      "text": ". Beyond social biases, LLMs have also been assessed by political tendency and personality traits based questionnaires like the Political Compass Test and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality type.",
      "metadata": {
        "source_file": "3641289.pdf",
        "title": null,
        "authors": null,
        "year": "2024",
        "detected_language": null,
        "page_count": 45,
        "origin_chunk_file": "3641289.chunks.json"
      },
      "query": "Compare and contrast the main theoretical perspectives on describe how fairness and bias are quantified in ai ethics discussions., grounding historical claims only in explicit snippet content.",
      "final_score": -6.73760649561882,
      "relevance": 3,
      "rank": 20,
      "id": "3641289.pdf::2024::66eee1a5ada3"
    }
  ]
}