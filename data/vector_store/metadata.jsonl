{"text": "The SP theory is a new theory of computing and cognition developed with the aim of integrating and simplifying a range of concepts in computing and cognitive science, with a particular emphasis on concepts in artificial intelligence. An overview of the theory is presented in Wolff(2003b) and more detail may be found in earlier publications cited there. Amongst other things, the SP theory provides an attractive model for database applications, especially those requiring a measure of human-like 'intelligence'. There is, of course, a wide variety of existing database systems that exhibit varying degrees and kinds of intelligence (Bertino et al., 2001) and it is reasonable to ask what may be gained by creating yet another system in that domain. In brief, the attractions of the SP model in this connection are that: • It provides an extraordinarily simple yet versatile format for representing knowledge that facilitates the seamless integration of different kinds of knowledge. The SP theory is an abstract model of any system for processing information, either natural or artificial. The system is Turing-equivalent in the sense that it can model the workings of a universal Turing machine but, unlike the universal Turing machine and equivalent models such as Lamda Calculus or Post's Canonical System, it has much more to say about the nature of 'intelligence' (Wolff, 1999a). The entire theory is based on principles of minimum length encoding pioneered by Ray Solomonoff(1964) and others (see Li and Vit´anyi (1997)). • It provides a framework for processing that knowledge that integrates and simplifies a range of artificial intelligence functions including probabilistic and exact forms of reasoning, unsupervised learning, fuzzy pattern recognition, best-match information retrieval, planning, problem solving and others. Prototypes of the SP system have been developed as software simulations running on an ordinary computer. These prototypes serve to demonstrate what can be done with the system and they provide the examples shown in this paper.  But a programme of further development will be required to translate the prototypes into a system with 'industrial strength'.  •  To describe how the SP model can emulate other models used in database applications and to compare the SP model with those other models.  This paper does not aim to provide a comprehensive view of the SP theory and applications because this has already been provided in Wolff(2003b) and earlier publications.  The narrower focus of this paper is on the SP system as an intelligent database system.  In the next section, the SP theory is described in outline.  After that, Sections 3, 4 and 5 are concerned with the first aim listed above, Section 6 is concerned with the second aim and Section 7 with the third.  In broad terms, the system receives 'New' information from its environment and transfers it to a repository of 'Old' information.  At the same time, it tries to compress the information as much as possible by finding patterns that match each other and merging or 'unifying' patterns that are the same.  An important part of this process is the building of 'multiple alignments' as described below.  This provides the key to recognition, information retrieval, reasoning, learning, and other aspects of intelligence to be reviewed in Section 6.  In the SP system, all knowledge is stored as arrays of atomic symbols in one or two dimensions called patterns.  In work to date, the main focus has been on 1-D patterns but it is envisaged that, at some stage, the concepts will be generalised to patterns in two dimensions.  Although this may seem to be a very limited format, it is possible within the system to model a wide range of existing formats for knowledge, including context-free and context-sensitive grammars, condition-action rules, tables, networks and trees of various kinds, including class-inclusion hierarchies, partwhole-hierarchies and discrimination networks.  Some examples will be seen below.  In the SP system, a symbol is simply a 'mark' that can be matched with other symbols to decide in each case whether it is the 'same' or 'different'.  There are no symbols in the system with 'hidden' meaning such as 'multiply' as the meaning of '×' in arithmetic or 'add' as the meaning of '+'.  However, it is possible to define the meaning of any given symbol in the SP system in terms of other symbols and patterns that are associated with it in the system.  Within the system, constructs such as 'variable', 'value', 'type', 'class', 'subclass', 'object', 'iteration', 'true', 'false', and 'negation' are not provided explicitly.  However, the effect of these constructs can be achieved by the use of patterns and symbols, and we shall see some examples below.  When any one New pattern is received, the system tries to find the best possible match between the New pattern and one or more of the Old patterns.  The result of this process is the creation of one or more multiple alignments, examples of which will be seen  below.1 Each multiple alignment is evaluated in terms of the principles of minimum length encoding as explained in Wolff(2003b) and earlier papers cited there.  Figure 1 is a simple example of the way in which a multiple alignment can achieve the effect of parsing a sentence, with SP patterns representing grammatical rules.  By convention, the New pattern—which in this case is the sentence to be parsed—is always shown in column 0.  All the other columns contain Old 1The concept of multiple alignment in the SP framework is similar to that concept in bioinformatics but there are important differences described in Wolff(2003b).  patterns, one pattern per column in any order.  The Old patterns in this example represent grammatical rules.  For example, the pattern 'S NP #NP V #V NP #NP #S' in column 7 is equivalent to 'S →NP V NP' in the convention of re-write rules, and the pattern 'NP D #D N #N #NP' in column 5 is equivalent to 'NP →D N'.  The entire multiple alignment divides the sentence into labelled parts and subparts like a conventional parsing and assigns a grammatical category to each word.  Contrary to what this example may suggest, the system has at least the expressive power of a context-sensitive grammar.  More elaborate examples of natural language processing in the SP system and a much fuller discussion of this area of application may be found in Wolff(2000).  S\ngirl -- girl #N --- #N #NP -----------------------------  #NP #S Figure 1: An example of a multiple alignment that achieves the effect of parsing a sentence, with SP patterns in columns 1 to 8 representing grammatical rules.  In one operation, the creation of multiple alignments achieves a range of computational effects, depending on the kinds of Old pattern that are stored in the system.  These effects include 'parsing' (as in the example just shown), 'recognition' of an unknown entity, 'retrieval' of stored information, probabilistic 'reasoning', logical 'deduction', mathematical 'calculation', and more.  In cases where the New pattern cannot be fully matched with the Old patterns, the system may 'learn' by creating patterns that are derived from multiple alignments where partial matching has been achieved or, if there are no such multiple alignments, from the New pattern itself (Wolff, 2002b).  These systemgenerated patterns are added to the repository of Old patterns.  Periodically, these patterns are evaluated in terms the principles of minimum length encoding and the repository of Old patterns may be purged of patterns that are least useful in those terms.  In the development of the SP theory, computer models have been created as a way of reducing vagueness and inconsistencies in the theory, as a way of verifying that the system really does work according to expectations, and as a means of demonstrating what the system can do.  Two main models have been developed to date: • SP61 which is a partial model of the system that builds multiple alignments from New and Old patterns (Wolff, 2000).  This model does not attempt any learning and it does not add any patterns to its repository of Old patterns.  All the Old patterns in the model must be supplied by the user when the program starts.  This model is relatively stable and provides all the examples in this article.  Since the SP system can model the operation of a universal Turing machine, it can, in principle, be used for any kind of arithmetic or mathematical operation and it can, in principle, perform any kind of 'procedure' that one might program in a procedural programming language such as C++ or Cobol.  That said, most applications that have been developed to date have a 'declarative' flavour and the ways in which the system may be applied to arithmetic or other mathematical operations have not yet been explored in any depth (but see Wolff (2002a)).  This has a bearing on how the system may be developed for database applications, as will be discussed in Section 7.  Many problems in artificial intelligence are known to be intractable if one wishes to obtain the best possible answer.  But if one is content with answers that are merely 'good enough', then it is often possible to achieve dramatic reductions in time complexity or space complexity or both.  These remarks apply to the multiple alignment problem in bioinformatics and to the version of that problem that has been developed in the SP system.  For any realistic example, an exhaustive search of the abstract space of possible multiple alignments is not possible and constraints must be applied, pruning away large parts of the search space.  In current models, the main emphasis is on hill climbing and related techniques that concentrate search in areas that are proving productive without ruling out any part of the search space a priori—and with enough flexibility to be able to escape from 'local peaks'.  In a serial processing environment, the time complexity of the SP61 model is approximately O(log2 Ns × NsOs), where Ns is the number of symbols in the New pattern and Os is the total number of symbols in the patterns in Old.  In a parallel processing environment, the time complexity may approach O(log2 Ns × Ns), depending on how well the parallel processing is applied.  The space complexity in serial or parallel environments is approximately O(Os).  In a serial processing environment, the time complexity of the SP70 model is approximately O(Np 2) where Np is the number of patterns in New and it is assumed that they are all of the same size or nearly so.  In a parallel processing environment, the time complexity may approach O(Np), depending on how well the parallel processing is applied.  In serial or parallel environments, the space complexity is approximately O(Np).  This section and the two that follow describe how the SP model may achieve the effect of popular database models used in 'mainstream' data processing applications.  This section discusses the relational model, Section 4 is concerned with the object-oriented model and Section 5 considers the hierarchical and network models.  Consider a typical table from a relational database like the one shown in Figure 1 (from the DreamHome example in Connolly and Begg (2002, p. 80)).  The same information can be represented using SP patterns, as shown in Figure 2.  Readers who are familiar with XML (Bray et al., 2000) will see that the way in which tables are represented with SP patterns is essentially the same as the way in which they are represented in XML.  Each pattern begins with a symbol '' that identifies it as a pattern representing a member of staff and there is a corresponding symbol, '', at the end.  Likewise, each field within each pattern is marked by start and end symbols such as ' ... ' and ' ... '.  StaffNo.  First Name Last Name Position Sex DoB Salary Branch No.  SL21 John White Manager M\nB005 SG37 Ann Beech Assistant F\nB003 SG14 David Ford Supervisor M 24-Mar-58\nB003 SA9 Mary Howe Assistant F\nB007 SG5 Susan Brand Manager F\nB003 SL41 Julie Lee Assistant F\nB005 0 SL21 John White Manager M 1-Oct-45 30000 B005 ) 1 SG37 Ann Beech Assistant F 10-Nov-60 12000 B003 )  Unlike XML, there is no restriction on the styles of symbols that may be used.  For example, ' ... ', ' ... ' and ' ... ' may be replaced by symbols such as 'staff...  #staff', 'staffno ... #staffno' and 'first name ... #first name', like the symbols used in Figure 1.  Any other style that is convenient may also be used.  In other applications, it may not be necessary to provide start and end symbols in some of the patterns, and in some cases, start and end symbols may not be needed at all.  At first sight, the SP (and XML) representation of a table is much more longwinded and cumbersome than the representation shown in Figure 2.  But a table in a relational database—as it appears on a computer screen or a computer printout—is a simplified representation of what is stored in computer memory or on a computer disk.  In relational database systems, the 'internal' representation of each table contains memory pointers or tags that are close analogues of symbols like '', ', '' and '' that appear in the SP and XML representations.  In short, the SP representation is essentially the same as the 'internal' representation of a table in a relational database.  The way tables are printed out or displayed on a computer screen is largely a cosmetic matter and there is no reason why tables in an SP database should not be printed or displayed in the conventional style.  In the SP system, the most natural way to retrieve information is in the manner of 'query-by-example'.  To achieve this, patterns like those shown in Figure 2 are stored as Old patterns and the query is created as a New pattern in the same format as the stored patterns but with fewer symbols.  For example, if we wish to identify all the female staffat branch number B003, our query pattern would be ' F B003 ' or even ' F B003 '.  Given this query as the New pattern and patterns like those in Figure 2 as Old patterns, SP61 creates a variety of multiple alignments but only two of them match all the symbols in the New pattern.  These two multiple alignments— shown in Figure 3—identify all the female staffin branch B003, as required.  As previously noted, the New pattern in any multiple alignment is always shown in column 0 and the remaining columns contain Old patterns, one pattern per column.  Of course, there is no need for the results of the user's query to be displayed in the manner shown in Figure 3.  As with the representation of tables, there is no reason in principle why information should not be displayed or printed in whatever format is convenient.  With relational databases, it is of course quite usual for a single query to retrieve information from two or more tables.  This subsection shows how this can be done in the SP model with an example corresponding to a simple join between two tables.  In the DreamHome example (Connolly and Begg, 2002, p. 80), there is one table for clients and another for viewings of properties by clients.  If we wish to know which clients have viewed one or more properties and the comments they have made (example 5.24 in Connolly and Begg (2002, p. 138)), we may achieve this with an SQL query like this: ------  ------\n-- -- B003 --------- B003 B003 --------- B003 - - ----- ----- Figure 3: The two best multiple alignments found by SP61 with the pattern ' F B003 ' in New and patterns in Old that include patterns representing Table 1.  These two multiple alignments are the only ones that provide a match for all the symbols in the New pattern (shown in row 0 in each case).  ------\n------ -- PG4 - 20-Apr-01 ------ too remote -----  ----- Figure 4: One of the five best multiple alignments created by SP61 with the pattern shown in column 0 in New and patterns representing tables for clients and viewings in Old.  as ' ... ' rather than ' ... '  (where '...' represents 'CR76' or other client number).  This allows the system to access details of the client such as 'first name' and 'last name'.2 2To be fully consistent, the same idea should also be applied to the way in which a property for rent is referenced from each of the 'viewing' patterns.  This would mean that ' ... ' in each 'viewing' pattern would be changed to The SP system does not, in itself, provide any query language like SQL for the retrieval of information.  However, the system has proved to be effective in the processing of natural languages (see Wolff(2000) and Section 6, below) and there is no reason in principle why the same should not be true of artificial languages like SQL.  If a query language is deemed necessary, it should be possible to specify the syntax of such a language using SP patterns and to process them within the multiple alignment framework to achieve information retrieval as required.  These are matters requiring further investigation.  One of the attractions of the relational model—and perhaps the main reason for its popularity—is the simplicity of the idea of storing all database knowledge in tables.  This format is very suitable for much of the knowledge to be stored in typical data processing applications but it is by no means 'universal'.  It is not, for example, a good medium for representing any kind of grammar or the kinds of if-then rules used in expert systems.  It can be used to represent the kinds of hierarchical structure associated with object-oriented design but it has shortcomings in this connection, as we shall see in the next section.  A major difference between the relational model and the SP model is that the SP model provides a format for knowledge that is even simpler than in the relational model.  Although this simplification may seem relatively slight, it has a dramatic impact on what can be represented in the system.  Many kinds of knowledge that are outside the scope of the relational model can be accommodated in the SP system and, as we shall see, it overcomes the weaknesses of the relational model in representing hierarchical structures.  At the same time, it can accommodate the relational model when that is required.  The second main difference between the two models is that the relational model is designed purely for the storage and retrieval of knowledge while the SP model can, in addition, support a range of different kinds of intelligence, to be reviewed in Section 6.  Since the invention of the Simula language for programming and simulation in the 1960s (Birtwistle et al., 1973), there has been a growing recognition of the value of organising software and databases into hierarchies of 'classes' and 'subclasses', with 'inheritance' of 'attributes' down each hierarchy to individual 'objects' at the bottom level.  An associated idea is that any object may be structured into a hierarchy of 'parts' and 'subparts'.  These 'object-oriented' ' ... '  and it would also mean that each of the five alignments would include a column showing the property for rent.  concepts allow software and databases to model the structure of human concepts (thus making them more comprehensible) and they help to minimise redundancies in knowledge.  And this makes it easier to modify any given body of knowledge without introducing unwanted inconsistencies.  In the database world, object-oriented concepts have been developed in the 'entity-relationship model' and the 'enhanced entity-relationship model' (see Connolly and Begg (2002)) and also in a variety of 'object-oriented databases' (see Bertino et al. (2001)).  (In the remainder of this paper, the entity-relationship model and enhanced entity-relationship model will be referred to collectively as the entityrelationship model.)  In the SP system, all the object-oriented concepts mentioned in the previous paragraph may be expressed and integrated using SP patterns, as illustrated in Figure 5.  As previously noted, column 0 contains the New pattern and the remaining columns contain Old patterns, one pattern per column.  The order of the Old patterns is entirely arbitrary, without special significance.  In this figure, column 2 contains a pattern representing the class 'vehicle'.  At this abstract level, a 'vehicle' in this example is something with a registration number, an engine, steering wheel, seats, and so on, but the details are unspecified.  Some of that detail is provided by the pattern in column 4 that represents the subclass 'car'.  In this example, a car is a vehicle with 4 seats, 4 doors and 4 wheels and a relatively small space for carrying luggage.  Yet more detail is supplied by the pattern shown in column 1 that represents a specific instance of a car with an identifier ('v4'), a registration number ('LMN 888'), and with a gasoline type of engine with a capacity of 2 litres.  So far, we know relatively little about the engine in v4.  More information is supplied by the pattern in column 3 which represents the structure of the class of internal combustion engines.  At this abstract level, an engine is something with fuel, a 'capacity', some level of 'compression', and a cylinder block, crank shaft, piston and valves.  More detail about the engine in this vehicle is provided by the pattern in column 5 which tells us that, as a gasoline-type engine, it runs on gasoline fuel, that it has a (relatively) low compression and that, in addition to the parts mentioned earlier, it has spark plugs and a carburettor.  The main alternative to gasoline-type engines is, of course, the diesel type—not shown in the figure— which runs on diesel fuel, has a (relatively) high compression, and does not need spark plugs or a carburettor.  Readers may wonder why the symbols '' and '' are used in the patterns shown in columns 1, 2 and 4 and why '' and '' appear in columns 1, 3 and 5.  These symbols are, in effect, 'punctuation' symbols that are needed to ensure that multiple alignments can be formed according to the principles described in Wolff(2003b) and earlier publications.  The multiple alignment concept, as it has been developed in the SP framework, provides a means of expressing all the main constructs associated with object- • Classes, subclasses and objects.  In Figure 5, there is a hierarchy of classes from 'vehicle' at the top level (column 2) through 'car' at an intermediate level (column 4) to an individual object ('v4' shown in column 1) at the bottom level.  The class 'engine' is also shown at an abstract level (column 3) and at a more concrete level (column 5).  --------  ------------------------  ----------------------------------------------- -------------- ------------------------------ --- LMN - LMN 888 - 888 -- ---------  --------  ----------------------- ----------------------------------------------------- spark_plugs carburettor --------------------------------  ---------------------------- ------------------------- gasoline ------------------------  ------------------------- 2000cc ------------------------  ------------------ low -----------------  cylinder_block crank_shaft pistons valves -------------------------------  ---------------------------  ---------------------------------------------------- -------- -------  ---------------------- steering_wheel --------------------------\n------------------------  ------------- -----------------------------  ---------------------------------------------- ------- ----------------------- • Inheritance of attributes.  From the multiple alignment in Figure 5 we can infer that v4 has a cylinder block, crank shaft, pistons and valves, that the engine has a low compression, that the vehicle has 4 wheels, and so on.  None of these 'attributes' are specified in the pattern for v4 shown in column 1.  They are 'inherited' from patterns representing other classes, in much the same way as in other object-oriented systems.  •  Cross-classification and multiple inheritance.  The multiple alignment framework supports cross-classification with multiple inheritance just as easily as it does simple class hierarchies with single inheritance.  With our 'vehicle' example, it would be easy enough to introduce patterns representing, say, 'military vehicles' or 'civilian vehicles', a classification which cuts across the division of vehicles into categories such as 'car', 'bus', 'van', and so on.  In a similar way, vehicles can be cross-classified as 'gasoline type' or 'diesel type' on the strength of the engines they contain, as shown in our example.  • Parts and subparts.  In our example, the class 'vehicle' has parts such as 'engine', 'steering wheel', 'seats', and so on, and the 'engine' has parts such as 'cylinder block', 'crank shaft' etc.  If there was only one type of engine, then all the parts and other attributes of engines could be expressed within the 'vehicle' pattern, without the need for a separate pattern to represent the engine.  The reason that a separate pattern is needed—with a corresponding slot in the 'vehicle' pattern—is that there is more than one kind of engine.  Another reason for representing the class of engines with separate patterns is that engines may be used in a variety of other things (e.g., boats, planes and generators), not just in road vehicles.  It should be apparent that, in the SP system, a pair of neighbouring symbols like '' and '' function very much like a 'variable' in a conventional system.  By appropriate alignment within a multiple alignment, such a variable may receive a 'value' such as 'gasoline' or 'diesel' in this example.  The range of possible values that a given variable may take—the 'type' of the variable—is defined implicitly within any given set of patterns in Old.  Column 4 in Figure 5 shows a car as something with 4 seats, 4 doors and 4 wheels but of course we know that all of these values can vary.  Sports cars often have 2 seats and 2 doors, some budget cars have 3 wheels, and a stretch limo may have many more seats and doors.  In a more fully-developed example, numbers of seats, doors and wheels would be unspecified at the level of 'car' and would be defined in subclasses like those that have been mentioned.  Perhaps the most striking difference between the SP system and other objectoriented systems is the extraordinary simplicity of the format for knowledge in the SP system, compared with the variety of constructs used in other system— such as 'classes', 'objects', 'methods', 'messages', 'isa' links, 'part-of' links, and more.  This subsections considers a selection of other differences that are somewhat more subtle but are, nevertheless, important.  In Simula and most object-oriented systems that have come after, there is a distinction between 'attributes' of objects and 'parts' of objects.  The former are defined at compile time while the aggregation of parts to form wholes is a run-time process.  This means that the inheritance mechanism applies to attributes but not to parts.  In the SP system, this distinction disappears.  Parts of objects can be defined at any level in a class hierarchy and inherited by all the lower level.  There is seamless integration of class hierarchies with part-whole hierarchies.  By contrast with most object-oriented systems, the SP system makes no formal distinction between 'class' and 'object'.  This accords with the observation that what we perceive to be an individual object, such as 'our car', can itself be seen to represent a variety of possibilities: 'our car taking us on holiday', 'our car carrying the shopping', and so on.  A pattern like the one shown in column 1 of Figure 5 could easily function as a class with vacant slots to be filled at a more specific level by details of the passengers or load being carried, the rˆole the vehicle is playing, the colour it has been painted, and so on.  This flexibility is lost in systems that do make a formal distinction between classes and objects.  Another consequence of making a formal distinction between objects and classes is that it points to the need for the concept of a 'metaclass': \"If each object is an instance of a class, and a class is an object, the [object-oriented] model should provide the notion of metaclass.  A metaclass is the class of a class.\"  (Bertino et al., 2001, p. 43).  It is true that this construct is not provided in most object-oriented database systems but it has been introduced in some artificial intelligence systems so that classes can be derived from metaclasses in the same way that objects are derived from classes.  Of course, this logic points to the need for 'metametaclasses', 'metametametaclasses', and so on without limit.  Because the SP system makes no distinction between 'object' and 'class', there is no need for the concept of 'metaclass' or anything beyond it.  All these constructs are represented by patterns.  With minor variations, the entity-relationship model has become the mainstay of data processing applications for business and administration.  Diagrammatic representations of entities and relationships are normally implemented with a relational database and there are efficient software tools to do the translation, hiding many of the details.  Since this combination of entity-relationship model and relational database has come to be so widely used, it will be the focus of our discussion here.  A table can be used to represent a class, with the columns (fields) representing the attributes of the class and the rows representing individual instances of the class.  Each class or subclass in a class hierarchy can also be represented by a table but in this case it is necessary to provide additional fields so that the necessary connections can be made.  For example, the class of 'staff' in a company may be represented by a table like the one shown in Figure 1 and separate tables may be created for each of the subclasses 'manager', 'supervisor' and 'assistant', each of these with columns relevant to the particular subclass but not for other subclasses.  In addition, each of the tables for the subclasses needs a column such as 'StaffNumber' so that the record of an individual in any one subclass can be connected to the corresponding record in the superclass.  Similar principles apply to the division of concepts into parts and subparts.  This system works quite well for many applications but it has a number of shortcomings compared with the SP system: •  Using tables to represent classes means that the description of a class must always take the form of a set of variables corresponding to the fields in the table.  In the SP system, it is possible to describe a class using any combination of variables and literals, according to need.  It is, for example, possible to record that a vehicle has a steering wheel (as in column 2 of Figure 5) without any implication that there may be alternative kinds of steering wheel to be recorded in a field with that name.  It is also possible to provide a verbal description of any class, something that is outside the scope of the relational model.  •  Using tables to represent classes means that the record for every individual must have start and end tags for every field in the table regardless of whether or not that field is used.  In the SP system, start and end tags are only needed for the fields that contain a value in the record for any individual.  •  The SP system allows the description of class hierarchies and part-whole hierarchies to be separated from the description of individual members of those hierarchies.  By contrast, the use of tables to represent class hierarchies and part-whole hierarchies means that the structure of these hierarchies must be reproduced, again and again, in every instance.  Using tables to represent either kind of hierarchy means that information that is specific to any one individual is fragmented and must be pieced together using keys.  In the SP system, by contrast, information that is specific to any one individual can always be represented with a single pattern.  The SP system provides for the smooth integration of class hierarchies and part-whole hierarchies in a way that cannot be achieved using tables.  Although the hierarchical and network models for databases have fallen out of favour in ordinary data processing applications, the network model has seen a revival, first with the development of the hypertext concept and then more dramatically with the application of that concept to the world wide web.  The hierarchical model is the mainstay of hierarchical file systems and finds niche applications in directories of various kinds.  In the SP system, any network or hierarchy can be represented using connections between patterns like the connection between 'engine' and 'vehicle' in Figure 5 (columns 3 and 2).  The basic idea is that one pattern, 'A', may contain the start and end symbols of another pattern, 'B', so that the two patterns can be connected in a multiple alignment like this: In effect, the pair of symbols ' ' in the 'A' pattern (column 1) are a 'reference' to the 'B' pattern (column 2).  With this simple device, it is possible to link patterns in hierarchies and networks of any complexity.  Any one pattern may appear recursively, two or more times within a multiple alignment, as described in Wolff(2003b) and earlier publications.  Where the full versatility of this scheme is not needed, it is also possible to create networks and hierarchies from patterns like ' ... ', ' ... ' and ' ... ' that can be linked end-to-end by alignment within a multiple alignment.  This section briefly reviews aspects of intelligence that have been shown to fall within the scope of the SP system, highlighting those with particular relevance to intelligent databases.  The main points of difference between the SP system and other artificial intelligence systems are also reviewed.  Readers are referred to Wolff(2003b) and other cited sources for more detail about artificial intelligence capabilities of the system outlined here: • Representation of knowledge.  As previously mentioned, the format that has been adopted for representing knowledge within the SP system has proved to be remarkably versatile, despite its extreme simplicity.  Given the system for forming multiple alignments, flat patterns can be used to represent context-free and context-sensitive grammars (Wolff, 2000), networks, trees (including class-inclusion hierarchies and part-whole hierarchies), tables, if-then rules and more.  Some of this versatility has been demonstrated above.  In the context of knowledge-based systems, a benefit of this versatile 'universal' format for knowledge is the scope that it offers for the seamless integration of different kinds of knowledge, minimising the awkward incompatibilities that arise in many computing systems.  •  Ontologies and 'semantic' retrieval of information.  The SP system provides a powerful framework for the representation and processing of ontologies and for the retrieval of information by meanings rather than literal matching of patterns (Wolff, 2003a).  •  Analysis and production of natural languages.  The syntax of natural languages may be represented with SP patterns and both the parsing and the production of sentences may be achieved by the formation of multiple alignments (Wolff, 2000).  Non-syntactic 'semantic' structures may also be represented and processed in the SP system.  Recent work, not yet published, has shown how syntax and semantics may be integrated within the SP framework.  • Probabilistic reasoning.  A major strength of the SP system is its support for probabilistic 'deduction' in one step or via chains of reasoning, abductive reasoning, and nonmonotonic reasoning with default values (Wolff, 1999b).  Relative probabilities of inferences may be calculated strictly in 3The technique that has been developed in the SP models has advantages compared with standard techniques for dynamic programming: it can process arbitrarily long patterns without excessive demands on memory, it can find many alternative matches, and the 'depth' or thoroughness of searching can be determined by the user.  •  Unsupervised learning.  In its overall abstract structure, the SP system is conceived as a system for unsupervised learning—and capabilities in this area have now been demonstrated in the SP70 computer model (Wolff, 2003c, 2002b).  The results are good enough to show that the approach is sound but further development is needed to realise the full potential of this model.  • Exact forms of reasoning.  Although this area is less well developed, there are good reasons to think that the SP system may also be applied to the 'exact' kinds of reasoning found in many areas of logic and mathematics, where answers are either 'true' or 'false', with nothing in between (Wolff, 2002a).  •  Planning and problem solving.  The SP system has been applied successfully to the problem of finding a route between two places (Wolff, 2003b) and it can solve geometric analogy problems translated into textual form (Wolff, 1999b).  If this potential can be realised, this should reduce or eliminate the need for human judgement in the normalisation of knowledge structures.  The SP system should be able to organise its knowledge automatically in a way that minimises redundancies and reveals the natural structures in that knowledge, including class hierarchies, part-whole hierarchies and their integration.  It should also be able to abstract rules and other generalisations from its stored knowledge, in the manner of datamining systems.  Of course, existing datamining techniques may also be applied to an SP database.  It would take us too far afield to attempt a detailed comparison with artificial intelligence systems in the kinds of areas mentioned above.  As an attempt to integrate ideas across a wide area, the SP system naturally has points of similarity with many existing systems, but at the same time, it has its own distinctive features.  Chief amongst these is the remarkable simplicity of the system combined with its very wide scope, much wider than the great majority of artificial intelligence systems, with the possible exception of unified theories of cognition such as Soar (Laird et al., 1987;  Rosenbloom et al., 1993) and ACT-R (Anderson and Lebiere, 1998).  Like those two systems, the development of the SP system was inspired by the writings of Allen Newell, putting the case for greater breadth and depth in theories of cognition (Newell, 1973, 1990).  Unlike hybrid systems, of which there are many, the SP system is not merely a conjunction of two or more different systems, combining their capabilities and also their complexities.  The SP system is the result of a radical rethink of concepts in artificial intelligence and beyond, aiming for integration in a radically simplified structure.  The result is a conceptual framework with distinctive features of which the main ones are: The SP computer models (SP61 and SP70) are good enough to demonstrate what can be done with the system but fall short of what would be needed for applications in industry, commerce or administration.  This section considers how the SP concepts that have been developed to date may be translated into a practical system.  Although the computational complexity of both models in a serial processing environment is within the bounds of what is normally considered to be acceptable, significant improvements are to be expected if the system can be developed with the benefit of parallel processing (Section 2.5).  And of course, parallel processing brings the additional benefit of faster processing in absolute terms and, with suitable design, greater robustness in the face of system failures.  Parallel processing is now a recognised requirement to meet the high computational demands of large scale databases (Abdelguerfiand Lavington, 1995; Abdelguerfiand Wong, 1998) and large-scale applications in artificial intelligence.  At the heart of the SP system is the building of multiple alignments and the core operation here is a process for finding good full and partial matches between patterns in the manner of dynamic programming.  At this level, there is considerable scope for the application of parallel processing because there are often many pairs of patterns that need to be matched and this can be done in parallel just as well as it can be done in sequence.  There is also scope for parallel processing at a more fine-grained level because the process of matching involves a process of 'broadcasting' symbols to make yes/no matches with other symbols and this is an intrinsically parallel operation.  The SP machine does not necessarily have to be developed in silicon.  One futuristic possibility is to exploit the potential of organic molecules such as DNA or proteins—in solution—to achieve the effect of high-parallel pattern matching.  •  Given that SIMD and MIMD high-parallel computers are already in existence, an alternative strategy is to create the SP machine as a software virtual machine running on one of these platforms.  •  An existing high-parallel database system (see, for example, Page (1992); Mahapatra and Mishra (2000)) may be modified to support the SP model.  Other models may, of course be retained alongside the SP model.  •  The system may be developed using low-cost computers connected together in a LAN or even a WAN.  Systems like Google have been developed in this way and they already provide high-speed pattern matching of a kind that may be adapted for use within a software implementation of the SP machine.  A graphical user interface to the SP system is needed for the input of data and queries, for the setting of parameters, and for viewing data and results.  Facilities that would be useful include: • The possibility of translating SP patterns like those shown in Figure 2 into a conventional tabular format (without start and end tags) for viewing or printing.  •  The representation of class hierarchies, part-whole hierarchies or other kinds of hierarchy or network in graphical form, without showing the tags that are used to link patterns together.  •  Facilities for scrolling and zooming to view any large structure such as a large multiple alignment, or a large hierarchy, network, or table.  For some applications there may be a need to provide an SQL-like query language.  As indicated in Section 3.2, it seems likely that this may be achieved within the SP framework by means of an appropriate set of SP patterns—but the details of how that should be done would need investigation.  In the development and application of information systems, it is rarely possible to introduce a new model and simultaneously discard all pre-established models.  There is normally a transition phase, which may be very prolonged, where two or more models coexist as alternatives for different applications or are used in some combination, according to need.  As the SP system matures, it may form hybrids with other systems in at least three different ways: • As noted previously (Section 2.4), the SP system is not yet a rival for wellestablished procedural languages like C++ or Cobol, and its application to arithmetic and other areas of mathematics needs development.  However, there is no reason why the system should not be used in conjunction with existing procedural languages, and with arithmetic or mathematical functions, in very much the same way that the relational database model is standardly used in conjunction with these non-relational languages and functions in many data processing applications.  •  Although in principle the SP system is a model for any kind of software, it is likely to be some time before it would be feasible or sensible to translate all existing applications into the form of SP patterns.  Meanwhile, there is no reason why an SP database system should not serve as a framework within which existing applications may be embedded, in much the same way that a relational or object-oriented database—or, indeed, an hierarchical file system—may contain pointers to executable files of many different kinds (see, for example, Cari˜no and Sterling (1998)).  •  As noted previously, there is no reason why existing datamining techniques should not be used with an SP database although, in the long run, this kind of processing should fall within the scope of the SP model.  The SP model is an alternative to existing database models that offers significant benefits compared with those models.  Within the multiple alignment framework it is possible to represent knowledge in a format that is both simple and versatile, and processing within the framework provides a key to intelligence in the recognition of patterns, retrieval of information, probabilistic and exact kinds of reasoning, planning, problem solving and others.  The versatility of the SP framework means that existing database models can be accommodated within the system and it can function in accordance with any one of those models where that is required.  At the same time, it offers a range of options that are not available in systems that are dedicated to any of the existing models.  Although more work is required in understanding how the model may be developed for learning, other aspects are sufficiently robust and mature for development into an industrial strength working system.  I am grateful to Thomas Connolly for constructive comments on this article.  The responsibility for all errors and oversights is, of course, my own.  M. Abdelguerfiand S. Lavington, editors.  Emerging Trends in Database and Knowledge-base Machines: The Application of Parallel Architectures to Smart Information Systems.  IEEE Computer Science Press, Los Alamitos, Calif., 1995.  T. Bray, J. Paoli, C. M. Sperberg-McQueen, and E. Maler.  Extensible Markup Language (XML) 1.0 (second edition).  Technical report, World Wide Web Consortium, 2000.  W3C recommendation, 6 October 2000.  Copy: <URL> F. Cari˜no and W. Sterling.  Parallel strategies and new concepts for a petabyte multimedia database computer.  In Parallel Database Techniques Abdelguerfiand Wong (1998), pages 139–164.  O. H. Cho and R. M. Colomb.  Associative random access machines and data-parallel multiway binary-search join.  Future Generation Computer Systems, 13(6):451–467, 1998.  A. Louri and J. A. Hatch.  An optical associative parallel processor for high-speed database-processing—theoretical concepts and experimental results.  Computer, 27(11):65–72, 1994.  A. Newell.  You can't play 20 questions with nature and win: projective comments on the papers in this symposium.  In W. G. Chase, editor, Visual Information Processing.  Academic Press, New York, 1973.  J. Page.  A study of a parallel database machine and its performance: the NCR Teradata DBC-1012.  Lecture Notes in Computer Science, 618:115–137, 1992.  P. S. Rosenbloom, J. E. Laird, and A. Newell.  The Soar Papers: Readings on Integrated Intelligence.  MIT Press, Cambridge, Mass., 1993.  D. Sankoffand J. B. Kruskall.  Time Warps, String Edits, and Macromolecules: the Theory and Practice of Sequence Comparisons.  Addison-Wesley, Reading, MA, 1983.  R. J. Solomonoff.  A formal theory of inductive inference.  parts I and II.  Information and Control, 7:1–22 and 224–254, 1964.  J. G. Wolff.  A scaleable technique for best-match retrieval of sequential information using metrics-guided search.  Journal of Information Science, 20 (1):16–28, 1994.  Copy: <URL> J. G. Wolff.  Mathematics and logic as information compression by multiple alignment, unification and search.  Technical report, CognitionResearch.org.uk, 2002a.  Copy: <URL> J. G. Wolff.  Unsupervised grammar induction in a framework of information compression by multiple alignment, unification and search.  In C. de la Higuera, P. Adriaans, M. van Zaanen, and J. Oncina, editors, Proceedings of the Workshop and Tutorial on Learning Context-Free Grammars, pages 113–124, 2003c.  This workshop was held in association with the 14th European Conference on Machine Learning and the 7th European Conference on Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD 2003), September 2003, Cavtat-Dubrovnik, Croata.", "metadata": {"source_file": "0311031v1.pdf", "title": "arXiv:cs/0311031 [cs.DB] 21 Nov 2003 Towards an Intelligent Database System Founded on the SP Theory of Computing and Cognition", "authors": [], "year": "2018", "detected_language": "en", "page_count": 25, "origin_chunk_file": "0311031v1.chunks.json"}}
{"text": "A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines. What is intelligence? It is a concept that we use in our daily lives that seems to have a fairly concrete, though perhaps naive, meaning. We say that our friend who got an A in his calculus test is very intelligent, or perhaps our cat who has learnt to go into hiding at the first mention of the word \"vet\". Although this intuitive notion of intelligence presents us with no difficulties, if we attempt to dig deeper and define it in precise terms we find the concept to be very difficult to nail down. Perhaps the ability to learn quickly is central to intelligence? Or perhaps the total sum of one's knowledge is more important? Perhaps communication and the ability to use language play a central role? What about \"thinking\" or the ability to perform abstract reasoning? How about the ability to be creative and solve problems? Intelligence involves a perplexing mixture of concepts, many of which are equally difficult to define. Psychologists have been grappling with these issues ever since humans first became fascinated with the nature of the mind. Debates have raged back and forth concerning the correct definition of intelligence and how best to measure the intelligence of individuals.  These debates have in many instances been very heated as what is at stake is not merely a scientific definition, but a fundamental issue of how we measure and value humans: Is one employee smarter than another?  Are men on average more intelligent than women?  Are white people smarter than black people?  As a result intelligence tests, and their creators, have on occasion been the subject of intense public scrutiny.  Simply determining whether a test, perhaps quite unintentionally, is partly a reflection of the race, gender, culture or social class of its creator is a subtle, complex and often politically charged issue  [Gou81, HM96].  Not surprisingly, many have concluded that it is wise to stay well clear of this topic.  In reality the situation is not as bad as it is sometimes made out to be.  Although the details of the definition are debated, in broad terms a fair degree of consensus about the scientific definition of intelligence and how to measure it has been achieved [Got97a, SB86].  Indeed it is widely recognised that when standard intelligence tests are correctly applied and interpreted, they all measure approximately the same thing [Got97a].  Furthermore, what they measure is both stable over time in individuals and has significant predictive power, in particular for future academic performance and other mentally demanding pursuits.  The issues that continue to draw debate are the questions such as whether the tests test only a part or a particular type of intelligence, or whether they are somehow biased towards a particular group or set of mental skills.  Great effort has gone into dealing with these issues, but they are difficult problems with no easy solutions.  Somewhat disconnected from this exists a parallel debate over the nature of intelligence in the context of machines.  While the debate is less politically charged, in some ways the central issues are even more difficult.  Machines can have physical forms, sensors, actuators, means of communication, information processing abilities and environments that are totally unlike those that we experience.  This makes the concept of \"machine intelligence\" particularly difficult to get a handle on.  In some cases, a machine may display properties that we equate with human intelligence, in such cases it might be \"Innumerable tests are available for measuring intelligence, yet no one is quite certain of what intelligence is, or even just what it is that the available tests are measuring.\"  — R. L. Gregory  [Gre98] reasonable to describe the machine as also being intelligent.  In other situations this view is far too limited and anthropocentric.  Ideally we would like to be able to measure the intelligence of a wide range of systems; humans, dogs, flies, robots or even disembodied systems such as chat-bots, expert systems, classification systems and prediction algorithms  [Joh92, Alb91].  One response to this problem might be to develop specific kinds of tests for specific kinds of entities; just as intelligence tests for children differ to intelligence tests for adults.  While this works well when testing humans of different ages, it comes undone when we need to measure the intelligence of entities which are profoundly different to each other in terms of their cognitive capacities, speed, senses, environments in which they operate, and so on.  To measure the intelligence of such diverse systems in a meaningful way we must step back from the specifics of particular systems and establish the underlying fundamentals of what it is that we are really trying to measure.  The difficulty of developing an abstract and highly general notion of intelligence is readily apparent.  Consider, for example, the memory and numerical computation tasks that appear in some intelligence tests and which were once regarded as defining hallmarks of human intelligence.  We now know that these tasks are absolutely trivial for a machine and thus do not appear to test the machine's intelligence in any meaningful sense.  Indeed even the mentally demanding task of playing chess can be largely reduced to brute force search  [HCH95].  What else may in time be possible with relatively simple algorithms running on powerful machines is hard to say.  What we can be sure of is that as technology advances, our concept of intelligence will continue to evolve with it.  How then are we to develop a concept of intelligence that is applicable to all kinds of systems?  Any proposed definition must encompass the essence of human intelligence, as well as other possibilities, in a consistent way.  It should not be limited to any particular set of senses, environments or goals, nor should it be limited to any specific kind of hardware, such as silicon or biological neurons.  It should be based on principles which are fundamental and thus unlikely to alter over time.  Furthermore, the definition of intelligence should ideally be formally expressed, objective, and practically realisable as an effective intelligence test.  In this paper we approach the problem of defining machine intelligence as follows: Section 2 overviews well known theories, definitions and tests of intelligence that have been developed by psychologists.  Our objective in this section is to gain an understanding of the essence of intelligence in the broadest possible terms.  In particular we are interested in commonly expressed ideas that could be applied to arbitrary systems and contexts, not just humans.  Section 3 takes these key ideas and formalises them.  This leads to universal intelligence, our proposed formal definition of machine intelligence.  We then examine some of the properties of universal intelligence, such as its ability to sensibly order simple learning algorithms and connections to the theory of universal optimal learning agents.  Section 4 overviews other definitions and tests of machine intelligence that have been proposed.  Although surveys of the Turing test and its many variants exist, for example [SCA00], as far as we know this section is the first general survey of definitions and tests of machine intelligence.  Given how fundamental this is to the field of artificial intelligence, the absence of such a survey is quite remarkable.  For any field to mature as a science, questions of definition and measurement must be meticulously investigated.  We conclude The genesis of this work lies in Hutter's universal optimal learning agent, AIXI, described in 2, 12, 60 and 300 pages in [Hut01b, Hut01a, Hut05, Hut07b], respectively.  In this work, an order relation for intelligent agents is presented, with respect to which the provably optimal AIXI agent is maximal.  The universal intelligence measure presented here is a derivative of this order relation.  A short description of the universal intelligence measure appeared in [LH05], from which two articles followed in the popular scientific press [GR05, Fi´e05].  An 8 page paper on universal intelligence appeared in [LH06b], followed by an updated poster presentation [LH06a].  In the current paper we explore universal intelligence in much greater detail, in particular the way in which it relates to mainstream views on human intelligence and other proposed definitions of machine intelligence.  Human intelligence is an enormously rich topic with a complex intellectual, social and political history.  For an overview the interested reader might want to consult \"Handbook of Intelligence\"  [Ste00] edited by R. J. Sternberg.  Our objective in this section is simply to sketch a range of tests, theories and definitions of human and animal intelligence.  We are particularly interested in common themes and general perspectives on intelligence that could be applicable to many kinds of systems, as these will form the foundation of our definition of machine intelligence in the next section.  children  [Bin11].  It was found that Binet's test results were a good predictor of children's academic performance.  Lewis Terman of Stanford", "metadata": {"source_file": "0712.3329v1.pdf", "title": "Universal Intelligence:  [cs.AI] 20 Dec 2007 A Definition of Machine Intelligence", "authors": ["Marcus Hutter"], "year": "2024", "detected_language": "en", "page_count": 49, "origin_chunk_file": "0712.3329v1.chunks.json"}}
{"text": "Bayesian networks consisting of the random variables B, L and S. At each choice point in this tree we choose how dom variables under consideration then there is probability Pl that Y is chosen to be a parent of X, probability P2 that Y is chosen to be a child of X, and probability p3 that there tached to the choices that constitute x. For example if x2 is the derivation leading to leaf 2 in ENTREE, then 1/J>. (xz) = PiP2. !>. (x) is simply 1/J>. (xlx is successful), so: Of a choice Ci in a SUCCeSsful derivation X be L'i(X), then f>.(x) = ZJ:1llip@•(x) = Z):1 exp(L:;i .\\iv,(x)). In the choice point we choose between either an edge from A to E or an edge from E to A. An undirected edge corresponds yield the undirected graph MA-B· We define P>. so that P>.(MA-B) = f>.(x2) + f>.(x3) = P1P2 + P2Pl = 2PIP2· In general, P>.(M) is just the distribution over models de suppose we set PI = P2 = p3 = 1/3. This means Vx1/J>.(x) = 1/27. Z>. = 25/27, so f>.(x) = 1/25 for all successful x and P>.(M) = 1/25 for all BNs. Increasing P3 at the expense of p1 and P2 expresses a preference BN at leaf 27 in Fig 5) the mode of the prior. Setting PI = 0 almost surely removes each leftmost choice from stop backtracking from Mi. Let Ci be the choice from Node(Mi, M*) that leads to Mi, and define C* analo of moving from M; to Mi in t steps, is positive, for all t > 0. So a fortiori the Markov chain is irreducible and, let prior over () x, 1 u; for all X; and u;. We also assume global parameter independence and parameter modular ity. The former says that the total density over the com plete parameter set is the product of the individual Ox,]u; densities and the latter that Bx;ǣu; is the same for any two %RV parent of H, p_l=l/3 1/3 :: which edge( [H]T] ,RV, [H-RV]Rest]) choose edges(T,RV,Rest). tRV child of H, p 2=1/3 l/3 :: which edgel[H]TI ,RV, [RV-H!Rest]) choose edges(T,RV,Reat). o edge¶ p_3=1/3 l/3 :: which edge([ HjT] ,RV,Rest) choose_edges(T,RV·Rest). represents the ancestor sets for each node, so for BNs it would be [b- [1] , 1-  [] , s-  [b,l]].  The ancestor sets A logic program P together with a goal G, defines an SLD tree each branch of which is a refutation of G using P. no ENs with s, l and b as nodes\") we (essentially) get BNT REE as an SLD-tree.  Each successful branch re (logs of) the probabilities added be>.  = (>.1, >.2, . .  . , An).  For any goal G, S has an associated SLD-tree: the one for of the well-known 'Asia' network given in Fig 3.  This is BN19 in Fig 5.  We then used a uniform prior over the set ing a cyclic transition kernel.  We cycle through the values Pb == 1 - z-n, for n == 1, . . .  , 28, so that on every 28th to estimate the posterior probability that xi is a parent of Xi for all i, j consistent with the ordering.  There are 28 Fig 7), we get a few really bad points.  In the worst case A was almost always the parent of D in the first run, but chain graph D*, the essential graph.  It is easy to de fine a prior over essential graphs and hence over Markov equivalence classes using an SLP.  These can be extended to define a prior over BNs if desired.  SEa, a fragment of which is given in Fig 4, does this and is most eas ily understood by looking at the steps used to sample from the prior it defines.  Given a set of random variables RVs, skeleton/2 first probabilistically chooses an undi rected graph Skel, then essential_graph/3 proba bilistically chooses an essential graph EG with immoralities Imms by adding arrows to Skel.  bn/3 is then defined to probabilistically choose a particular BN from the equiva lence class of BNs defined by EG.  It is more natural to de fine a prior over essential graphs, but we extended the prior to be over BNs to compare with our previous experiments.  Although it was easy to define the prior, the associated probability tree had far too many failure derivations to be useful.  A similar problem occurred when, in a sepa rate experiment, we used a prior with constraints enforcing marginal independence between pairs of nodes.  Although all the experiments here are for BNs our approach is general.  Our SLP implementation, which works by trans lating a human-readable SLP as in Fig 2 to Prolog, does not 'know' that the first-order terms are BNs.  If we want to apply our method to different models, we just write the appropriate SLP priors and write code to compute the ap propriate Bayes factors.  Our final conclusion is that further work is required to fully understand the advantages and limitations of our approach.  There are many methods in the literature for improving MCMC and we have only tried one of them (the use of a cyclic transition kernel).  One big problem was that natural ways of defining priors generally led to inefficient MCMC due to the existence of many failures.  The same problem arises in logic programming where it is addressed using source-to-source program transformation.  We expect pro gram transformation to be necessary for real applications, where, if we can exploit prior information using our frame work, substantial benefits are possible.  Denison, D. G. T., Mallick, B. K., & Smith, A. F. M. (1998).  A Bayesian CART algorithm.  Biometrika, 85(2), 363-377.  Gilks, W. R., Richardson, S., & Spiegelhalter, D. (Eds.).  (1996).  Markov Chain Monte Carlo in Practice.  Chapman & Hall, London.  Gillespie, S. B., & Perlman, M. D. (2001).  Enumerating Markov equivalence classes of acyclic digraph mod els.  In Uncertainty in Artificial Intelligence: Pro Madigan, D., Andersson, S. A., Perlman, M. D., & Volin sky, C. T. (1996).  Bayesian model averaging and model selection for Markov equivalence classes of acycic digraphs.  Communications in Statistics: The Philps, D. B., & Smith, A. F. M. (1996).  Bayesian model comparison via jump diffusions.  In Gilks, W. R., Richardson, S., & Spiegelhalter, D. (Eds.), Markov Roberts, G. 0. (1996).  Markov chain concepts related to sampling algorithms.  In Gilks, W. R., Richardson, S., & Spiegelhalter, D. (Eds.), Markov Chain Monte Figure 5: BNT REE probability tree defining a distribution over all Bayes nets consisting of the three random variables A,B,C. Figure 6:  Comparing estimates of the posterior probabilities of child-parent features from two different MCMC runs.  BNs restricted to those consistent with a specific ordering.  Fork = 2 (2po) runtimes=1472,1492; k = 2, half data (2po...half) runtimes = 1479,1447; and k = 3 (3po) runtimes=1960,1944.  Figure 7: Comparing estimates of the posterior probabilities of child-parent features from two different MCMC runs.  No variable ordering imposed.  Fork = 2 (2pun) runtimes=2273,2258; k = 3 (3pun) runtimes=2882,2891; and without any restriction on k (Spun) runtimes=3340,3122  Figure 8: 'Recovered' BNs where only links with estimated posterior probability over 95% are created.  For (i) the totally unconstrained case, (ii) when only a maximum of k = 3 parents were allowed and (iii) when only a maximum of k = 3 parents were allowed, but the parents of E were forced to beT and L.  No variable ordering was imposed.", "metadata": {"source_file": "1301.2254v1.pdf", "title": null, "authors": ["Monte Carlo", "Nicos Angelopoulos", "James Cussens"], "year": "2011", "detected_language": "en", "page_count": 8, "origin_chunk_file": "1301.2254v1.chunks.json"}}
{"text": "The currently most efficient algorithm for infer ence with a probabilistic network builds upon a triangulation of a network's graph. In this paper, we show that pre-processing can help in finding good triangulations for probabilistic networks, that is, triangulations with a minimal maximum clique size. We provide a set of rules for step wise reducing a graph. The reduction allows us to solve the triangulation problem on a smaller graph. From the smaller graph's triangulation, a triangulation of the original graph is obtained by reversing the reduction steps. Our experimental results show that the graphs of some well-known real-life probabilistic networks can be triangu lated optimally just by pre-processing; for other networks, huge reductions in size are obtained. The currently most efficient algorithm for inference with a probabilistic network is the junction-tree propagation al gorithm that builds upon a triangulation of a network's moralised graph [1, 2]. The running time of this algorithm depends on the triangulation used. In general, it is hard to find a triangulation for which this running time is minimal. As there is a strong relationship between the running time of the algorithm and the maximum of the triangulation's clique sizes, for real-life networks triangulations are sought for which this maximum is minimal. The minimum of the maximum clique size over all triangulations of a graph is a well-studied notion, both by researchers in the field of probabilistic networks and by researchers in graph theory and graph algorithms. In the latter field of research, the no tion of treewidth is used to denote this minimum minus one. Unfortunately, computing the treewidth of a given graph is an NP-complete problem [3]. of the problem under study, using relatively little compu tation time and without losing optimality. The smaller, and presumably easier, problem is subsequently solved. In this paper, we discuss pre-processing for triangulation of probabilistic networks.  We provide a set of rules for step wise reducing the problem of finding a triangulation for a network's moralised graph with minimal maximum clique size, to the same problem on a smaller graph.  Various al gorithms can then be used to solve the smaller problem.  Given a triangulation of the smaller graph, a triangulation of the original graph is obtained by reversing the reduction steps.  Our reduction rules are guaranteed not to destroy op timality with respect to maximum clique size.  Experiments with pre-processing revealed that our rules can effectively reduce the problem size for various real-life probabilistic networks.  In fact, the graphs of some well-known networks are triangulated optimally just by pre-processing.  In this paper, we do not address the second phase in the pre processing approach outlined above, that is, we do not ad dress actually constructing triangulations with a minimal or close to minimal maximum clique size.  Recent research re sults indicate, however, that for small graphs optimal trian gulations can be feasibly computed.  Building upon a vari ant of an algorithm by Am borg, Corneil, and Proskurowski  [3], K. Shoikhet and D. Geiger performed various exper iments on randomly generated graphs [4].  Their results indicate that this algorithm allows for computing optimal triangulations of graphs with up to I 00 vertices.  The paper is organised as follows.  In Section 2, we review some basic definitions.  In Section 3, we present our pre processing rules.  The computational model in which these rules are employed, is discussed in Section 4.  In Section 5, we report on our experiments with well-known real-life probabilistic networks.  The paper ends with some conclu sions and directions for further research in Section 6.  Let G = (V, A) be a directed acy clic graph.  The moral isation of G is the undirected graph M(G) obtained from G by adding edges between every pair of non-adjacent ver tices that have a common successor (vertices v and w have a common succ essor if there is a vertex x with ( v, x)  E A a nd ( w, x) E A), and then dropping the arcs' directions.  Let G = (V, E) be an undirected graph.  A set of vertices W ũ V is called a clique in G if there is an edge between every pair of disjoint vertices from W; the cardinality of W is the clique's size.  For a set of vertices W Ǵ V, the subgraph induced by W is the graph G[WJ = (W, (W x W) n E); for a single vertex v, we write G-v to denote G[V - { v} ].  The graph G is triangulated if it does not contain an induced subgraph that is a simple cycle of length at least four.  A triangulation of G is a triangulated graph H (G) minus 1.  The treewidth of G, denoted r (G), is the minimum treewidth over all triangulations of G. A graph H is a minor of a graph G if H can be obtained from G by zero or more vertex deletions, edge deletions, and edge contractions (edge contra cti on is the operation that replaces two adjacent vertices v and w by a single ver tex that is connected to all neighbours of v and w ).  It is well known (see for example  [5]), that the treewidth of a minor of G is never larger than the treewidth of G itself.  A linear ordering of an undirected graph G = (V, E) is a bijection V t-t {1, . . . , jVI}.  For v E V and a linear or dering f of G- v, we denote by ( v; f) the linear ordering F of G that is obtained by adding v at the beginning off, that is, f'(v)  = 1 and, for all w =/= v, F(w) = f(w) + 1.  if, for each v E V, its higher ordered neighbours form a clique, that is, if every pair of distinct verti ces in the set {wE V I {v, w} E E and f(v) < f(w)} is adjacent.  It is well known (see for example  [6]), that a graph is triangu lated if and only if it allows a perfect elimination scheme.  For a graph G and a linear ordering f of G, there is a unique minimal triangulation H (G) of G that has f for its perfect elimination scheme.  This triangulation, which we term the fill-in given j, can be constructed by, fori = 1, ... , !VI, turning the set of higher numbered neighbours of f-1 ( i) into a clique.  The maximum clique size minus 1 of this fill-in is called the treewidth of f.  The treewidth of a lin ear ordering of a triangulated graph equals the maximum number of higher numbered neighbours of a vertex [ 6].  To conclude, a junction tree of an undirected graph G = (V, E) is a tree T = (I, F), where every node i E I has associated a vertex set Vi, such that the following two properties hold: the set {Vi I i E I} equals the set Tv = {  i I v E Vi} c ons titu tes a connected subtree ofT.  It is well known (see for example [6]), that a graph is trian gulated if and only if it has a junction tree.  Pre-processing a probabilistic network for triangulation builds upon a set of reduction rules.  These rules allow for stepwise reducing a network's moralised graph to another graph with fewer vertices.  The steps applied during the re duction can be reversed, thereby enabling us to compute a triangulation of the original graph from a triangulation of the smaller graph.  In this section, we discuss the various rules; a discussion of the computational method in which these rules are employed, is deferred to Section 4.  During a graph's reduction, we maintain a stack of elimi nated vertices and an integer low that gives a lower bound for the treewidth of the original graph.  Application of a re duction rule serves to modify the current graph G to G' and to possibly update low to low'.  We say that the rule is safe if max(r(G), low) = max(r(G'), low').  By applying safe rules, therefore, we have as an invariant that the treewidth of the original graph equals the maximum of the treewidth of the reduced graph and the value low.  In the sequel, we assume that the original moralised graph has at least one edge and that low is initialised at 1.  Our first reduction rule applies to simplicial vertices.  A vertex v is simplicial in an undirected graph G if the neigh bours ofv form a clique in G. Proof.  Since G contains a clique of size d + 1, we have that r(G) ;:: d. We further observe that r(G) ;:: r(G - v), because G - v is a minor of G.  We therefore have that r(G) ;:: max(d, r(G- v)) .  Now, let f be a linear ordering of G-v of treewidth k::; max(d, r(G- v)).  Let H be the fill-in of G- v given f.  Adding vertex v and its (formerly) incident edges to H yields a graph H' that is still triangu lated: as every pair of neighbours of v is adjacent, v cannot belong to a simple (ch ordless ) cycle of length at least four.  The maximum clique size of H' therefore equals the maxi mum ofd+1andk+l.  Hence,r(G) Ū max(d,r(G-v)), from which we conclude the first property stated in the lemma.  To prove the second property, we observe that the linear ordering (v; f) is a perfect elimination scheme for H', as removal of v upon computing the fill-in of H' does not create any additional edges.  Reduction Rule 1: Simplicial vertex rule Let v be a simplicial vertex of degree d  u 0.  Removev.  Set low to max(/ow, d).  From we have that the simplicial vertex rule is safe.  The second property stated in the lemma further pro vides for the rule's reversal when computing a triangulation of the original graph from one of the reduced graph.  Because the digraph G of a probabilistic network is moralised before it is triangulated, it is likely to give rise to many simplicial vertices.  We consider a vertex v with outdegree zero in G. Since all neighbours of v have an arc pointing into v, moralisation will connect every two neigh bours that are not yet adjacent, thereby effectively turning v into a simplicial vertex.  The simplicial vertex rule wiii thus remove at least all vertices that have outdegree zero in the network's original digraph.  As every directed acyclic graph has at least one vertex of outdegree zero, at least one reduc tion will be performed.  As the reduced graph need not be the moralisation of a directed acyclic graph, it is possible that no further reductions can be applied.  The digraph G of a probabilistic network may also include vertices with indegree zero and outdegree one.  These ver tices will always be simplicial in the moralisation of G.  We consider a vertex v with indegree zero and a single arc pointing into a vertex w. In the moralisation of G, w and its (former) predecessors constitute a clique.  As all neigh bours of v belong to this clique, v is simplicial.  A special case of the simplicial vertex rule applies to ver tices of degree 1; it is termed the twig rule, after [7].  The twig rule is based upon the observation that vertices of degree one are always simplicial.  Another special case is the islet rule that serves to remove vertices of degree zero.  We would like to note that many heuristic triangulation al gorithms, such as the algorithm described in [8], remove simplicial vertices.  Our second reduction rule applies to almost simplicial ver tices.  A vertex v is almost simplicial in an undirected graph G if there is a neighbour w of v such that all other neigh bours of v form a clique in G. Figure 2 illustrates the basic idea.  As we allow other neighbours of v to be adjacent to Let G be an undirected graph and let v be an almost simplicial vertex in G with degree d u 0.  Let G' be the graph that is obtained/rom G by turning the neighbours of v into a clique and then removing v. Then, • the linear ordering ( v; f) of G, with f a linear or dering ofG' oftreewidth at most max(d, r(G')), has treewidth at most max(d, r(G')).  Proof.  Let w be a neighbour of v such that the other neighbours of v form a clique.  As we can obtain G' from G by contracting the edge { v, w}, G' is a minor of G.  We therefore have that r(G') :-:; r(G).  Now, let f be a linear ordering ofG' of treewidth k :-:; max(d, r(G')).  Let H be the fill-in ofG' given f.  If we add v and its (formerly) adja cent edges to H, then v is simplicial in the resulting graph H1• Using Lemma l, we find that r(H')  = max(k, d).  The two properties stated in the lemma now follow.  D\nBuilding upon we find that the almost simpli cial vertex rule is safe.  Suppose that we have G and low before, and G' and low1 after application of the rule.  Then, r(G') :-:; r(G), r(G) :-:; max(d,r(G')), and d :-:; low = low1•  We conclude that max(r(G),low)  =\nmax(r(G'),low1).  Examples can be constructed, unfortu nately, that show that the rule is not safe for low < d. ducing any graph of treewidth one to the empty graph.  The islet, twig and series rules suffice for reducing any graph of Let G be an undirected graph and let v, w be two vertices of degree three having the same set of neigh bours.  Let G' be the graph that is obtained from G by turning the set of neighbours of v into a clique and then removing v and w. Then, • the linear ordering (v; (w; f)), with f a linear or dering ofG' oftreewidth at most max(3, r(G')), has treewidth at most max(3, r(G')).  contracting the edges { v, x} and { w, y} in G, we obtain G'.  So, G' is a minor of G.  We find that r(G1) :-:; r(G) .  graph and the islet, twig, series, triangle, buddy and cube rules cannot be applied, then we know that the graph has moralised graph of the entire ALARM network is thus re duced to the empty graph, which indicates that our reduc 5.  Let H be the triangulation that results from step 4.  For H, a perfect elimination scheme f is constructed.  6.  Until S is empty, the top element v is popped from S and f is replaced by (v; f).  tain lists of the vertices of degree zero, one, and two.  The buddy, triangle and cube rules can also be implemented to a vertex of degree d, investigating simpliciality then takes O(d2) time.  In a graph with n vertices, we may have to costs OCL d(v)2)  = O(ne) time, where d(v) is the dev .  gree of vertex v and e denotes the number of edges m the would like to note that addition of the almost simplicial ver tex rule to P R-4 did not result in any further reductions.  network before pre-pro pre-pro pre-pro pre-pro pre-pro pre-pro withS with PR-1 with PR-2 with PR-3 with PR-4 lVI lEI lVI lEI lVI lEI lVI lEI lVI lEI lVI lEI WILSON  I I\nALARM  I  I\nVSD\nOESOPHAGUS\nOESOPHAGUS+\nMUNIN\nICEA\nPATHFINDER\nOESOPHAGUS+ _0\n0.01 0.01 0.06 with probabilities on graphical structures and their applica tion to expert systems.  The Journal of the Royal Statistical Society.  Series B, vol.  50, pp.  157-224, 1988.  ity of finding embeddings in a k-tree.  SIAM Journal on Al gebraic and Discrete Methods, vol. 8, pp.  277-284, 1987.  [4] K. Shoikhet and D. Geiger.  A practical algorithm for finding optimal triangulations.  Proceedings of the National Con ference on Artificial Intelligence (AAAI 97), pp.  185-190.  [8] A. Becker and D. Geiger.  A sufficiently fast algorithm for finding close to optimal junction trees.  Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, [10] R.E. Tarjan and M. Yannakakis.  Simple linear time algo rithms to test chordality of graphs, test acyclicity of graphs, [11] D.J. Rose, R.E. Tarjan, and G.S. Lueker.  Algorithmic as pects of vertex elimination on graphs.  SIAM Journal on Computing, vol. 5, pp.  266-283, 1976.  [12] D.P. Sanders.  On linear recognition of tree-width at most four.  SIAM Journal on Discrete Mathematics, vol. 9, pp.  101-117, 1996.", "metadata": {"source_file": "1301.2256v1.pdf", "title": "Pre-processing for Triangulation of Probabilistic Networks 6. 2", "authors": ["Hans L. Bodlaender", "Koster", "Frank van den Eijkhof", "Linda C. van der"], "year": "2011", "detected_language": "en", "page_count": 8, "origin_chunk_file": "1301.2256v1.chunks.json"}}
{"text": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]). However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques. With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10].  For example, neural network based language models significantly outperform N-gram models  [1, 27, 17].  The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.  As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.  We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20].  This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings  [13, 14].  Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities.  Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(\"King\") - vector(\"Man\") + vector(\"Woman\") results in a vector that is closest to the vector representation of the word Queen  [20].  In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words.  We design a new comprehensive test set for measuring both syntactic and semantic regularities1, and show that many such regularities can be learned with high accuracy.  Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.  Representation of words as continuous vectors has a long history  [10, 26, 8].  A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model.  This work has been followed by many others.  Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer.  The word vectors are then used to train the NNLM.  Thus, the word vectors are learned even without constructing the full NNLM.  In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.  Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).  In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words  [20, 31]; LDA moreover becomes computationally very expensive on large data sets.  Similar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.  Next, we will try to maximize the accuracy, while minimizing the computational complexity.  where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture.  Common choice is E = 3 −50 and T up to one billion.  All models are trained using stochastic gradient descent and backpropagation  [26].  The probabilistic feedforward neural network language model has been proposed in [1].  It consists of input, projection, hidden and output layers.  At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary.  The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix.  As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.  The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense.  For a common choice of N = 10, the size of the projection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units.  Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V .  Thus, the computational complexity per each training example is In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree.  This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16].  Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V )).  For example when the vocabulary size is one million words, this results in about two times speedup in evaluation.  While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N ×D×H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization.  Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks  [15, 2].  The RNN model does not have a projection layer; only input, hidden and output layer.  What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections.  This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.  where the word representations D have the same dimensionality as the hidden layer H. Again, the term H × V can be efficiently reduced to H × log2(V ) by using hierarchical softmax.  Most of the complexity then comes from H × H.  To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief  [6], including the feedforward NNLM and the new models proposed in this paper.  The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters.  For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7].  Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.  In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity.  The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model.  While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.  The new architectures directly follow those proposed in our earlier work  [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words.  While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.  Note that related models have been proposed also much earlier [26, 8].  The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).  We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.  Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.  Training complexity is then Q = N × D + D × log2(V ).  (4) We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.  The model architecture is shown at Figure 1.  Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.  The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.  More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.  We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity.  Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.  where C is the maximum distance of the words.  Thus, if we choose C = 5, for each training word we will select randomly a number R in range , and then use R words from history and Figure 1: New model architectures.  The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.  R words from the future of the current word as correct labels.  This will require us to do R  × 2 word classifications, with the current word as input, and each of the R + R words as output.  In the following experiments, we use C = 10.  Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words.  To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(\"biggest\")−vector(\"big\")+ vector(\"small\").  Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search).  When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this method.  Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin.  Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.  Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic Word Relationship test set.", "metadata": {"source_file": "1301.3781v3.pdf", "title": "[cs.CL] 7 Sep 2013 Efficient Estimation of Word Representations in Vector Space", "authors": ["Kai Chen  .", "Greg Corrado", "Jeffrey Dean  ."], "year": "2013", "detected_language": "en", "page_count": 12, "origin_chunk_file": "1301.3781v3.chunks.json"}}
{"text": "Type of relationship Word Pair 1 Word Pair 2 Common capital city Athens Greece Oslo Norway All capital cities Astana Kazakhstan Harare Zimbabwe Currency Angola kwanza Iran rial City-in-state Chicago Illinois Stockton California Man-Woman brother sister grandson granddaughter Adjective to adverb apparent apparently rapid rapidly Opposite possibly impossibly ethical unethical Comparative great greater tough tougher Superlative easy easiest lucky luckiest Present Participle think thinking read reading Nationality adjective Switzerland Swiss Cambodia Cambodian Past tense walking walked swimming swam Plural nouns mouse mice dollar dollars Plural verbs work works speak speaks To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York). We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric.  Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.  We have used a Google News corpus for training the word vectors.  This corpus contains about 6B tokens.  We have restricted the vocabulary size to 1 million most frequent words.  Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy.  To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.  The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2.  It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements.  So, we have to increase both vector dimensionality and the amount of the training data together.  While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary.  Only questions containing words from the most frequent 30k words are used.  Dimensionality / Training words 24M 49M 98M 196M 391M 783M\n24.0 30.1 36.5 40.8 46.6 50.4 Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors.  The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20] Model Semantic-Syntactic Word Relationship test set MSR Word Relatedness Architecture Semantic Accuracy  [%] Syntactic Accuracy [%] Test Set [20] RNNLM\nNNLM\nCBOW\nSkip-gram\n(such as 50 - 100).  Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.  For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation.  We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.  First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors.  In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary.  We also include results on a test set introduced in [20] that focuses on syntactic similarity between words3.  The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary).  We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU.  We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8).  In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions.  The NNLM vectors perform significantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer.  The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one.  Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.  Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors.  The comparison is given in Table 4.  The CBOW model was trained on subset Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models.  Full vocabularies are used.  Semantic Syntactic Total Collobert-Weston NNLM\n37M 1.4 2.6 2.1 Turian NNLM\n37M 3.3 13.2 8.8 Mikolov RNNLM\n783M 50.0 55.9 53.3 Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch.  Accuracy is reported on the full Semantic-Syntactic data set.  Model Vector Training Accuracy  [%] Training time Dimensionality words [days] Semantic Syntactic Total 3 epoch CBOW\n3 epoch Skip-gram\n1 epoch CBOW\n1  epoch Skip-gram\n1 epoch Skip-gram\n783M 56.7 54.5 55.5 2.5 of the Google News data in about a day, while training time for the Skip-gram model was about three days.  For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training).  Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup.  As mentioned earlier, we have implemented various models in a distributed framework called DistBelief.  Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad [7].  We used 50 to 100 model replicas during the training.  The number of CPU cores is an Table 6: Comparison of models trained using the DistBelief distributed framework.  Note that training of NNLM with 1000-dimensional vectors would take too long to complete.  Model Vector Training Accuracy  [%] Training time Dimensionality words [days x CPU cores] Semantic Syntactic Total NNLM\n6B 66.1 65.1 65.6 2.5 x 125 Architecture Accuracy  [%] 4-gram [32]\nAverage LSA similarity [32]\nLog-bilinear model  [24] 54.8 RNNLMs [19] 55.4 Skip-gram 48.0 Skip-gram + RNNLMs 58.9 estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit.  Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations.  The result are reported in Table 6.  The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32].  This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices.  Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model  [32], log-bilinear model  [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19].  We have explored the performance of Skip-gram architecture on this task.  First, we train the 640dimensional model on 50M words provided in [32].  Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence.  The final sentence score is then the sum of these individual predictions.  Using the sentence scores, we choose the most likely sentence.  A short summary of some previous results together with the new results is presented in Table 7.  While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).  Table 8 shows words that follow various relationships.  We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word.  Thus for example, Paris - France + Italy = Rome.  As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skipgram model trained on 783M words with 300 dimensionality).  Relationship Example 1 Example 2 Example 3 France - Paris Italy: Rome Japan: Tokyo  Florida: Tallahassee big - bigger small: larger cold: colder quick: quicker Miami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii Einstein - scientist Messi: midfielder Mozart: violinist Picasso: painter Sarkozy - France Berlusconi:  Italy Merkel: Germany Koizumi: Japan copper - Cu zinc:  Zn gold:  Au uranium: plutonium Berlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack Microsoft - Windows Google: Android IBM: Linux Apple: iPhone Microsoft - Ballmer Google: Yahoo IBM:  McNealy Apple: Jobs Japan - sushi Germany: bratwurst France:  tapas USA: pizza assumes exact match, the results in Table 8 would score only about 60%).  We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications.  Another way to improve accuracy is to provide more than one example of the relationship.  By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.  It is also possible to apply the vector operations to solve different tasks.  For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector.  This is a popular type of problems in certain human intelligence tests.  Clearly, there is still a lot of discoveries to be made using these techniques.  In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.  We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent).  Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set.  Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary.  That is several orders of magnitude larger than the best previously published results for similar models.  An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2  [11].  The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman's rank correlation over the previous best result [31].  The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection  [28].  It can be expected that these applications can benefit from the model architectures described in this paper.  Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.  Results from machine translation experiments also look very promising.  In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.  We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors.  We also expect that high quality word vectors will become an important building block for future NLP applications.  [1] Y. Bengio, R. Ducharme, P. Vincent.  A neural probabilistic language model.  Journal of Machine Learning Research, 3:<PHONE>, 2003.  [3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.  Large language models in machine translation.  In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007.  [4] R. Collobert and J. Weston.  A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.  In International Conference on Machine Learning, ICML, 2008.  [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa.  Natural Language Processing (Almost) from Scratch.  Journal of Machine Learning Research, 12:24932537, 2011.  [6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.  [7] J.C. Duchi, E. Hazan, and Y. Singer.  Adaptive subgradient methods for online learning and stochastic optimization.  Journal of Machine Learning Research, 2011.  [9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng.  Improving Word Representations via Global Context and Multiple Word Prototypes.  In: Proc.  Association for Computational Linguistics, 2012.  [10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart.  Distributed representations.  In: Parallel distributed processing: Explorations in the microstructure of cognition.  Volume 1: Foundations, MIT Press, 1986.  [11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak.  Semeval-2012 task 2: Measuring degrees of relational similarity.  In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.  [12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts.  Learning word vectors for sentiment analysis.  In Proceedings of ACL, 2011.  [14] T. Mikolov, J. Kopeck´y, L. Burget, O. Glembek and J. ˇCernock´y.  Neural network based language models for higly inflective languages, In: Proc. ICASSP 2009.  [15] T. Mikolov, M. Karafi´at, L. Burget, J. ˇCernock´y, S. Khudanpur.  Recurrent neural network based language model, In: Proceedings of Interspeech, 2010.  [16] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock´y, S. Khudanpur.  Extensions of recurrent neural network language model, In: Proceedings of ICASSP 2011.  [17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇCernock´y.  Empirical Evaluation and Combination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.  [18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇCernock´y.  Strategies for Training Large Scale Neural Network Language Models, In: Proc.  Automatic Speech Recognition and Understanding, 2011.  [21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean.  Distributed Representations of Words and Phrases and their Compositionality.  Accepted to NIPS 2013.  [23] A. Mnih, G. Hinton.  A Scalable Hierarchical Distributed Language Model.  Advances in Neural Information Processing Systems 21, MIT Press, 2009.  [24] A. Mnih, Y.W. Teh.  A fast and simple algorithm for training neural probabilistic language models.  ICML, 2012.  [26] D. E. Rumelhart, G. E. Hinton, R. J. Williams.  Learning internal representations by backpropagating errors.  Nature, 323:533.536, 1986.  [28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning.  Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.  In NIPS, 2011.  [29] J. Turian, L. Ratinov, Y. Bengio.  Word Representations: A Simple and General Method for Semi-Supervised Learning.  In: Proc. Association for Computational Linguistics, 2010.  [30] P. D. Turney.  Measuring Semantic Similarity by Latent Relational Analysis.  In: Proc.  International Joint Conference on Artificial Intelligence, 2005.  [31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov.  Combining Heterogeneous Models for Measuring Relational Similarity.  NAACL HLT 2013.", "metadata": {"source_file": "1301.3781v3.pdf", "title": "[cs.CL] 7 Sep 2013 Efficient Estimation of Word Representations in Vector Space", "authors": ["Kai Chen  .", "Greg Corrado", "Jeffrey Dean  ."], "year": "2013", "detected_language": "en", "page_count": 12, "origin_chunk_file": "1301.3781v3.chunks.json"}}
{"text": "Functional dependencies restrict the potential interactions among variables connected in a probabilistic network. This restriction can be exploited in qualitative probabilistic reasoning by introducing deterministic variables and mod ifying the inference rules to produce stronger conclusions in the presence of functional rela tions. I describe how to accomplish these modi fications in qualitative probabilistic networks by exhibiting the update procedures for graphical transformations involving probabilistic and de terministic variables and combinations. A sim ple example demonstrates that the augmented scheme can reduce qualitative ambiguity that would arise without the special treatment of functional dependency. Analysis of qualitative synergy reveals that new higher-order relations are required to reason effectively about syn ergistic interactions among deterministic vari ables. A degenerate special case of probabilistic relations arises when the variables are connected by a de terministic, or functional, relationship. Although strictly a subclass of the general probabilistic case, it often pays to distinguish relations of this type and earmark them for special treatment. For example, Neufeld and Poole [1988] rely on the distinction be tween deterministic (implication) and probabilistic (confirmation) relations in their application of qual itative probability to default reasoning. The impor tant difference between functional and probabilistic dependencies is in the restrictions they impose on potential interactions among connected variables. In this paper, I investigate the opportunity to exploit these constraints where deterministic and probabilis tic variables coexist in networks of qualitative rela tions. The product is a special set of inference rules for manipulating combinations of these relations. In corporating these rules in a hybrid representation scheme results in a language more expressive and powerful than would be obtained from the simple union of its deterministic and probabilistic compo nents.  The advantage of a functional relation is that the arguments of the function completely determine (hence the term deterministic) its value.  Any other variables added to the argument list would be su perfluous.  In contrast, introducing additional con ditioning variables to a conditional probability can cause its value to change arbitrarily.  Because de terministic relations impose stricter limits on poten tial interactions, they are inherently more modular than probabilistic relations  [Beckerman and Horvitz, 1988].  A formal expression of this enhanced modularity can be found in graphical criteria for conditional in dependence in probabilistic networks  [Pearl et al., 1989].  A network containing deterministic vari ables entails more conditional independencies than an identical structure representing purely probabilis tic relations.  As demonstrated below, a similar improvement can be achieved for other qualitative properties of relations in probabilistic networks-in particular, monotonicity.  Indeed, the ability to de rive stronger qualitative conclusions from networks containing deterministic relations is ultimately due to the extra independencies sanctioned by the func tional constraints.  Specification of deterministic variables in proba bilistic network representations was introduced by Shachter for numeric influence diagrams [1988].  The hybrid representation scheme presented here extends the qualitative probabilistic network (QPN) formal ism  [Wellman, 1990a] to accommodate determin istic relations.  Its manipulation of functional de pendencies draws on the work of Michelena and Agogino [1989] on deterministic monotonic influ ence diagrams ( dMIDs).  In addition, the synthesis yields new inference rules not expressible in QPNs or dMIDs alone.  A probabilistic network (also called a belief network or influence diagram, with some variations) is a di rected acyclic graph composed of nodes denoting random variables and edges indicating their prob abilistic dependencies.  A network represents a valid dependency structure, called an Independence- or I map  [Pearl et al., 1989], if the joint distribution over the entire variable set can be factored into the con ditional probabilities of each node given its prede cessors.  When a network is an I-map, the graphi cal d-separation criterion is a sufficient condition for conditional independence.  In the presence of deter ministic variables, the stronger D-separation (note uppercase) condition may be applied.  For defini tions of these conditions and thorough discussion of their properties, see the work of Pearl et al.  ([1989], for example).  The probabilistic network of Figure 1 provides an example of the distinction between d- and D separation.  Nodes x andy may be marginally depen dent, as they have a common predecessor, w. (The dashed inner ellipse indicates that w may or may not be deterministic.)  They are conditionally inde pendent given w, as w blocks the only path (undi rected) between them.  Suppose w is a probabilis tic node.  Then x and y are not conditionally inde pendent given z, because z provides only probabilis tic information about w. Even given z, information about x is potential evidence impinging on w, and hence affects belief about y. These independence and dependence assertions are in accordance with the d-separation criterion applied to the graph.  If w is functionally determined by z, however, then knowledge of z leaves no room for further influence from x on belief about w.  Therefore there is no ef fective path from x to y, and the two variables are conditionally independent given z.  This is verified by the stronger separation criterion: if w is deter ministic, x and y are D-separated by { z}.  The determinism of w impacts relations beyond conditional independence.  Consider the qualitative probabilistic network of Figure 2.  This network is similar to the one above, with one extra edge and signs 6i E {+, -,?} on each link indicating the qualitative influences holding among the variables.  Qual itative influences are a type of monotonicity con straint on the probabilistic dependence between the associated variables.  Inference in QPNs consists of combinations and manipulations of these influences to derive new influences among variables not directly connected in the original network.  Suppose we are interested in determining the qual itative relation of z on y given x in this model.  In the given network, the value of y is specified in terms of w; z and x are only indirectly related to y. To compute the relation of interest, we transform the network via a series of node reductions and link re versals until the relation is displayed directly.  In our example, z must replace w as a predecessor of y.  The probabilistic semantics of qualitative influ ences sanctions simple graphical transformation op erations based on sign multiplication ( ®) and addi tion (EB)  [Wellman, 1990a; Wellman, 1990b].  Treat ing the network as a standard QPN, the best trans formation consists of a reversal of the link from w to x, followed by a reduction of w from the network.  The resulting network is depicted in Figure 3a.  Note that regardless of the signs of the original relations (as long as they are nonzero), the qualitative rela tion of z on y given x is ambiguous (that is, 6 = ?).  The ambiguity in this case is not spurious; the po tential interaction of z and w in their relation to x admits an arbitrary probabilistic relation of z on w given x.  This ambiguity then propagates directly to the conditional influence of z on y.  In contrast, if w is a function of z, then knowing x can provide no additional information, and the potential interaction of z and w on x is irrelevant.  In this case, shown in Figure 3b, the relation of z on y (whether x is given or not) depends only on z's influence on w and w's on y. Note also that this network reflects the conditional independence of x and y given z, whereas the network of Figure 3a does not.  Thus, by recognizing the special case of determin istic relations, we are able to obtain strictly stronger qualitative conclusions (unless of course 61 or 6a are ?, in which case the results are equivalent).  Not only can we detect more independencies, we can Figure 3: (a) The transformed network using stan dard QPN operations.  (b) The result obtained by exploiting the deterministic nature of w. also resolve ambiguities that are prevented by these additional independence conditions.  Moreover, the example above demonstrates that this phenomenon can be manifested even when all the variables of in terest are probabilistic.  We can achieve reductions of ambiguity without resorting to explicit identification of independen cies via D-separation.  In the remaining sections, I demonstrate how the advantages of functional re lations can be realized locally by simple modifica tions to the QPN transformation operations for cases where one or more of the nodes involved are deter ministic.  QPNs support two types of qualitative probabilis tic relations.  Influences describe the direction of a probabilistic relation, and synergies describe the in teraction among influences.  The bulk of this analysis concerns qualitative influences; further discussion of synergy is deferred to Section 5.  A qualitative influence is a kind of probabilistic monotonicity constraint on random variables.  We say that that a positively influences b iff the proba bility distribution for b given a is increasing in a, in the sense of first-order stochastic dominance (FSD).  When b has other predecessors besides a, the rela tion must hold for any assignment of values to those variables.  In symbols, Va1, a2.  a1 X a2 => Fb(·lalxo)  FSD Fb(·ia2xo), (1) where Fb is the cumulative probability distribution for b, and xo is any assignment of values to the other predecessors of b.1  An equivalent statement is that Pr(b X bolaxo) is non-decreasing in a, for any values of bo and xo.  A negative influence is defined analogously with the appropriate changes in sign.  When a non monotonic or unknown probabilistic dependence holds, we assign a \"?\" influence.  Independence is denoted by a zero influence, by convention repre sented implicitly in the absence of a link.  For fur ther motivation and implications of this definition see  [Wellman, 1990a].  '  Qualitative influences on deterministic variables are simply functional monotonicity constraints.  Let b be a deterministic variable with predecessors a and x, that is, b = f(a,x) for some function f.  Then a positively influences b iff, for any a1 > a2 and x0, f(al,xo) > f(a2,xo).  When f is differentiable with respect to a, we can exgress this as an inequality on the partial derivative, a! > 0.  If we regard deterministic functions as an extreme class of conditional probability distributions, we see that the definition of influences on deterministic variables is a special case of the probabilistic defi nition (1).  Thus, any sound inference procedure for QPNs will produce sound conclusions in the pres ence of deterministic variables, ignoring the func tional nature of the relations.  The example of Sec tion 2 demonstrated, however, that ignoring this in formation can lead to weakened conclusions, includ ing spurious ambiguity.  Virtually all research in qualitative reasoning has been directed toward deterministic variables [Weld and de Kleer, 1989].  Even in a probabilistic setting, deterministic qualitative relations are likely to play a significant role in definitions, accounting relations, and constraints (for instance, in constrained opti mization problems  [Michelena and Agogino, 1989]).  Conversely, the ability to express probabilistic re lationships in otherwise deterministic models adds veridicality, since real-world problems invariably present elements of uncertainty.  The reader may find it curious that the determin istic relations are defined to be strictly monotonic while the probabilistic definition employs non-strict inequalities.  This practice follows the conventions of previous work: the original definitions of qual itative influences in QPNs [Wellman, 1990a] and the monotonicities used in dMIDs  [Michelena and Agogino, 1989].  These conventions are not entirely arbitrary; the non-strict interpretation is notation ally simpler and more broadly applicable in the prob abilistic case, and strictness is required for invertibil ity in the deterministic.  Invertibility is necessary for the arc reversal op eration in deterministic models, and can be qual itatively guaranteed only for strict monotone rela tions.  In hybrid models, it makes sense to relax this requirement, allowing deterministic nodes to be come probabilistic when their relations are inverted if the prerequisites do not hold.  This suggests that it would be generally useful to admit both strict and non-strict qualitative relations, carefully distinguish ing them and maintaining this information through network transformations.  For notational simplicity, I adopt the convention in this paper that probabilis tic relations are non-strict, while deterministic ones are strict, unless stated otherwise.  Where strictness (or invertibility in general) is critical for the valid ity of an inference rule, the details are spelled out explicitly.  The use of partial derivatives to describe deter ministic relationships presumes continuity and dif ferentiability of the corresponding functions.  Al though the results presented here generally do not depend on these properties, I make use of them in proofs and illustrations for expository simplicity.  The inference rules below could be justified by ar guments based on differences as well as differentials.  The only requirement is that the domain of every variable be ordinally scaled, so that monotonicity is a well-defined property.  The probabilistic relations among any subset of a network's variables can be rendered direct via trans formations composed of sequences of two basic op erations  [Shachter, 1988; Wellman, 1990b].  Node reduction is the process of removing a node from the network by averaging out its effects.  The arc reversal operation changes the orientation of the di rected edge, updating the probabilistic relation using Bayes's rule.  A third operation, deterministic node propagation (DNP), removes links emanating from deterministic nodes.  Each of these operations is as sociated with an update formula or inference rule describing how the qualitative relations need to be modified to reflect the changes in network structure.  Since qualitative deterministic relations are a spe cial case of probabilistic ones, the inference rules for QPNs [Wellman, 1990a] are valid for hybrid net works.  When the nodes of interest are all deter ministic, the stronger dMID rules [Michelena and Agogino, 1989] apply.  The rules below extend these sets to handle combinations of deterministic and probabilistic relations as well.  In the descriptions below, we consider a simple network fragment with nodes a, c, and d (b is saved for the discussion of synergy in Section 5).  Node c has a link to node d, and a optionally has a link to each of the others.  The situation is depicted in Figure 4.  When a node is functionally determined by its prede cessors, its outgoing arcs may be deleted via the op eration of deterministic node propagation  [Shachter, 1988].  The idea is that since the predecessors com pletely describe the node, it would be valid to de scribe the node's relation to its successors directly in terms of these predecessors.  Therefore, the edge removal in DNP is accompanied by an update of the relation from the node's predecessors to its succes sors, adding new links if necessary.  Let c be a deterministic node.  Therefore, c = f(a, x), where a is the predecessor of interest and x denotes the \"other\" predecessors.  We consider first the case where its successor d is also deterministic.  Since a may also be a predecessor of d (as in Fig ure 4), d = g(a, c, y) in general, where y denotes the other predecessors of d, which may overlap with x.  Substituting the expression for c, we see that Thus, d is functionally determined by the union of c's predecessors and its own, excepting c.  The inference problem we face is how to deter mine the qualitative relation of a on d with c fac tored out.  The solution is provided by the expres sion for d's partial derivative with respect to a. To distinguish the perspectives of g and g, I use the notation % ( z, z '\n, . . . )  to indicate the partial deriva tive expressed in terms of the variables in { z, z '\n, . . .  } .  According to the chain rule,2 The signs of the terms on the right-hand side of (2) are given by the qualitative influences holding in the network before DNP.  The sign of the influence When dis probabilistic, it will generally remain so after DNP.  Nevertheless, the same update equation applies in this case.  This fact is a consequence of the analogous result for node conditionalization in regular QPNs [Wellman, 1990a].  Although the con ditionalization operation is valid in QPNs only for nodes with at most one successor, this restriction can be waived in the case of deterministic nodes due to the stronger D-separation condition for indepen dence.  Since c is a function of its predecessors, the dependence of don any other variable in the context of these predecessors must be the same as it was in the context of c and its own predecessors.  Figure 5 displays the general result of determinis tic node propagation in QPNs.  Node dis determin istic iff it was so before the operation.  An arc reversal operation transforms the network by flipping the orientation of a particular influence link and updating the incoming links of the incident variables.  A link from c to dis eligible for reversal as long as there are no other paths from c to d, in which case the operation would create a directed cycle.  Figure 6 depicts the structure of the network after reversal.  The updated signs on the links (6') are computed from the pre-reversal signs ( 6) according to Table 1.  (Where the table indicates 6' = 0, that link would be omitted from the network.)  There are five cases, distinguished by the following factors: The determinism of nodes c and d after reversal also depends on these factors.  The rules for deterministic d also require that the function be invertible.  If the qualitative influence of c on d is not strict and monotone, then the appro priate update procedure is to select the rule from Table 1 as if d were probabilistic.  Cases I and II are simply the reversal rules for dMIDs  [Michelena and Agogino, 1989].  As they im pose the strongest prerequisites on determinism of the variables, they yield the strongest results.  At the other extreme, case V corresponds to the rever sal rule for QPNs [Wellman, 1990a], which is valid for any reversible link.  The results for any of the cases are at least as strong as the QPN results.  Cases III and IV cover hybrid situations, where one of the nodes is deterministic and the other probabilistic.  Case III is actually not a reversal at all, but a complete removal of the link (6d,e = 0).  It never makes sense to actually reverse the link from a de terministic node c to a probabilistic node d, because, unlike the dMID case (I), node d and its predecessors cannot be substituted for the original predecessors of c. Since these predecessors must remain anyway, adding a link from d to already-deterministic c would be superfluous.  At best, the reversal would achieve the same results as when both nodes are probabilis tic (case V).  As deterministic node propagation (Sec tion 4.1) dominates these results, for case III this operation should always be chosen over reversal.  An examination of the derivation of the rule for case I [Michelena and Agogino, 1989] reveals that the result does not depend on the determinism of c. Therefore, the updated signs for case IV are iden tical to those for the dMID rule.  After reversal, c becomes a deterministic function of d and its pre decessors, while d turns into a probabilistic variable dependent on the union of c's original predecessors with its own.  The determinism of c follows from invertibility of the original relation on d.  The prob abilistic nature of d is a consequence of the stochas tic relation between c and its original predecessors.  Without c, the remaining variables are insufficient to determine d with certainty.  And, in a departure from dMIDs (case II), an absence of original prede cessors of c does not permit us to separate d from a, because the two variables may not be marginally independent.  Thus, there is no special provision for pred( c) = 0 in the case of probabilistic c. I. det det f;0  VJ bc,d e<CUR> bc,d @ ba,d) ba,d EB ( ba,c ®be, d) dMID II. det det 6c,d e( 6c,d ® 6a,d)\ndMID III. det prob -\n6a,c 6a,d EB ( Ca,c ® 6c,d) DNP IV.  prob det -\n6c,d e( 6c,d ® 6a,d)  Oa,d EB (Sa,c ® Oc,d) det+-+prob v. prob prob -\nOc,d  Oa,c EB ( Oa,d ® ?)  Oa,d EB ( Ca,c ® Oc,d) QPN Table 1: Rules for reversing the link from c to d.  Except for case IV, the deterministic or probabilistic nature of nodes is unchanged by the operation.  Node reduction (also called removal or conditional ization) is the process of splicing a node out of the network, connecting its predecessors directly to its successors.  In the simplest case, nodes without suc cessors (barren nodes) can be summarily cut from the network.  Nodes with successors can be reduced by reversing or deleting (via DNP) their outgoing links until they are barren, then removing them.  If node c has a single successor, d, then the in fluence from any predecessor a to d after reducing c O!,d = Oa,d EB ( 6a,c ® Oc,d)·  Recall this expression is the same as the update for deterministic node propagation (3).  In fact, when c is deterministic, this update is valid regardless of the number of successors.  To see this, note that the link from a to c remains unchanged by DNP, and therefore a series of these operations for various successors can be performed independently.  This in variance does not hold for arc reversal; the result of the first reversal generally affects subsequent ones.  Thus probabilistic nodes cannot be directly reduced if they have more than a single successor.  After reducing c, node d is deterministic iff both c and d were deterministic before the operation.  Re ducing a probabilistic node renders its successors probabilistic regardless of their former status.  Adopting special provisions for deterministic nodes strengthens the QPN inference rules in three pri mary ways.  1.  An additional operation, deterministic node prop agation, is available for eliminating links with less information loss than arc reversal.  2.  Sharper results are obtained for arc reversals in volving deterministic nodes.  In Table 1, cases I-IV dominate case V in the sense that the conclusions are at least as strong for any assignment to the 6s.  3.  It is possible to directly reduce deterministic nodes with multiple successors.  The reduction of ambiguity demonstrated by the example of Section 2 is attributable to the last item.  The network of Figure 3b is the direct result of re ducing w, chaining the influence from z to both of w's successors.  If one of w's outgoing links had to be reversed first (treating w as probabilistic), the ambiguity of Figure 3a would be inevitable.  Given the augmented update rules, the inference task is to choose the appropriate sequence of oper ations to transform the network to answer specified queries.  This choice is critical, as the strength of conclusions may vary depending on the transforma tion applied.  I have addressed this issue in the con text of QPNs [Wellman, 1990b], though the presence of deterministic variables presents some new ques tions.  For example, when should reversal be chosen over deterministic node propagation?  DNP is al ways preferred when d is probabilistic (case III of Table 1), but neither operation is dominant in the other cases.  Further work is required to resolve this and other inferential issues for QPN s with functional dependencies.  In the discussion thus far I have considered only qualitative influences.  QPNs also include qualitative synergies describing the interaction of two variables in their influence on a third.  Consider the network of Figure 7.  A potential synergy O{x,y},z E {+, -, 0, ?} exists between every pair of variables x and y with a common successor z .3  The definition for qualitative synergy is based on the concept of supermodularity [Topkis, 1978].  A bivariate function z = f( x, y) is supermodular if it is more than additive in its arguments.  Formally, for any x1 > x2, Y1 > Y2, Figure 7: Qualitative synergies describe the interac tion between influences on a common variable.  In this example the relevant synergies are {a, b} on c and {a,b}, {a,c}, and {b,c} on d. ()2z oxoy 2: o. For deterministic variables, the synergy condition is simply supermodularity with respect to the specified predecessor variables.  This is the definition provided for synergy on utility, a distinguished deterministic variable in QPNs [Wellman, 1990a].  Synergy on probabilistic variables is defined in terms of an inequality on differences in cumulative probability distributions for various combinations of conditioning variables [Wellman, 1990a].  The essen tial property of probabilistic synergy for our pur poses is that it is equivalent to supermodularity of an expectation function.  Specifically, a and b are synergistic on c iff E[tP(c)!abx] is supermodular in a, b (5) for any monotone transform tP and any assignment to the other predecessors x.  The requirement that (5) hold for all monotone transforms is a strong one, but it is precisely this condition that enables us to define synergy for merely ordinally scaled variables.  The deterministic synergy condition, in contrast, is not invariant un der monotone transforms, and therefore makes sense only for cardinally scaled variables.  This departs also from qualitative influences, which are robust to monotone transforms in both the deterministic and probabilistic cases.  Thus, unlike the situation with influences, deter ministic synergy is not a special case of its prob abilistic counterpart.  It is actually weaker in the sense that it permits us to presume a cardinal scale.  However, it is stronger in another respect, namely that it mandates a functional dependency.  Because neither relation subsumes the other, special treat ment of deterministic variables for reasoning about synergy is required for soundness if the special in terpretation of deterministic synergy is adopted.  The question, then, is what inference rules are sanc tioned for probabilistic and deterministic variables and combinations.  Let us focus on the operation of reducing c in the network fragment of Figure 7.  If both c and d are probabilistic, the QPN update rule dictates the new synergy of a and b on d:  This rule is also valid if dis the value node and syner gies on d are defined as supermodularity [Wellman, 1990a].  This follows from the invariance of utility under positive linear transforms.  However, the situation is different when c or d are deterministic.  To see this, let us examine the case where both variables represent differentiable func tions of their predecessors.  Then the reduction op eration transforms d from a function of a, b, and c to a function of a and b alone, The first four additive terms on the right-hand side of (7) correspond exactly to the sign expressions of the four terms of the QPN synergy update equa tion (6).  However, there is an additional term (ar guments omitted), oc oc 82d oa ob 8c2' with no counterpart in (6).  The first two factors of this term are described by the qualitative influences on c, but there is no qualitative relation correspond ing to univariate second partial derivatives.  Such a concept was not defined for QPNs, as it makes lit tle sense for ordinally scaled variables.  But it is not surprising that this is a factor in deterministic syn ergy, since the objects of that relation are in fact cardinally scaled variables.  Indeed, Topkis [1978] has also shown that extension of submodularity de pends on the convexity or concavity of the transfor mation function.  Consider the simple tax-planning model of Figure 8.  We define income as the sum of salary and interest, and assert that taxes are a function of income and deductions.  Salary and interest have a zero synergy on income, as the combination function is additive, or modular.  Suppose we wish to determine the synergistic rela tion of salary and interest on taxes.  This relation can be rendered direct by reducing income from the net work.  Because all synergies in the original network are zero, the first four terms of (7) drop out of the equation.  The qualitative synergy of concern, there fore, depends entirely on whether the relation of in come on taxes is concave or convex, that is, whether taxes are regressive or progressive.  Unfortunately, this information cannot be expressed by qualitative influences and synergies alone.  It seems reasonable, then, that further investiga tion of qualitative deterministic synergy should be preceded by incorporation of univariate second-order qualitative relations.  These relations represent nat ural concepts (concavity or convexity), and can also be propagated through graphical transformations (Nestor Michelena, personal communication).  Es tablishing a probabilistic analog of these and devel oping methods to take advantage of their decision theoretic implications are subjects for future work.  The foregoing analysis has demonstrated that aug menting QPNs to identify and exploit functional de pendencies can strengthen inference in hybrid net works of deterministic and probabilistiC variables.  Moreover, much of the improvement can be real ized by simple modifications to existing graphical inference rules.  I have described these modifications in detail for qualitative influences and pointed out that carrying out a similar exercise for qualitative synergy will require the construction of new secondorder qualitative relations.  Exploiting functional dependencies is likely to prove profitable for other representations based on probabilistic constraints.  Any such scheme presents the potential for information loss when the con straints expressible in the specified language are not closed under the transformation operations  [Fertig and Breese, 1989; Wellman, 1990b].  Functional de pendencies, because they restrict the allowable in teractions among variables, can significantly reduce this information loss in some cases.  [Fertig and Breese, 1989]  K. W. Fertig and J. S. Breese.  Interval influence diagrams.  In Proceed ings of the Workshop on Uncertainty in Artificial Intelligence, pages 102-111, Windsor, ON, 1989.  [Beckerman and Horvitz, 1988]  David E. Hecker man and Eric J. Horvitz.  The myth of mod ularity in rule-based systems for reasoning with uncertainty.  In John F. Lemmer and Laveen N. Kanal, editors, Uncertainty in Artificial Intelli gence 2, pages 23-34.  North-Holland, 1988.  [Michelena and Agogino, 1989] Nestor Michelena and Alice Agogino.  Determinis tic monotonic influence diagrams.  Working Paper 89-1101-1, Berkeley Expert Systems Technology Laboratory, December 1989.  [Neufeld and Poole, 1988] Eric Neufeld and David Poole.  Probabilistic semantics and defaults.  In Proceedings of the Workshop on Uncertainty in Artificial Intelligence, pages 275-282, Minneapo lis, MN, 1988.  [Pearl et a/., 1989]  Judea Pearl, Dan Geiger, and Thomas Verma.  Conditional independence and its representations.  Kybernetika, 25:33-44, 1989.  [Shachter, 1988] Ross D. Shachter.  Probabilistic in ference and influence diagrams.  Operations Re search, 36:589-604, 1988.  [Topkis, 1978] Donald M. Topkis.  Minimizing a sub modular function on a lattice.  Operations Re search, 26:305-321, 1978.  [Weld and de Kleer, 1989]  Daniel S. Weld and Jo han de Kleer, editors.  Readings in Qualitative Reasoning About Physical Systems.  Morgan Kauf mann, 1989.  [Wellman, 1990a] Michael P. Wellman.  Fundamen tal concepts of qualitative probabilistic networks.  Artificial Intelligence, 1990.  [Wellman, 1990b]  Michael P. Wellman.  Graphical inference in qualitative probabilistic networks.  Networks, 1990.", "metadata": {"source_file": "1304.1081v1.pdf", "title": null, "authors": ["Michael P. Wellman"], "year": "2011", "detected_language": "en", "page_count": 8, "origin_chunk_file": "1304.1081v1.chunks.json"}}
{"text": "Comprehensible explanations of probabilistic reasoning are a prerequisite for wider acceptance of Bayesian methods in expert systems and decision support systems. A study of human reasoning under uncertainty suggests two different strategies for explaining probabilistic reasoning specially attuned to human thinking: The first, qualitative belief propagation, traces the qualitative effect of evidence through a belief network from one variable to the next. This propagation algorithm is an alternative to the graph reduction algorithms of Wellman (1988) for inference in qualitative probabilistic networks. It is based on a qualitative analysis of intercausal reasoning, which is a generalization of Pearl's \"explaining away\", and an alternative to Wellman's definition of qualitative synergy. The other, Scenario-based reasoning, involves the generation of alternative causal \"stories\" accounting for the evidence. Comparing a few of the most probable scenarios provides an approximate way to explain the results of probabilistic reasoning. Both schemes employ causal as well as probabilistic knowledge. Probabilities may be presented as phrases and/or numbers. Users can control the style, abstraction and completeness of explanations. The developers of expert systems and decision support systems have long been aware of the importance of facilities to explain the computer based reasoning to users as a prerequisite to their more widespread acceptance (e.g. Teach & Shortliffe, 1981). Unless users can come to * This work was supported by the National Science Foundation under grant IRI-<PHONE> to Carnegie Mellon and by the Rockwell International Science Center. understand the assumptions and reasoning of such systems, it is impossible to develop the kind of human-machine collaboration that is the basis for successful use of such systems. For explanations to be effective, their form and content must be carefully matched to the users' competence, knowledge, and styles of reasoning. The approach that underlies the classic \"expert systems\" paradigm is to employ computer representations and inference mechanisms intended to emulate human reasoning.  To the extent that this emulation is successful, the computer-based reasoning ought to seem familiar to people and so relatively easy to explain.  While there is ample evidence that normatively appealing probabilistic and decision theoretic schemes are poor models of human reasoning under uncertainty (e.g. Kahneman et a/. 1982), there is surprisingly little experimental evidence that the rule-based alternatives, such as certainty factors or fuzzy logic, are any better as descriptive models.  And even if successful descriptively, the emulative approach would merely reproduce the documented deficiencies of our intuitive reasoning rather than complement and enhance it.  Are we forced to choose between the unreliable and the inexplicable?  Our approach to this dilemma is to explore whether in fact it may be possible to explicate probabilistic reasoning in ways better attuned to human thinking.  Only recently has much attention begun to be paid to the automatic generation of comprehensible explanations for probabilistic and decision analytic schemes.  Horvitz et a/. (1986) present a system which can explain its recommendations about what test to perform to gather diagnostic evidence.  Langlotz et a/. (1986) present a scheme for quantitative analysis of decision trees, which explains qualitatively how one decision may outweigh another in terms of expected utility.  Klein (1990) presents a scheme for qualitatively explaining the implications of hierarchical additive value functions.  Elsaesser (1988) provides some empirical evidence on the efficacy of explanations of simple Bayesian inference, with one variable and one observation.  Strat's (1988) system explains the dynamics of Dempster-Shafer reasoning based on sensitivity analysis, with interesting implications for probabilistic schemes.  Sember and Zukerman's (1989) scheme generates micro explanations, that is local propagation of evidence between neighbouring variables in a belief net.  Our focus here is on approaches for generating macro explanations, intended to explain probabilistic reasoning over larger networks.  We wish to avoid dogmatism about what kinds of explanation scheme will be most effective, but rather explore a variety of approaches, including graphical, numerical, and linguistic representations.  We are interested in both quantitative and qualitative forms of explanation in various combinations.  This paper gives an account of several of the key ideas that have emerged from our initial work.  Since our goal is to produce interpretations of probabilistic reasoning that are more compatible with human reasoning styles, we started out with an empirical study of human strategies for uncertain reasoning.  This provided us with the inspiration for the design of two new and contrasting modes of explaining probabilistic reasoning, namely qualitative belief propagation and scenario-based reasoning.  It is useful to distinguish explanation as the communication of static knowledge or beliefs from explanation of the dynamics, of how beliefs are changed in the light of new evidence.  Explanation of the statics, though relatively straightforward, is a prerequisite for explanation of the dynamics.  Among the issues in static explanation we discuss are the use of belief nets, the use of linguistic phrases to express probabilities, and the importance of causal knowledge.  Next we outline the use of qualitative belief propagation as a means of dynamic explanation.  This includes an analysis of qualitative intercausal reasoning, generalizing Pearl's notion of \"explaining away\", with a theorem giving a precise characterization of when it applies.  Finally, we describe a scenario based approach to explanation.  This is illustrated by explanations generated from our prototype implementation in Allegro Common Lisp, QIQ (Qualitative Interface to the Quantitative).  The essence of effective explanation is to design its content and form to mesh with the knowledge and modes of thought of the person to whom you are explaining.  Thus, producing good explanations of formal reasoning under uncertainty requires an understanding of the way people reason intuitively under uncertainty.  There is a vast literature on human judgment under uncertainty for very simple inference problems, typically with a single hypothesis variable and a single observation (e.g. see Kahneman et al., 1982 and Morgan & Henrion, 1990 for reviews), but relatively little is known about cognitive processes in more complex situations.  To improve our insights into this and to seek inspiration for alternative approaches to explanation, we conducted a series of cognitive process-tracing studies.  We recorded and analyzed verbal protocols from subjects asked to think aloud as they performed uncertain reasoning tasks (Druzdzel, 1989).  Here is a sample task: Harry is in the house of a new acquaintance and suddenly finds himself sneezing.  This could be due to an incipient cold, or to an allergy attack brought on by a cat.  Before he started sneezing he would have judged the cold and allergy both about equally unlikely.  (b) Suppose Harry now notices small paw-prints on the furniture.  How should this affect his degree of belief that he is getting a cold?  (c) Suppose he then hears a barking of a small dog in the room next-door.  How does this further affect his degree of belief that he is getting a cold?  Most subjects were able to provide qualitative answers to these questions rather easily.  In (a) they judged a cold was about as likely as not.  In (b) that the paw-prints should decrease his belief in the cold, since the sneezing might be explained by a cat, suggested by the paw-prints.  And in (c), that hearing the barking dog should increase belief in the cold again, since the dog provides an alternative explanation of the paw prints.  One unsurprising finding was that subjects generally used qualitative terms for probabilities, using quantitative terms almost not at all.  This finding is consistent with previous studies of intuitive reasoning (e.g. Kuipers, Moskovitz & Kassirer, 1988}.  Another finding confirming previous work (e.g. Kahneman and Tversky, 1980)  Less expected and of considerable interest in the current context, was evidence of two quite different strategies for plausible reasoning.  One, which we call qualitative belief propagation, involves propagating the qualitative impact of evidence from event to event, following local causal and diagnostic relationships.  For example, barking indicates the presence of the dog.  The dog explains the pawmarks, which are thȽn .  weaker evidence for the cat.  Reduced belief 1n the cat in turn reduces belief in the allergy.  This is noJ. less of an explanation for the sneezing and so requires an increased belief in the cold.  The other strategy, scenario-based reasoning, is quite different, and was more common in the protocols.  The reasoner identifies one or more scenarios, that is consistent instantiations of the variables, forming a coherent, often causal,-story, compatible with the known evidence.  For example, Harry has a cold, which explains the sneezing; there is no cat, and so no allergy; the dog explains the paw-prints and barking.  Subjects often appeared to develop one or more such quasi-deterministic scenarios.  Figure 1 shows two such scenarios, as a subset of the event tree.  By now the most familiar display of qualitative probabilistic information is the Bayesian belief net (and influence diagram), which prȿvi3es a .  perspicuous display of purely quahtɀt1ve beliefs about conditional dependence and Independence.  Figure 2 provides a belief network for probabilistic knowledge for the \"sneeze\" example.  The nodes depict the key variables.  (NB, we use the abbreviated term \"Cat\" to mean \"the presence of a cat in the vicinity\", and so on.)  As usual, the directed arcs depict dependences between them, (or more strictly the absence of arcs depicts independence).  The same.  information could, of course, be represented in text as a list of the dependencies, such as, The probability of sneezing is affected by cold.  The probability of sneezing depends on allergy.  The probability of some target event (e.g. the cold) can then be judged by the relative probability of the scenario(s) that contain(s) it.  If one considered all possible scenarios, then this strategy is an exact algorithm for Bayesian reasoning.  This is generally too much mental effort, but it can be a good approximation if one considers only the few most probable scenarios.  On the other hand, if a likely scenario is ignored or its relativɁ probab2lity misestimated, it can lead to severe b1ases.  Th1s scenario-based reasoning appears related to explanation-based reasoning identified by a number of psychologists as strategies for complex reasoning tasks (e.g. Pennington & Hastie, 1988).  The improved perspicuity of the graphical representation in showing the locality of relationships is immediately clear.  Although for some purposes the textual form is valuable, particularly for those not familiar with the belief network notation.  To complete the static explanation we need to add the probabilities in some form.  One appealing approach to render numerical probabilities more digestible is to translate them Into verbal phrases, such as \"very likely\" or \"somewhat improbable\".  A considerable empirical literature reports people's interpretations of verbal probability phrases in terms of numerical probabilities or ranges.  In general this research has found a degree of consistency in usage, at least in the ordering people assign to sets of such phrases (Budescu & Wallsten, 1985; Wallsten et a/, 1986; Kong et a/. 1986).  But it has also found significant variability in interpretation between people, and considerable context dependence (Brun & Teigen, 1988).  Nuclear safety engineers mean something quite different by \"uncommon\" than physicians.  People interpret other people's use of phrases somewhat differently (and with wider range of uncertainty) from what they themselves claim to mean by the phrases (Wallsten et al; 1986); that is, they are sensitive to the variability among people.  This suggests mappings from phrases to numbers, needed for encoding, should be somewhat different, with broader ranges than mappings from phrases to numbers, as used here for explanations.  even by event within a network.  For example, \"unlikely\" may mean something quite different when applied to the chance of allergy to an antibiotic than the chance of dying in an operation.  If desired, a different mapping may be used for each event and influence.  However we expect a small number of mappings will be sufficient to cover the contexts for a given network.  Relevant phrases can be divided into belief phrases, such as \"very probable\" or \"unlikely\", and frequency phrases, such as \"common\" or \"rare\".  Most come in both adjectival form, as above, and adverbial form, such as \"probably\" or \"commonly\".  We have found it most natural to express marginal prior and posterior probabilities of events in terms of adjectival probabilities, for example, The above examples are generated by 010 as Probability part of the static explanation of the sneeze belief Adjectives Adverbs network.  ..-- unlikely rarely remains.  To some this is part of the attraction of verbal phrases over numerical probabilities.  1.-- fairly unlikely fairly rarely Others may wish to see the numerical probability less likely than not less often than notin addition to the verbal phrase, as in the -1(1.-- as likely as not as often as not examples above.  This allows users to pay more likely than not more often than n\"'÷øention tù whatever they find most helpful • .  and, '\\Vith expenence, perhaps to learn the mapp1ngs.  ,_ __ fairly likely fairly often .", "metadata": {"source_file": "1304.1082v1.pdf", "title": null, "authors": [], "year": "2011", "detected_language": "en", "page_count": 11, "origin_chunk_file": "1304.1082v1.chunks.json"}}
{"text": "absolute probabJhtJes, one recent study (Eisaesser very likely very commonly & Henrion, 1990) has examined mappings from 1.fl-IMK.- certain always relative probabilities or changes in probabilities to To cope with differences in personal preference and the context-dependence of interpretations, our explanation system, 010, provides a variety of mappings, including two mappings from the literature (Wallsten et al. 1986; Kong et a/. 1986), and our own synthesis from the literature, illustrated in figure 3. We have tried to use terms which minimize ambiguity and variability among people and contexts. Users can select from these mappings or provide their own. The context and interpretation may vary not only by domain, but phrases such as \"more likely than\". They found that a fixed mapping to phrases from differences in probabilities provided a better model than ratios of probabilities or odds. These phrases are useful for comparing the probabilities of events or updates in degrees of belief, such as: Cold is slightly less likely than cat (0. 08/0. 10) . No cat is a great deal more likely than cat (0. 1/0. 9) . structure in uncertain reasoning (Tversky & Kahneman, 1980). People find it easier to reason from cause to effect than vice versa. As we mentioned, this was also apparent in our protocol studies. Some, notably Pearl (1988), have explicitly identified the directed arcs of belief nets with cause-effect relations. Others have argued that there is no inherent relationship with causality: After all, the arcs can be reversed simply by application of Bayes' rule, but causality cannot. But in any case, it is usually most natural to assess influences in causal direction. We view knowledge of causal relations as an important semantic enrichment to the pure belief net. It is not essential for Bayesian inference, but can be of great help in communicating with people, both for encoding expert opinion and for explanation. QIQ can encode a cause-effect relationship as supplementary knowledge about each influence arc.  This information is used in generating text descriptions of influences, for example: Cat commonly (p=0.  8) causes allergy.  Cat is the only cause of allergy.  The quantities described here are causal strengths, that is the probability that the specified precursor event, if present, is sufficient to cause the successor.  If no other cause of the successor is present then the causal strength is the same as the conditional probability of the effect given the cause.  This is the case with the link from cat to allergy, where no other cause is known (in this example).  However, if other causes are possible, then the causal strengths may be different from the conditional probabilities.  Causal strengths are an equivalent representation to the conditional probability representation, and each can be derived from the other.  The best known application of causal strengths is in the noisy-OR gate, which often arises in situations with multiple alternative causes of a common effect.  Each link from cause to effect is characterized by its causal strength, the probability of the effect given only that cause is present.  The condition it embodies is sometimes called causal independence, namely that the probability that each present cause is sufficient to produce the effect is independent of the presence or sufficiency of other causes.  For example, we have, Cold very commonly (p=0. 9) causes sneezing.  Allergy very commonly (p=0. 9) causes sneezing.  Causal independence can be expressed as: Cold does not affect the tendency of allergy to cause sneezing, and vice versa.  In this case we also assume no leaks (Henrion, 1990), i.e. no \"spontaneous\" occurence of the effect in the absence of explicitly modelled causes: There is no other cause of sneezing than cold and allergy  The latter assertion gives the leak probability, that is the probability of paw marks given no cat or dog.  Note that the former two assertions do not give the simple conditional probability of paw marks given the cat (dog), but given also none of the \"other causes\" mentioned in the last assertion.  The goal of qualitative belief propagation is to find the direction of the impact of an observed variable on the degree of belief in another variable, whether increased, decreased, or unchanged (+- 0).  Wellman (1988) presents a scheme for qualitative probabilistic networks (QPNs) which provides an appealing formal basis for this task for arbitrary belief nets.  Wellman's scheme uses an inference algorithm for QPNs using arc reversal and graph reduction, modelled on Shachter's algorithms for inference in quantitative belief networks.  However, human qualitative belief propagation appears to trace the impact of evidence locally from node to node, which seems more reminiscent of the quantitative belief propagation or message-passing algorithms developed by Pearl and others than the reduction type algorithms.  Wellman (1988) provides a persuasive argument for first-order stochastic dominance (FSD) as the best formal interpretation of the informal notion of the sign of an influence, whether knowledge of A being true (high) increases or decreases belief in B being true (high).  Thus a positive influence of binary variable A on variable B is defined thus: denote variables, and lowercase their values: a means A is false.)  Signs may be assigned to arcs in the network either by direct assessment or, for qualitative explanation of quantitative reasoning, as an abstraction from quantitative assessments.  In the sneeze example, it is intuitively clear (and consistent with the numbers used in the example) that all influences are positive.  For simplicity, we will assume the network is singly connected and limit ourselves to binary variables.  As a prerequisite to describing the algorithm for qualitative belief propagation, we must first distinguish the three types of inference: Predictive (or causað inference is in the same direction as the original qualitative influence.  Diagnostic inference is in the reverse direction.  lntercausal inference gives the qualitative impact of evidence for one variable A on another variable B, when both have influences on a third variable C, about which we have independent evidence.  These three situations are illustrated below in Figure 4.  Qualitative predictive inference is quite simple.  If we have positive evidence E that increases our belief in A, and the influence of A on B is positive, then E should also increase our belief in B (actually, not decrease it, given Wellman's weak definition of the direction of influence).  More generally, concatenation (chaining) produces an influence whose sign is the product of the signs of its component influences.  Diagnostic inference is similar to predictive inference, although a little more complicated since variable  A inherits any relevant predecessors of B in inverting the direction of the arrow.  If we want to propagate the effect of evidence across two divergent arrows, we can simply chain the diagnostic and predictive inference.  Observation of pawmarks increase belief in the cat, which in turn increases belief in the allergy.  But propagating across convergent arrows is less straightforward: If there is no diagnostic evidence for the common effect (or direct observation), e.g. for C, then the two influencing variables A and B are of course independent, and so knowledge about A has no effect on B.  On the other hand, if we observe C (or have diagnostic evidence for it)  A and B become dependent.  Thus, intercausal inference is not a simple concatenation of predictive and diagnostic inference.  While there has been much informal discussion of \"explaining away\" (a form of intercausal reasoning) (Henrion, 1986; Pearl, 1988), a precise characterization seems to be lacking of the general conditions under which explaining away or other qualitative intercausal reasoning applies.  So we now turn to this issue.  \"Explaining away\" applies when A and B are two alternative causes of C, for example if the influence of A and B on C is a noisy OR.  Given evidence for C, then evidence for A generally produces a reduced belief in B. But what if the influence is not a noisy-OR?  What precisely are the conditions on the influence of A and B on C under which this qualitative pattern applies?  Figure 5 presents the question schematically.  •Theorem (qualitative lntercausal Influence): Suppose we have three logical variables, A, B. C. all with non-zero priors, where A and B can influence C, and A and B are marginally independent (there are no paths between A and B other than through C).  Suppose C is observed to be true.  Then a positive qualitative influence exists between A and B, i.e. P(c I a b x) P(c I a 6 X) 2: P(c I a 6 X) P(c I a b X), where x are the predecessors of C other than A and B.  The qualitative influence from A to B is zero or negative, i.e. we replace the 2: by= or::;; in [1 ], if we replace the 2: in condition [3] by = or ::;; respectively. •  This result is easily derived from [2] by the application of Bayes' rule.  Condition [3] is reminiscent of Wellman's (1988) definition of qualitative synergy, but instead of the multiplicative form he employs the additive form (for binary variables), It can be shown that if either or both of the influences from A to C and from B to C are positive, then multiplicative synergy  [3] implies ad1itive synergy  [4].  It is also easy to show that no1sy-OR gates are subsynergistic for product synergy, just as Wellman showed they are subsynergistic for additive synergy.  Hence, if P(CjA,B) is a noisy OR gate and C is observed present, there is a negative influence between A and B.  So any further evidence for B will decrease belief in A, and vice versa, since they provide alterȺative explanȻt.ions for C.  In other words [3] is prec1sely the cond1t1on under which explaining away applies.  Note that if the influence has positive multiplicative synergy, A has a positive influence on B, the 0nverse of.  explaining away.  For example, if the Influence 1s a \"leaky noisy AND gate\", in which C has an increased chance of occurring if A and B both occur, then given C, knowledge of A may increase belief in B.  For example, suppose that the presence of flammable material and an ignition source together can cause combustion, which in turn may cause smoke (which also has other possible causes).  Observation of smoke can create a positive influence from flammable material to the ignition source.  We illustrate how these ideas may be applied to provide qualitative explanation using the sneeze example, somewhat like that provided by some of our subjects.  Consider question (c) from above, how does observation of barking affect our belief in the cold, given we already have observed sneezing and pawmarks?  Since both the cases of convergent influences (sneezing and paw marks) are noisy ORs with observed effects, explaining away applies, that is they can be reduced to negative influences between the causes.  We can generate a trace of the explanation thus: Observe sneezing and paw-marks.  Impact of barking on cold?  3.  Reduced probability of cat reduces probability of allergy.  4.  Reduced probability of allergy reduces ability to explain sneezing, and so increases probability of cold.  This illustrates all three kinds of propagation.  Step 1 involves simple diagnostic inference over a positive influence.  Step 2 involves intercausal inference, producing a negative influence from dog to cat.  Step 3 involves simple predictive inference, propagating negative evidence over a positive influence.  And step 4 involves intercausal inference, producing another negative influence from allergy to cold.  Since there are two positive steps and two negative steps (the intercausal inferences), the chaining produces a cumulative positive influence between barking and cold as shown in Figure 6.  We should point out that the scheme as described is limited to singly connected networks of binary variables.  Whereas inference schemes using propagation operate on a belief network representation of knowledge, scenario-based explanations are based on scenario trees (also known as probability trees, or decision trees without the decision variables).  Each path from root to an end node represents a scenario, or sequence of events.  The psychological literature suggests that it may be easier to understand scenarios if they are presented as coherent causal stories.  So in an attempt to make scenarios easier to grasp, we can order the events in a scenario so that effects follow their causes, and employs causal conjunctions to link them when appropriate, for example: No cold; cat causes allergy, which causes sneezing.  Dog causes barking and paw marks; no cat, hence no allergy; cold causes sneezing.  In some scenarios, an event may deviate from what is expected, having a low probability given its predecessors.  Even though a cat is present there may be no allergic reaction.  In such cases, we can aid interpretation by indicating such surprises by an exception conjunction such as \"but\" for an event with low conditional probability: No cold; cat, but no allergy, hence no sneezing.  The probability of each scenario is the product of the conditional probabilities of all the events in it.  Exact Bayesian inference to find the posterior probability of an event can be performed by looking at the ratio of the sum of the probabilities of all scenarios compatible with the event to the sum of all those not compatible, after eliminating all scenarios not consistent with the observations.  The number of possible scenarios is generally large of course, and cognitively unmanageable.  But fortunately, it is often possible to understand the essentials of what is going on by examining only a few of the most probable scenarios.  The following is a simple scenario-based explanation from the sneeze example.  We ask it to explain the probability assigned to cold given sneezing has been observed: Scenario A is about as likely as scenario B (0. 47/0. 48) because cold in A is a great deal less likely than no cold in B (0. 08/0. 92) ' although no cat in A is a great deal more likely than cat in B (0. 9/0.  1) .  QIQ first displays what is known and what can be definitely inferred from it.  It then gives a list of one or more scenarios which are compatible with the target variable (cold) and a second list of scenarios which are incompatible with it.  Since currently our only observation is sneezing, the variables paw marks, dog, and barking are irrelevant, and so the scenarios mention only cold, cat, and allergy.  In general there may be a vast number of possible scenarios (exponential in the number of uncertain variables), so it only gives the most probable one(s) in each list.  The rest are grouped as \"other less probable scenario(s)\", those which collectively contribute less than 15% of the overall probability for that list.  This parameter can be varied to control the length and precision of the explanation.  The next part of the explanation compares the probabilities of the most important pairs of scenarios in terms of significant differences in the probabilities of their component events.  Any contrasts that are significant (probabilities differing by a factor of more than 1 .2 in the default option) are mentioned in explaining the relative probabilities.  The explanation lists, after \"because\", the contrasts favoring the more probable sȼenario, and then, after \"although\", the contrasts, if any, supporting the other scenario.  This scheme is based on the principle that it is easier to judge the relative probability of two scenarios by comparing their differences than by judging their absolute probabilities.  The co'!lparisons use the relative probability phrases calibrated against numerical probability differences by Elsaesser & Henrion (1990).  In this case the Note that several techniques are provided to abstract and simplify the explanation.  First, only relatively low probability of cold in scenario A is just about balanced by the low probability of cat in scenario B. Another possible scenario which has both cold and cat (hence allergy) is not even mentioned, because having two very unlikely events its relative probability is negligible relative to the two scenarios each with a single unlikely event.  So, it is contained in the \"other less probable scenario(s)\" group.  Paw Marks could have been caused by cat or dog or another unknown cause.  Barking must have been caused by dog.  Scenario A is much more likely than scenario B (0. 38/0. 05) because no cat in A is a great deal more likely than cat in B (0.90/0. 10) .  Scenario A is somewhat less likely than scenario C (0. 38/0. 56) because cold in A is a great deal less likely than no cold in C (0. 08/0.92) 1 although no cat in A is a great deal more likely than cat in C (0. 90/0. 10) .  relevant events are considered, that is events whose consideration affects the target probability given available observations.  Second, only those scenarios that contribute more than 1 0% of the probability to the target event (or its complement) are listed explicitly.  This can·drastically simplify the explanation, since many real cases seem to be like the example above, where a few (two to four) scenarios turn out to have the bulk of the probability, and the vast mass can be ignored without significant error.  Thirdly, in comparisons of the relative probability of pairs of scenarios, only those events with substantially different probabilities are mentioned.  Additional abstraction techniques could provide further simplification.  Some linked variables might be combined so that they can be treated as one.  For example, allergy might be combined into cat, considering cat to cause sneezing directly.  This reduction of variables can reduce the number of distinct scenarios and also the complexity of each scenario.  Another improvement in a decision context would be to consider the importance of a scenario in terms of expected utility rather than simply probability.  In a medical context, low probability scenarios leading to death may have a stronger claim to be listed explicitly than higher probability scenarios with less interesting consequences.  There is considerable psychological evidence for the prevalence of scenario-based reasoning in human thinking, and of the importance of coherent, causal stories (e.g. Pennington & Hastie, 1988).  The psychological literature has focussed generally on the ways in which this leads to systematic distortions and biases in probabilistic judgment.  Our approach here is to generate explanations matched to human p", "metadata": {"source_file": "1304.1082v1.pdf", "title": null, "authors": [], "year": "2011", "detected_language": "en", "page_count": 11, "origin_chunk_file": "1304.1082v1.chunks.json"}}
{"text": "An experiment replicated and extended recent findings on psychologically realistic ways of modeling propagation of uncertainty in rule based reasoning. Within a single production rule, the antecedent evidence can be summarized by taking the maximum of disjunctively connected antecedents and the minimum of conjunctively connected antecedents. The maximum certainty factor attached to each of the rule's conclusions can be scaled down by multiplication with this summarized antecedent certainty. Beckerman's modified certainty factor technique can be used to combine certainties for common conclusions across production rules. Rule based systems have proven to be successful techniques for the computational modeling of human reasoning. They can be used to model human procedural knowledge in a convenient, homogeneous, modular fashion that is consistent with a great deal of psychological evidence (Klahr, Langley, & Neches, 1987). Curiously, very few of the rule based human simulations employ techniques for representing and propagating uncertainty. Although it is widely acknowledged that much of human knowledge is uncertain, it is in the field of artificial intelligence that research on the representation and management of uncertainty in rule based reasoning has been focused (Kanal & Lemmer, 1986; Hink & Woods, 1987). Most of the work on uncertainty in artificial intelligence has so far been normative, stressing issues of mathematical correctness and effectiveness. The approach taken in this paper is not normative, but descriptive. It seeks to determine how ordinary reasoners propagate uncertainty in the context of rule based reasoning. The problem of uncertainty in rule based architectures can be broken into two sub problems. One concerns how the possibly uncertain evidence in a rule's antecedents affects the rule's conclusions. Consider the general case of a production rule with i antecedents and j conclusions. Attached to each of the rule's j conclusions would typically be a numerically represented maximum certainty factor.  If the evidence contained in the rule's antecedents is believed with perfect certainty, then each conclusionj would be drawn with its maximum-certainty factor} However, in the general case, the evidence in each of the rule's antecedents is believed with varying degrees of certainty.  How should the uncertainty of antecedent evidence be summarized?  And how should this sumarized antecedent certainty afect the maximum certainty factor of each conclusion?  Slightly complicating the frrst question is the fact that the antecedents can be connected either conjunctively or disjunctively.  With conjunctive connectives, all of the antecedents must hold in order for the rule to fire.  For disjunctive connectives, satisfaction of only a single antecedent enables the rule to fire.  The other uncenainty sub-problem in rule based systems concerns the issue of combining evidence across different rules with the same conclusion.  Imagine that a particular conclusion exists in more than one rule.  As rules fire, their conclusions come to be believed with varying degrees of cenainty, as just outlined.  How should these cenainties be combined in cases where a previously frred rule has an overlapping conclusion with a newly frred rule?  This is not a problem in deterministic production systems that do not quantify uncertainty since they typically avoid drawing the same conclusion more than once.  However, it is a problem in any production system that attempts to propagate quantitative uncenainties as its rules frre.  Solution of these two sub-problems is critical for rule based effons to model human cognition because algorithms implementing a solution to each of these two sub-problems are typically invoked every time a rule with quantitative representation of uncenainty fires.  If these algorithms lack psychological validity, simulation errors will tend to accumulate and compound as rules fire.  A recent psychological experiment evaluated a number of different solutions to these problems (Shultz, Zelazo, & Engelberg, 1989).  In a study of the problem of propagating uncenainty within a rule, subjects learned hypothetical production rules in which each of three antecedents was believed with varying degrees of cenainty.  A maximum certainty factor for each rule's conclusion was also specified.  Subjects were then asked to rate the cenainty with which they believed the rule's conclusion.", "metadata": {"source_file": "1304.1083v1.pdf", "title": null, "authors": ["Thomas R. Shultz", "Beckerman"], "year": "2011", "detected_language": "en", "page_count": 6, "origin_chunk_file": "1304.1083v1.chunks.json"}}
{"text": "Seven different quantitative models for summarizing the antecedent certainty and two models for using this sumarized antecedent certainty to scale down The results indicated that the best fitting model for summarizing antecedent cenainties was the so-called maximin model, which took the maximum cenainty factor from disjunctively connected antecedents and the minimum certainty factor from conjunctively connected antecedents. The better technique for scaling down the maximum certainty factor in the conclusion was to multiply the maximum certainty factor by this summarized antecedent certainty factor. Among the alternative, less successful models for summarizing antecedent cenainties were minimum, maximum, product, sum minus overlap, mean, and median. The best-fitting maximin model was a\nhybrid model combining the maximum and minimum models. Another assessed hybrid model combined the product and sum minus overlap models. It used the product of conjunctively connected antecedent cenainties and the sum minus overlap of disjunctively connected antecedent certainties. The less successful model for scale down the maximum cenainty factor in the rule's conclusion was a model that computed the mean of this value and the summarized antecedent certainty. In a second experiment, Shultz et al. (1989) compared five diferent quantitative models for combining cenainties across rules with the same conclusion. Here, subjects learned two rules relevant to the same conclusion, each of which had antecedent evidence of varying certainty. They were asked to rate the cenainty of the common conclusion. Two models for scaling down the conclusion's maximum cenainty factor by the antecedent certainty were also tested, with the same result as in the frrst experiment. That is, scaling was better approximated by multiplication than by computing the mean. Two models for combining cenainties across rules fit the data equally well: the classic cenainty factor approach used in MYCIN (Shonliffe, 1976) and Heckerman's (1986) modified certainty factor approach, which sums the cenainties contributed by each of two rules and divides The letters symbolizing events in these items (A, B, C) were not instantiated with more realistic events.  It was part of the research strategy to begin with these abstract items before investigating the effects of context on reasoning under uncertainty.  by 1 plus their product.  Heckennan's model was favored over the tri-partite classic certainty factor approach because it employs a simpler, more unified formula.  Heckerman (1986) showed that both his modified certainty factor model and the classic certainty factor approach are valid probabilistic interpretations of certainty factors, under the assumptions that the evidence provided by the rules is conditionally independent and that the rule base forms a tree structure.  Grosof (1986) showed that Heckerman's modified certainty factor model is equivalent to a special case of Dempster-Shafer theory.  We found that the Dempster-Shafer technique (as described in Gordon & Shortliffe, 1984) yielded identical results to the classic certainty factor model when both of two rules either confirmed or disconfmned a conclusion but differed slightly when one rule was confmning and the other was disconfmning.  Other, less successful models for combining certainties across rules included those that computed the mean, maximum, or minimum of the two certainties.  A criticism addressed to these conclusions at the meeting in which they were frrst presented was that the problem had been broken into unrealistically small parts.  It was argued that the favored algorithms might not work on more natural problems that required full integration of all of the algorithmic elements.  The purpose of the present study was to address this critique by using problems that required full integration of the algorithms.  If events A, B, and C al happen, then event X is highly certain to happen.  Event A is highly certain to happen.  Event B moderately certain to happen.  Event C is slightly certain to happen.  If event D, E, or F happens, then event X is moderately certain not to happen.  Event D is highly certain to happen.  Event E moderately certain to happen.  Event F is slightly certain to happen.", "metadata": {"source_file": "1304.1083v1.pdf", "title": null, "authors": ["Thomas R. Shultz", "Beckerman"], "year": "2011", "detected_language": "en", "page_count": 6, "origin_chunk_file": "1304.1083v1.chunks.json"}}
{"text": "In each item, the subject was asked to rate the certainty of event X happening by placing a slash on a 16 em line with certain not to happen anchoring the left end, certain to happen anchoring the right end, and uncertain labelling the midpoint Across the 12 rule items there was systematic variation in the type of connective for the antecedents (conjunctive vs. disjunctive) and the certainty and direction of the conclusion (highly vs. moderately certain to happen vs. not-happen). As in the original experiments (Shultz et al., 1989), additional items were presented at the end of this questionnaire in order to calibrate each subject's use of the certainty descriptors highly, moderately, and slightly certain. For each of these certainty descriptors, the subject was asked to place a slash on a 16 em line to represent the described degree of certainty. The calibration line was anchored at one end with uncertain and at the other end with completely certain. Two different forms of the questionnaire were created by using two different random orders of the same 12 items. Each of the two forms was given to 22 subjects. Responses to the calibration items were converted to certainty factors by measuring the placement of the slash to the nearest 1/2 em and dividing by 16. These calibrated certainty factor values were then used to generate model predictions for each subject. Responses to the 12 rule items were converted to certainty A limitation of this approach is that it is restricted to reasoning with abstract, de contextualized material. Future research will be necessary to extend these effects to more realistic items. As that happens, theoretical ideas about the impact of context on reasoning under uncertainty can be developed and Two different models were used to predict the human data. One employed the combination of best fitting algorithms from the previous experiments (Shultz et al., 1989): maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor technique.  For convenience, I call this the MMH model.  Because the previous experiments had included so many unsuccessful models, it was thought unnecessary to replicate all of those models here.  Just one model, suggested by the spontaneous comments of some subjects in the current experiment was used as a foil to MMH.  This was what I call the mean model since it summarizes antecedent evidence within a rule by taking the mean certainty factor, scales down the maximum certainty factor in the conclusion by taking the mean of the maximum certainty factor and the summarized antecedent certainty factor, and combines certainties across rules by taking the mean.  Mean models were included in our previous experiments, but were not found to be among the best fitting.  Nonetheless, taking the mean is not an unreasonable strategy for people faced with a complex task of combining numerical estimates.  Moreover, some subjects do insist that that is what they do, even if their data do not always confirm this introspection.  For each subject, both the MMH and the mean models were used to generate the subject's predicted ratings of the certainty of event X happening on the 12 items.  Each subject's own calibration ratings of the relevant certainty descriptors were used to generate these predictions.  Then the predictions generated by each of the two models were correlated with the subject's actual certainty ratings.  Many of these correlation coefficients were statistically significant at p < .05 even with only 10 df.  Thirty-nine of the 44 MMH correlations were significant and 35 of the 44 mean correlations were significant.  To evaluate the relative success of the two tested models, the corelation coefficients were subjected to an analysis of variance in which the form of the questionnaire served as a between subject factor and model served as a within subject factor.  This analysis yielded only a main effect for model, F(l, 42)  =\n25.41, p < .001.  The mean correlation for the MMH model was .75, and that for the mean model was .65.  The median correlations for the two models were .83 and .  7 1, respectively.  The mean and median correlations between the predictions generated by the two tested models were .877 and .879, respectively.  With regard to mean correlations, the partial correlation between the mean model and actual data, with the MMH model partialled out, was -.03.  With regard to median correlations, the partial correlation between the mean model and actual data, with the MMH model partialled out, was -.07.  The predictions generated by the MMH model were significantly better than those generated by the mean model.  This replicates our previous results and extends them to a more realistic scenario in which all of the relevant algorithms need to function.  A realistic way to simulate the data of human subjects in rule based propagation of uncertainty is to use maximin summarizing, multiplication scaling, and Beckerman's modified certainty factor model for combining evidence across rules.  All of the relation between the mean model and subject data can be attributed to the fact that both were highly correlated with the predictions of the MMH model.  With the MMH model predictions partialled out, the correlation between the mean model and subject data disappeared.  contextualized results can be compared to the those generated in these abstract settings.  Although quite successful in accounting for human data, the modified certainty factor approach advocated here does suffer from a subtle flaw that needs correcting.  The original certainty factor approach for combining disconfirming evidence across rules involves a simple summing of the two certainty factors (Shortliffe, 1976).  Later, it was recognized that this could create anomalies when a conclusion with plenty of accumulated support (many rules) could be completely overwhelmed by a single piece of new evidence (one rule).  Consequently, the impact of new disconfirming evidence was reduced by dividing the sum by 1 minus the minimum of the two absolute certainty factors.  One difficulty is that this is a shot gun solution that minimizes the impact of all new evidence equally, no matter how much evidence has accumulated in favor of the alternate conclusion.  A more precise solution would Another difficulty is that the amount of evidence underlying a certainty factor ought to affect combinations of confirming as well as disconfirming evidence.  A more general solution would incorporate adjustment for amount of support into all updates, whether confirming or disconfirming.  These adjustments might apply, for example, to Beckerman's (1986) modified certainty factor model.  Such considerations would not have affected the present results, where there was no accumulation of evidence, but could be quite important in cases where evidence does accumulate.  Development of a more precise, yet more generally applicable, solution to these problems is another topic for future investigation.  Learning is another problem for approaches (like the present one) that build relative certainties directly into the knowledge representation.  A variety of techniques have been developed to enable production systems to modify their own rule base, essentially by learning new rules (Klahr et al., 1987; Laird, Newell, & Rosenbloom, 1987).  It is likely that the learning of maximum certainty factors for rule conclusions would significantly complicate these learning algorithms.  Yet the effort required to dynamically adjust certainty factors in rule conclusions as a function of corrective feedback might well be worthwhile.  A system that was sensitive to its own relative certainties would be more intelligent and effective than one which ignored such differences.  The certainty factor approach to reasoning under uncertainty can be criticized for not making use of a full, optimal Bayesian analysis, which would take account of prior probabilities of the conclusion and conditional probabilities of the evidence given the conclusion.  However, the certainty factor approach can be regarded as Bayesian under restricted assumptions (Beckerman, 1986; Wise & Henrion, 1986).  The present results show that the consequences of these assumptions are not disastrous for psychological modeling .  Also, psychological experiments have reve'aled a tendency for people to ignore prior probabilities even if they are well aware of them (Tversky &\nKahneman, 1980).  Furthermore, it is unlikely that people would typically possess the knowledge of prior and conditional probabilities required by a full Bayesian analysis.  The most that ordinary reasoners can realistically be expected to know, at least implicitly, is how certain they are of various instantiated antecedent conditions and the maximum certainty factors contained in the conclusions of their production rules.  Any of the various normative models of reasoning under uncertainty could be candidates for descriptive models of how ordinary people reason.  Some of these candidates could conceivably be ruled out as psychological models in terms of the amount of prior knowledge or working memory capacity that they require.  Those remaining normative models could serve as inspiration for descriptive psychological investigation.  Conversely, it is also possible that psychological evidence might inspire or at least constrain the normative study of reasoning under uncertainty.  Gordon, I., & Shortliffe, E. H. (1984).  The Dempster-Shafer theory of evidence.  In B. G. Buchanan & E. H. Shortliffe (Eds.), Rule-based expert systems (pp. 272-292).  Reading, MA: Addison-Wesley.  Grosof, B. N. (1986).  Evidential confmnation as transformed probability: On the duality of priors and updates.  In L. N. Kanal & I. F. Lemmer (Eds.), Uncertainty in Artificial Intelligence (pp. 153-166).  North-Holland: Elsevier Science Publishers.  Heckerman, D. (1986).  Probabilistic interpretations for MYCIN's certainty factors.  In L. N. Kanal & I. F. Lemmer (Eds.), Uncertainty in Artificial Intelligence (pp. 167-196).  North Holland: Elsevier Science Publishers.  Klahr, D., Langley, P., & Neches, R. (Eds.).  (1987).  Production system models of learning and development.  Cambridge, MA: MIT Press.  Kuipers, B., Moskowitz, A. J., & Kassirer, I. P. (1988).  Critical decisions under uncertainty: Representation and structure.  Cognitive Science, 12, 177-210.  Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987).  Soar: An architecture for general intelligence.  Artificial Intelligence, 33, 1-64.  Shultz, T. R., Zelazo, P. R., & Engelberg, D. I. (1989).  Managing uncertainty in rule based reasoning.  Proceedings of the Eleventh Annual Conference of the Cognitive Science Society.  (pp. 227-234).  Hillsdale, NJ: Lawrence Erlbaum.  Tversky, A., & Kahneman, D. (1980).  Causal schemas in judgment under uncertainty.  In M. Fishbein (Eel.), Progress in social psychology (Vol 1, pp. 49-72).  Hillsdale, NJ: Lawrence Erlbaum.  Wise, B. P., & Henrion, M. (1986).  A framework for comparing uncertain inference systems to probability.  In L. N. Kanal &\nI. F. Lemmer (Eds.), Uncertainty in Artificial Intelligence (pp.  69-83).  North-Holland: Elsevier Science Publishers.  This research was supported in part by a grant from the Natural Sciences and Engineering Research Council of Canada and by the McGill-ffiM Cooperative Project", "metadata": {"source_file": "1304.1083v1.pdf", "title": null, "authors": ["Thomas R. Shultz", "Beckerman"], "year": "2011", "detected_language": "en", "page_count": 6, "origin_chunk_file": "1304.1083v1.chunks.json"}}
{"text": "in the user interface; its data is stored in the knowledge base and manipulated by the infer ence engine. There is a great deal of literature of a connection between two variables is rarely dificult to make, and most experts can easily provide the information necessary to draw arcs. The determination of appropriate conditional probability distributions to serve as arc weights, however, is quite dificult and time consuming. Increased efficiency in probability assessments could greatly reduce the time needed to design a system. The idea that ballpark probabilities may be as powerful as carefully refined assessments is not as radical as it sounds. An analysis of MYCIN [28), a rule-based expert system that used certainty factors, indicated that the cer tainty factors associated with the rules were not very important to the system's performance MYCIN's diagnoses were, in general, identi cal with the certainty factor engine on or off [3, pages 217-219). In the clinical psychology literature on bootstrapping, Dawes described a series of tests in which he compared judg ments made directly by experts to those gen erated by linear model of the experts. In addition to the standard, regression weighted models, Dawes included some improper linear models in which the weights were either ran dom or uniform. The result was that al lin ear models outperformed raw expert judgment [7). Abramson's experiments on chess material advantage functions yielded similar results [1] [2); in a 28,000 game tournament among expert designed, regression-learned, uniform, and ran domly generated weights, no single set was able to demonstrate superiority over the others. The goals of a sensitivity analysis are (i) to gain insight into the nature of a problem, (ii) to find a simple and elegant structure that does justice to the problem, and (ii) to check the correct ness of the numbers and the need for precision in refining them [31, ). Although this characterization of a sensitivity analysis should be familiar to decision analysts, it may appear somewhat unusual to people from other fields.  have reached a certain degree of precision, fur ther refinement on these numbers has little ef fect on the decisions.  Whether similar obser vations are true for diagnostic problems, (i.e., once the prior and conditional probabilities have reached certain quality, further improvement on these probabilities has little effect on its diag noses), such as Pathfinder's, is the subject of this paper.  The information stored in a Bayes net can be divided into two components: Network structure plays an obvious role in system performance.  The full extent of struc ture's importance, however, remains to be es tablished.  Both Dawes' improper linear mod els  [7) and Abramson's tournaments  [2) indicate that, at least at times, structure is almost the sole determining factor of strength.  This study was devised to examine the role of parameters to Pathfinder's performance.  Experiments were run on a body of 60 \"clas sic\" cases in which the diagnosis was known.  Since a network's parameters include prior and conditional probabilities, both sets of probabili ties had to be varied.  The experiments reported in the next section used two sets of prior prob abilities (those specified by the experts and a uniform distribution across the hypotheses) and three types of conditionals: 3.  The values assessed by experts plus ran domly generated noise, using both uni formly and normally distributed noise func tions.  against which others may be judged, the ran dom parameters addressed the relative impor tance of structure and parameters, and the ran dom noise addressed the issue of sensitivity.  The use of two different sets of priors addressed the effect of priors on system performance.  We found that system performance degraded so significantly with randomly generated probabil ities that the resulting system had negligible dis criminating power.  These results were observed regardless of the distribution function used for generating the conditional probabilities or the selection of priors.  These findings led us to con dude that parameters are crucial to a Bayes net (or at least to Pathfinder's Bayes net) and that experts are needed to provide the parameters.  Having shown that parameters play an im portant role in system performance, our next task was to assess the quality of Pathfinder's parameters with respect to their sensitivity to noise.  Some minor changes have been made to some conditional probabilities used in Pathfinder during system evaluation, to cater for mis-diagnoses on several test cases (9], in dicating that refining parameters can lead to improvement in performance.  Our analysis was intended to assess al of Pathfinder's parame ters.  Our experiments studied variations in both prior and conditional probabilities.  Priors were fixed either at the expert-assessed set or at a uniform set.  Conditionals were varied by augmenting the expert's assessment with ran domly generated noise.  The resultant condi tional probabilities are then renormalized.  The random noise functions followed uniform or nor mal distributions with p = 0 and several val ues of tr.  For each noise function, :five parame ter sets were created.  Sixty cases were run on each network, for a total of 300 data points per noise function.  A total of 7 noise generating schemes were used, including uniformly gener ated noise (uniform noise), normaly generated noise (normal noise) with standard deviations ( 0') of 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, al with a mean (p) of 0 (to ensure that any probabil ity has equal chance of being increased or de creased).  A sumary of the test results with expert priors is shown in Table 1.  In the ta ble, the percentage of correct diagnoses, (i.e., the number of cases in which the known diag nosis was assigned the highest probability di vided by the total number of cases), is intended to provide a measure of the diagnostic power.", "metadata": {"source_file": "1304.1106v1.pdf", "title": null, "authors": ["Bruce Abramson"], "year": "2011", "detected_language": "en", "page_count": 8, "origin_chunk_file": "1304.1106v1.chunks.json"}}
{"text": "The average confidence, (average diference in posterior probabilities between the two diseases with the highest posterior probabilities on the differential diagnosis for al the cases run), with respect to the correct diagnosis2 and incorrect diagnosis, provides a measure of the discrimi nating power of the leading disease (disease with the highest posterior probability on the differ ential diagnosis) from the other diseases on the differential diagnosis. It should be pointed out that although the percentage of correct diagno sis is more important than average confidence in system performance, average confidence is also useful in gauging system performance. Systems scoring perfectly (100%) in the correct diagnosis column with 0 average confidence (e.g., al dis eases have the same posterior probability with respect to al the test cases) could be as useless a system as one with no correct diagnoses and absolute average confidence (1.0). Also shown in Table 1 (in the column headed \"Percentage Better\"), is the percentage of cases in which the noisy network assigned the correct diagnosis with a higher probability than did the original network. Table 1 indicates that the original knowledge base had the highest score in both percentage of correct diagnoses and average confidence; aug mentation with uniform noise produced the low est scores on both items. Adding normal noise to the original knowledge base produced a sys tem with scores that lie between these extremes, with better results for systems with smaller standard deviations (or less noise). Furtheris based entirely on the presence of hairy cells. Noisy probabilities associated with that one key symptom have a significant negative impact on the system's ability to detect the disease. More complicated diagnoses, in which many features play a role, are less susceptible to the vagaries of a single noisy datum. This observation is not surprising; it occurs in many test settings. In a complex collection of logic gates, for example, simple functions calculated by passing through a single gate will be completely unreliable i<CUR> that gate is faulty.  More complex functions, how ever, may be robust enough to be recovered.  In the same way, simple and obvious diseases will be misdiagnosed if noise is introduced in the wrong place.  Complicated diagnoses are more robust.  Since these are precisely the data in which an expert's assessments are least likely to err, analyses like the one described above may be biased against the system.  In this paper, we have shown that parameters play an important role in Pathfinder's perfor mance; of these parameters, conditional probabilities are more important than priors.  Our analysis of Pathfinder confirmed the diagnostic prowess claimed by its designers.  In addition, they indicated a well-performed elicitation; the assessed probabilities were accurate enough to produce near-optimal performance.  Although these results, in and of themselves, may not ap pear earth-shattering, they do highlight an im portant point: outsiders (i.e., people other than the system's designers) were able to investigate and experimentally validate a knowledge engi neering exercise.  This type of experimentation is rare in AI and almost unheard of in knowl edge engineering; it was possible, in large part, because of the transparency of the Bayes net formalism.  Verifiable, reproducible, and controlled ex perimentation is an important part of science, and it is one of the areas in which AI has been traditionally weak [1).  The recent wave of work on Bayes nets, however, has suggested several diferent types of experiments: comparisons of different uncertainty formalisms [8], competi tions between Bayes nets and rule bases [14)  [20]  [30] [32), and several diferent approaches to (and motivations for) sensitivity analyses [18] (29].  For the most part, these studies ad dress the behavior of a system; although they are al system-specific, they should have some general implications to the way in which we approach system design.  Our results, for ex ample, suggest that future system designers consider their underlying model's sensitivity to noisy parameters before expending time and ef fort on parameter refinement.  We believe that stronger results should be possible, and we hope to see many of the experimental techniques of behavioral psychology modified to investigate knowledge-based systems.  Our sensitivity anal yses represent what we hope is one step towards the development of reproducible controlled ex periments for AI systems.  [1] Bruce Abramson.  On Learning and Test ing Evaluation Functions.  In Proceedings of the Sizth Israeli Conference on Artificial Intelligence, pages 7-16, 1989.  [2] Bruce Abramson.  Expected-Outcome: A General Model of Static Evaluation.  IEEE Transactions on Pattern Analysis and Ma chine Intelligence, 12(2):182-193, 1990.  (3] B. G. Buchanan and E. H. Shortlife, editors.  Rule-Based Ezpert Systems: The MYCIN Ezperiments of the Stanford Heuristic Programming Project.  Addison Wesley, 1984.  (4] R. M. Chavez and G. F. Cooper.  KNET:  Integrating Hypermedia and Bayesian Modeling.  In Proceedings of The Fourth Workshop on Uncertainty in Artificial In telligence, pages 49-54, 1988.  [5] P. Cheeseman.  In Defence of Probabil ity.  In Proceedings of the 9th International Joint Conference on Artificial Intelligence, pages <PHONE>, 1985.  [6] G. F. Cooper.  A Method for Using Belief Networks as Infiuence Diagram.  In Pro ceedings of The Fourth Workshop on Un- [8] D. E. Beckerman.  An Empirical Compar ison of Three Scoring Schemes.  In Pro ceedings of the Fourth Workshop on Uncer tainty in Artificial Intelligence, pages 158169, 1988.  [11] D. E. Beckerman, E. J. Horvitz, and B. N. Nathwani.  Update on the Pathfinder Project.  In Proceedings of the 13th Sympo sium on Computer Applications in Medical Care, pages 203-207, 1989.  (14] M. Henrion and D. R. Cooley.  An Ex perimental Comparison of Knowledge En gineering for Expert Systems and Decision Analysis.  In Proceedings of the Sizth N a tional Conference on Artificial Intelligence, pages 471-476, 1987.  [15] E. J. Horvitz, J. S. Breese, and M. Hen rion.  Decision Theory in Expert Systems and Artificial Intelligence.  International Journal of Approzimate Reasoning, 2:247302, 1988.", "metadata": {"source_file": "1304.1106v1.pdf", "title": null, "authors": ["Bruce Abramson"], "year": "2011", "detected_language": "en", "page_count": 8, "origin_chunk_file": "1304.1106v1.chunks.json"}}
{"text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. Neural machine translation is a newly emerging approach to machine translation, recently proposed by Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the traditional phrase-based translation system (see, e.g., Koehn et al., 2003) which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a correct translation. Most of the proposed neural machine translation models belong to a family of encoder– decoders (Sutskever et al., 2014; Cho et al., 2014a), with an encoder and a decoder for each language, or involve a language-specific encoder applied to each sentence whose outputs are then compared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sentence into a fixed-length vector.  A decoder then outputs a translation from the encoded vector.  The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.  A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.  This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.  Cho et al. (2014b) showed that indeed the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.  In order to address this issue, we introduce an extension to the encoder–decoder model which learns to align and translate jointly.  Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated.  The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.  The most important distinguishing feature of this approach from the basic encoder–decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector.  Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.  This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.  We show this allows a model to cope better with long sentences.  In this paper, we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder–decoder approach.  The improvement is more apparent with longer sentences, but can be observed with sentences of any length.  On the task of English-to-French translation, the proposed approach achieves, with a single model, a translation performance comparable, or close, to the conventional phrase-based system.  Furthermore, qualitative analysis reveals that the proposed model finds a linguistically plausible (soft-)alignment between a source sentence and the corresponding target sentence.  From a probabilistic perspective, translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x, i.e., arg maxy p(y | x).  In neural machine translation, we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus.  Once the conditional distribution is learned by a translation model, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability.  Recently, a number of papers have proposed the use of neural networks to directly learn this conditional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and ˜Neco, 1997).", "metadata": {"source_file": "1409.0473v7.pdf", "title": "[cs.CL] 19 May 2016 N M", "authors": ["Dzmitry Bahdanau", "KyungHyun Cho Yoshua", "Kalchbrenner", "Blunsom", "Cho et al", "Koehn", "Cho et al.", "Hermann"], "year": "2016", "detected_language": "en", "page_count": 15, "origin_chunk_file": "1409.0473v7.chunks.json"}}
{"text": "This neural machine translation approach typically consists of two components, the first of which encodes a source sentence x and the second decodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by (Cho et al., 2014a) and (Sutskever et al., 2014) to encode a variable-length source sentence into a fixed-length vector and to decode the vector into a variable-length target sentence. Despite being a quite new approach, neural machine translation has already shown promising results. Sutskever et al. (2014) reported that the neural machine translation based on RNNs with long shortterm memory (LSTM) units achieves close to the state-of-the-art performance of the conventional phrase-based machine translation system on an English-to-French translation task.1 Adding neural components to existing translation systems, for instance, to score the phrase pairs in the phrase table (Cho et al., 2014a) or to re-rank candidate translations (Sutskever et al., 2014), has allowed to surpass the previous state-of-the-art performance level. Here, we describe briefly the underlying framework, called RNN Encoder–Decoder, proposed by Cho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns to align and translate simultaneously. In the Encoder–Decoder framework, an encoder reads the input sentence, a sequence of vectors x = (x1, · · · , xTx), into a vector c.2 The most common approach is to use an RNN such that ht = f (xt, ht−1) (1) and c = q ({h1, · · · , hTx}) , where ht ∈Rn is a hidden state at time t, and c is a vector generated from the sequence of the hidden states. f and q are some nonlinear functions. Sutskever et al. (2014) used an LSTM as f and q ({h1, · · · , hT }) = hT , for instance. 1 We mean by the state-of-the-art performance, the performance of the conventional phrase-based system without using any neural network-based component. 2 Although most of the previous works (see, e.g., Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013) used to encode a variable-length input sentence into a fixed-length vector, it is not necessary, and even it may be beneficial to have a variable-length vector, as we will show later.  The decoder is often trained to predict the next word yt′ given the context vector c and all the previously predicted words {y1, · · · , yt′−1}.  In other words, the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals: where y =\ny1, · · · , yTy\n.  With an RNN, each conditional probability is modeled as where g is a nonlinear, potentially multi-layered, function that outputs the probability of yt, and st is the hidden state of the RNN.  It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).  In this section, we propose a novel architecture for neural machine translation.  The new architecture consists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching through a source sentence during decoding a translation (Sec. 3.1).  Figure 1: The graphical illustration of the proposed model trying to generate the t-th target word yt given a source sentence (x1, x2, . . . , xT ).  It should be noted that unlike the existing encoder–decoder approach (see Eq. (2)), here the probability is conditioned on a distinct context vector ci for each target word yi.  The context vector ci depends on a sequence of annotations (h1, · · · , hTx) to which an encoder maps the input sentence.  Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i-th word of the input sequence.  We explain in detail how the annotations are computed in the next section.  where eij = a(si−1, hj) is an alignment model which scores how well the inputs around position j and the output at position i match.  The score is based on the RNN hidden state si−1 (just before emitting yi, Eq. (4)) and the j-th annotation hj of the input sentence.  We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system.  Note that unlike in traditional machine translation, the alignment is not considered to be a latent variable.  Instead, the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through.  This gradient can be used to train the alignment model as well as the whole translation model jointly.  We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation, where the expectation is over possible alignments.  Let αij be a probability that the target word yi is aligned to, or translated from, a source word xj.  Then, the i-th context vector ci is the expected annotation over all the annotations with probabilities αij.  The probability αij, or its associated energy eij, reflects the importance of the annotation hj with respect to the previous hidden state si−1 in deciding the next state si and generating yi.  Intuitively, this implements a mechanism of attention in the decoder.  The decoder decides parts of the source sentence to pay attention to.  By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector.  With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.  The usual RNN, described in Eq. (1), reads an input sequence x in order starting from the first symbol x1 to the last one xTx.  However, in the proposed scheme, we would like the annotation of each word to summarize not only the preceding words, but also the following words.  Hence, we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been successfully used recently in speech recognition (see, e.g., Graves et al., 2013).  A BiRNN consists of forward and backward RNN's.  The forward RNN −→f reads the input sequence as it is ordered (from x1 to xTx) and calculates a sequence of forward hidden states (−→h 1, · · · , −→h Tx).  The backward RNN ←−f reads the sequence in the reverse order (from xTx to x1), resulting in a sequence of backward hidden states (←−h 1, · · · , ←−h Tx).  backward one ←−h j, i.e., hj = h−→h ⊤ j ; ←−h ⊤ j\ni⊤ .  In this way, the annotation hj contains the summaries of both the preceding words and the following words.  Due to the tendency of RNNs to better represent recent inputs, the annotation hj will be focused on the words around xj.  This sequence of annotations is used by the decoder and the alignment model later to compute the context vector (Eqs. (5)–(6)).  We evaluate the proposed approach on the task of English-to-French translation.  We use the bilingual, parallel corpora provided by ACL WMT '14.3 As a comparison, we also report the performance of an RNN Encoder–Decoder which was proposed recently by Cho et al. (2014a).  We use the same training procedures and the same dataset for both models.4 WMT '14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words.  Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5 We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder.  We concatenate news-testFigure 2: The BLEU scores of the generated translations on the test set with respect to the lengths of the sentences.  The results are on the full test set which includes sentences having unknown words to the models.  2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test set (news-test-2014) from WMT '14, which consists of 3003 sentences not present in the training data.  After a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to train our models.  Any word not included in the shortlist is mapped to a special token ([UNK]).  We do not apply any other special preprocessing, such as lowercasing or stemming, to the data.  We train two types of models.  The first one is an RNN Encoder–Decoder (RNNencdec, Cho et al., 2014a), and the other is the proposed model, to which we refer as RNNsearch.  We train each model twice: first with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then with the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).  The encoder and decoder of the RNNencdec have 1000 hidden units each.7  The encoder of the RNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000 hidden units.  Its decoder has 1000 hidden units.  In both cases, we use a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer to compute the conditional probability of each target word (Pascanu et al., 2014).  We use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model.  Each SGD update direction is computed using a minibatch of 80 sentences.  We trained each model for approximately 5 days.  Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013).  Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model.  For more details on the architectures of the models and training procedure used in the experiments, see Appendices A and B.  In Table 1, we list the translation performances measured in BLEU score.  It is clear from the table that in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec.  More importantly, the performance of the RNNsearch is as high as that of the conventional phrase-based translation system (Moses), when only the sentences consisting of known words are considered.  This is a significant achievement, considering that Moses uses a separate monolingual corpus (418M words) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.  6  We used the tokenization script from the open-source machine translation package, Moses.  7  In this paper, by a 'hidden unit', we always mean the gated hidden unit (see", "metadata": {"source_file": "1409.0473v7.pdf", "title": "[cs.CL] 19 May 2016 N M", "authors": ["Dzmitry Bahdanau", "KyungHyun Cho Yoshua", "Kalchbrenner", "Blunsom", "Cho et al", "Koehn", "Cho et al.", "Hermann"], "year": "2016", "detected_language": "en", "page_count": 15, "origin_chunk_file": "1409.0473v7.chunks.json"}}
{"text": "Deep Neural Networks (DNNs) are extremely powerful machine learning models that achieve excellent performance on difficult problems such as speech recognition [13, 7] and visual object recognition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation for a modest number of steps. A surprising example of the power of DNNs is their ability to sort N N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are related to conventional statistical models, they learn an intricate computation. Furthermore, large DNNs can be trained with supervised backpropagation whenever the labeled training set has enough information to specify the network's parameters. Thus, if there exists a parameter setting of a large DNN that achieves good results (for example, because humans can solve the task very rapidly), supervised backpropagation will find these parameters and solve the problem. Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori. For example, speech recognition and machine translation are sequential problems. Likewise, question answering can also be seen as mapping a sequence of words representing the question to a sequence of words representing the answer. It is therefore clear that a domain-independent method that learns to map sequences to sequences would be useful. Sequences pose a challenge for DNNs because they require that the dimensionality of the inputs and outputs is known and fixed. In this paper, we show that a straightforward application of the Long Short-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems. The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixeddimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (fig. 1).  The second LSTM is essentially a recurrent neural network language model  [28, 23, 30] except that it is conditioned on the input sequence.  The LSTM's ability to successfully learn on data with long range temporal dependencies makes it a natural choice for this application due to the considerable time lag between the inputs and their corresponding outputs (fig. 1).  There have been a number of related attempts to address the general sequence to sequence learning problem with neural networks.  Our approach is closely related to Kalchbrenner and Blunsom  [18] who were the first to map the entire input sentence to vector, and is related to Cho et al.  [5] although the latter was used only for rescoring hypotheses produced by a phrase-based system.  Graves  [10] introduced a novel differentiable attention mechanism that allows neural networks to focus on different parts of their input, and an elegant variant of this idea was successfully applied to machine translation by Bahdanau et al.  [2].  The Connectionist Sequence Classification is another popular technique for mapping sequences to sequences with neural networks, but it assumes a monotonic alignment between the inputs and the outputs  [11].  The main result of this work is the following.  On the WMT'14 English to French translation task, we obtained a BLEU score of 34.81 by directly extracting translations from an ensemble of 5 deep LSTMs (with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beamsearch decoder.  This is by far the best result achieved by direct translation with large neural networks.  For comparison, the BLEU score of an SMT baseline on this dataset is 33.30  [29].  The 34.81 BLEU score was achieved by an LSTM with a vocabulary of 80k words, so the score was penalized whenever the reference translation contained a word not covered by these 80k.  This result shows that a relatively unoptimized small-vocabulary neural network architecture which has much room for improvement outperforms a phrase-based SMT system.  A useful property of the LSTM is that it learns to map an input sentence of variable length into a fixed-dimensional vector representation.  Given that translations tend to be paraphrases of the source sentences, the translation objective encourages the LSTM to find sentence representations that capture their meaning, as sentences with similar meanings are close to each other while different sentences meanings will be far.  A qualitative evaluation supports this claim, showing that our model is aware of word order and is fairly invariant to the active and passive voice.  The Recurrent Neural Network (RNN)  [31, 28] is a natural generalization of feedforward neural networks to sequences.  Given a sequence of inputs (x1, . . . , xT ), a standard RNN computes a sequence of outputs (y1, . . .  , yT ) by iterating the following equation: The RNN can easily map sequences to sequences whenever the alignment between the inputs the outputs is known ahead of time.  However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships.  The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN (this approach has also been taken by Cho et al.  [5]).  While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies (figure 1)  [14, 4, 16, 15].  However, the Long Short-Term Memory (LSTM) [16] is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.  The goal of the LSTM is to estimate the conditional probability p(y1, . . .  , yT ′|x1, . .  . , xT )  where (x1, . .  . , xT ) is an input sequence and y1, . .  . , yT ′ is its corresponding output sequence whose length T ′ may differ from T .  The LSTM computes this conditional probability by first obtaining the fixeddimensional representation v of the input sequence (x1, . .  . , xT )  given by the last hidden state of the LSTM, and then computing the probability of y1, . .  . , yT ′ with a standard LSTM-LM formulation whose initial hidden state is set to the representation v of x1, . . .  , xT : In this equation, each p(yt|v, y1, . .  . , yt−1) distribution is represented with a softmax over all the words in the vocabulary.  We use the LSTM formulation from Graves [10].  Note that we require that each sentence ends with a special end-of-sentence symbol \"\", which enables the model to define a distribution over sequences of all possible lengths.  The overall scheme is outlined in figure 1, where the shown LSTM computes the representation of \"A\", \"B\", \"C\", \"\" and then uses this representation to compute the probability of \"W\", \"X\", \"Y\", \"Z\", \"\".  We applied our method to the WMT'14 English to French MT task in two ways.  We used it to directly translate the input sentence without using a reference SMT system and we it to rescore the n-best lists of an SMT baseline.  We report the accuracy of these translation methods, present sample translations, and visualize the resulting sentence representation.  We used the WMT'14 English to French dataset.  We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean \"selected\" subset from [29].  We chose this translation task and this specific training set subset because of the public availability of a tokenized training and test set together with 1000-best lists from the baseline SMT [29].  As typical neural language models rely on a vector representation for each word, we used a fixed vocabulary for both languages.  We used 160,000 of the most frequent words for the source language and 80,000 of the most frequent words for the target language.  Every out-of-vocabulary word was replaced with a special \"UNK\" token.  The core of our experiments involved training a large deep LSTM on many sentence pairs.  We trained it by maximizing the log probability of a correct translation T given the source sentence S, so the training objective is 1/|S| X\nwhere S is the training set.  Once training is complete, we produce translations by finding the most likely translation according to the LSTM:  We search for the most likely translation using a simple left-to-right beam search decoder which maintains a small number B of partial hypotheses, where a partial hypothesis is a prefix of some translation.  At each timestep we extend each partial hypothesis in the beam with every possible word in the vocabulary.  This greatly increases the number of the hypotheses so we discard all but the B most likely hypotheses according to the model's log probability.  As soon as the \"\" symbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete hypotheses.  While this decoder is approximate, it is simple to implement.  Interestingly, our system performs well even with a beam size of 1, and a beam of size 2 provides most of the benefits of beam search (Table 1).  We also used the LSTM to rescore the 1000-best lists produced by the baseline system [29].  To rescore an n-best list, we computed the log probability of every hypothesis with our LSTM and took an even average with their score and the LSTM's score.  While we do not have a complete explanation to this phenomenon, we believe that it is caused by the introduction of many short term dependencies to the dataset.  Normally, when we concatenate a source sentence with a target sentence, each word in the source sentence is far from its corresponding word in the target sentence.  As a result, the problem has a large \"minimal time lag\" [17].  By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged.  However, the first few words in the source language are now very close to the first few words in the target language, so the problem's minimal time lag is greatly reduced.  Thus, backpropagation has an easier time \"establishing communication\" between the source sentence and the target sentence, which in turn results in substantially improved overall performance.  Initially, we believed that reversing the input sentences would only lead to more confident predictions in the early parts of the target sentence and to less confident predictions in the later parts.  However, LSTMs trained on reversed source sentences did much better on long sentences than LSTMs trained on the raw source sentences (see sec. 3.7), which suggests that reversing the input sentences results in LSTMs with better memory utilization.  We found that the LSTM models are fairly easy to train.  We used deep LSTMs with 4 layers, with 1000 cells at each layer and 1000 dimensional word embeddings, with an input vocabulary of 160,000 and an output vocabulary of 80,000.  Thus the deep LSTM uses 8000 real numbers to represent a sentence.  We found deep LSTMs to significantly outperform shallow LSTMs, where each additional layer reduced perplexity by nearly 10%, possibly due to their much larger hidden state.  We used a naive softmax over 80,000 words at each output.  The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the \"encoder\" LSTM and 32M for the \"decoder\" LSTM).  The complete training details are given below: • We used stochastic gradient descent without momentum, with a fixed learning rate of 0.7.  After 5 epochs, we begun halving the learning rate every half epoch.  We trained our models for a total of 7.5 epochs.  •  We used batches of 128 sequences for the gradient and divided it the size of the batch (namely, 128).  •  Although LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients.  Thus we enforced a hard constraint on the norm of the gradient [10, 25] by scaling it when its norm exceeded a threshold.  For each training batch, we compute s = ∥g∥2, where g is the gradient divided by 128.  If s > 5, we set g = 5g • Different sentences have different lengths.  Most sentences are short (e.g., length 20-30) but some sentences are long (e.g., length > 100), so a minibatch of 128 randomly chosen training sentences will have many short sentences and few long sentences, and as a result, much of the computation in the minibatch is wasted.  To address this problem, we made sure that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.  A C++ implementation of deep LSTM with the configuration from the previous section on a single GPU processes a speed of approximately 1,700 words per second.  This was too slow for our purposes, so we parallelized our model using an 8-GPU machine.  Each layer of the LSTM was executed on a different GPU and communicated its activations to the next GPU / layer as soon as they were computed.  Our models have 4 layers of LSTMs, each of which resides on a separate GPU.  The remaining 4 GPUs were used to parallelize the softmax, so each GPU was responsible for multiplying by a 1000 × 20000 matrix.  The resulting implementation achieved a speed of 6,300 (both English and French) words per second with a minibatch size of 128.  Training took about a ten days with this implementation.  We used the cased BLEU score  [24] to evaluate the quality of our translations.  We computed our BLEU scores using multi-bleu.pl1 on the tokenized predictions and ground truth.  This way of evaluating the BELU score is consistent with [5] and [2], and reproduces the 33.3 score of [29].  However, if we evaluate the best WMT'14 system [9] (whose predictions can be downloaded from statmt.org\\matrix) in this manner, we get 37.0, which is greater than the 35.8 reported by statmt.org\\matrix.  The results are presented in tables 1 and 2.  Our best results are obtained with an ensemble of LSTMs that differ in their random initializations and in the random order of minibatches.  While the decoded translations of the LSTM ensemble do not outperform the best WMT'14 system, it is the first time that a pure neural translation system outperforms a phrase-based SMT baseline on a large scale MT Method test BLEU score (ntst14)  Bahdanau et al.  [2] 28.45 Baseline System  [29] 33.30 Single forward LSTM, beam size 12 26.17 Single reversed LSTM, beam size 12 30.59 Ensemble of 5 reversed LSTMs, beam size 1 33.00 Ensemble of 2 reversed LSTMs, beam size 12 33.27 Ensemble of 5 reversed LSTMs, beam size 2 34.50 Ensemble of 5 reversed LSTMs, beam size 12 34.81 Table 1: The performance of the LSTM on WMT'14 English to French test set (ntst14).  Note that an ensemble of 5 LSTMs with a beam of size 2 is cheaper than of a single LSTM with a beam of size 12.  Method test BLEU score (ntst14)  Baseline System  [29] 33.30 Cho et al.", "metadata": {"source_file": "1409.3215v3.pdf", "title": "[cs.CL] 14 Dec 2014", "authors": ["Quoc V. Le"], "year": "2014", "detected_language": "en", "page_count": 9, "origin_chunk_file": "1409.3215v3.chunks.json"}}
{"text": "[5] 34.54 Best WMT'14 result [9] 37.0 Rescoring the baseline 1000-best with a single forward LSTM 35.61 Rescoring the baseline 1000-best with a single reversed LSTM 35.85 Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs 36.5 Oracle Rescoring of the Baseline 1000-best lists ∼45 Table 2: Methods that use neural networks together with an SMT system on the WMT'14 English to French test set (ntst14). task by a sizeable margin, despite its inability to handle out-of-vocabulary words. The LSTM is within 0.5 BLEU points of the best WMT'14 result if it is used to rescore the 1000-best list of the baseline system. We were surprised to discover that the LSTM did well on long sentences, which is shown quantitatively in figure 3. Table 3 presents several examples of long sentences and their translations. Figure 2: The figure shows a 2-dimensional PCA projection of the LSTM hidden states that are obtained after processing the phrases in the figures. The phrases are clustered by meaning, which in these examples is primarily a function of word order, which would be difficult to capture with a bag-of-words model. Notice that both clusters have similar internal structure. One of the attractive features of our model is its ability to turn a sequence of words into a vector of fixed dimensionality. Figure 2 visualizes some of the learned representations. The figure clearly shows that the representations are sensitive to the order of words, while being fairly insensitive to the Type Sentence Our model Ulrich UNK , membre du conseil d' administration du constructeur automobile Audi , affirme qu' il s' agit d' une pratique courante depuis des ann´ees pour que les t´el´ephones portables puissent ˆetre collect´es avant les r´eunions du conseil d' administration afin qu' ils ne soient pas utilis´es comme appareils d' ´ecoute `a distance . Truth Ulrich Hackenberg , membre du conseil d' administration du constructeur automobile Audi , d´eclare que la collecte des t´el´ephones portables avant les r´eunions du conseil , afin qu' ils ne puissent pas ˆetre utilis´es comme appareils d' ´ecoute `a distance , est une pratique courante depuis des ann´ees .  Our model \" Les t´el´ephones cellulaires , qui sont vraiment une question , non seulement parce qu' ils pourraient potentiellement causer des interf´erences avec les appareils de navigation , mais nous savons , selon la FCC , qu' ils pourraient interf´erer avec les tours de t´el´ephone cellulaire lorsqu' ils sont dans l' air \" , dit UNK .  Truth \" Les t´el´ephones portables sont v´eritablement un probl`eme , non seulement parce qu' ils pourraient ´eventuellement cr´eer des interf´erences avec les instruments de navigation , mais parce que nous savons , d' apr`es la FCC , qu' ils pourraient perturber les antennes-relais de t´el´ephonie mobile s' ils sont utilis´es  `a bord \" , a d´eclar´e Rosenker .  Our model Avec la cr´emation , il y a un \" sentiment de violence contre le corps d' un ˆetre cher \" , qui sera \" r´eduit `a une pile de cendres \" en tr`es peu de temps au lieu d' un processus de d´ecomposition \" qui accompagnera les ´etapes du deuil \" .  Truth Il y a , avec la cr´emation , \" une violence faite au corps aim´e \" , qui va ˆetre \" r´eduit `a un tas de cendres \" en tr`es peu de temps , et non apr`es un processus de d´ecomposition , qui \" accompagnerait les phases du deuil \" .  Table 3: A few examples of long translations produced by the LSTM alongside the ground truth translations.  The reader can verify that the translations are sensible using Google translate.  Figure 3: The left plot shows the performance of our system as a function of sentence length, where the x-axis corresponds to the test sentences sorted by their length and is marked by the actual sequence lengths.  There is no degradation on sentences with less than 35 words, there is only a minor degradation on the longest sentences.  The right plot shows the LSTM's performance on sentences with progressively more rare words, where the x-axis corresponds to the test sentences sorted by their \"average word frequency rank\".  There is a large body of work on applications of neural networks to machine translation.  So far, the simplest and most effective way of applying an RNN-Language Model (RNNLM)  [23] or a Feedforward Neural Network Language Model (NNLM)  [3] to an MT task is by rescoring the nbest lists of a strong MT baseline  [22], which reliably improves translation quality.  More recently, researchers have begun to look into ways of including information about the source language into the NNLM.  Examples of this work include Auli et al.  [1], who combine an NNLM with a topic model of the input sentence, which improves rescoring performance.  Devlin et al.  [8] followed a similar approach, but they incorporated their NNLM into the decoder of an MT system and used the decoder's alignment information to provide the NNLM with the most useful words in the input sentence.  Their approach was highly successful and it achieved large improvements over their baseline.  Our work is closely related to Kalchbrenner and Blunsom  [18], who were the first to map the input sentence into a vector and then back to a sentence, although they map sentences to vectors using convolutional neural networks, which lose the ordering of the words.  Similarly to this work, Cho et al.  [5] used an LSTM-like RNN architecture to map sentences into vectors and back, although their primary focus was on integrating their neural network into an SMT system.  Bahdanau et al.  [2] also attempted direct translations with a neural network that used an attention mechanism to overcome the poor performance on long sentences experienced by Cho et al.  [5] and achieved encouraging results.  Likewise, Pouget-Abadie et al.  [26] attempted to address the memory problem of Cho et al.  [5] by translating pieces of the source sentence in way that produces smooth translations, which is similar to a phrase-based approach.  We suspect that they could achieve similar improvements by simply training their networks on reversed source sentences.  End-to-end training is also the focus of Hermann et al.  [12], whose model represents the inputs and outputs by feedforward networks, and map them to similar points in space.  However, their approach cannot generate translations directly: to get a translation, they need to do a look up for closest vector in the pre-computed database of sentences, or to rescore a sentence.  In this work, we showed that a large deep LSTM, that has a limited vocabulary and that makes almost no assumption about problem structure can outperform a standard SMT-based system whose vocabulary is unlimited on a large-scale MT task.  The success of our simple LSTM-based approach on MT suggests that it should do well on many other sequence learning problems, provided they have enough training data.  We were surprised by the extent of the improvement obtained by reversing the words in the source sentences.  We conclude that it is important to find a problem encoding that has the greatest number of short term dependencies, as they make the learning problem much simpler.  In particular, while we were unable to train a standard RNN on the non-reversed translation problem (shown in fig. 1), we believe that a standard RNN should be easily trainable when the source sentences are reversed (although we did not verify it experimentally).  We were also surprised by the ability of the LSTM to correctly translate very long sentences.  We were initially convinced that the LSTM would fail on long sentences due to its limited memory, and other researchers reported poor performance on long sentences with a model similar to ours  [5, 2, 26].  And yet, LSTMs trained on the reversed dataset had little difficulty translating long sentences.  Most importantly, we demonstrated that a simple, straightforward and a relatively unoptimized approach can outperform an SMT system, so further work will likely lead to even greater translation accuracies.  These results suggest that our approach will likely do well on other challenging sequence to sequence problems.  We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolfgang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba, and the Google Brain team for useful comments and discussions.", "metadata": {"source_file": "1409.3215v3.pdf", "title": "[cs.CL] 14 Dec 2014", "authors": ["Quoc V. Le"], "year": "2014", "detected_language": "en", "page_count": 9, "origin_chunk_file": "1409.3215v3.chunks.json"}}
{"text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. Deep convolutional neural networks [22, 21] have led to a series of breakthroughs for image classification [21, 50, 40]. Deep networks naturally integrate low/mid/highlevel features [50] and classifiers in an end-to-end multilayer fashion, and the \"levels\" of features can be enriched by the number of stacked layers (depth). Recent evidence [41, 44] reveals that network depth is of crucial importance, and the leading results [41, 44, 13, 16] on the challenging ImageNet dataset [36] all exploit \"very deep\" [41] models, with a depth of sixteen [41] to thirty [16]. Many other nontrivial visual recognition tasks [8, 12, 7, 32, 27] have also Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer \"plain\" networks.  The deeper network has higher training error, and thus test error.  Similar phenomena on ImageNet is presented in Fig.  4. greatly benefited from very deep models.  Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers?  An obstacle to answering this question was the notorious problem of vanishing/exploding gradients  [1, 9], which hamper convergence from the beginning.  This problem, however, has been largely addressed by normalized initialization  [23, 9, 37, 13] and intermediate normalization layers  [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation  [22].  When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly.  Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments.  Fig.  1 shows a typical example.  The degradation (of training accuracy) indicates that not all systems are similarly easy to optimize.  Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it.  There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model.  The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart.  But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).  In this paper, we address the degradation problem by introducing a deep residual learning framework.  Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping.  Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x) := H(x)−x.  The original mapping is recast into F(x)+x.  We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping.  To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.  The formulation of F(x)+x can be realized by feedforward neural networks with \"shortcut connections\" (Fig. 2).  Shortcut connections [2, 34, 49] are those skipping one or more layers.  In our case, the shortcut connections simply perform identity mapping, and their outputs are added to the outputs of the stacked layers (Fig. 2).  Identity shortcut connections add neither extra parameter nor computational complexity.  The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe  [19]) without modifying the solvers.  We present comprehensive experiments on ImageNet  [36] to show the degradation problem and evaluate our method.  We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart \"plain\" nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.  Similar phenomena are also shown on the CIFAR-10 set  [20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset.  We present successfully trained models on this dataset with over 100 layers, and explore models with over 1000 layers.  On the ImageNet classification dataset [36], we obtain excellent results by extremely deep residual nets.  Our 152layer residual net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets  [41].  Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition.  The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in ILSVRC & COCO 2015 competitions.  This strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.  Shortcut Connections.  Practices and theories that lead to shortcut connections [2, 34, 49] have been studied for a long time.  An early practice of training multi-layer perceptrons (MLPs) is to add a linear layer connected from the network input to the output  [34, 49].  In [44, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients.  The papers of [39, 38, 31, 47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections.  In [44], an \"inception\" layer is composed of a shortcut branch and a few deeper branches.  Concurrent with our work, \"highway networks\" [42, 43] present shortcut connections with gating functions [15].  These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free.  When a gated shortcut is \"closed\" (approaching zero), the layers in highway networks represent non-residual functions.  On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned.  In addition, highLet us consider H(x) as an underlying mapping to be fit by a few stacked layers (not necessarily the entire net), with x denoting the inputs to the first of these layers.  If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions2, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., H(x) −x (assuming that the input and output are of the same dimensions).  So rather than expect stacked layers to approximate H(x), we explicitly let these layers approximate a residual function F(x) := H(x) −x.  The original function thus becomes F(x)+x.  Although both forms should be able to asymptotically approximate the desired functions (as hypothesized), the ease of learning might be different.  This reformulation is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left).  As we discussed in the introduction, if the added layers can be constructed as identity mappings, a deeper model should have training error no greater than its shallower counterpart.  The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.  With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.  In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition the problem.  If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.  We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.  We adopt residual learning to every few stacked layers.  A building block is shown in Fig.  2.  Formally, in this paper we consider a building block defined as:  Here x and y are the input and output vectors of the layers considered.  The function F(x, {Wi}) represents the residual mapping to be learned.  For the example in Fig. 2 that has two layers, F = W2σ(W1x) in which σ denotes ReLU [29] and the biases are omitted for simplifying notations.  The operation F + x is performed by a shortcut connection and element-wise addition.  We adopt the second nonlinearity after the addition (i.e., σ(y), see Fig. 2).  The shortcut connections in Eqn.(1) introduce neither extra parameter nor computation complexity.  This is not only attractive in practice but also important in our comparisons between plain and residual networks.  We can fairly compare plain/residual networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).  The dimensions of x and F must be equal in Eqn.(1).  If this is not the case (e.g., when changing the input/output channels), we can perform a linear projection Ws by the shortcut connections to match the dimensions: We can also use a square matrix Ws in Eqn.(1).  But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus Ws is only used when matching dimensions.  The form of the residual function F is flexible.  Experiments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible.  But if F has only a single layer, Eqn.(1) is similar to a linear layer: y = W1x + x, for which we have not observed advantages.  We also note that although the above notations are about fully-connected layers for simplicity, they are applicable to convolutional layers.  The function F(x, {Wi}) can represent multiple convolutional layers.  The element-wise addition is performed on two feature maps, channel by channel.  We have tested various plain/residual nets, and have observed consistent phenomena.  To provide instances for discussion, we describe two models for ImageNet as follows.  Plain Network.  Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets  [41] (Fig. 3, left).  The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer.  We perform downsampling directly by convolutional layers that have a stride of 2.  The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax.  The total number of weighted layers is 34 in Fig. 3 (middle).  It is worth noticing that our model has fewer filters and lower complexity than VGG nets  [41] (Fig. 3, left).  Our 34layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).  Figure 3.  Example network architectures for ImageNet.  Left: the VGG-19 model  [41] (19.6 billion FLOPs) as a reference.  Middle: a plain network with 34 parameter layers (3.6 billion FLOPs).  Right: a residual network with 34 parameter layers (3.6 billion FLOPs).  The dotted shortcuts increase dimensions.  Table 1 shows more details and other variants.  Our implementation for ImageNet follows the practice in [21, 41].  The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation [41].  A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21].  The standard color augmentation in [21] is used.  We adopt batch normalization (BN)  [16] right after each convolution and before activation, following [16].  We initialize the weights as in [13] and train all plain/residual nets from scratch.  We use SGD with a mini-batch size of 256.  The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60 × 104 iterations.  We use a weight decay of 0.0001 and a momentum of 0.9.  We do not use dropout [14], following the practice in [16].  In testing, for comparison studies we adopt the standard 10-crop testing [21].  For best results, we adopt the fullyconvolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in {224, 256, 384, 480, 640}).  We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes.  The models are trained on the 1.28 million training images, and evaluated on the 50k validation images.  We also obtain a final result on the 100k test images, reported by the test server.  We evaluate both top-1 and top-5 error rates.  Plain Networks.  We first evaluate 18-layer and 34-layer plain nets.  The 34-layer plain net is in Fig.  3 (middle).  The 18-layer plain net is of a similar form.  See Table 1 for detailed architectures.  The results in Table 2 show that the deeper 34-layer plain net has higher validation error than the shallower 18-layer plain net.  To reveal the reasons, in Fig. 4 (left) we compare their training/validation errors during the training procedure.  We have observed the degradation problem - the Table 1.  Architectures for ImageNet.  Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked.  Downsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.  Figure 4.  Training on ImageNet.  Thin curves denote training error, and bold curves denote validation error of the center crops.  Left: plain networks of 18 and 34 layers.  Right: ResNets of 18 and 34 layers.  In this plot, the residual networks have no extra parameter compared to their plain counterparts.  Table 2.  Top-1 error (%, 10-crop testing) on ImageNet validation.  Here the ResNets have no extra parameter compared to their plain counterparts.  Fig.  4 shows the training procedures.  34-layer plain net has higher training error throughout the whole training procedure, even though the solution space of the 18-layer plain network is a subspace of that of the 34-layer one.  We argue that this optimization difficulty is unlikely to be caused by vanishing gradients.  These plain networks are trained with BN  [16], which ensures forward propagated signals to have non-zero variances.  We also verify that the backward propagated gradients exhibit healthy norms with BN.  So neither forward nor backward signals vanish.  In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the solver works to some extent.  We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the Residual Networks.  Next we evaluate 18-layer and 34layer residual nets (ResNets).  The baseline architectures are the same as the above plain nets, expect that a shortcut connection is added to each pair of 3×3 filters as in Fig.  3  (right).  In the first comparison (Table 2 and Fig. 4 right), we use identity mapping for all shortcuts and zero-padding for increasing dimensions (option A).  So they have no extra parameter compared to the plain counterparts.  We have three major observations from Table 2 and Fig.  4.  First, the situation is reversed with residual learning – the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%).  More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data.  This indicates that the degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth.  Second, compared to its plain counterpart, the 34-layer 3We have experimented with more training iterations (3×) and still observed the degradation problem, suggesting that this problem cannot be feasibly addressed by simply using more iterations.  Table 3.  Error rates (%, 10-crop testing) on ImageNet validation.  VGG-16 is based on our test.  ResNet-50/101/152 are of option B that only uses projections for increasing dimensions.  GoogLeNet  [44] (ILSVRC'14) -\n7.89 VGG  [41] (v5) 24.4 7.1 PReLU-net [13] 21.59 5.71 BN-inception  [16] 21.99 5.81 ResNet-34 B 21.84 5.71 ResNet-34 C 21.53 5.60 ResNet-50 20.74 5.25 ResNet-101 19.87 4.60 ResNet-152 19.38 4.49 Table 4.  Error rates (%) of single-model results on the ImageNet validation set (except † reported on the test set).  method top-5 err.  (test) VGG  [41] (ILSVRC'14) 7.32 GoogLeNet  [44] (ILSVRC'14) 6.66 VGG [41] (v5) 6.8 PReLU-net  [13] 4.94 BN-inception  [16] 4.82 ResNet (ILSVRC'15) 3.57 Table 5.  Error rates (%) of ensembles.  The top-5 error is on the test set of ImageNet and reported by the test server.  ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced training error (Fig. 4 right vs. left).  This comparison verifies the effectiveness of residual learning on extremely deep systems.  Last, we also note that the 18-layer plain/residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).  When the net is \"not overly deep\" (18 layers here), the current SGD solver is still able to find good solutions to the plain net.  In this case, the ResNet eases the optimization by providing faster convergence at the early stage.  Figure 5.  A deeper residual function F for ImageNet.  Left: a building block (on 56×56 feature maps) as in Fig. 3 for ResNet34.  Right: a \"bottleneck\" building block for ResNet-50/101/152.  parameter-free, identity shortcuts help with training.  Next we investigate projection shortcuts (Eqn.(2)).  In Table 3 we compare three options: (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameterfree (the same as Table 2 and Fig. 4 right); (B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections.  Table 3 shows that all three options are considerably better than the plain counterpart.  B is slightly better than A.  We argue that this is because the zero-padded dimensions in A indeed have no residual learning.  C is marginally better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts.  But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem.  So we do not use option C in the rest of this paper, to reduce memory/time complexity and model sizes.  Identity shortcuts are particularly important for not increasing the complexity of the bottleneck architectures that are introduced below.  Deeper Bottleneck Architectures.  Next we describe our deeper nets for ImageNet.  Because of concerns on the training time that we can afford, we modify the building block as a bottleneck design4.  For each residual function F, we use a stack of 3 layers instead of 2 (Fig. 5).  The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions.  Fig.  5 shows an example, where both designs have similar time complexity.  The parameter-free identity shortcuts are particularly important for the bottleneck architectures.  If the identity shortcut in Fig.  5 (right) is replaced with projection, one can show that the time complexity and model size are doubled, as the shortcut is connected to the two high-dimensional ends.  So identity shortcuts lead to more efficient models for the bottleneck designs.  50-layer ResNet: We replace each 2-layer block in the 4Deeper non-bottleneck ResNets (e.g., Fig.  5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets.  So the usage of bottleneck designs is mainly due to practical considerations.  We further note that the degradation problem of plain nets is also witnessed for the bottleneck designs.  34-layer net with this 3-layer bottleneck block, resulting in a 50-layer ResNet (Table 1).  We use option B for increasing dimensions.  This model has 3.8 billion FLOPs.  101-layer and 152-layer ResNets: We construct 101layer and 152-layer ResNets by using more 3-layer blocks (Table 1).  Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).  The 50/101/152-layer ResNets are more accurate than the 34-layer ones by considerable margins (Table 3 and 4).  We do not observe the degradation problem and thus enjoy significant accuracy gains from considerably increased depth.  The benefits of depth are witnessed for all evaluation metrics (Table 3 and 4).  Comparisons with State-of-the-art Methods.  In Table 4 we compare with the previous best single-model results.  Our baseline 34-layer ResNets have achieved very competitive accuracy.  Our 152-layer ResNet has a single-model top-5 validation error of 4.49%.  This single-model result outperforms all previous ensemble results (Table 5).  We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting).  This leads to 3.57% top-5 error on the test set (Table 5).  This entry won the 1st place in ILSVRC 2015.  We conducted more studies on the CIFAR-10 dataset  [20], which consists of 50k training images and 10k testing images in 10 classes.  We present experiments trained on the training set and evaluated on the test set.  Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we intentionally use simple architectures as follows.  The plain/residual architectures follow the form in Fig.  3 (middle/right).  The network inputs are 32×32 images, with the per-pixel mean subtracted.  The first layer is 3×3 convolutions.  Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size.  The numbers of filters are {16, 32, 64} respectively.  The subsampling is performed by convolutions with a stride of 2.  The network ends with a global average pooling, a 10-way fully-connected layer, and softmax.  There are totally 6n+2 stacked weighted layers.  The following table summarizes the architecture: When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts).  On this dataset we use identity shortcuts in all cases (i.e., option A), method error (%) Maxout [10] 9.38 NIN [25] 8.81 DSN  [24] 8.22 # layers # params FitNet  [35]\n19.4M 7.93 Table 6.  Classification error on the CIFAR-10 test set.  All methods are with data augmentation.  For ResNet-110, we run it 5 times and show \"best (mean±std)\" as in [43].  so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.  We use a weight decay of 0.0001 and momentum of 0.9, and adopt the weight initialization in [13] and BN  [16] but with no dropout.  These models are trained with a minibatch size of 128 on two GPUs.  We start with a learning rate of 0.1, divide it by 10 at 32k and 48k iterations, and terminate training at 64k iterations, which is determined on a 45k/5k train/val split.  We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip.  For testing, we only evaluate the single view of the original 32×32 image.  We compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and 56-layer networks.  Fig. 6 (left) shows the behaviors of the plain nets.  The deep plain nets suffer from increased depth, and exhibit higher training error when going deeper.  This phenomenon is similar to that on ImageNet (Fig. 4, left) and on MNIST (see [42]), suggesting that such an optimization difficulty is a fundamental problem.  Fig. 6 (middle) shows the behaviors of ResNets.  Also similar to the ImageNet cases (Fig. 4, right), our ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases.  We further explore n = 18 that leads to a 110-layer ResNet.  In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5.  So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training.  The rest of the learning schedule is as done previously.  This 110-layer network converges well (Fig. 6, middle).  It has fewer parameters than other deep and thin 5With an initial learning rate of 0.1, it starts converging (3% over VGG-16.  This gain is solely because of the improved features learned by ResNet.  MS COCO The MS COCO dataset  [26] involves 80 object categories.  We evaluate the PASCAL VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = .5:.05:.95).  We use the 80k images on the train set for training and the 40k images on the val set for evaluation.  Our detection system for COCO is similar to that for PASCAL VOC.  We train the COCO models with an 8-GPU implementation, and thus the RPN step has a mini-batch size of 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images.  The RPN step and Fast RCNN step are both trained for 240k iterations with a learning rate of 0.001 and then for 80k iterations with 0.0001.  Table 8 shows the results on the MS COCO validation set.  ResNet-101 has a 6% increase of mAP@[.5, .95] over VGG-16, which is a 28% relative improvement, solely contributed by the features learned by the better network.  Remarkably, the mAP@[.5, .95]'s absolute increase (6.0%) is nearly as big as mAP@.5's (6.9%).  This suggests that a deeper network can improve both recognition and localization.  For completeness, we report the improvements made for the competitions.  These improvements are based on deep features and thus should benefit from residual learning.  MS COCO Box refinement.  Our box refinement partially follows the iterative localization in [6].  In Faster R-CNN, the final output is a regressed box that is different from its proposal box.  So for inference, we pool a new feature from the regressed box and obtain a new classification score and a new regressed box.  We combine these 300 new predictions with the original 300 predictions.  Non-maximum suppression (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6].  Box refinement improves mAP by about 2 points (Table 9).  Global context.  We combine global context in the Fast R-CNN step.  Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling  [12] (with a \"single-level\" pyramid) which can be implemented as \"RoI\" pooling using the entire image's bounding box as the RoI. This pooled feature is fed into the post-RoI layers to obtain a global context feature.  This global feature is concatenated with the original per-region feature, followed by the sibling classification and box regression layers.  This new structure is trained end-to-end.  Global context improves mAP@.5 by about 1 point (Table 9).  Multi-scale testing.  In the above, all results are obtained by single-scale training/testing as in [32], where the image's shorter side is s = 600 pixels.  Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using maxout layers.  In our current implementation, we have performed multi-scale testing following [33]; we have not performed multi-scale training because of limited time.  In addition, we have performed multi-scale testing only for the Fast R-CNN step (but not yet for the RPN step).  With a trained model, we compute conv feature maps on an image pyramid, where the image's shorter sides are s ∈{200, 400, 600, 800, 1000}.", "metadata": {"source_file": "1512.03385v1.pdf", "title": "[cs.CV] 10 Dec 2015 Deep Residual Learning for Image Recognition Kaiming He", "authors": ["Xiangyu Zhang", "Jian Sun"], "year": "2015", "detected_language": "en", "page_count": 12, "origin_chunk_file": "1512.03385v1.chunks.json"}}
{"text": "training data COCO train COCO trainval test data COCO val COCO test-dev mAP @.5 @[.5, .95] @.5 @[.5, .95] baseline Faster R-CNN (VGG-16) 41.5 21.2 baseline Faster R-CNN (ResNet-101) 48.4 27.2 +box refinement 49.9 29.9 +context 51.1 30.0 53.3 32.2 +multi-scale testing 53.8 32.5 55.7 34.9 ensemble 59.0 37.4 system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv baseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 baseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0 baseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8 Table 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \"baseline+++\" include box refinement, context, and multi-scale testing in Table 9. system net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv baseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 baseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6 baseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0 Table 11. Detection results on the PASCAL VOC 2012 test set ( displaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system \"baseline+++\" include box refinement, context, and multi-scale testing in Table 9. We select two adjacent scales from the pyramid following [33]. RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33]. Multi-scale testing improves the mAP by over 2 points (Table 9).  Using validation data.  Next we use the 80k+40k trainval set for training and the 20k test-dev set for evaluation.  The testdev set has no publicly available ground truth and the result is reported by the evaluation server.  Under this setting, the results are an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9% (Table 9).  This is our single-model result.  Ensemble.  In Faster R-CNN, the system is designed to learn region proposals and also object classifiers, so an ensemble can be used to boost both tasks.  We use an ensemble for proposing regions, and the union set of proposals are processed by an ensemble of per-region classifiers.  Table 9 shows our result based on an ensemble of 3 networks.  The mAP is 59.0% and 37.4% on the test-dev set.  This result won the 1st place in the detection task in COCO 2015.  Table 12.  Our results (mAP, %) on the ImageNet detection dataset.  Our detection system is Faster R-CNN  [32] with the improvements in Table 9, using ResNet-101.  we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11)6.  The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6].  ImageNet Detection The ImageNet Detection (DET) task involves 200 object categories.  The accuracy is evaluated by mAP@.5.  Our object detection algorithm for ImageNet DET is the same as that for MS COCO in Table 9.  The networks are pretrained on the 1000-class ImageNet classification set, and are fine-tuned on the DET data.  We split the validation set into two parts (val1/val2) following [8].  We fine-tune the detection models using the DET training set and the val1 set.  The val2 set is used for validation.  We do not use other ILSVRC 2015 data.  Our single model with ResNet-101 has top-5 LOC error on predicted CLS VGG's [41] VGG-16 1-crop 33.1 [41] RPN ResNet-101 1-crop 13.3 RPN ResNet-101 dense 11.7 RPN ResNet-101 dense ResNet-101 14.4 RPN+RCNN ResNet-101 dense ResNet-101 10.6 RPN+RCNN ensemble dense ensemble 8.9 Table 13.  Localization error (%) on the ImageNet validation.  In the column of \"LOC error on GT class\" ([41]), the ground truth class is used.  In the \"testing\" column, \"1-crop\" denotes testing on a center crop of 224×224 pixels, \"dense\" denotes dense (fully convolutional) and multi-scale testing.  58.8% mAP and our ensemble of 3 models has 62.1% mAP on the DET test set (Table 12).  This result won the 1st place in the ImageNet detection task in ILSVRC 2015, surpassing the second place by 8.5 points (absolute).  The ImageNet Localization (LOC) task [36] requires to classify and localize the objects.  Following [40, 41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes.  We adopt the \"per-class regression\" (PCR) strategy  [40, 41], learning a bounding box regressor for each class.  We pre-train the networks for ImageNet classification and then fine-tune them for localization.  We train networks on the provided 1000-class ImageNet training set.  Our localization algorithm is based on the RPN framework of [32] with a few modifications.  Unlike the way in [32] that is category-agnostic, our RPN for localization is designed in a per-class form.  This RPN ends with two sibling 1×1 convolutional layers for binary classification (cls) and box regression (reg), as in [32].  The cls and reg layers are both in a per-class from, in contrast to [32].  Specifically, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not being an object class; the reg layer has a 1000×4-d output consisting of box regressors for 1000 classes.  As in [32], our bounding box regression is with reference to multiple translation-invariant \"anchor\" boxes at each position.  As in our ImageNet classification training (Sec. 3.4), we randomly sample 224×224 crops for data augmentation.  We use a mini-batch size of 256 images for fine-tuning.  To avoid negative samples being dominate, 8 anchors are randomly sampled for each image, where the sampled positive and negative anchors have a ratio of 1:1  [32].  For testing, the network is applied on the image fully-convolutionally.  Table 13 compares the localization results.  Following [41], we first perform \"oracle\" testing using the ground truth class as the classification prediction.  VGG's paper  [41] reval test OverFeat [40] (ILSVRC'13) 30.0 29.9 GoogLeNet  [44] (ILSVRC'14) -\n26.7 VGG  [41] (ILSVRC'14) 26.9 25.3 ours (ILSVRC'15) 8.9 9.0 ports a center-crop error of 33.1% (Table 13) using ground truth classes.  Under the same setting, our RPN method using ResNet-101 net significantly reduces the center-crop error to 13.3%.  This comparison demonstrates the excellent performance of our framework.  With dense (fully convolutional) and multi-scale testing, our ResNet-101 has an error of 11.7% using ground truth classes.  Using ResNet-101 for predicting classes (4.6% top-5 classification error, Table 4), the top-5 localization error is 14.4%.  The above results are only based on the proposal network (RPN) in Faster R-CNN [32].  One may use the detection network (Fast R-CNN [7]) in Faster R-CNN to improve the results.  But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar RoI-pooled features.  As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training.  Motivated by this, in our current experiment we use the original RCNN  [8] that is RoI-centric, in place of Fast R-CNN.  Our R-CNN implementation is as follows.  We apply the per-class RPN trained as above on the training images to predict bounding boxes for the ground truth class.  These predicted boxes play a role of class-dependent proposals.  For each training image, the highest scored 200 proposals are extracted as training samples to train an R-CNN classifier.  The image region is cropped from a proposal, warped to 224×224 pixels, and fed into the classification network as in R-CNN [8].  The outputs of this network consist of two sibling fc layers for cls and reg, also in a per-class form.  This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the RoI-centric fashion.  For testing, the RPN generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals' scores and box positions.  This method reduces the top-5 localization error to 10.6% (Table 13).  This is our single-model result on the validation set.  Using an ensemble of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set.  This number significantly outperforms the ILSVRC 14 results (Table 14), showing a 64% relative reduction of error.  This result won the 1st place in the ImageNet localization task in ILSVRC 2015.", "metadata": {"source_file": "1512.03385v1.pdf", "title": "[cs.CV] 10 Dec 2015 Deep Residual Learning for Image Recognition Kaiming He", "authors": ["Xiangyu Zhang", "Jian Sun"], "year": "2015", "detected_language": "en", "page_count": 12, "origin_chunk_file": "1512.03385v1.chunks.json"}}
{"text": "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.  Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.  †Work performed while at Google Brain.  ‡Work performed while at Google Research.  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5].  Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures  [38, 24, 15].  Recurrent models typically factor computation along the symbol positions of the input and output sequences.  Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t.  This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.  Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation  [32], while also improving model performance in case of the latter.  The fundamental constraint of sequential computation, however, remains.  Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19].  In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.  In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.  The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.  The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU  [16], ByteNet  [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.  In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.  This makes it more difficult to learn dependencies between distant positions [12].  In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.  Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.  Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].  End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks  [34].  To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.  In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].  Most competitive neural sequence transduction models have an encoder-decoder structure  [5, 2, 35].  Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn).  Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time.  At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.  The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.  Encoder: The encoder is composed of a stack of N = 6 identical layers.  Each layer has two sub-layers.  The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.  We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization  [1].  That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.  To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.  Decoder: The decoder is also composed of a stack of N = 6 identical layers.  In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.  We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.  We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2).  The input consists of queries and keys of dimension dk, and values of dimension dv.  We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.  In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.  The keys and values are also packed together into matrices K and V .  We compute the matrix of outputs as: The two most commonly used attention functions are additive attention  [2], and dot-product (multiplicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of\n√dk .  Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.  While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3].  We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4.  To counteract this effect, we scale the dot products by\n√dk .  4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1.  Then their dot product, q · k = Pdk i=1 qiki, has mean 0 and variance dk.  Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.  With a single attention head, averaging inhibits this.  Where the projections are parameter matrices W Q i\n∈Rdmodel×dk, W K  i\n∈Rdmodel×dk, W V i\n∈Rdmodel×dv and W O ∈Rhdv×dmodel.  In this work we employ h = 8 parallel attention layers, or heads.  For each of these we use dk  = dv = dmodel/h = 64.  Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.  •  In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.  This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].  •  The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.  Each position in the encoder can attend to all positions in the previous layer of the encoder.  •  Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.  See Figure 2.  In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.  This consists of two linear transformations with a ReLU activation in between.  While the linear transformations are the same across different positions, they use different parameters from layer to layer.  Another way of describing this is as two convolutions with kernel size 1.  The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.  Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel.  We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30].  In the embedding layers, we multiply those weights by √dmodel.  Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.  n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.  Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1)  Recurrent O(n · d2) O(n) O(n)  Convolutional O(k · n · d2) O(1) O(logk(n))  Self-Attention (restricted) O(r · n · d) O(1) O(n/r)  Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.  There are many choices of positional encodings, learned and fixed [9].  where pos is the position and i is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from 2π to 10000 · 2π.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.  In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden layer in a typical sequence transduction encoder or decoder.  Motivating our use of self-attention we consider three desiderata.  One is the total computational complexity per layer.  Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.  The third is the path length between long-range dependencies in the network.  Learning long-range dependencies is a key challenge in many sequence transduction tasks.  One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.  The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12].  Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.  As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.  In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.  To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position.  This would increase the maximum path length to O(n/r).  We plan to investigate this approach further in future work.  As side benefit, self-attention could yield more interpretable models.  We inspect attention distributions from our models and present and discuss examples in the", "metadata": {"source_file": "1706.03762v7.pdf", "title": "[cs.CL] 2 Aug 2023 Attention Is All You Need Provided proper attribution is provided, Google hereby grants permission to", "authors": ["Noam", "Aidan N. Gomez∗"], "year": "2024", "detected_language": "en", "page_count": 15, "origin_chunk_file": "1706.03762v7.chunks.json"}}
{"text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \"next sentence prediction\" task that jointly pretrains text-pair representations.  The contributions of our paper are as follows: •  We demonstrate the importance of bidirectional pre-training for language representations.  Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations.  This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.  •  We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures.  BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures.  •  BERT advances the state of the art for eleven NLP tasks.  The code and pre-trained models are available at google-research/bert.  There is a long history of pre-training general language representations, and we briefly review the most widely-used approaches in this section.  Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods.  Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010).  To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013).  These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014).  To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).  ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension.  They extract context-sensitive features from a left-to-right and a right-to-left language model.  The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations.  When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003).  Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs.  Similar to ELMo, their model is feature-based and not deeply bidirectional.  Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models.  As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008).  More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).  The advantage of these approaches is that few parameters need to be learned from scratch.  At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang et al., 2018a).  Left-to-right language modelFigure 1: Overall pre-training and fine-tuning procedures for BERT.  Apart from output layers, the same architectures are used in both pre-training and fine-tuning.  The same pre-trained model parameters are used to initialize models for different down-stream tasks.  During fine-tuning, all parameters are fine-tuned.  [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token  (e.g. separating questions/answers).  ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).  There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017).  Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).  We introduce BERT and its detailed implementation in this section.  There are two steps in our framework: pre-training and fine-tuning.  During pre-training, the model is trained on unlabeled data over different pre-training tasks.  For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks.  Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters.  The question-answering example in Figure 1 will serve as a running example for this section.  A distinctive feature of BERT is its unified architecture across different tasks.", "metadata": {"source_file": "1810.04805v2.pdf", "title": "[cs.CL] 24 May 2019 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Wei Chang", "Kenton Lee Kristina Toutanova"], "year": "2019", "detected_language": "en", "page_count": 16, "origin_chunk_file": "1810.04805v2.chunks.json"}}
{"text": "There is miniModel Architecture BERT's model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as \"The Annotated Transformer.\"2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensorflow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/filter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional TransInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨Question, Answer ⟩) in one token sequence. Throughout this work, a \"sentence\" can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \"sequence\" refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]).  The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks.  Sentence pairs are packed together into a single sequence.  We differentiate the sentences in two ways.  First, we separate them with a special token ([SEP]).  Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B.  As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as C ∈RH, and the final hidden vector for the ith input token as Ti ∈RH.  For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.  A visualization of this construction can be seen in Figure 2.  Unlike Peters et al. (2018a) and Radford et al. (2018), we do not use traditional left-to-right or right-to-left language models to pre-train BERT.  Instead, we pre-train BERT using two unsupervised tasks, described in this section.  This step is presented in the left part of Figure 1.  Task #1: Masked LM  Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model.  Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \"see itself\", and the model could trivially predict the target word in a multi-layered context.  In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.  We refer to this procedure as a \"masked LM\" (MLM), although it is often referred to as a Cloze task in the literature (Taylor, 1953).  In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.  In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random.  In contrast to denoising auto-encoders (Vincent et al., 2008), we only predict the masked words rather than reconstructing the entire input.  Although this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning.  To mitigate this, we do not always replace \"masked\" words with the actual [MASK] token.  The training data generator chooses 15% of the token positions at random for prediction.  If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.  Then, Ti will be used to predict the original token with cross entropy loss.  We compare variations of this procedure in", "metadata": {"source_file": "1810.04805v2.pdf", "title": "[cs.CL] 24 May 2019 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Wei Chang", "Kenton Lee Kristina Toutanova"], "year": "2019", "detected_language": "en", "page_count": 16, "origin_chunk_file": "1810.04805v2.chunks.json"}}
{"text": "Metro News Column. Mercedes Olivera. The Dallas Morning News. May 6, 2006 (<URL>). \"The first time 11year-old Christian Gomez saw a real robot, it was two months ago at NASA's Johnson Space Center. He was impressed. But he decided he doesn't trust robots. 'Someday they're going to build them with artificial intelligence, and terrorists might take control of them and use them to hurt us,' he said this week. It's a very precocious observation. But his reaction about the trip to NASA was very typical: 'It was the most important thing I've ever done.' That's how the rest of the 11 students from Obadiah Knight Elementary School also felt about the annual field trip to Houston. ... It was the high point in the school year for the students who are in the school's talented and gifted program and who had been studying a space-centered curriculum. ... For the past 12 years, Southwest Airlines has funded the trip to help students see firsthand the wonders of science.\" Curriculum—Fascinating, fun and, yes, that's science. Deedee Cuddihy. TES—The Times Educational Supplement. February 24, 2006 (<URL>). \"Forget about the facts; concentrate on asking questions. Get your geek on. Ethan Gilsdorf. The Boston Globe. February 14,\n(<URL>). \"'Popular culture is a great way for getting people interested in science,' says Ed Rodley, a [Boston] Museum of Science curator. 'There's a whole generation of roboticists who saw \"2001: A Space Odyssey\" and wanted to do artificial intelligence. Ten years behind them are \"Star Wars\" fans.' Last year, the museum's 'Lord of the Rings' exhibit was a huge hit. The current show 'Star Wars: Where Science Meets Imagination' set a record, attracting 15,193 visitors in one day.\" Second thoughts about the mission? Leslie Brokaw. The Boston Globe. February 19, 2006 (<URL>). \"Artificial intelligence founding father, MIT professor emeritus, and voice-of-HAL-consultant Marvin Minsky will be taking questions tomorrow after the 7 p.m. screening of Stanley Kubrick's '2001: A Space Odyssey' at the Coolidge Corner Theatre.  Back when Kubrick was developing the movie, which was released in 1968, he turned to Minsky for advice on making the talking computer accurate.\"  Open the pod bay doors, HAL.  New Scientist (Issue 2550: ).  May 4, 2006 (<URL>).  \"HAL 9000, the chatty computer from 2001: A Space Odyssey, has come a step closer to reality.  A team crewing NASA's Mars Desert Research Station, a simulated planetary environment in the Utah desert, has been experimenting this week with software that can talk to the crew about the status of their spacecraft's systems.  Using wireless headsets, crew members ask the computer questions ….\"  That's the philosophy behind the new <CUR>1 million Connect science and technology gallery.  Opened just last week, with the help of National Museums of Scotland funds and a gaggle of sponsors, the gallery at the Royal Museum offers a wealth of interactive, visually-stunning and unique displays. ...  The exhibition space has been divided into five main subject areas covering transport (Move It!), artificial intelligence (Robots), cloning (Me2), space travel (Blast Off!) and energy (Power Up).  Each subject area is designed around a number of significant museum objects, complemented by a range of specially designed interactives.\"  The items in this collage were selected from the AI TOPICS Web site's \"AI in the News\" collection that can be found—complete with links to the item's source and related AI TOPICS pages—at www.  aaai.org/aitopics/ html/current.html.  Please note that: (1) an excerpt may not reflect the overall tenor of the item, nor contain all of the relevant information; and, (2) all items are offered \"as is\" and the fact that an item has been selected does not imply any endorsement whatsoever.", "metadata": {"source_file": "1887-Article Text-1883-1-10-20080129.pdf", "title": "AI in the News", "authors": ["Jon Glick"], "year": "2006", "detected_language": "en", "page_count": 1, "origin_chunk_file": "1887-Article Text-1883-1-10-20080129.chunks.json"}}
{"text": "Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1 Training a machine learning model to perform natural language processing (NLP) tasks often requires that the model can process text in a way that is amenable to downstream learning. This can be loosely viewed as developing general-purpose knowledge that allows the model to \"understand\" text. This knowledge can range from low-level (e.g. the spelling ©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. or meaning of words) to high-level (e.g. that a tuba is too large to fit in most backpacks). In modern machine learning practice, providing this knowledge is rarely done explicitly; instead, it is often learned as part of an auxiliary task. For example, a historically common approach is to use word vectors (Mikolov et al., 2013b,a; Pennington et al., 2014) to map word identities to a continuous representation where, ideally, similar words map to similar vectors. These vectors are often learned through an objective that, for example, encourages co-occurring words to be positioned nearby in the continuous space (Mikolov et al., 2013b).  Recently, it has become increasingly common to pre-train the entire model on a data-rich task.  Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks.  In applications of transfer learning to computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014), pre-training is typically done via supervised learning on a large labeled data set like ImageNet (Russakovsky et al., 2015; Deng et al., 2009).  In contrast, modern techniques for transfer learning in NLP often pre-train using unsupervised learning on unlabeled data.  This approach has recently been used to obtain state-of-the-art results in many of the most common NLP benchmarks (Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019c; Lan et al., 2019).  Beyond its empirical strength, unsupervised pre-training for NLP is particularly attractive because unlabeled text data is available en masse thanks to the Internet—for example, the Common Crawl project2 produces about 20TB of text data extracted from web pages each month.  This is a natural fit for neural networks, which have been shown to exhibit remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger data set (Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018; Huang et al., 2018b; Keskar et al., 2019a).", "metadata": {"source_file": "1910.10683v4.pdf", "title": "[cs.LG] 19 Sep 2023 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Adam Roberts∗", "Katherine Lee∗ katherinelee@.com", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Ivan Titov"], "year": "2023", "detected_language": "en", "page_count": 67, "origin_chunk_file": "1910.10683v4.chunks.json"}}
{"text": "This synergy has resulted in a great deal of recent work developing transfer learning methodology for NLP, which has produced a wide landscape of pre-training objectives (Howard and Ruder, 2018; Devlin et al., 2018; Yang et al., 2019; Dong et al., 2019), unlabeled data sets (Yang et al., 2019; Liu et al., 2019c; Zellers et al., 2019), benchmarks (Wang et al., 2019b, 2018; Conneau and Kiela, 2018), fine-tuning methods (Howard and Ruder, 2018; Houlsby et al., 2019; Peters et al., 2019), and more. The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field. The basic idea underlying our work is to treat every text processing problem as a \"text-to-text\" problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document Figure 1: A diagram of our text-to-text framework. Every task we consider—including translation, question answering, and classification—is cast as feeding our model text as input and training it to generate some target text. This allows us to use the same model, loss function, hyperparameters, etc. across our diverse set of tasks.  It also provides a standard testbed for the methods included in our empirical survey.  \"T5\" refers to our model, which we dub the \"Text-to-Text Transfer Transformer\".  summarization, and sentiment classification, to name a few.  With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered.  We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands.  As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques.  We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.  In order to perform experiments at this scale, we introduce the \"Colossal Clean Crawled Corpus\" (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web.", "metadata": {"source_file": "1910.10683v4.pdf", "title": "[cs.LG] 19 Sep 2023 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Adam Roberts∗", "Katherine Lee∗ katherinelee@.com", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Ivan Titov"], "year": "2023", "detected_language": "en", "page_count": 67, "origin_chunk_file": "1910.10683v4.chunks.json"}}
{"text": "Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1 The remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4. Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our \"Colossal Clean Crawled Corpus\" (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the \"Text-to-Text Transfer Transformer\" (T5). mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal selfattention, which only allows the model to attend to past outputs. The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent \"heads\" whose outputs are concatenated before being further processed. Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer.  While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).  Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the \"key\" and \"query\" being compared in the self-attention mechanism.  We use a simplified form of position embeddings where each \"embedding\" is simply a scalar that is added to the corresponding logit used for computing the attention weights.  For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding.  Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets.  In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding.  Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.  To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme.  Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work.  As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers.  Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation.  As a result, we use a combination of model and data parallelism and train models on \"slices\" of Cloud TPU Pods.5  TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines.  We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).  Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning.  In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data.  To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web.", "metadata": {"source_file": "1910.10683v4.pdf", "title": "[cs.LG] 19 Sep 2023 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Adam Roberts∗", "Katherine Lee∗ katherinelee@.com", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Ivan Titov"], "year": "2023", "detected_language": "en", "page_count": 67, "origin_chunk_file": "1910.10683v4.chunks.json"}}
{"text": "Common Crawl has previously been used as a source of text data for NLP, for example to train an n-gram language model (Buck et al., 2014), as training data for commonsense reasoning (Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013), as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and even simply as a giant text corpus for testing optimizers (Anil et al., 2019). Common Crawl is a publicly-available web archive that provides \"web extracted text\" by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl's web extracted text: • We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark). • We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words. • We removed any page that contained any word on the \"List of Dirty, Naughty, Obscene or Otherwise Bad Words\".6 • Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript. • Some pages inadvertently contained code. Since the curly bracket \"{\" appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket. • Since some of the scraped pages were sourced from Wikipedia and had citation markers (e.g. [1], [citation needed], etc.), we removed any such markers.  •  Many pages had boilerplate policy notices, so we removed any lines containing the strings \"terms of use\", \"privacy policy\", \"cookie policy\", \"uses cookies\", \"use of cookies\", or \"use cookies\".  •  To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set.  Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect7 to filter out any pages that were not classified as English with a probability of at least 0.99.  Our heuristics are inspired by past work on using Common Crawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication.  However, we opted to create a new data set because prior data sets use a more limited set of filtering heuristics, are not publicly available, and/or are different in scope (e.g. are limited to News data (Zellers et al., 2019;  Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training data for machine translation (Smith et al., 2013)).  To assemble our base data set, we downloaded the web extracted text from April 2019 and applied the aforementioned filtering.  This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text.  We dub this data set the \"Colossal Clean Crawled Corpus\" (or C4 for short) and release it as part of TensorFlow Datasets.8 Our goal in this paper is to measure general language learning abilities.  As such, we study downstream performance on a diverse set of benchmarks, including machine translation, question answering, abstractive summarization, and text classification.  Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.", "metadata": {"source_file": "1910.10683v4.pdf", "title": "[cs.LG] 19 Sep 2023 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Adam Roberts∗", "Katherine Lee∗ katherinelee@.com", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Ivan Titov"], "year": "2023", "detected_language": "en", "page_count": 67, "origin_chunk_file": "1910.10683v4.chunks.json"}}
{"text": "All data was sourced from TensorFlow Datasets.9 GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a collection of text classification tasks meant to test general language understanding abilities: In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a \"text-to-text\" format—that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using \"teacher forcing\" (Williams and Zipser, 1989)) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model. As an example, to ask the model to translate the sentence \"That is good.\" from English to German, the model would be fed the sequence \"translate English to German: That is good.\" and would be trained to output \"Das ist gut.\" For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies (\"entailment\"), contradicts (\"contradiction\"), or neither (\"neutral\") a hypothesis. With our preprocessing, the input sequence becomes \"mnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\" with the corresponding target word \"entailment\". Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs \"hamburger\" when the only possible labels for a task were \"entailment\", \"neutral\", or \"contradiction\"). In this case, we always count the model's output as wrong, though we never observed this behavior in any of our trained models.  Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices.  A diagram of our text-to-text framework with a few input/output examples is shown in Figure 1.  We provide full examples of preprocessed inputs for every task we studied in", "metadata": {"source_file": "1910.10683v4.pdf", "title": "[cs.LG] 19 Sep 2023 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "authors": ["Adam Roberts∗", "Katherine Lee∗ katherinelee@.com", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu", "Ivan Titov"], "year": "2023", "detected_language": "en", "page_count": 67, "origin_chunk_file": "1910.10683v4.chunks.json"}}
{"text": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Approach\n2.1 Model and Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Results\n3.1 Language Modeling, Cloze, and Completion Tasks . . . . . . . . . . . . . . . . . . . . . . . . . .  .\n3.2 Closed Book Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  3.5 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  3.6 Reading Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  3.9 Synthetic and Qualitative Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  Broader Impacts\n6.1 Misuse of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .  First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models.  There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story.  For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task.  Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer.", "metadata": {"source_file": "2005.14165v4.pdf", "title": "[cs.CL] 22 Jul 2020 Language Models are Few-Shot Learners OpenAI", "authors": ["Tom B. Brown∗", "Benjamin Mann∗", "Nick Ryder∗", "Melanie Subbiah∗", "Amanda Askell Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger Tom Henighan Rewon", "Daniel M. Ziegler", "Jeffrey Wu", "Christopher Hesse", "Mark Chen Eric Sigler", "Mateusz Litwin", "Scott Gray Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford"], "year": "2020", "detected_language": "en", "page_count": 75, "origin_chunk_file": "2005.14165v4.chunks.json"}}
{"text": "First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons. Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [YdC+19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].  Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. \"please tell me if this sentence describes something happy or something sad\") or at most a tiny number of demonstrations (e.g. \"here are two examples of people acting brave; please give a third example of bravery\") is often Figure 1.1: Language model meta-learning.  During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities.  It then uses these abilities at inference time to rapidly adapt to or recognize the desired task.  We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence.  The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence.  Figure 1.2:  Larger models make increasingly efficient use of in-context information.  We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2).  The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information.  We see qualitatively similar behavior across a wide range of tasks.  sufficient to enable a human to perform a new task to at least a reasonable degree of competence.  Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue.  To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.  One potential route towards addressing these issues is meta-learning1 – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1).  Recent work [RWC+19] attempts to do this via what we call \"in-context learning\", using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.  While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example [RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art.  Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks.  Another recent trend in language modeling may offer a way forward.  In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters  [DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19], and finally 17 billion parameters [Tur20].  Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20].  Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.  1In the context of language models this has sometimes been called \"zero-shot transfer\", but this term is potentially ambiguous: the method is \"zero-shot\" in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples.  To avoid this confusion, we use the term \"meta-learning\" to capture the inner-loop / outer-loop structure of the general method, and the term \"in context-learning\" to refer to the inner loop of meta-learning.  We further specialize the description to \"zero-shot\", \"one-shot\", or \"few-shot\" depending on how many demonstrations are provided at inference time.  These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which we discuss later in the paper, but \"meta-learning\" is intended to encompass both possibilities, and simply describes the inner-outer loop structure.  Figure 1.3:  Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning.  See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite.  In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities.  Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.", "metadata": {"source_file": "2005.14165v4.pdf", "title": "[cs.CL] 22 Jul 2020 Language Models are Few-Shot Learners OpenAI", "authors": ["Tom B. Brown∗", "Benjamin Mann∗", "Nick Ryder∗", "Melanie Subbiah∗", "Amanda Askell Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger Tom Henighan Rewon", "Daniel M. Ziegler", "Jeffrey Wu", "Christopher Hesse", "Mark Chen Eric Sigler", "Mateusz Litwin", "Scott Gray Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford"], "year": "2020", "detected_language": "en", "page_count": 75, "origin_chunk_file": "2005.14165v4.chunks.json"}}
{"text": "For each task, we evaluate GPT-3 under 3 conditions: (a) \"few-shot learning\", or in-context learning where we allow as many demonstrations as will fit into the model's context window (typically 10 to 100), (b) \"one-shot learning\", where we allow only one demonstration, and (c) \"zero-shot\" learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model's context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these \"learning\" curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning. Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting. GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once.  We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles.  At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3.  This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.  By presenting a broad characterization of GPT-3's strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.  A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself).  We also undertake a systematic study of \"data contamination\" – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web.  In this paper we develop systematic tools to measure data contamination and quantify its distorting effects.  Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity.  In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings.  Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners.  Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3's characteristics in this regard.  The remainder of this paper is organized as follows.  In Section 2, we describe our approach and methods for training GPT-3 and evaluating it.  Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.  Section 4 addresses questions of data contamination (train-test overlap).  Section 5 discusses limitations of GPT-3.  Section 6 discusses broader impacts.  Section 7 reviews related work and Section 8 concludes.  Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training.  Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context.  Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on.  These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on.  Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): • Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task.  Typically thousands to hundreds of thousands of labeled examples are used.  The main advantage of fine-tuning is strong performance on many benchmarks.  The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data  [GSL+18, NK19], potentially resulting in an unfair comparison with human performance.  In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work.  •  Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed.  As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one final example of context, with the model expected to provide the completion.  We typically set K in the range of 10 to 100 as this is how many examples can fit in the model's context window (nctx = 2048).  The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset.  The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models.  Also, a small amount of task specific data is still required.  As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL+16] – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task.  •  One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1.  The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task.  By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given.  Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning.  The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time.  We typically present the model with a few dozen examples in the few shot setting.  Exact phrasings for all task descriptions, examples and prompts can be found in", "metadata": {"source_file": "2005.14165v4.pdf", "title": "[cs.CL] 22 Jul 2020 Language Models are Few-Shot Learners OpenAI", "authors": ["Tom B. Brown∗", "Benjamin Mann∗", "Nick Ryder∗", "Melanie Subbiah∗", "Amanda Askell Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger Tom Henighan Rewon", "Daniel M. Ziegler", "Jeffrey Wu", "Christopher Hesse", "Mark Chen Eric Sigler", "Mateusz Litwin", "Scott Gray Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford"], "year": "2020", "detected_language": "en", "page_count": 75, "origin_chunk_file": "2005.14165v4.chunks.json"}}
{"text": "Text Generation aims to produce plausible and readable text in a human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (PLMs). Text generation based on PLMs is viewed as a promising approach in both academia and industry. In this paper, we provide a survey on the utilization of PLMs in text generation. We begin with introducing three key aspects of applying PLMs to text generation: 1) how to encode the input into representations preserving input semantics which can be fused into PLMs; 2) how to design an effective PLM to serve as the generation model; and 3) how to effectively optimize PLMs given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on PLMs. Finally, we highlight the future research directions which will further improve these PLMs for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on PLMs. INTRODUCTION Text generation, also known as natural language generation, has been one of the most important sub-fields in natural language processing (NLP). It aims to produce plausible and readable text in a human language, from the input data in various forms including text, image, table and knowledge base. In the last decades, text generation techniques have been extensively applied to a wide range of applications. For example, they have been used in dialog systems to generate responses to user Fig. 1. An illustrative process of applying PLMs to text generation. We divide the process into three main steps: input representation learning, model architecture design and selection, model parameters optimization.  utterances in a conversation [224], in machine translation to translate a text from one language into another [31]; and in text summarization to generate an abridged summary of the source text  [40].  The primary goal of text generation is to automatically learn an input-to-output mapping from the data to construct an end-to-end solution with minimal human intervention.  This mapping function allows the generation system to generalize in a broader field and to generate free text under the given conditions.  Earlier approaches usually adopt statistical language models for modeling the conditional probabilities of words given an 𝑛-gram context  [11, 13].  Such a statistical approach is known to suffer from the data sparsity issue, and a number of smoothing methods have been developed to alleviate this problem so as to better estimate unobserved term occurrences [179, 210].  Still, word tokens are used as the basic representation units in these approaches, which leads to the issue that similar tokens cannot be easily mapped with each other.  With the emergence of deep learning techniques [96], neural network models have dominated the mainstream methods in text generation and make exceptional success in generating natural language texts.  Deep neural generation models usually adopt the sequence-to-sequence framework  [177] based on the encoder-decoder scheme: the encoder first maps the input sequence into fix-sized lowdimensional vectors (called input embeddings), and then the decoder generates a target text based on the input embeddings.  The representation by embeddings makes a key difference from earlier statistical approaches, which makes it easier to cope with the possible relations between inputs and outputs.  Various neural models have been proposed with different designs for the encoderdecoder architecture, such as graph neural networks (GNN) for encoding graph inputs [102] and recurrent neural networks (RNN) for decoding texts [108].  Besides, the attention mechanism  [2] and copy mechanism [164] are widely used to improve the performance of text generation models.  An important merit of deep neural networks for text generation is that they enable end-to-end learning of semantic mappings from the input data to output texts without labor-intensive feature engineering.  Moreover, deep neural models employ low-dimensional semantic representations  [82] to capture linguistic features of language, which is useful to alleviate data sparsity.  Despite the success of deep neural models for text generation, a major performance bottleneck lies in the availability of large-scale labelled datasets.  Most of text generation methods require substantial amounts of manually labelled parallel data, which restricts their applicability in many domains that suffer from a dearth of annotated examples.  To date, most of existing labelled datasets for text generation tasks are usually small.  In such cases, deep neural networks are likely to overfit on these small datasets and do not generalize well in practice.  Moreover, the early neural models for text generation were still relatively shallow with only 1~3 neural layers.  Therefore, these models have difficulties in modeling intricate relationships between the context and word meanings and deriving contextual word representations for better generation [151].  In recent years, the paradigm of pre-trained language models (PLMs) is thriving in NLP  [151].  The basic idea is to first pre-train the models on large-scale unsupervised corpora and then finetune these models in downstream supervised tasks.  Such a pretraining-finetuning framework In this section, we first give a general task definition of text generation, then describe the background of PLMs, and finally introduce the three key aspects on PLM-based text generation methods.  Generally, a text can be modeled as a sequence of tokens 𝑦= ⟨𝑦1, ...,𝑦𝑗, ...,𝑦𝑛⟩, where each token 𝑦𝑗 is drawn from a vocabulary V.  The task of text generation aims to generate plausible and readable text in a human language.  In most cases, text generation is conditioned on some input data (e.g., text, image, tabular, and knowledge base), which is denoted as 𝑥. In particular, the generated text is expected to satisfy some desired language properties such as fluency, naturalness, and coherence.  We denote the desired properties for output text as a property set P. Based on the above notations, the task of text generation can be formally described as: where the text generation model 𝑓M produces the output text 𝑦given the input data 𝑥, satisfying some special proprieties from the property set P.  In this survey, the text generation model 𝑓M is specially crafted based on a PLM M. Specifically, according to the type of the input data 𝑥and the property set P, text generation can be instantiated into different kinds of tasks: • When the input data 𝑥is not provided or is a random vector, text generation will degenerate into language modeling or unconditional text generation  [152, 153].  In this case, the output text is required to satisfy some common language properties, such as fluency and naturalness.  •  When the input data 𝑥is a set of discrete attributes (e.g., topic words and sentiment labels), it becomes topic-to-text generation  [33] or attribute-based generation  [90].  The input data 𝑥plays the role of controlling the content of the generated text.  In such a situation, the output text should be relevant to the input topics or adhere to the required attributes.  •  When the input data 𝑥is structured data such as knowledge base or table, it is considered as data-to-text generation [60, 105].  This task aims to generate a descriptive text about the structured data.  Therefore, the output text should be objective and accurate.  •  When the input data 𝑥is multimedia input such as image and speech, it becomes image captioning [191] or speech recognition  [45].  We may expect that the caption text be lively for attracting children's attention, and the converted speech text be faithful to the original speech.  •  The most common form of input data 𝑥is a text sequence.  This form spans a number of applications such as machine translation [31], text summarization  [161] and dialog system  [215].  For a specific task, the output text is expected to satisfy desired properties.  For example, the summaries in text summarization should not contradict the facts described in the input text, and the responses in dialog should be relevant to the input dialog history and context.  Pre-trained language models (PLMs) are deep neural networks that are pre-trained on large-scale unlabelled corpora, which can be further fine-tuned on various downstream tasks.  It has been shown that PLMs can encode a significant amount of linguistic knowledge into their vast amounts of parameters  [106, 158].  Therefore, it is promising to apply PLMs to enhance the understanding of language and improve the generation quality.  Owing to the great success of Transformer [180], almost all PLMs employ it as the backbone.  As two typical PLMs, GPT [152] and BERT [35] are first built upon Transformer decoder and encoder respectively.  Following GPT and BERT, PLMs such as XLNet [198], RoBERTa  [124], ERNIE [217], T5  [154] and BART [99] are propopsed in the literature.  Among them, XLNet, RoBERTa and ERNIE are developed based on the BERT model, while T5 and BART are encoder-decoder based PLMs.  Recent studies have shown that the performance of PLMs can be boosted by increasing the scale of model parameters  [89], which triggered the development of large-scale PLMs such as GPT-3 (175B)  [14], PANGU (200B)  [207], GShard (600B)  [98] and Switch-Transformers (1.6T)  [46], which consist of billions or trillions of parameters.  In addition, PLMs are designed for other tasks such as named entity recognition [147], programming [49], and networking [127].  According to the pre-training objectives, PLMs for text generation can be categorized as masked LMs, causal LMs, prefix LMs, and encoder-decoder LMs, which will be detailed in Section 4.  To effectively leverage PLMs for downstream text generation tasks, we need to consider three key aspects from the perspectives of data, model, and optimization, respectively: • Input Data: How to encode the input 𝑥into a representation preserving the input semantics that can be fused into the PLM M?  For text generation, the input data, containing critical semantic information for the target output, often appears in various data types for different tasks (e.g., sequential text, structured table, multimedia), whereas most PLMs are typically pre-trained on the sequential text data.  Therefore, it is a major challenge to develop effective, flexible representation learning approaches for PLMs to capture semantic information from various types of input data.  •  Model Architecture: How to design an effective PLM M to serve as the generation function 𝑓M and adapt to various text generation tasks?  In the literature, a number of PLMs have been developed with generalized architectures for general purposes (e.g., denoised auto-encoder  [99] or auto-regressive decoder  [153]), While these general architectures cannot cope with some special text generation cases.  Therefore, it is important to make specific designs on the underlying PLMs for achieving good task performance when adapting to different text generation tasks.  •  Optimization Algorithm: How to optimize the text generation function (i.e., PLMs) 𝑓M given the reference text 𝑦and ensure that the generated text satisfies special text properties P?  In order to produce satisfactory text, it is critical to learn the text generation function by developing effective optimization algorithms.  A major challenge stems from the fact that some desired properties for output text are difficult to be formulated or optimized.  In the following sections, we will present recent research efforts on PLM-based text generation, with an emphasis on the three aforementioned aspects.  The overall organization of our description follows the schema shown in Figure 2.  As discussed in Section 2, the first aspect is the encoding of input data 𝑥into meaningful representations preserving input semantics for PLMs.  In this section, we will present three main types of input data for text generation, i.e., unstructured input, structured input, and multimedia input.  3.1 Unstructured Input In text generation, most studies focus on modeling unstructured text input (e.g., sentence, paragraph, and document), which requires to accurately understand the input information and derive meaningful text representations.  The aim of text representation learning is to condense the input text into low-dimensional vectors that can preserve the core semantic meanings.  In what follows, we will discuss how to derive effective semantic representations for three kinds of unstructured text data, namely paragraphs, documents and multi-lingual texts.  3.1.1 Paragraph Representation Learning.  A paragraph usually consists of multiple sentences describing different topics and each sentence contains a sequence of words.  To capture both low-level word meanings and high-level topic semantics in a paragraph, many studies proposed hierarchy-based or graph-based methods to learn the paragraph representation.  Hierarchy-based Representation Learning.  For a multi-sentence paragraph such as a multiturn dialogue, a typical approach is to concatenate sentences as a whole text and predict the output Modeling Inter-Sentential Semantics: BERTSumExt  [123]; HIBERT et al.  [214]; Capturing Critical Semantics:  Nguyen et al.  [136]; Liu et al.  [118]; RL Efficiency: Huang et al.  [79]; DANCER  [56]; Manakul et al.  [133] Incorporating Additional Objectives: TableGPT  [60]; Mager et al.  [130]; Adding Structural Information as Input: Ribeiro et al.  [158]; Fan et al.  [43]; Employing Structural Encoding Module: StructAdapt  [159]; Li et al.  [105] Incorporating Additional Objectives: TableGPT  [60]; Harkous et al.  [72]; Utilizing Copying Mechanism: Li et al.  [105]; Suadaa et al.  [172]; Adding Target Information as Input: Chen et al.  [28] Pure Multi-Task Fine-Tuning: Goodwin et al.  [61]; Bai et al.  [3]; Hybrid Multi-Task Fine-Tuning: Liu et al.  [118]; Li et al.  [105] Adapter-based Fine-Tuning: Houlsby et al.  [77]; Ribeiro et al.  [159]; Freezing-based Fine-Tuning: Gheini et al.  [55]; Distillation-based Fine-Tuning: Chen et al.  [26] text  [7, 215].  However, flat concatenation cannot effectively capture the semantic dynamics across utterances which is likely to cause inaccurate generation.  To deal with this issue, hierarchical encoders have been proposed to model the input paragraph [64, 112].  Gu et al.  [64] represented the dialogue context using DialogBERT, a hierarchical framework that utilizes sentence- and discourselevel Transformer encoders to encode each dialogue utterance and the sequence of utterance vectors, respectively.  However, when encoding each individual utterance, it does not consider the history information, which is essential for understanding dialogue utterances.  Thus, Li et al.  [112] employed a Transformer to encode each utterance into a dense vector, upon which a left-to-right flow module was designed to capture the utterance-level dynamic information flow.  Graph-based Representation Learning.  A long paragraph is likely to contain repeated, redundant or contradictory information.  How to exploit the key semantics and remove minor information from the intricate paragraph text is critical to promote paragraph-based generation performance.  Compared with sequences, by explicitly representing words or phrases as nodes and their relations (e.g., similarity) as edges, graphs can easily aggregate relevant but disjoint context in the text [138, 190].  As a representative example, Wu et al.  [190] leveraged a phrase-level unified semantic graph, where nodes are phrases extracted by dependency parsing and relations are dependency relations.  This graph can be used to aggregate co-referent phrases that are scattered in context for better capturing the long-range relations and global paragraph structures.  Besides, in conversational machine reading, Ouyang et al.  [138] formulated the input text as two complementary graphs, i.e., explicit and implicit discourse graphs, to fully capture the discourse relations and latent vector interactions among all the elementary discourse units.  3.1.2 Document Representation Learning.  In many text generation tasks such as document translation and document summarization, the input text might be a long document consisting of multiple paragraphs.  When encoding the documents, it is challenging to model cross-sentence (paragraph) semantics and capture the most critical semantics.  Modeling Inter-Sentential Semantics.  Most of PLMs are trained as masked language models.  They mainly focus on learning token-level representations instead of sentence-level ones.  Although segment embeddings are used to represent different sentences separately, they cannot capture the cross-sentence semantics.  To encode inter-sentential semantics, several studies [123, 214, 222] proposed to learn document representations in a hierarchical way.  For example, Liu et al.  [123] inserted the \"[CLS]\" token at the beginning of each sentence to aggregate sentence-level features in lower layers and then combine them with self-attention in higher layers.  Besides, Zhang et al.  [214] proposed HIBERT for learning document representations in a hierarchical fashion by using a sentence encoder to map sentences into sentence vectors and a document encoder to further learn context-sensitive sentence representations given their surrounding sentence vectors as context.  Capturing Critical Semantics.  In practice, sentences or paragraphs in long documents will inevitably complement, overlap, or conflict with one another.  Therefore, it is necessary to retain the most critical contents and verbalize them in the generated text.  To address the issue of key points missing in output text, Nguyen et al.  [136] introduced a topic model to capture the global topic semantics of the document and a gate mechanism to control the amount of global semantics provided to the text generation module.  Similarly, Liu et al.  [118] proposed two topic-aware contrastive learning objectives, among which the coherence detection objective identifies topics of a dialogue by detecting the coherence change among topics and the sub-summary generation objective forces the model to capture the most salient information and generate a sub-summary for each topic.  Representation Learning Efficiency.  Efficiency is a crucial aspect for modeling long documents, especially when generating long text.  Since the self-attention mechanism grows quadratically with sequence length, a number of studies aimed to improve the encoding efficiency of self-attention  [79, 133].  A representative example is Manakul et al.  [133], which proposed local self-attention, allowing longer input spans during training; and explicit content selection, reducing memory and compute requirements.  Furthermore, several researchers adopted divide-and-conquer encoding methods.  By splitting the long document into short sentences, it is easier to summarize each short part of the document separately [56], reducing the computational complexity.  3.1.3 Multi-lingual Representation Learning.  Existing PLMs are mainly pre-trained on English text while ignoring other low-resource languages.  It is difficult to apply English-based PLMs to solve multi-lingual text generation tasks (e.g., multi-lingual machine translation).  Several approaches have been proposed to cope with multilingual texts.  Cross-lingual Representations.  The core idea of cross-lingual representation learning is to learn a shared embedding space for two languages, in order to improve PLMs' ability to translate between them.  A well-known cross-lingual PLM is XLM  [31], which leveraged both monolingual and parallel data to learn cross-lingual representations.  However, these learned representations on shared Byte-Pair Encoding (BPE) spaces is implicit and limited.  Therefore, Ren et al.  [157] further computed cross-lingual 𝑛-gram embeddings and derived an 𝑛-gram translation table based on them for providing explicit representation learning signals.  Multi-lingual Representations.  Given more than two languages, multi-lingual PLMs aim to learn representations for any of the languages.  Based on English PLMs, BART and T5, Liu et al.  [122] and Xue et al.  [194] proposed mBART and mT5, respectively, which are pre-trained once for all languages.  Considering the differences across languages (e.g., syntactic rules), several studies utilized contrastive learning to learn multi-lingual representations [139, 185].  In particular, Wang et al.  [185] proposed two training objectives: contrastive sentence ranking (CSR) and sentence aligned substitution (SAS).  CSR creates positive and negative sentence pairs based on their saliency scores, while SAS replaces sentences with those in another language.  By contrastively learning these languages in a common text, the model can learn shared representation spaces across languages.  3.2 Structured Input Structured data (e.g., table, graph, and tree) is a critical kind of input for text generation in many real-world applications, such as medical report [73] and weather report [57] generation.  However, it is non-trivial to model structured input for PLMs due to three major challenges: (1) there exists a semantic gap between structured data and PLMs, since PLMs are typically pre-trained on natural language texts; (2) it is non-trivial to encode the structural information in the input data; (3) it requires to maintain fidelity of the generated text with respect to the input.  3.2.1 Bridging the Semantic Gap.  In general, PLMs are pre-trained on unstructured text, which differs in form from the structured data.  Several methods have been proposed to bridge this gap.  Structured Data Linearization.  In order to fit the structured input for PLMs, a simple approach is to linearize the input data into a sequence [43, 130, 158].  Specifically, Ribeiro et al.  [158] linearized knowledge graph (KG) into a sequence of triples by concatenating the relational triples.  Besides, some studies adopted template-based heuristic methods to serialize the input data [60].  For example, the attribute-value pair \"name: james beattie\" will be serialized as a sentence \"name is james beattie\".  Representation Alignment.  The semantic gap makes it difficult to effectively inject structured data representations into PLMs while directly serializing structured data.  Therefore, some people proposed to align the structured data representations with PLM-based word embeddings in semantic spaces.  For example, Li et al.  [105] utilized graph neural networks (GNN) to project KG entities into embeddings, and then performed representation alignment by minimizing the Euclidean distance between the GNN-based and PLM-based entity embeddings.  3.2.2 Capturing the Structural Information.  An important feature of structured data is that it represents data in a structural way, such as the ⟨𝑎𝑡𝑡𝑟𝑖𝑏𝑢𝑡𝑒, 𝑣𝑎𝑙𝑢𝑒⟩pair in table or the ⟨ℎ𝑒𝑎𝑑,𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛,𝑡𝑎𝑖𝑙⟩ triple in KB.  Such structural information can be used to help generate faithful text by modeling the input in a more accurate way.  Incorporating Additional Training Objectives.  To enhance the preservation of structural information, a typical approach is to incorporate auxiliary training objectives related to structural information  [60, 105, 130].  One kind of objectives is to reconstruct the semantic structure of the input data.  For example, Gong et al.  [60] utilized the attribute names of input tables as the labels to reconstruct table structure based on the attribute value representations from PLMs, which enforces PLMs to embed table structure into table representations.  Another method is to adjust the output text based on the structural information.  Mager et al.  [130] proposed cycle-consistency based losses to assess the quality of output text based on how well it can reconstruct the input structure.  Adding Structural Information as Input.  As opposed to prior studies that implicitly capture structural information with training losses, several studies explicitly took structural information as input [43, 158].  Ribeiro et al.  [158] directly prepended \"〈H〉\", \"〈R〉\", and \"〈T〉\" tokens before the head entity, relation and tail entity of a KG triple to reveal the relations between entities.  Besides, Fan et al.  [43] used the graph embedding of an Abstract Meaning Representation (AMR) graph as input.  The graph embedding provides the graph structure information by encoding the depth of each node (from the node to the root node) and the subgraph each node belongs to.  Employing Structural Encoding Module.  Since PLMs are originally developed for sequential input, it makes sense to incorporate additional modules to encode the structured input.  A representative example is StructAdapt  [159], which adds layer-wise graph convolution modules to learn representations built upon the graph connectivity over the PLM encoder.  Similarly, Li et al.  [105] employed GNN to encode KG relations as embeddings, which will be taken as input of PLMs.  In addition to the above textual data, multimedia data (e.g., image, video, and speech) has also been utilized as input of text generation algorithms, e.g., image captioning and speech recognition.  3.3.1 Image Captioning.  Image captioning, which aims to generate a textual description for an image, has been extensively studied in the field of computer vision (CV).  Many studies have proposed multi-modal PLMs to combine textual and visual modalities.  A well-known multi-modal PLM is XGPT  [191].  Inspired by text-based GPT, XGPT takes images as inputs and uses the image captioning task as the pre-training task in the pre-training stage.  Chen et al.  [21] also proposed an image captioning PLM, called VisualGPT.  They designed a self-resurrecting attention mechanism to learn how to encode the visual information and adapt it to the PLM decoder.  However, traditional vision-language pre-training fails to capture the relationship between the visual and text modalities.  Yang et al.  [200] proposed three pre-training tasks to effectively learn better aligned representations among three kinds of input data: text word, visual object, and scene text.  3.3.2 Video Captioning.  Video captioning focuses on generating natural language text that can describe the video content.  VideoBERT  [174] and CBT  [173] are two early attempts to investigate video-language pre-training with regard to the video captioning task.  However, previous studies usually adopted one single encoder-decoder framework, which is not flexible for diverse downstream tasks.  UniVL [129] employed two single-modal encoders to encode text and video separately and a sentence decoder to generate video captions.  3.3.3 Speech Recognition.  In practice, speech recognition is hungry for human-transcripted supervised data.  Thus, a number of unsupervised and semi-supervised methods were developed to integrate PLMs for weakly-supervised learning.  For example, Fan et al.  [45] proposed an unsupervised approach to pre-training encoder-decoder model with unpaired speech and transcripts.  Liao et al.  [114] proposed a speech recognition post-processing model that attempts to transform the incorrect and noisy recognition output into natural language text for humans and downstream tasks by leveraging the Metadata Extraction (MDE) corpus to construct a small task-specific dataset.", "metadata": {"source_file": "2201.05273v4.pdf", "title": "Pre-trained Language Models for Text Generation: A Survey", "authors": ["JUNYI LI∗", "WAYNE XIN ZHAO", "Junyi Li", "Tianyi Tang", "Wayne Xin Zhao", "Jian-Yun Nie", "Ji-Rong Wen", "Montréal"], "year": "2022", "detected_language": "en", "page_count": 35, "origin_chunk_file": "2201.05273v4.chunks.json"}}
{"text": "After encoding the input data into low-dimensional representations, the next step is to develop an effective PLM M as the text generation function 𝑓M. Based on such an architecture of PLM, the text generation objective can be modeled as the conditional probability of the output text 𝑦given the input data 𝑥, which can be formally factorized by tokens: where 𝑦𝑖denotes the 𝑖-th output token, and 𝑦<𝑖denotes the previous tokens 𝑦1, ...,𝑦𝑖−1. To compute the conditional probability, traditional neural models mainly adopt the RNN architecture [177] with several variants [164]. In recent years, solely based on attention mechanisms, Transformer [180] can better capture long-range dependency in texts, which is beneficial for modeling and generating texts. With the excellent parallelization capacities, Transformer has become the backbone for developing very large PLMs. When trained on large-scale unlabeled corpora [124], PLMs built on the Transformer architecture can encode rich semantic or linguistic knowledge. Furthermore, it has been shown that PLMs can be effectively fine-tuned to different text generation tasks [99, 168]. All these make PLMs the first choice to implement the text generation function 𝑓M. Existing PLMs for text generation adopt either a single Transformer or a Transformer-based encoderdecoder as the backbone. PLMs, such as GPT-3 [14] and UniLM [36], use a single Transformer encoder/decoder to simultaneously implement the process of input encoding and output decoding. This includes three major variants: masked LMs, causal LMs, and prefix LMs, with different attention mask strategies. In contrast, PLMs built upon Transformer encoder-decoder perform input encoding and output decoding separately. In the following, we describe these four variants in detail. 4.1.1 Masked Language Models. Masked LMs use a full-attention Transformer encoder. Equipped with the full attention, models are usually pre-trained with masked language modeling (MLM) task, i.e., predicting the masked tokens using the bidirectional information.  The most representative model is BERT  [35], which is used extensively in natural language understanding (NLU).  However, due to the discrepancy between the pre-training task of masked LMs and the downstream generation function, masked LMs are rarely utilized for text generation tasks  [198].  It is more common to use masked LMs as the encoder part for text generation, allowing to leverage the excellent bidirectional encoding capacities.  For example, Rothe et al.  [161] proposed to initialize both the encoder and decoder of the generation model with BERT [35], which yields comparable performance with other PLMs specially designed for text generation.  4.1.2 Causal Language Models.  Similar to Transformer decoder, causal LMs adopt the diagonal mask matrix.  Causal LMs are designed for language modeling, which is to determine the probability of a given sequence of words occurring in a sentence.  Causal LMs are straightforward for text generation, predicting the next word conditioned on all previous words.  In the literature, GPT [152] was the first causal LM for the text generation task.  Then, GPT-2 [153] explored the transfer capacity of language models for zero-shot generation task, highlighting the significance of sufficient data.  Furthermore, GPT-3  [14] showed that massive model parameters can significantly improve the downstream generation tasks, with a few examples or prompts.  CTRL [90] is proposed as a conditional causal LM to generate text based on control codes that govern style, content, and task-specific behavior.  Causal LMs are simple and straightforward for text generation, but they have several structural and algorithmic limitations: Causal LMs encode the tokens just from left to right, thus ignore the bidirectional information on the input side.  Moreover, causal LMs are not specially designed for the sequence-to-sequence generation tasks, thus in practice they do not achieve high performance in tasks such as summarization and translation [153].  4.1.3 Prefix Language Models.  Upon a single Transformer, prefix LMs adopt bidirectional encoding scheme in the input side and natural left-to-right generation pattern in the output side.  By utilizing the mixture attention mask, the tokens in the input text 𝑥can attend to each other, while the tokens in the target text 𝑦can only attend to all input tokens and previous generated tokens.  UniLM  [36] was the first prefix LM.  Compared to causal LMs, UniLM used prefix attention mask to solve conditional generation tasks, similar to the encoder-decoder architecture. UniLMv2  [5] and GLM  [39] improved vanilla prefix masking strategy by introducing permuted language modeling in XLNet [198].  Although prefix LMs have several advantages, Raffel et al.  [154] compared singleTransformer prefix LMs to Transformer-based encoder-decoder LMs and concluded that adding explicit encoder-decoder attention is more effective to capture conditional dependencies.  4.1.4 Encoder-Decoder Language Models.  Encoder-decoder LMs follow the standard Transformer architecture for text generation, consisting of stacks of both encoder and decoder layers.  During pre-training, MASS [168] and ProphetNet  [150] took the sequence with one masked segment as the input of encoder and then the decoder generates the masked tokens in an auto-regressive way.  T5  [154] randomly replaced several spans in the source text with different special tokens, and then the decoder predicted every replaced span in turn.  BART  [99] was pre-trained with denoising auto-encoder (DAE), i.e., the model learns to recover the original text from corrupted text, which is corrupted with different noising methods, such as sentence permutation and token deletion.  To derive performant PLMs for text generation, many studies proposed to improve the Transformer backbone of PLMs.  In this part, we will introduce two major improved techniques, i.e., extended input embeddings and improved attention mechanism.  4.2.1 Extended Input Embeddings.  Besides (sub-)word embeddings, almost all PLMs use position embeddings to indicate the indices of input words.  Compared to CNN and RNN, the self-attention operation is usually order-independent.  Hence, it is essential to provide explicit position information to capture the sequential nature of text.  Original Transformer  [180] utilized the pre-determined absolute position embeddings with sinusoidal functions, while most PLMs (e.g., BERT and GPT) adopted learned absolute position embeddings.  Instead of absolute ones, relative position embeddings produce position embeddings according to the offset between two tokens.  For example, T5 [154], UniLMv2 [5] and ProphetNet  [150] employed an bucket relative positional method.  In addition, hierarchical position embeddings are utilized to indicate inter- and intra- sentence position information, which is often used in some fixed-format text such as poem [109] and lyric [195].  Moreover, it is necessary to incorporate auxiliary embeddings to enrich the input information  [88].  Similar to segment embeddings used in BERT, dialogue state embeddings  [6, 189] are used to assign each utterance, and user embeddings  [6, 69] are utilized to differentiate characters involved in a conversation.  In the multilingual scenario, language embedding [30, 168] is commonly introduced to inform the model about the language of each sentence.  In addition, rhyme embeddings [109] and vowel embeddings [195] are proposed to indicate acoustics information in poem and lyric.  4.2.2 Improved Attention Mechanism.  Although there exist various modules in Transformer (e.g., position-wise FFN, self-attention, etc.), related works mainly focused on improving the self- and cross-attention mechanism for text generation [88].  In order to adapt to long-form text input and alleviate quadratic complexity of full-attention computation, sparse attention is proposed to replace the original self-attention for long-form input.  Rather than attending to all other tokens, every token only attends to specific tokens with strategies such as window attention  [133, 143, 205], global attention  [143, 205], random attention [205] and Sinkhorn attention  [223].  In practice, many text generation tasks need to process input data from multiple sources.  It is common to leverage one or more encoders to encode multiple inputs.  Therefore, several works proposed to utilize different strategies to aggregate multi-source inputs in the cross-attention module.  Golovanov et al.  [58] conducted mean pooling for dialogue history, current state and persona information.  Chen et al.  [22] and Liu et al.  [120] proposed multi-view attention and knowledgeaware attention to process embeddings from multiple views or knowledge sources.  In addition, VECO [128] pluged a cross-attention technique into the Transformer encoder to explicitly build the inter-dependence between multiple languages.  BASS  [190] and Ribeiro et al.  [159] substituted the self-attention module with GNN to better extract structural information.  Zeng et al.  [209] appended the gating mechanism after self-attention to inject condition-aware information.  To obtain good performance, it is critical to develop effective optimization algorithms for PLM-based text generation models.  We consider three main types of optimization methods, namely fine-tuning, prompt-tuning, and property-tuning.  We will detail each optimization method below.  During pre-training, PLMs are able to capture general linguistic knowledge from large-scale corpora.  However, it requires task-specific knowledge to perform downstream text generation tasks.  For this purpose, fine-tuning is a popular approach to incorporating task-specific information into PLMs by adjusting their weights using downstream text generation datasets [153].  According to how the parameters of PLMs are updated [88], exiting fine-tuning methods for text generation can be categorized as 1) vanilla fine-tuning, 2) intermediate fine-tuning, 3) parameterefficient fine-tuning, and 4) multi-task fine-tuning.  Compared with vanilla fine-tuning, intermediate and multi-task fine-tuning can alleviate the overfitting issue on small text generation datasets to some extent.  As the vanilla fine-tuning requires adjusting the entire model, parameter-efficient methods such as adapters [77] can fine-tune PLMs in a lightweight manner.  5.1.1 Vanilla Fine-Tuning.  Vanilla fine-tuning directly updates PLMs using downstream text generation datasets with task-specific losses (e.g., cross-entropy loss [153]).  Zhang et al.  [215] trained the DialoGPT model on the basis of the GPT-2 architecture by modeling a multi-turn dialogue session as a long text and optimizing the generation model with language modeling objective.  Ribeiro et al.  [158] investigated two recent PLMs, BART and T5, for graph-to-text generation and fine-tuned them using the typical auto-regressive cross-entropy loss.  A major issue of vanilla finetuning is that it is often not sufficiently optimized on small datasets, which is prone to overfitting.  5.1.2 Intermediate Fine-Tuning.  The basic idea of intermediate fine-tuning is to incorporate an intermediate dataset consisting of sufficient labeled instances.  The intermediate dataset can focus on the same target text generation task but from a different domain, or a similar NLP task from the same target domain.  It is helpful to infuse domain- or task-specific knowledge from the intermediate dataset to alleviate the overfitting issue and enhance the performance on small target text generation datasets  [146].  According to the relatedness between the intermediate dataset and the target text generation dataset  [88], intermediate fine-tuning can be divided into two categories, i.e., domain adaptive intermediate fine-tuning (DAIFT) and task adaptive intermediate fine-tuning (TAIFT).  Domain Adaptive Intermediate Fine-Tuning.  According to Kalyan et al.  [88], DAIFT utilizes an intermediate dataset, which focuses on a similar NLP task (not text generation tasks) from the same target domain, consisting of sufficient labeled instances.  By leveraging such an intermediate dataset, PLMs can be enriched with domain-specific knowledge, which is helpful to improve the performance of the target text generation task within the same domain.  DAIFT is commonly used in machine translation to eliminate the issue of unseen languages in translation pairs.  For example, to improve the translation quality of the low-resource target language (e.g., Kazakh), Liu et al.  [126] constructed a large-scale intermediate monolingual corpus of the target language and fine-tuned mBART by reconstructing the corrupted target-language text.  The intermediate dataset comes from the same language domain as the target dataset (e.g., Kazakh), which can impart language-related linguistic knowledge to PLMs for a better translation performance.  Task Adaptive Intermediate Fine-tuning.  In contrast with DAIFT, TAIFT incorporates an intermediate dataset on the same target text generation task but from a different domain.  It aims to infuse task-specific knowledge from the massive intermediate labeled dataset for improving the same target text generation task.  It has been shown that the additional training with generalpurpose text corpora (e.g., Wikipedia, WebText) on the same text generation task can improve the performance on a specific domain (e.g., Movie)  [42, 134].  For example, Fabbri et al.  [42] performed summarization on intermediate pseudo-summaries created from Wikipedia to improve the zeroshot and few-shot performance of abstractive summarization, and Mao et al.  [134] conducted generation on intermediate BookCorpus dataset (built from WebText) to improve commonsense story generation on the target WritingPrompts dataset.  5.1.3 Multi-Task Fine-Tuning.  Multi-task fine-tuning can exploit cross-task knowledge to improve the primary text generation task by incorporating auxiliary tasks.  Furthermore, by obtaining knowledge from related NLP tasks, multi-task fine-tuning can enhance the robustness of PLMs and reduce the need for large amounts of labeled instances in the text generation task.  According to the similarity between the primary text generation task and auxiliary tasks, multi-task fine-tuning (MTFT) can be divided into two categories, i.e., pure MTFT and hybrid MTFT.  Pure Multi-Task Fine-Tuning.  Pure MTFT incorporates auxiliary tasks that are the same as the primary text generation task but from different domains.  Previous studies mainly utilized additional datasets to eliminate the data scarcity issue of the primary text generation task [3, 61].  Specifically, Goodwin et al.  [61] leveraged twenty-one additional summarization datasets to improve zero-shot summarization on previously unseen datasets.  Besides, Bai et al.  [3] incorporated an auxiliary monolingual summarization task to improve the primary cross-lingual summarization task in a low-resource language.  Hybrid Multi-Task Fine-Tuning.  Hybrid MTFT incorporates auxiliary tasks that are different from the primary text generation task.  These diverse auxiliary tasks can enhance the primary generation task in different aspects.  For example, Liu et al.  [118] and Jin et al.  [86] fine-tuned PLMs with auxiliary tasks (e.g., coherence detection, style-carrying text reconstruction) to control the content of the generated text according to the topic change and text style (humor, romance, and clickbait).  Besides, to improve the faithfulness of the generated text, Li et al.  [105] and Gong et al.  [60] introduced auxiliary input reconstruction tasks to reconstruct KG triples and table values for aligning the input information with the generated content.  5.1.4 Parameter-Efficient Fine-Tuning.  As the above fine-tuning methods require updating all PLM parameters, it is time-consuming to perform the entire fine-tuning in resource-limited scenarios.  Many studies developed parameter-efficient fine-tuning (PEFT) for text generation tasks.  Adapter-based Parameter-Efficient Fine-Tuning.  Adapter is a special neural layer proposed by Houlsby et al.  [77] to fine-tune PLMs in a parameter-efficient way.  The adapter module projects the input vector into a small vector and then projects back into the original dimension using two feed-forward layers and a non-linear layer.  Specifically, the adapters first project the original 𝑑-dimensional features into a smaller dimension, 𝑚, apply a non-linearity, then project back to 𝑑dimensions.  The total number of parameters added per layer, including biases, is 2𝑚𝑑+ 𝑑+ 𝑚. By setting 𝑚≪𝑑, we can limit the number of additional parameters per task.  Thus, it is highly efficient to fix the parameters of original PLMs but only fine-tune the adapters  [27, 170].  To address the inefficiency and overfitting issues in low-resource abstractive summarization, Chen et al.  [27] inserted adapters into both encoder and decoder of PLMs and only fine-tuned the adapters.  A number of studies have shown that adapters can help PLMs efficiently capture some input characteristics for generating more accurate output text with a low extra cost in terms of parameters  [95, 159].  For example, Ribeiro et al.  [159] utilized adapters to model the input graph structure effectively when fine-tuning PLMs on graph input.  Freezing-based Parameter-Efficient Fine-Tuning.  This approach refers to freezing most parameters and only updating a small proportion of PLM parameters.  Recent studies have shown that not all parameters of PLMs are necessary to be fine-tuned for text generation tasks, and some of them can be fixed during fine-tuning without large impact on the model performance.  Several studies also revealed that cross-attention (or encoder-decoder attention) layers are more important than self-attention layers when fine-tuning PLMs for machine translation  [55, 203].  Therefore, Gheini et al.  [55] only fine-tuned cross-attention layers while kept the encoder and decoder fixed.  This approach achieved comparable translation performance to fine-tuning all parameters.  Distillation-based Parameter-Efficient Fine-Tuning.  Another parameter-efficient fine-tuning approach is to distill large teacher PLMs into small student models.  By distilling the knowledge in PLMs for text generation into small generative models (e.g., LSTM), the student models can be efficiently fine-tuned for better generation performance  [26, 167].  As a representative example, Chen et al.  [26] leveraged BERT as the teacher model that generates sequences of word probability logits and treated the Seq2Seq model as the student network, which can effectively learn from the teacher's outputs.  Most generative PLMs are pre-trained using language modeling objectives and then fine-tuned on text generation tasks with task-specific objectives.  Such a discrepancy between pre-training and fine-tuning affects the performance of PLMs on text generation tasks.  As a new learning paradigm, 5.2.1 Background.  According to Liu et al.  [119], a prompt function 𝑓𝑝𝑟𝑜𝑚𝑝𝑡(·) converts the input text 𝑥into a prompt 𝑥′ = 𝑓𝑝𝑟𝑜𝑚𝑝𝑡(𝑥) through a two-step process: 1. Apply a textual template containing two slots: an input slot [𝑋] for input 𝑥and an answer slot [𝑍] for an intermediate generated answer text 𝑧that will later be mapped into 𝑦. 2.  Fill the input slot [𝑋] with the input text 𝑥. Here the prompt can be cloze or prefix style.  The cloze-style prompt is usually adopted in language understanding tasks, where the empty slot [𝑍] is either in the middle of the prompt or at the end.  For example, in sentiment analysis where 𝑥=\"I love this movie\", the template may take a clozed form such as \"[𝑋] It was a really [𝑍] movie.\"  to predict the answer in [𝑍].  While in the prefix-style prompt, the input text comes entirely before the empty slot [𝑍] such as \"English: [𝑋] German: [𝑍]\" in machine translation.  Prefix prompts are widely used in text generation, as they mesh well with the left-to-right nature of language modeling.  In the above prompt examples, the template is composed of discrete natural language tokens, but the tokens can also be virtual words (e.g., represented by numeric IDs), which would be mapped into continuous embeddings later.  5.2.2 Discrete Prompts.  Early prompting studies create prompts by manually designing templates based on human introspection.  As a pioneering study, GPT-2 [153] performed text generation tasks using various manually-created prompts.  For example, the prompt \"translate to french, [input], [output]\" is used in machine translation.  The prompt defines the semantic mapping from input data to output text in a specific text generation task.  By utilizing diverse prompts, a single PLM is able to perform a number of different text generation tasks.  These approaches heavily relied on manual efforts to create prompts; but PLMs are highly sensitive to prompts: improperly-created prompts lead to low performance [83].  To avoid the need to manually specify prompts, Shin et al.  [166] proposed AutoPrompt to automatically search for template tokens.  Several other methods have also been proposed to discover discrete prompts automatically such as paraphrasing existing prompts [83], generating prompts using PLMs [51], and mining prompts from a corpus [83].  5.2.3 Continuous Prompts.  Continuous prompts (a.k.a., soft prompts), consisting of embedding vectors, are widely explored for text generation tasks.  Two major advantages are expected: 1) relaxing the constraint that the prompt template should be natural language words; 2) removing the restriction that the template is parameterized by PLMs' parameters.  Instead, continuous prompts have their own parameters that can be optimized based on training data of the text generation tasks.  The most well-known method using continuous prompts for text generation is prefix-tuning [110], which freezes the generative PLMs (e.g., GPT-2, BART) and optimizes a sequence of task-specific vectors (called prefix).  In contrast to full-parameter fine-tuning, which requires storing a tuned copy of the model for each text generation task, prefix-tuning only optimizes the prefix for each text generation task.  Similar to prefix-tuning, several studies used continuous prompts to solve other text generation tasks such as dialogue generation  [65].  For different generation tasks, we need to consider specific language properties when tuning PLMs.  In this section, we discuss three major properties that are widely desired for text generation.  5.3.1 Relevance.  According to the linguistic literature [107], in text generation, relevance means that the topical semantics conveyed in output text is highly related to the input text.  As a representative example, in dialogue systems, the generated responses should be relevant to the historical utterances and other conditions, such as speaker persona and discourse topic.  Compared with traditional neural generative models, PLMs utilize more powerful multi-layer cross-attention mechanism to model the semantic associations between input and output, which can enhance the relevance of generated text to the input data (e.g., the dialogue systems  [189, 215]).  A good example is DialoGPT  [215] based on an auto-regressive language model GPT-2.  Specially, DialoGPT was first trained on large-scale dialogue pairs/sessions, which could enable DialoGPT to capture the joint distribution of Pr(ℎ𝑖𝑠𝑡𝑜𝑟𝑦,𝑟𝑒𝑠𝑝𝑜𝑛𝑠𝑒) in conversational flow for generating relevant responses to the history utterance.  Furthermore, Zeng et al.  [208] utilized the masked language modeling objective to solve generate responses based on various types of dialogue context.  Specifically, they proposed a TF-IDF based masking which selects more condition-related tokens to be masked, so that PLMs can generate condition-related expressions rather than the general language patterns.  Besides, they adopted a non-parametric attention-based gating mechanism to switch between generating a general word or a condition-related word at each position.  5.3.2  Faithfulness.  Faithfulness is also an important language property to consider for text generation, which means the generated content should adhere to the semantics of input text.  For example, text summarization aims to generate faithful text conveying the salient information of the input text.  Faithfulness sometimes refers to the fact that the generated text is in accord with world facts.  To generate faithful texts, PLMs should be able to accurately understand the core semantics of input and acquire sufficient world knowledge for solving the downstream task.  It has been shown that PLMs have excellent natural language understanding capacities in capturing core semantics from plain text [35], and they indeed encode a large amount of world knowledge  [83], which is potentially beneficial to generate faithful summary by injecting background knowledge into text.  For example, Kryscinski et al.  [93] utilized a contextual network in the PLM decoder to retrieve the most salient parts from the source document to improve the level of faithfulness of generated summaries.  Besides, several studies proposed to generate faithful texts by introducing additional losses besides the text generation loss [161, 202].  Specifically, Yang et al.  [202] fine-tuned PLMs through a theme modeling loss which aims to make the generated summary semantically close to the original article for achieving faithful generation.  5.3.3 Order-Preservation.  In the NLP field, order-preservation is a special property that refers that the order of semantic units (word, phrase, etc.) in both input and output text is consistent.  Such a property is key to several important text generation tasks, such as text paraphrasing and machine translation.  In machine translation, when translating from source language to target language, it often requires preserving some order of phrases in the source and target text for ensuring the accuracy of the translation results.  In machine translation, word alignment is an extensively studied approach to achieve the orderpreservation property.  A representative study is Code-Switching Pre-training (CSP)  [199].  CSP first automatically extracted the word-pair alignment information from the source and target monolingual corpora.  Then, to enhance the order-preservation property during translation, CSP continually pre-trained PLMs by predicting the sentence fragment on the source side given the aligned fragment in the target language.  Moreover, to relax the restriction of discrete word alignment, another line of research aims to conduct continuous representation alignment to improve the orderpreservation property.  Wada et al.  [182] focused on aligning word representations of each language by mapping word embeddings of each language into a common latent space.  Lin et al.  [116] proposed mRASP to enforce words and phrases that have similar meanings across multiple languages, to be aligned in the representation space.  prior knowledge transfer [120, 144, 227], data augmentation [23, 131, 142, 193], multi-task learning  [3, 61] Bias in Pretraining Corpora Quantization by truncating PLMs weights [171, 204], pruning less critical weights [44, 62, 68, 76], knowledge distillation  [26, 85, 103].  Model Enhancement\nCHALLENGES AND SOLUTIONS  The three previous sections described three key aspects together with the basic methods used in PLM-based text generation.  In this section, we further discuss the major challenges in each of the aspects and possible solutions.  A summary of these challenges and solutions is presented in Table 1.  6.1.1  Lacking Sufficient Training Data.  In a number of text generation tasks, it is difficult to obtain sufficient annotated data.  Transfer learning provides an effective solution by transferring the knowledge of data-rich source tasks into data-scarce target text generation tasks.  Besides, data augmentation and multi-task learning can be also used to address this problem.  Transfer Learning.  To deal with the data scarcity issue, several studies proposed first finetuning PLMs on large amounts of external labeled corpora and then transferring into data-scarce target text generation tasks  [120, 144, 227].  In particular, Peng et al.  [144] and Zou et al.  [227] first fine-tuned PLMs on substantial labeled dialog/summary data and then fine-tuned for the target dialog/summarization task in a new domain with limited labeled data.  Similarly, Liu et al.  [120] first trained models on large-scale ungrounded dialogs and unstructured knowledge base separately to improve the low-resource knowledge-grounded dialog generation task.  Data Augmentation.  In recent literature, data augmentation has emerged as a critical method for increasing the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.  One line of research is to use retrieval models to obtain real data from external corpora as the augmented data  [142, 193].  For the query-focused summarization task, Pasunuru et al.  [142] used a search engine, i.e., Bing, to retrieve the answer paragraph as the synthetic summary and used the top ranked documents as input text.  Another line of work is to use perturbation-based methods by corrupting the original text  [23, 131].  For example, Chen et al.  [23] presented a set of data augmentation methods for conversation summarization, such as random swapping/deletion to randomly swap or delete utterances in conversations.  Multi-Task Learning.  Leveraging other data-rich tasks and datasets can also overcome the data scarcity issue.  Most studies usually incorporated similar auxiliary generation tasks for enhancing the primary text generation task [61].  However, these methods usually adopt independent decoders for each task, thus breaking the semantic connections between high- and low-resource text generation tasks.  To bridge this gap, Bai et al.  [3] employed a unified decoder which learns the alignments and patterns across multiple languages in machine translation.  6.1.2 Data Bias from Pre-training Corpora.  In sociology, bias is an unjustified prejudice in favour of or against a person, group, or thing [54].  PLMs are generally trained using real-world data in such a way that they model the statistical properties of the training data.  As a result, they inherit the biases and stereotypes that are common in the data [54].  These biases and stereotypes can pose significant challenges in downstream text generation tasks [14].  It has been shown that the generated texts from PLMs are likely to be biased towards some attributes [14], i.e., favoring a particular race, gender or aged people, which is not desired for the text generation tasks.  These undesirable biases are unexpectedly hidden in model components such as word embeddings [10] and attention heads [181].  A simple approach to mitigating the gender bias in word embeddings is to \"swap\" gendered terms in training data when generating word embeddings [221].  Furthermore, simply masking names and pronouns may also reduce biases and improve the performance of certain language tasks [34].  However, to date, there is still no general, unified approach to reducing the data bias from PLMs for text generation.  Some of these techniques for bias detection and mitigation have been critiqued as merely capturing over-simplified dimensions of bias with proper debiasing requiring more holistic evaluation [59].  6.2.1 Model Compression.  Although PLMs have achieved great success on text generation, the backbone Transformers are still bulky and resource-hungry, resulting in high memory consumption, computational overhead, and energy cost.  To address these issues, more and more approaches are proposed to compress PLMs  [50], such as quantization, pruning, and knowledge distillation.  Quantization.  Quantization means reducing the number of unique values used to represent PLMs weights, which in turn allows to represent them using fewer bits [50].  As most PLMs are built upon Transformer, quantization can be generally applied to those weights residing in fullyconnected layers (i.e., embedding layers, linear layers, and feed-forward network layers).  However, when the model parameters are compressed, the generation capacity might be reduced.  To alleviate the issue of generating unsatisfactory text with truncated PLMs, a promising solution is to first identify important weights and then avoid truncating them during the quantization step [204].  Pruning.  Pruning refers to identifying and removing redundant and/or less important weights [50].  Pruning methods for text generation largely fall into two categories [50].  The first type of unstructured pruning prunes individual weights by locating the set of least important weights in PLMs.  The importance of weights can be measured by specific metrics such as absolute values [62] and gradients  [68].  The second type of structured pruning prunes structured blocks of weights or even complete components of PLMs by reducing and simplifying certain modules such as attention heads [76] and Transformer layers  [44].  Knowledge Distillation.  Knowledge distillation refers to training a smaller model (called the student) using the output of PLMs (called the teacher).  First, the student model can directly learn from the output word distribution of the final softmax layer in PLMs, which allows the student to mimic the generated text of the teacher by replicating the word distribution across the whole vocabulary [26].  Second, the student can also learn from the output tensors of PLMs encoders [103].  Intuitively, the representations of PLMs encoder may contain meaningful semantics and contextual relationships between input tokens, which is helpful for generating accurate text.  Third, by replicating attention distributions between input data and output text, the student can also learn the contextual dependency between input and output [85].  6.2.2 Model Enhancement.  Although PLMs have achieved great success nowadays, they are still far from our expectations.  Recently, there has been a surge of interest in the research community to strengthen existing PLMs to improve the performance of text generation.  Large-scale PLMs.  Kaplan et al.  [89] have shown that the performance of PLMs can be boosted by scaling up the amount of PLMs' parameters.  This observation sparked the development of large-scale PLMs in text generation  [14, 207].  The most representative large-scale PLMs for text generation is GPT-3  [14], which contains 175 billion parameters, 10x more than any previous non-sparse PLMs.  With a large number of parameters, GPT-3 can achieve strong performance in various text generation tasks without any gradient updates or fine-tuning.  Knowledge-Enriched PLMs.  Recent research has found that integrating knowledge from external knowledge sources can enhance the text generation performance of PLMs  [175, 225].  Specifically, ERNIE 3.0 [175] was pretrained on a 4TB corpus consisting of plain texts and a large-scale knowledge graph for both language understanding and generation tasks.  Without incorporating explicit knowledge, CALM [225] can encode commonsense knowledge into parameters by teaching PLMs to write and reason with common concepts through pre-training strategies, yielding better performance on text generation tasks.  Efficient PLMs.  Pre-training PLMs on large-scale text data is prohibitively expensive.  Recently, it has been demonstrated that by meticulously structuring the model architecture, it is possible to obtain equivalent or higher text generation performance with less pre-training data  [225] or lower pre-training costs [84].  For example, CALM [225] developed a mutually reinforced pre-training framework with generative and contrastive objectives, thus achieving comparable results to other larger PLMs such as T5 while only being pre-trained on a small corpus for a few steps.  6.3.1  Satisfying Special Text Properties.  In Section 5.3, we introduced three basic text properties.  In this section, we will present three more difficult properties for text generation tasks, i.e., coherence, factuality, and controllability.  Coherence.  In linguistics [101], language coherence is what makes a multi-sentence text meaningful, both logically and syntactically.  An essential technique to improving coherence is to elaborately plan the generated content, which is known as text planning  [78, 107].  For example, Li et al.  [107] designed a text generation model based on a two-level text plan: (1) the document plan is modeled as a sequence of sentence plans in order, and (2) the sentence plan is modeled as an entity-based subgraph from KG.  The local coherence is naturally enforced by KG subgraphs, and the global coherence can be improved by generating a coherent sequence of subgraphs.  Wang et al.  [186] proposed a two-stage planning, i.e., the first stage is to organize the story outline which illustrates the story plots and events, and the second stage is to expand the outline into a complete story.  Factuality.  The input data (e.g., infobox) for text generation tasks (e.g., table-to-text generation) usually contains some factual information.  In such cases, the generated content should adhere to the original input facts.  However, lacking direct access to the input facts or explicit supervision makes PLMs unable to retain text factuality in generation process.  For data-to-text generation, the pointer generator [164] is usually adopted to copy the input facts into output for preserving factuality [29, 105].  Furthermore, to make summarization models produce more factual summaries, some studies proposed evaluation metrics or correction methods to measure and revise the generated text for preserving factuality  [37, 135].  Controllability.  In text generation, many applications need a good control over the output text.  For example, to generate reading materials for kids, we would like to guide the output stories to be safe, educational and easily understandable by children.  The Plug and Play Language Model, also known as PPLM  [33], is an example of a controllable PLM that combines a PLM with one or more simple attribute classifiers that direct text generation without further PLM training.  Several studies achieved controllablility from a distributional view [91, 141].  Pascual et al.  [141] described a plug-and-play decoding approach in a single sentence: given a topic or keyword, the model adds a shift to the probability distribution over the vocabulary towards semantically similar words.  6.3.2 Mitigating Tuning Instabilities.  Due to the catastrophic forgetting nature of PLMs and small size of text generation datasets, tuning PLMs for text generation is usually unstable i.e., fine-tuning the model with different random seeds results in a wide variance of performance.  The possible solutions include intermediate fine-tuning, mixout and using supervised contrastive loss.  Intermediate Fine-Tuning.  Recent studies have shown that first training PLMs on data-rich intermediate labeled datasets (e.g., a similar NLP task from the same target domain) before finetuning them on data-scarce target text generation tasks can achieve better performance in target tasks  [126, 146].  For example, Liu et al.  [126] constructed an intermediate monolingual corpus of the target language (e.g., Kazakh) and fine-tuned mBART to reconstruct the corrupted monolingual text for improving the translation quality of the low-resource target language.  Mixout Strategy.  When fine-tuning PLMs, dropout [169] has been used as a regularization method to prevent performance degeneration if there are only a small number of training instances.  Lee et al.  [97] introduced a variant of dropout, mixout, which stochastically mixes parameters of two PLMs.  The mixout strategy can regularize learning by minimizing the deviation from one of the two PLMs and the strength of regularization adapts along the optimization trajectory.  Contrastive Learning.  The most used cross-entropy loss in text generation, i.e., the KL-divergence between one-hot vectors of labels and the distribution of model's outputs, lacks robustness to noise labels [219] or adversarial examples [41].  Thus, fine-tuning PLMs with cross-entropy loss tends to be unstable, especially when labeled data is limited.  An effective solution is to capture the similarity between examples in one class and contrast them with examples in other classes  [67].  To this end, Gunel et al.  [67] combined the cross-entropy loss with a supervised contrastive learning loss that pushes the words from the same class close and the words from different classes further apart.  EVALUATION AND RESOURCES  In this section, we will discuss several commonly used evaluation metrics and resources with respect to PLMs for text generation.  With the growing variety of text generation applications and datasets, there are several advantages of automatic evaluation: it is potentially much cheaper and quicker than human evaluation, and it is repeatable [8].  Therefore, we mainly concentrate on automatic evaluation metrics for text generation in this part.  Following Celikyilmaz et al.  [19], we present four categories of metrics, i.e., 𝑛-gram overlap metrics, diversity metrics, semantic similarity metrics, and logit-based metrics.  We list the metrics used in each text generation task in Table 2.  7.1.1 N-Gram Overlap Metrics.  These metrics measure the degree of word \"matching\" between machine-generated and ground-truth texts at the word level.  BLEU.  The Bilingual Evaluation Understudy (BLEU)  [140] is one of the first metrics used to compare the similarity of two sentences.  This metric was originally proposed for machine translation by comparing a candidate translation of text with one or more reference translations and now applied in various generation tasks.  BLEU-𝑛measures the precision of the co-occurrences of 𝑛grams between the generated and real text and conducts length penalty on shorter generated text.  Specially, SacreBLEU  [149] is recommended for use in machine translation to avoid inconsistency issue.  Several smoothing methods [20] are also proposed to evaluate short sentences.  ROUGE.  Recall-Oriented Understudy for Gisting Evaluation (ROUGE)  [115] is a set of metrics for measuring automatic summarization of long texts consisting of multiple sentences.  ROUGE-𝑛 counts the F1 score of the overlapping 𝑛-grams between generated and ground-truth texts.  METEOR.  The Metric for Evaluation of Translation with Explicit ORdering (METEOR)  [4] is proposed to address some issues found in BLEU.  Compared to BLEU, METEOR is computed based on the harmonic mean of the unigram precision and recall, and measures word-to-word matches between generated and real text based on WordNet.  ChrF++.  Character 𝑛-gram F-score (ChrF++)  [148] is an automatic evaluation metric for machine translation.  Different from the word level co-occurrence of BLEU, ChrF++ is mainly focused on the character-level matching so as to consider morpheme overlapping.  7.1.2 Diversity Metrics.  Lexical diversity is desirable in many text generation tasks, such as dialogue systems and story generation.  For these tasks, it is necessary to conduct diversity evaluation on generated texts.  Distinct.  Distinct-𝑛measures the degree of diversity by calculating the number of distinct 𝑛-grams in generated text [100].  This metric is scaled by total number of generated tokens to avoid favoring long sentences.  7.1.3 Semantic Similarity Metrics.  The above metrics are focused on the literal word comparison.  Many studies also proposed to compare the implicit semantics between generated text and groundtruth text.  A typical approach is to map both generated text and ground-truth text into sentence vectors and then compare their embedding similarity.  BERTScore.  Given the excellent performance of BERT across many tasks, BERTScore [213] leverages the pre-trained contextual embeddings from BERT and compares words in candidate and reference texts by cosine similarity.  BERTScore has proven to correspond well with human judgments on sentence-level and system-level evaluations [19].  7.1.4  Logit-Based Metrics.  In text generation, the probability of a generated text𝑦= ⟨𝑦1, . . .  ,𝑦𝑛⟩ can be formulated as Pr(𝑦)  = Î𝑛 𝑗=1 Pr(𝑦𝑗|𝑦1:𝑗−1;𝑥)  , where 𝑥denotes the input data, and 𝑦1:𝑗−1 denotes the previous tokens ⟨𝑦1, . .  . ,𝑦𝑗−1⟩. Logit-based metrics evaluate the generated text from a probabilistic view.  PPL.  In information theory, perplexity (PPL) is a measurement of how well a probability distribution or probability model predicts a sample compared with the ground-truth [12].  A low perplexity indicates the probability distribution is good at predicting the sample.  Therefore, the perplexity of the discrete probability distribution Pr(·) is defined as: 7.2.1 Open-Source Libraries.  There are a number of public text generation libraries that can be used to implement PLM-based text generation models.  Transformers [188] is an all-featured library for Transformer-based PLMs, and Fairseq  [137] is a library to train custom models for translation, summarization, language modeling and other text generation tasks.  Besides, some of libraries like FastSeq [196], DeepSpeed [156], and LightSeq [187] are useful to increase the inference speed of models.  TextBox  [104] supports 21 text generation models, including several prevalent PLMs, and diverse generation strategies (e.g., top-𝑘, beam search) and evaluation metrics (e.g., BLEU, Distinct).  One can easily choose different PLMs, optimization methods, and evaluation metrics by setting corresponding hyper-parameters with just a few lines of code.  7.2.2 Evaluation Benchmarks.  In order to evaluate the comprehensive capacities of PLMs, several important evaluation benchmarks are created and released, which involve multiple evaluation tasks from different aspects.  In addition to GLUE [184] and SuperGLUE [183] which are general language understanding evaluation benchmarks, an increasing number of general benchmarks targeted for text generation have recently been proposed.  Liu et al.  [117] introduced the General Language Generation Evaluation (GLGE) benchmark, a new multi-task benchmark for evaluating the generalization capabilities of text generation.  GLGE contains 8 English language generation tasks, covering summarization, question generation, generative question answering, and dialogue.  For each task, GLGE designs three sub-tasks in terms of task difficulty (i.e., GLGE-Easy, GLGEMedium, and GLGE-Hard).  APPLICATION As discussed in Section 2, text generation can be instantiated into different kinds of applications.  To summarize existing text generation applications, we present an overview of different tasks (as well as corresponding common datasets and metrics) in Table 2.  In what follows, we will highlight three classic applications, i.e., machine translation, text summarization and dialogue system, and briefly discuss how to design a task-specific PLM to adapt to specific text generation tasks.  8.1 Machine Translation Machine translation (MT) is the process of automatically translating one language into another.  With the advent of deep learning, Neural Machine Translation (NMT) has emerged as the dominant method in both academic research and commercial use [32].  Machine translation can be classified into two types: unsupervised machine translation and supervised machine translation, depending on whether parallel corpora are available for fine-tuning PLMs.  8.1.1 Unsupervised Machine Translation.  Unsupervised Machine Translation (UMT) refers to the use of solely monolingual corpora without any parallel data for both pre-training and fine-tuning PLMs.  UMT enables machine translation to no longer rely on large-scale annotated corpora, and also brings remarkable advances in low-resource language translation.", "metadata": {"source_file": "2201.05273v4.pdf", "title": "Pre-trained Language Models for Text Generation: A Survey", "authors": ["JUNYI LI∗", "WAYNE XIN ZHAO", "Junyi Li", "Tianyi Tang", "Wayne Xin Zhao", "Jian-Yun Nie", "Ji-Rong Wen", "Montréal"], "year": "2022", "detected_language": "en", "page_count": 35, "origin_chunk_file": "2201.05273v4.chunks.json"}}
{"text": "When using PLMs for UMT, there are typically two steps involved [94]: 1) PLMs are pre-trained on monolingual corpora in a variety of languages, learning word embeddings and modeling probabilities for each sentence in each language; 2) Iterative back-translation is then leveraged to combine the source-to-target and target-to-source model with the denoising auto-encoding and back-translation objectives. Pre-training on Monolingual Corpora. Recent PLM-based research has mainly focused on the first step of UMT. Specifically, XLM [31] and mBERT [35] were pre-trained on multiple monolingual data using MLM task, and then the PLM was used to initialize both the encoder and the decoder for machine translation. mBART [122] followed the pre-training scheme of BART [99] on multiple languages, while these PLMs just performed the original pre-training task with mixed monolingual corpora, without considering the relationship between languages. CMLM [157] further proposed cross-lingual MLM to randomly mask tokens in monolingual sentences and predicted corresponding translation candidates. Therefore, CMLM was able to align the embeddings of different languages. CSP [199] shared the similar idea, replacing some words in the source sentences with their translation words and then predicting the replaced words. Leveraging Iterative Back-translation. In the back-translation stage, Garcia et al. [53] proposed using multi-task learning. They investigated multilingual UNMT, which involved the use of a third language when translating one language into another. The extra language can provide auxiliary monolingual data or parallel data containing only one language in the source or target language. They aggregated back-translation loss and introduced a cross-translation term to incorporate the auxiliary corpus. Li et al. [113] also applied the cross-translation term and additionally included a knowledge distillation objective for the third (intermediate) language. 8.1.2 Supervised Machine Translation.  Supervised machine translation (SMT) refers to fine-tuning PLMs based on parallel corpora.  Here, we will discuss how to utilize existing self-supervised PLMs and how to design PLMs for parallel corpora.  Directly Fine-tuning Unsupervised PLMs.  Almost all PLMs mentioned above using unsupervised (self-supervised) pre-training, such as XLM [31] and mBART [122], can be directly fine-tuned with bilingual pairs.  Moreover, considering the excellent encoding capability of BERT, BERT-fused model [226] leveraged BERT to extract contextual embedding for the source sentence, and fused the representations with each layer of the encoder and decoder.  CTNMT  [197] leveraged asymptotic distillation and dynamic switching gate to integrate the BERT embedding.  Graformer  [176] grafted mBERT as the encoder and mGPT as the decoder, and then trained a cross-attention module to combine them.  Tang et al.  [178] proposed to fine-tune mBART on multiple language pairs, which is called multilingual fine-tuning.  Designing PLMs for Parallel Corpora.  Most of PLMs are pre-trained on monolingual corpora using self-supervised pre-training tasks such as MLM and DAE.  Nevertheless, these pre-training objectives are different from the downstream translation task.  Hence, mRASP [116] pre-trained the model on bilingual pairs with supervised Seq2Seq loss by randomly replacing the words in the source sentence with the words which have the same meaning in other languages.  As a result, words with similar meaning across different languages are encouraged to share similar representations.  mRASP2  [139] applied contrastive learning to minimize the representation gap of similar sentences Text summarization is the process of condensing text into a brief summary that retains key information from the source text  [40].  The mainstream approaches to text summarization based on PLMs are either extractive or abstractive.  Extractive summarization selects a subset of sentences from the source text and concatenates them to form the summary  [123, 214].  In contrast, abstractive summarization generates the summary automatically from the abstract representation of input texts [164, 211].  As abstractive summarization is more related to text generation, we only discuss abstractive summarization in this section.  and maximize that of unrelated sentences.  Despite significant success, pre-training on parallel data requires massive labour and financial resources to create vast amounts of bilingual pairs.  8.3 Dialogue System Dialogue system (a.k.a., conversational agent) aims to make machines communicate with human fluently.  Technically, machines are required to generate a response conditioned on history contexts.  According to downstream applications, dialogue systems are commonly categorized into opendomain and task-oriented dialogue systems.  The former intends to converse with humans engaged on open topics such as daily life, sports and entertainment [80], while the latter is focused on assisting users to complete specific tasks, such as hotel reservation and product purchase [220].  8.3.1 Open-domain dialogue System.  Open-domain dialogue system is also known as chat-bots focusing on daily chat.  For example, Microsoft XiaoIce is a well-known open-domain dialogue system to satisfy human needs for communication, affection, and social belonging [224].  Continuous Pretraining with dialogue Corpora. PLMs, such as GPT-2, are pre-trained on general text corpora, thus various studies continually pre-trained general-purpose PLMs to fit dialogue systems.  Due to the difficulty in obtaining large-scale dialogue corpora, informal text resources (such as forum posts and comments in Reddit, Twitter and Weibo) are usually employed for continual pre-training.  As two typical models, DialoGPT [215] and Meena  [1] used English or Chinese dialogue corpora to continually pre-train casual LMs like GPT-2.  Besides, Blender [160] and PLATO [6] utilized the Seq2Seq loss to generate the next utterance based on previous utterances.  Moreover, PLATO [6] incorporated the next utterance classification (NUC) loss, similar to the next sentence prediction task in BERT, to judge whether the response is relevant to history dialogues to enhance the coherence of utterances.  In order to penalize bland responses and decrease repetitions, DialoGPT [215] employed mutual information maximization to predict the input given generated response and Blender [160] adopted unlikelihood training objective to penalize repetitive 𝑛-grams.  Directly Fine-tuning Existing PLMs.  In addition to pre-training on dialogue corpora, researchers also explored fine-tuning existing PLMs on dialogue tasks.  TransferTransfo  [189] adapted GPT to the dialogue task through multi-task learning.  Based on TransferTransfo, Golovanov et al.  [58] modified the architecture to better model multiple inputs including dialogue history, persona information, and current state.  Besides, to capture the hierarchical structure of dialogue, hierarchical encoders have been proposed to model the dialogue input  [64, 112].  Gu et al.  [64] proposed a hierarchical framework, dialogueBERT, that uses sentence- and discourse-level Transformer encoders to encode each dialogue utterance and the sequence of utterance vectors, respectively.  Furthermore, controllability is also important to consider in dialogue systems.  Zeng et al.  [209] utilized conditionaware Transformer block to steer the response in a specific topic label.  StyleDGPT  [201] attempted to enforce the target style of the generated response with KL loss at both word and sentence levels.  8.3.2  Task-Oriented Dialogue System.  Task-oriented (a.k.a., goal-oriented) dialogue system is a widely-used text generation application in real life, such as helping users order tickets.  Generally, task-oriented dialogue system was divided into four modules, i.e., natural language understanding, dialogue state tracking, dialogue policy learning and natural language generation [220].  Most previous work only focused on the last generation module in task-oriented dialogue system by using generative PLMs (e.g., GPT).  For example, SC-GPT [144] used the ground-truth results of previous three modules (e.g., dialogue state) and serialized them as input of the last generation module to generate response.  Kale et al.  [87] further designed a manual schema to better convert previous results into a natural language.  Shalyminov et al.  [165] proposed to generate and retrieve several responses based on the dialogue context and utilized the NUC task to select the best one.  PRAL  [63] utilized two separate GPT-2 to model the user and system, and adopted a third GPT-2 to perform knowledge distillation and incorporate commonsense knowledge into the final dialogue generation.  Besides, more and more studies proposed to jointly learn these four modules based on a shared PLM.  Budzianowski et al.  [15] and Hosseini-Asl et al.  [75] generated the dialogue state, system action and final response successively, based on the original dialogue history.  In this part, we will briefly introduce other text generation tasks, such as question generation, story generation and data-to-text generation.  8.4.3 Data-to-text Generation.  The above tasks take unstructured text as input, while the data-totext generation task generates descriptive text about structured input data, such as table, knowledge graph (KG) and abstract meaning representation (AMR).  First, a naive and straightforward approach is to directly linearize the structured table  [29, 60] and KG [72, 158] into textual form as the input of PLMs.  Considering the graph structure of KG and AMR, Li et al.  [105] and Ribeiro et al.  [159] employed graph neural network to learn a better representation for each node.  Moreover, to cope with the structural information, a typical approach is to incorporate auxiliary training objectives such as predicting the value of table  [60] and the relation of knowledge graph [105].  8.4.1 Question Generation.  Question generation can be seen as a dual task of question answering (QA), i.e., generate coherent questions based on given passages and answers.  Existing PLMs, such as UniLM [5, 36] and ProphetNet  [150], can be employed for this task by taking as input the concatenation of the passage and answer.  Moreover, researchers explored this task in different QA settings.  For example, Huang et al.  [81] proposed a two-stage model to solve multi-hop question generation, and Cao et al.  [17] attempted to generate open-ended questions which are answered by multiple sentences.  Moreover, Majumder et al.  [132] proposed a clarification question generation task to ask questions about the missing information in the passage in order to reduce the ambiguity.  8.4.4 Other Generation Tasks.  Besides the aforementioned tasks, there are also other text generation applications.  ColdGANs  [163] explored the unconditional language generation.  KG-BART [125] investigates the commonsense generation, i.e., generating a natural language consisting of provided commonsense concept (word), which can be considered as the hard-constrained conditional generation [52].  Moreover, text style transfer aims to convert a text into another style while preserving the basic semantics of input [52], such as sentiment transfer and writing style transfer [92].  In addition, some researchers devoted to literary creation, such as poem [109] and lyric [195].  In this survey, we presented an overview of current representative research efforts on PLMs-based text generation, and expect it can facilitate future research.  We began with introducing three key aspects when applying PLMs to text generation, based on which the main content of our survey is divided into three sections from the view of input representation learning, model architecture design, and parameter optimization.  Besides, we discussed several non-trivial challenges related to the above three aspects.  Finally, we reviewed various evaluation metrics, open-source libraries, and common applications to help practitioners evaluate, choose and employ PLMs for text generation.  Despite the great progress made in recent years, we are faced with several open problems and several future directions are promising to deal with them.  Controllable Generation.  Controllable text generation with PLMs is an interesting direction but still at a very early stage.  Controlling some attributes of the generated text has many practical use cases, such as generating positive responses to patients suffering from depression in dialogue systems.  However, PLMs are usually pre-trained in universal corpora, which is difficult to control the multi-grained attributes of the generated text (e.g., sentiment, topic, and coherence).  Keskar et al.  [90] has explored text generation with control codes that govern style, content and task-specific behavior.  However, these control codes are preset and coarse-grained.  Future work can explore multi-grained control and develop PLMs that are sufficiently steerable.  Optimization Exploration.  Fine-tuning is the predominant optimization way to distill the linguistic knowledge stored in PLMs to downstream generation tasks.  Now, prompt-based learning has become a performant and lightweight optimization method  [119].  Future work can explore a broader range of optimization approaches that can combine the advantages of current methods.  Language-agnostic PLMs.  Nowadays, almost all the PLMs for text generation are mainly for English.  These PLMs will encounter challenges when dealing with non-English generation tasks.  Therefore, language-agnostic PLMs are worthy to be investigated.  This requires us to capture universal linguistic and semantic features across different languages.  An interesting direction is explore how to reuse existing English-based PLMs for text generation in non-English languages.  Ethical Concern.  Currently, PLMs are pre-trained on large-scale corpora crawled from web without fine-grained filtering, potentially causing ethical issues such as generating private content about users.  Therefore, researchers should try their best to prevent misusing PLMs.  Besides, the text generated by PLMs might be prejudiced, which is in line with the bias in training data along the dimensions of gender, race, and religion  [14].  As a result, we should intervene PLMs for preventing such biases.  The research on the general approach is extensive but still preliminary for PLMs.  In conclusion, text generation based on PLMs has greatly contributed to the advance of the state of the art in this field.  However, the current state of the art in different text generation tasks is still far from what one could expect.  Extensive research efforts are needed to better adapt PLMs to text generation tasks.", "metadata": {"source_file": "2201.05273v4.pdf", "title": "Pre-trained Language Models for Text Generation: A Survey", "authors": ["JUNYI LI∗", "WAYNE XIN ZHAO", "Junyi Li", "Tianyi Tang", "Wayne Xin Zhao", "Jian-Yun Nie", "Ji-Rong Wen", "Montréal"], "year": "2022", "detected_language": "en", "page_count": 35, "origin_chunk_file": "2201.05273v4.chunks.json"}}
{"text": "Susan Zhang∗, Stephen Roller∗, Naman Goyal∗, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott†, Sam Shleifer†, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer Meta AI {susanz,roller,naman}@fb.com Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models. Large language models (LLMs) trained on massive text collections have shown surprising emergent capabilities to generate text and perform zero- and few-shot learning (Brown et al., 2020; Lieber et al., 2021; Smith et al., 2022; Rae et al., 2021; Chowdhery et al., 2022). While in some cases the public can interact with these models through paid APIs, full model access is currently limited to only a few highly resourced labs.2 This restricted access has limited researchers' ability to study how and why these large language models work, hindering ∗Equal contribution. †Work done while at Meta AI. 1Following Brown et al. (2020), we use GPT-3 to refer to both the 175B model and the smaller scale models as well. 2Exceptions include work by EleutherAI, who released dense models up to 20B in size (Black et al., 2022), Salesforce (Nijkamp et al., 2022), and Meta AI, who released dense models up to 13B and sparse models up to 1.1T (Artetxe et al., 2021).  There is also ongoing work from the BigScience workshop ( huggingface.co/), which aims to open source very large multilingual language models and datasets.  progress on improving known challenges in areas such as robustness, bias, and toxicity.  In this technical report, we present Open Pretrained Transformers (OPT), a suite of decoderonly pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.  We train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training.  Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.  Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the collective research community as a whole, which is only possible when models are available for study.  We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request.  Access will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; and those in industry research laboratories.  We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU.  From this implementation, and from using the latest generation of NVIDIA hardware, we are able to develop OPT-175B using only 1/7th the carbon footprint of GPT-3.  While this is a significant achievement, the energy cost of creating such a model is still nontrivial, and repeated efforts to replicate a model of this size will only amplify the growing compute footprint of these LLMs.  We believe the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear 125M\n1.2e−4 2M Table 1: Model architecture details.  We report the number of layers (#L), number of attention heads (#H), and the embedding size (dmodel).  We also report the peak Learning Rate (LR) and global batch size in number of tokens (Batch).  guidelines around responsible AI in general and responsible LLMs in particular, given their centrality in many downstream language applications.  A much broader segment of the AI community needs access to these models in order to conduct reproducible research and collectively drive the field forward.  With the release of OPT-175B and smaller-scale baselines, we hope to increase the diversity of voices defining the ethical considerations of such technologies.  We present results on eight Transformer language models ranging from 125 million to 175 billion parameters.  Architectural details are displayed in Table 1.  In the interest of transparency, and to reduce risk of training instabilities, our models and hyperparameters largely follow Brown et al. (2020), with variations in batch size mostly to obtain increased computational efficiency.  For weight initialization, we follow the same settings provided in the Megatron-LM codebase,4 using a normal distribution with zero mean and standard deviation of 0.006.  Standard deviation for output layers are scaled by a 1.0/ √\n2L term where L is the total number of layers.  All bias terms are initialized as 0, and all models are trained with ReLU activation and a sequence length of 2048.  We use an AdamW optimizer (Loshchilov and Hutter, 2017) with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1.  We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.  A number of mid-flight changes to LR were also required (see Section 2.5).  Our batch sizes range from 0.5M to 4M depending on the model size (see Table 1) and is kept constant throughout the course of training.  We use a dropout of 0.1 throughout, but we do not apply any dropout to embeddings.  We clip gradient norms at 1.0, except for some midflight changes that reduce this threshold down from 1.0 to 0.3 (see Section 2.5).  We also include a gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks (splitting the division by the world size of N into two division operations by √\nThe pre-training corpus contains a concatenation of datasets used in RoBERTa  (Liu et al., 2019b), the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021).  All corpora were previously collected or filtered to contain predominantly English text, but a small amount of non-English data is still present within the corpus via CommonCrawl.  We removed duplicated documents across all datasets by filtering out documents via MinhashLSH (Rajaraman and Ullman, 2011) with a Jaccard similarity ≥.95.  We found the Pile was particularly full of duplicate documents, and advise future researchers using the Pile to perform additional de-duplication processing.  We tokenize all corpora using the GPT-2 byte level BPE tokenizer (Sennrich et al., 2016; Radford et al., 2019; Brown et al., 2020).  Our final corpus contains roughly 180B tokens.  PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021).  To convert the conversational trees into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree.  This reduced the corpus by about 66%.  We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019).  We achieve utilization of up to 147 TFLOP/s per GPU.  We keep Adam state in FP32, since we shard it across all hosts, while the model weights remained in FP16.  To avoid underflows, we used dynamic loss scaling, as described in Micikevicius et al. (2017).  Hardware Failures We faced a significant number of hardware failures in our compute cluster while training OPT-175B. In total, hardware failures contributed to at least 35 manual restarts and the cycling of over 100 hosts over the course of 2 months.  During manual restarts, the training run was paused, and a series of diagnostics tests were conducted to detect problematic nodes.  Flagged nodes were then cordoned off and training was resumed from the last saved checkpoint.  Given the difference between the number of hosts cycled out and the number of manual restarts, we estimate 70+ automatic restarts due to hardware failures.  Loss Divergences Loss divergences were also an issue in our training run.  When the loss diverged, we found that lowering the learning rate and restarting from an earlier checkpoint allowed for the job to recover and continue training.  We noticed a correlation between loss divergence, our dynamic loss scalar crashing to 0, and the l2-norm of the activations of the final layer spiking.  These observations led us to pick restart points for which our dynamic loss scalar was still in a \"healthy\" state (≥1.0), and after which our activation norms would trend downward instead of growing unboundedly.  Our empirical LR schedule is shown in Figure 1.  Early in training, we also noticed that lowering gradient clipping from 1.0 to 0.3 helped with stability; see our released logbook for exact details.  Figure 2 shows our validation loss with respect to training iterations.  We evaluate our model on 16 standard NLP tasks utilized in the literature: HellaSwag (Zellers et al., 2019), StoryCloze (Mostafazadeh et al., 2016), PIQA (Bisk et al., 2020), ARC Easy and Challenge (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), WinoGrad (Levesque et al., 2011), WinoGrande (Sakaguchi et al., 2020), and SuperGLUE (Wang et al., 2019).  We follow GPT-3 (Brown et al., 2020) by using their prompts and overall experimental setup.  We compare primarily to GPT-3, having aimed to re-implement their evaluation settings, but include reported performance of other LLMs on a per-task basis when available (Lieber et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Black et al., 2022)  We report performance in accuracy (omitting F1 for MultiRC and ReCoRD for consistency in evaluation metrics).  For the Winograd Schema Challenge (WSC) task in the SuperGLUE benchmark, we follow (Brown et al., 2020) and formulate the task as multiple choice questions, which is known to affect performance (Liu et al., 2020).  Zero-shot Overall average zero-shot performance across all 14 tasks may be seen in Figure 3.  Overall, we see our average performance follows the trend of GPT-3.  However, performance can vary radically across the tasks: for a full breakdown, see", "metadata": {"source_file": "2205.01068v4.pdf", "title": "[cs.CL] 21 Jun 2022 OPT: Open Pre-trained Transformer Language Models Susan Zhang", "authors": ["Susan Zhang∗", "Stephen Roller∗", "Artetxe", "Moya Chen", "Shuohui Chen", "Christopher Dewan", "Mona Diab", "Xian Li", "Xi Victoria Lin", "Todor Mihaylov", "Myle", "Sam Shleifer", "Kurt Shuster", "Daniel Simig", "Anjali Sridhar", "Tianlu Wang"], "year": "2022", "detected_language": "en", "page_count": 30, "origin_chunk_file": "2205.01068v4.chunks.json"}}
{"text": "CCS Concepts: • Computing methodologies →Machine learning approaches; Neural networks; Natural language generation; • Security and privacy →Human and societal aspects of security and privacy. Additional Key Words and Phrases: machine learning, artificial intelligence, neural networks, trustworthy AI, machine generated text, transformer, text generation, threat modeling, cybersecurity, disinformation, generative AI Since the release of GPT-2 [143] and subsequent explosion of high-quality Transformer-based NLG models, there has been only one general survey on detection of machine generated text [85]. The scope of this previous survey is constrained to detection methods specifically targeting the several generative Transformer models that had been released at the time. Prior to this, a systematic review of machine generated text predating the Transformer architecture covered approaches to detecting previous NLG approaches, such as Markov chains [18]. Our survey differs from previous work in three major ways. First, our survey of machine generated text detection is more comprehensive than previous work. We consider literature on feature-based detection of machine-generated text that was omitted from prior review [61, 106, 133]. Such approaches are a worthy inclusion as feature-based approaches still apply against contemporary NLG models [41, 61, 99], and may provide benefits such as improved robustness against adversarial attacks targeting neural networks [41], or enhanced explainability [99]. Additionally, as research on both NLG and detection has continued to rapidly advance in the years following the previous survey, we must now cover a wider range of generative models and defensive research. Second, in addition to a comprehensive review of detection methods targeting contemporary models, this survey provides an in-depth analysis of the risks posed by NLG models via the process of threat modeling (i.e., identifying potential adversaries, their capabilities and objectives)  [24].  The result of our threat modeling process is a series of threat models that describe scenarios where machine generated text may be abused, the likely methodology of attackers, and existing research related to each threat.  To date, there has yet to be any survey of machine generated text detection with a focus on the risks presented by machine generated text.  Consideration of threat models is vital to set the groundwork for trustworthy development of NLG technology, encourage early development of defensive measures, and minimize potential harms.  Third, guided by the EU Ethics Guidelines for Trustworthy AI  [57] and research community efforts [90], we present our survey with sociotechnical and human-centric considerations integrated throughout, focusing not only on NLG systems and machine text detection technologies, but on the humans who will be exposed to both text generation and detection systems in daily life.  The goal of trustworthy AI is to ensure that AI systems are developed in ways that are lawful, ethical, and robust both from a technical and social perspective.  Abuse of NLG models threatens all three of these areas, representing safety risks to those who may be targeted by NLG-enabled attacks, threats to the integrity of online social spaces, and challenges to the resilience of the technical and social systems that comprise modern society.  Machine text detection is part of protecting against abuse of NLG models, enhancing the robustness and safety of NLG development.  Critically, our survey also includes insight into ensuring defensive machine text detection systems themselves are transparent, fair, and accountable.  To summarize, the major contributions of this work are as follows: • The most complete survey of machine generated text detection to date, including previously omitted feature-based work and findings from recent contemporary research.  •  The first detailed review of the threat models enabled by machine generated text, at a critical juncture where NLG models and tools are rapidly improving and proliferating.  •  A meaningful exploration of both topics through the lens of Trustworthy AI (TAI), considering the ethical and trust impacts of both threat models and detection systems.  MACHINE GENERATED TEXT Before reviewing threat models and detection methodologies for machine generated text, it is helpful to briefly provide a formal definition of machine generated text, and a condensed overview of natural language generation (NLG) models.  We recommend further reading of dedicated surveys on natural language generation for greater insight into the wide breadth of NLG models and applications  [51, 67, 112, 140, 150, 155].  2.1 Definition and Scope In this survey, we use a broad definition of the term \"machine generated text\" which we believe includes all relevant research in the field: We focus our definition of machine generated text on natural language — i.e., text written in human languages that are \"acquired naturally (in [an] operationally defined sense) in association with speech\"  [122] — and exclude non-natural language — i.e., logical languages, programming languages, etc.  Exclusion of non-natural language aligns with other work in the field: the term \"text generation\" is currently considered synonymous with \"natural language generation\"  [112, 199].  We anticipate that \"text generation\" may be repurposed in future research as an umbrella term that includes non-natural language text as well.  This would accommodate common considerations between NLG models and contemporary code generation models, such as Codex  [35] and CodeGenX  [123].  As an example, attacks against StackOverflow or GitHub may include both NLG as well as vulnerable code generation.  Code generation models can also be used to complete programming assignments without triggering common plagiarism detection tools [23].  Our definition of machine generated text is intentionally broad, and covers a large number of possible use cases and associated threat models, which will be discussed in Section 3.  In the interests of managing a survey scope that already spans a wide range of literature and broad sociotechnical context, text generation by means of text adversarial attack will not be considered.  In the majority of cases, the production of new text is not the primary goal of a text adversarial attack, and text adversarial attacks and threat models are already covered by surveys in adversarial attack literature  [34, 80, 190].  We will nevertheless discuss the role machine generated text plays in adversarial contexts in Section 3, as well as adversarial robustness of detection models in Section 5.  Note that this analysis focuses on threat models where a threat actor leverages machine generated text as part of an attack — typically scenarios where the attacker is attempting to pass machine text as human, and where detection of machine generated text may be useful defensively.  We are not discussing attacks against NLG models themselves, unless they leverage NLG as part of the attack.  For example, a white-box training data extraction attack targeting the weights of a commercial speech-to-text model would not be included in our analysis, but using an NLG model to produce data for poisoning that model's training dataset would.  With this definition of machine generated text in mind, and with an understanding of the scope of research under consideration, we proceed to a brief overview of natural language generation.  Using a computer to produce human-like text is well-established in the history of computing.  Turing's proposed \"imitation game\" [179] in 1950 considered the question of machine intelligence based on the ability of a machine to conduct human-like conversation over a text channel, for which the first widely-published method dates back to 1966 with the ELIZA chatbot [192].  Given the large volume of NLG research over the past 55 years, we provide only a high-level taxonomy of major NLG tasks and approaches as groundwork for our analysis of threat models and detection methodologies, and leave detailed discussion to aforementioned dedicated surveys.  2.2.1 Natural Language Generation Tasks.  Recall from §1.1 that there are a wide variety of applications for natural language generation.  Leveraging previous surveys [51, 86, 112], we provide a summary of major tasks in the NLG domain, with examples of models that have been used for each task in Table 1.  Note that many of the models listed are multi-purpose and can be trained for numerous NLG tasks.  In Table 1, we provide a small selection of models that have been used for each task as representative examples.  The summary in Table 1 is not exhaustive, and in reality, a mutually exclusive delineation between input types does not exist.  Combinations of different input types are possible.  As an example, CTRL takes both a discrete control code attribute and conditional text prompt in generation  [93].  Question-answering systems may be able to answer questions about images, such as Unified VLP  [202] and TAG  [187].  We consider a \"topic\" as an attribute in this overview, and so include \"topic-to-text generation\" under the broader umbrella of \"attribute-based generation\", including work such as topic-to-essay generation [59].  Given the strong generative capabilities of Transformer language models, and the corresponding increased risk of associated threat models, Transformer-based models rightly warrant particular emphasis in review.  However, as mentioned in Section 1.2, consideration of the broader field of natural language generation and previous detection research is important as detection techniques that apply against pre-Transformer models have been shown to be useful in detection of modern generative models, and diverse approaches may offer increased adversarial robustness  [41] or better explainability  [99].  2.3 Natural Language Generation Approaches There are a wide range of model architectures and algorithmic approaches to natural language generation.  We categorize these approaches broadly into neural and non-neural methods, and then further break them down into more specific categories.  A diagram of our simplified breakdown can be found in Figure 1.  As previously mentioned, NLG encompasses a large variety of tasks and research areas, with this brief section serving as context for understanding machine generated text threat models and detection methods.  2.3.1 Non-Neural Models.  Predating the popularization of neural approaches in the NLG domain, a range of systems were used to accomplish NLG tasks.  These early approaches can broadly be summarized as \"rule-based\", though there existed variety in terms of processes, pipelines, and targets tasks.  A review of rule-based systems can be found in Reiter and Dale's book on the subject [150].  An alternative approach to purely rule-based approaches is to use an existing natural language corpus to generate rules for components of an NLG system, such as content selection  [53, 104] or template generation  [98].  These statistical approaches are meant to be more adaptable to different domains than strictly rule-based systems.  While many different statistical models have been integrated with NLG systems in various ways, Hidden Markov Models (HMM)  [15] feature prominently in past work.  More recent non-neural research has used reinforcement learning  [82] and hierarchical reinforcement learning [49] of Markov Decision Process (MDP) agents to learn optimal text generation policies.  2.3.2  Non-Transformer Neural Methods.  Natural language generation using neural networks was demonstrated to be highly effective using recurrent neural networks (RNN)  [20, 88, 130], including long short-term memory (LSTM) architectures [128] and gated recurrent units (GRUs)  [139].  However, RNN and LSTM architectures had to contend with the vanishing gradient problem, to which the multi-head attention mechanism of the Transformer architecture is more resilient [177].  Generative adversarial networks (GANs)  [71] — commonly used to generate continuous data (such as images) — can also be adapted to a discrete context for natural language generation  [117, 194].  Deep reinforcement learning (RL) has been used with neural networks to learn policy gradient methods that reward text characteristics associated with high-quality text generation [111].  A related area of work is the usage of inverse reinforcement learning (IRL), which has included work that aims to address reward sparsity and mode collapse problems in GAN-based text generation by learning an optimal reward function and generation policy  [113, 163].  2.3.3  Transformer.  The multi-head attention architecture of Transformer language models [185] currently represents the state-of-the-art in natural language generation across natural language tasks.  Among Transformer models, the unidirectional GPT-2 [143] and GPT-3 [30] models are the most studied in the field of machine generated text detection due to their groundbreaking performance on unconditional and conditional text generation — though like many other Transformer models, these architectures can be used for other NLG tasks as well.  In addition to GPT-2 and GPT-3, also notable are related autoregressive language models using similar architectures, with variations in sampling procedures or training datasets.  Such models include Grover [197] (a GPT-2 style model trained on a news dataset and using nucleus sampling instead of top-𝑘sampling), GPT-J  [186] (a 6-billion parameter autoregressive language model trained on The Pile  [66]), and GPT-NeoX-20B  [25] (a 20-billion parameter model similar to GPT-3, also trained on The Pile  [66]).  Unidirectional Transformer language models generate text by performing self-supervised distribution estimation to predict the next token based on previous tokens.  To do this, the model is trained on an existing set of variable-length example texts (𝑥1,𝑥2, ...,𝑥𝑛) each composed of symbols (𝑠1,𝑠2, ...,𝑠𝑚).  These symbols may be characters, or multi-character tokens obtained through a tokenization process.  The probability of a given text can then be expressed as the conditional probability of the final token, given each previous token.  That is: The self-attention mechanism in the Transformer architecture makes it possible to train neural network architectures that are effectively able to estimate such probabilities, given a suitable pre-training task.  In unidirectional models such as those in the GPT lineage, a common training task is prediction of the next token in sequence.  To generate text, such models can then receive a continue an input sequence by sampling from the probability distribution of all possible next tokens based on previous tokens.  An important parameter in this sampling process is \"temperature\" 𝑇∈(0, ∞), which can be raised above 1 to increase the likelihood of selecting a less-probable next token — improving diversity at the potential cost of choosing an unusual token — or lowered below 1 to bias sampling towards more common tokens.  There are three common decoding strategies used for sampling token probabilities from contemporary unidirectional generative Transformer models [78]: (1) No truncation →Sample from the entire probability distribution.  At 𝑇= 1, this is called \"pure sampling\".  (2) Top-𝑘truncation →Sample from the 𝑘most probable tokens.  (3) Nucleus sampling (also known as top-𝑝truncation) →Sample from tokens in the top-𝑝portion of the probability mass, rather than a fixed number of tokens 𝑘. Alternative methods of sampling are an active area of research in improving text generation.  Such methods include 'typical sampling', in which tokens are selected based on expected information gain, rather than strictly probability of occurrence [126].  While unidirectional generative models are key fixtures of machine generated text detection research, other Transformer architectures can be used for NLG tasks as well.  The architecture of BART [108] includes a bidirectional encoder (similar to BERT  [50]), but maintains a left-to-right decoder for sequential text generation.  Other Transformer architectures such as MASS  [170], T5  [145], and ULMFiT [79] can also be used for NLG tasks.  An important area of ongoing research centers around shaping the output produced by Transformer models.  This can include prompt engineering — carefully crafting the conditional text input for a language model to continue [30] — or by providing additional discrete attributes that can be used to influence the generation of the network, such as control code, topic, or sentiment as in CTRL  [93], PPLM  [47], or GeDi  [100].  Greater control over model output increases the risks posed by threat models [30].  As an example, when generating social media posts as part of an NLG-augmented online influence campaign, an attacker would benefit from being able to ensure that generated comments both 1) mention a targeted political opponent, and 2) demonstrate negative entity sentiment towards the opponent.  We will cover such potential abuses and others in more detail in the next section, which concerns threat models associated with machine generated text.  Machine generated text enables a diverse array of attacks.  These attacks may be performed by threat actors with specific objectives, such as to compromise a computer system, exploit a target individual for financial gain, or enable large-scale harassment of specific communities.  The EU ethics guidelines for trustworthy AI emphasize that unintended or dual-use applications of AI systems should be taken into account, and that steps should be taken to prevent and mitigate abuse of AI systems to cause harm [57].  As such, trustworthy AI in the context of NLG necessitates understanding the areas where such models may be abused, and how these abuses may be prevented (either with detection technologies, moderation mechanisms, government legislation, or platform policies).  When discussing attacks, we discuss not only the direct impact on targets, but also the broader impacts of both attacks and mitigation measures on trust.  To understand the risks that motivate research on detection of machine generated text, we draw from existing literature to present a series of threat models incorporating natural language generation.  Threat modeling reflects the process of thinking like an attacker, identifying vulnerabilities to systems by identifying potential attackers, their capabilities, and objectives.  The goal of threat modeling is to improve the security of systems by considering the greatest threats to systems and their users.  Many methods of threat modeling have been developed over the years, including producing system diagrams, stepping through itemized vulnerability checklists, and performing open-ended brainstorming  [29, 97, 181, 184].  In late 2020, a diverse set of experts formed a threat modeling working group to produce a high-level set of guidelines related to effective threat modeling approaches [27] — we leverage these guidelines in the open-ended attack-centric modeling approach in this section.  3.1 Threat Modeling Fundamentals As we anticipate an audience with varying exposure to cybersecurity topics, before we present threat models related to machine generated text, it is helpful to first provide an overview of threat modeling, and characterize the approach taken in this section.  A basic example of a common threat model is \"a thief who wants to steal your money\"  [164].  We can add detail to this threat model by considering more specific capabilities and objectives that such an attacker might have.  For example, we may consider \"a thief with lockpicks who wants to steal your TV\", or \"a thief who found your banking password in a database dump and wants to transfer money out of your account\".  With these threat models in mind, we can then propose mitigation strategies, such as \"install locks that are resistant to lockpicking\", or \"use multi-factor authentication for online banking\".  Finally, we evaluate whether our mitigation approach is sufficient to address the threat, and consider what other threat models we might need to consider.  Threat modeling is inherently an iterative process  [27, 164].  Shostack's Four Question Frame for Threat Modeling [164, 165] presents best a plain language foundation for threat modeling by posing four simple questions: (1) What are we working on?  →Identify the system under attack.  (2) What can go wrong?  →Determine potential attackers, their capabilities, and objectives.  (3) What are we going to do about it?  →Devise a mitigation strategy.  (4) Did we do a good job?  →Review whether the analysis is accurate and complete.  (1) Identify the system under attack: We provide a broad attack-centric analysis of machine generated text on society, rather than a system-centric analysis focusing on vulnerabilities to a specific IT system.  As such, we identify several discrete technological systems, within the broader societal supersystem.  (2) Determine potential attackers, their capabilities, and objectives: We consider threat actors of varying sophistication and motives, but with a common modus operandi — in all cases, our attacker is an individual or organization exploiting an NLG model.  We characterize the attacker when explaining each attack.  (3) Devise a mitigation strategy: After identifying a threat model, we propose mitigation measures to improve security and reduce risk.  Detection of computer-generated text is often part of the presented mitigation approaches, but policy changes and human moderation systems can also have a significant impact.  (4) Review whether the analysis is accurate and complete: We have given careful thought to the presented threat models, which are formed from perspectives gained across industry, academia, and government.  However, as threat modeling is an iterative process that benefits from diverse perspectives [27], we greatly encourage further analysis of potential attacks and mitigation measures in future research.  The remainder of this section comprises our threat model analysis, grouped according to a breakdown of attacks into four major categories, followed by a concluding discussion.  Within each category we discuss threat models associated with that category of attack, identifying systems at risk, and describing possible threat actors, their objectives, and capabilities.  For each attack, we propose mitigations, and then discuss the trust impacts of both the attack and — crucially — of the proposed mitigations as well.  A taxonomy of the broad categories of attacks using NLG models we discuss can be found in Figure 2.  While a completely exhaustive list of all possible future malicious applications of NLG models is not possible, the threats outlined here span a wide range of tangible dangers at this point in time, representing valuable areas of future investigation for preemptive ethical defensive research.  As previously mentioned, threat modeling is iterative, and it is hoped that these threat models should serve as the foundation for future work in improving security against machine generated text.  3.2.1 Phishing and Scamming.  Phishing attacks center on socially engineering a target individual to perform a desired action.  This might be to convince the target to open an unsafe document that contains an exploit, cause the target to navigate to a fake banking webpage, encourage them to share sensitive information that can be used for identity theft, among many other documented methods [5, 37].  Phishing attacks can target numerous channels, including email, phone, SMS, or chat applications.  Automated messaging approaches in the early stages of phishing campaigns are common [5].  Machine generated text can be a useful tool to an attacker attempting to scale or better target phishing or scam campaigns.  Rather than provide the same message to all targets, NLG can be used to generate target-specific text.  Research has demonstrated the effectiveness of NLG both for scaling of email masquerade attacks [13], and for communitytargeted phishing [69].  Carefully targeting a phishing attack (commonly referred to as \"spear phishing\"), greatly increases the likelihood of a specific target falling for the attack [31].  In the cases of chat messages, NLG models that serve as dialogue agents may be exploited to exchange messages with the target under a pretext before exploiting them [201].  Mitigation of NLG-enabled phishing attacks will be similar to established work on existing phishing attacks, including both automated detection systems, user reporting, and awareness campaigns [1].", "metadata": {"source_file": "2210.07321v4.pdf", "title": "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods", "authors": ["Evan Crothers", "Nathalie Japkowicz", "Herna Viktor"], "year": "2023", "detected_language": "en", "page_count": 36, "origin_chunk_file": "2210.07321v4.chunks.json"}}
{"text": "NLG may present an increased challenge for existing detection systems in that generated messages may have unique or highly-varied content — though attackers may be forced to include specific \"payload\" content for an attack to be effective (e.g., a phishing email may include a unique shortlink to the same fraudulent website, a malicious chatbot may need to socially engineer responses to the same security questions). As text content becomes more varied due to more powerful NLG models, detection of payload content may represent a stable detection feature. Algorithms for detection of machine generated text are also likely to be added to existing automated detection approaches. 3.2.4 Impacts of Attacks and Mitigation on Trust. The usage of NLG models to produce compelling, targetspecific messages as part of large scale phishing attacks and social worms is likely to further reduce trust in text communications, particularly those received from contacts not personally known. Individuals may become even more suspicious of unsolicited messages — even seemingly innocuous ones. As even a seemingly good-natured greeting may be just the first message from a malicious dialogue agent, individuals may decide it is safer to not reply to such messages, further reducing trust and social interaction with new individuals in online communities. NLG-based poisoning attacks against machine learning models will likely have the greatest trust impact on machine learning practitioners, who may be required to carefully scrutinize open-source training data for poisoned samples. Where mitigation of poisoning attacks involves limiting access to training datasets behind auditing and approval processes, such procedures may cause developers to feel distrusted, and undermine the relationship between these individuals and the organizations they work with. While the trust impact of NLGbased data poisoning attacks may be relatively minor among the general population, a high-profile attack (e.g., a poisoning attack against a medical diagnosis model) may cause individuals to lose trust in machine learning systems more broadly, based on concerns that such models are not be safe from malicious tampering.  3.3 Online Influence Campaigns An area of particular concern for abuse of machine generated text is facilitation of online influence campaigns.  The objectives of threat actors in this area may either be political in nature (e.g., disinformation, propaganda, election interference) or commercial in nature (e.g., product promotion, smearing competitors, fake reviews).  In either case, the goal is to promote a particular idea or prompt a particular action among the target audience.  Either type of campaign may both leverage or facilitate other threat models, such as spam, harassment, mass submission of agenda-driven content, phishing, or malware.  The distinction between commercial and political influence campaigns is useful for better understanding threat actors and threatened systems in more detail, as well as categorizing existing research.  3.3.1 Political Influence Campaigns.  Machine generated text as part of political influence campaigns has been analyzed in previous work  [166, 172, 197].  Papers related to the threat of generative language models on online influence operations may use terminology such as terminology \"fake news\"  [197] or \"disinformation\"  [28, 172], or \"domestic and foreign influence operations\"  [28].  The threat actor in a political influence campaign represents an entity who wishes to influence beliefs or prompt action among a target group.  These threat actors might include, as examples: • A political party hiring a group to post unflattering comments online about their political adversaries •  A nation-state disseminating fraudulent news reports to mask human rights abuses • A nation at war attempting to incite the citizenry of an opposing nation to overthrow the government Datasets from past political influence campaigns have been released by Facebook  [157], Reddit [148], and Twitter  [180], including operations attributed to 22 different countries.  Threat actors in this space can be expected to already have the capability of running online political influence campaigns using human employees or contractors, and be familiar with social media automation tools that facilitate registration and management of fraudulent social media accounts.  Large models with strong few-shot performance can simply be provided examples of the desired messaging to produce further propaganda.  We demonstrate this in Table 2, using Russian information operations executed on Twitter during the Syrian civil war to generate additional 'on-message' tweets that promote Syrian Arab Army (SAA) and Russian forces, while discrediting American involvement.  RT @TheArabSource: Confirmed: #US retrains #ISIS and #ISIS-like jihadists in east #Syria and re-brands them 'democratic forces' (VIDEO) - E... RT @Tasnimnews_EN: #Iraq Army Discovers #US Drone in #Daesh Arms Depot near Fallujah\n... ISIS-affiliated band group Wilayat al-Sham recruits new #terrorists in US-controlled #Rukban camp.  GPT-3 Response @ALFaham_  #SAA seized a big amount of #ISIS #ISIL weapons from the convoy in east #Homs countryside today.  #Syria #Damascus #Jobar #ISIS #Washington is willing to perform the functions of #MiddleEastern policeman and play the role of accessory to #Turkish#aggression against #Syria. ...  #Russia's #Air_Force has stepped up airstrikes against camps for the #Islamic_State in the #Terek_Val As consumption of text content is common online, there are many avenues where machine generated text might be utilized by a threat actor to improve scaling and targeting of influence operations.  Social media, due to the large volume of engaged users, is likely to continue be a valuable and vulnerable target for such campaigns [148, 157, 180].  Research on detection of machine generated text for political influence campaigns that focuses on 'fake news' only covers a very small subset of critical threat models.  Fake news detection research often imagines an adversary using an NLG model to produce news-like disinformation at scale [197].", "metadata": {"source_file": "2210.07321v4.pdf", "title": "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods", "authors": ["Evan Crothers", "Nathalie Japkowicz", "Herna Viktor"], "year": "2023", "detected_language": "en", "page_count": 36, "origin_chunk_file": "2210.07321v4.chunks.json"}}
{"text": "Producing massive volumes of news-like content may be a less desirable machine generated text disinformation scaling approach than social messages for several reasons: Regarding mitigation, past research has identified that the average user is overly trusting of profiles with AI-generated photos and GPT-2 text, accepting connection requests from deepfake profiles on LinkedIn 79%–85% of the time [131]. As such, it is unlikely that user reports will serve as an adequate first line of defense. Instead, a combination of automated detection models (including machine generated text detection) and platform moderation efforts should be used to detect political influence campaigns. Among these should be measures to protect against social media abuse more broadly, including detection of account automation, and scrutiny of coordinated inauthentic activity for content amplification. Investigations by disinformation researchers, such as those carried out on Twitter, are likely to remain relevant [180]. 3.3.2 Commercial Influence Campaigns. In commercial influence campaigns, the goal is to influence individuals in a manner that commercially benefits the threat actor. Examples of such campaigns include publishing fraudulent reviews, artificially boosting a website's search engine page ranking, spamming online communities with advertisements for a product, or attempting to inorganically cause promotional content to trend on social media. As with previous categories, there may be overlap between different attacker approaches. A threat model of particular interest is the usage of machine generated text to generate fraudulent reviews that either promote one's own product/service, or target a competitor [2, 99, 172]. Work has been published that demonstrates sentiment-preserving fake reviews, which might be used for such a purpose [2]. Fake reviews can be abused on marketplace websites themselves, or by targeting potential customers on social media platforms. Threat actors may operate such campaigns themselves, or may avail themselves of the thriving online market for fake reviews [77].  Organizations selling fake reviews may become early adopters of open-source NLG models to provide unique and specific reviews at lower cost.  Mitigation of NLG models used for fake reviews on online marketplaces might involve running machine generated text detection on the text of reviews, in addition to other detection systems currently used to combat this problem.  Advanced NLG models should not affect context-based detection methods (e.g., identifying patterns in reviewer usernames, similar account creation times, unusual purchase behaviour, etc.).  It may be more difficult to detect commercial influence campaigns if attackers post content outside marketplace websites.  For example, social media websites (e.g., Facebook, Instagram, Reddit, YouTube comments), map platforms (e.g., Google Maps), or dedicated review sites (e.g., Yelp) may all be locations where false reviews may be posted.  3.3.3 Impacts of Attacks and Mitigation on Trust.  In addition to the risks posed by machine generated text for online influence campaigns, the existence of NLG threat models causes additional damage to trust online.  The perception that any given user on social media may be a bot, can cause users of social media to dismiss others (particularly individuals whom they don't agree with) as \"bots\", rather than acknowledge that other real people may hold different viewpoints.  The net effect of this is reduced trust in the authenticity of online interactions.  Mitigation of NLG-enabled influence operations via automated detection of machine generated text also itself carries potential negative impacts.  Automated detection creates the possibility of mass-suppression of speech online.  Previous work has found that text written by non-native English speakers that included political topics was of high risk of being erroneously detected by a Transformer trained on previous political influence campaigns [42].  As methods based on RoBERTa (also a Transformer) are currently the state of the art for detection of machine generated text  [118, 169], classifiers for machine generated text detection leveraged to combat online influence campaigns must be carefully trained and ethically evaluated to minimize the risk of similar incidences of mass discrimination.  Continued public reporting of influence campaign datasets, such as the regular releases by Twitter for review by researchers [180], would be beneficial to protecting trust in social media moderation.  Language background considerations evoke another problem: there are legitimate reasons why a user may rely on machine generated text.  A person writing in their non-native language may leverage an online translation model to assist them.  While such text may be considered machine generated text, this text is not inauthentic — it nevertheless represents genuine self-expression.  Much of the world relies on translation tools to better participate in online discourse; recall that 1 in 3 internet users aged 16 to 64 have used an online translation tool in the last week [91].  Relying on machine generated text detection alone is therefore likely to produce a solution that is discriminatory, unreliable, and greatly damages trust in social media platforms.  Machine generated text detection should then be used among multiple features, such as account creation times, activity patterns, registered phone numbers, and IP addresses, to determine whether activity is linked together as part of an online influence operation.  3.4.2 Applications and Cover Letters.  Contemporary NLG models can be used to generate large numbers of cover letters or essays for applying for scholarships or to employment opportunities.  Commercial websites already exist for producing cover letters using GPT-3 [134].  While the overall usefulness of human-written cover letters has been debated in business media  [119], they are ostensibly meant to be an earnest reflection of a candidate.  Usage of AI models to generate a cover letter or essay submission is therefore likely to be considered exploitative by organizations who review such submissions.  The threat actors in this case may be individuals (perhaps understandably) looking to save time and improve their employment opportunities by bypassing a cumbersome application process, or a malicious attacker looking to flood a target company with fraudulent submissions (a threat actor which we will discuss further in \"Spam and Harassment\").  Detection of machine generated text may be able to identify artificial cover letters or essays given they are of sufficient length (the odds of successful detection improve with sequence length  [81, 143, 197]).  However, caution should be taken with this approach, as use of AI writing tools is not necessarily exploitative.  Again, individuals writing in a second or third language may rely on translation models or NLG writing assistants to help them write cover letters or scholarship applications.  It may be difficult to differentiate those who mean to exploit such systems (e.g., thoughtlessly spam submissions to as many avenues as possible), and those who are relying on AI writing tools to better express themselves.  As such, a better mitigation approach may be to develop alternative approaches to evaluating candidates, such as placing more emphasis face-to-face discussions with prospective job candidates or award recipients.  3.4.3 Content Generation.  A growing threat model for social media platforms is the possibility that a large number of users may begin using generative AI models (including NLG models) to produce social media content in ways that harms these platforms.  While threat actors in this case may not be overtly malicious, large volumes of content from generative models may dilute the perceived quality of content on a platform, undermine trust in platforms more generally, or create plagiarism concerns.  As a recent example, in response to the release of highly effective AI models for image generation (DALL-E [146], Stable Diffusion [152]), a number of art websites have enacted a blanket ban against all AI-generated art  [55].  Video is a particularly important medium on modern social media: there are approximately 4.95 billion Internet users on Earth  [92], of these, an estimated 92.6 percent watch digital videos each week  [91].  The interplay between social media creators and generative models represents important sociotechnical context to avoid common Fair ML traps [160].  Award-winning online commentator Drew Gooden performed a video demonstration of GPT-3-based writing assistant Jasper [84], critiquing applications of Jasper for production of video scripts and social media content [70].  When attempting to generate a bio for a company website, Gooden found that Jasper produced a sample that directly plagiarized a Newswire article (timestamp 11:55).  Gooden also noted that utilizing such a tool without disclosure would violate the trust of viewers (timestamp 4:22).  Mitigations of threats related to undesired inclusion of NLG content in social media may involve similar blanket bans to those targeting AI-generated art  [55], or policies that mandate pre-emptive disclosure of the usage of AI tools as part of a platform's terms of service (similar to the requirements mandated in the Responsible AI License  [60]).  The difficult enforcement of such policies would likely necessitate a combination of machine generated text detection algorithms and moderator investigations.  submissions they publish are to be completely written by humans.  Similarly, some organizations may also be concerned with spam of low-quality machine generated content submissions overwhelming editorial staff, or wish to reduce the risk of plagiarism or copyright infringement as some models have been found to memorize training data which can emerge during inference  [33].  We distinguish spam and harassment from other categories of attacks by focusing on cases where the goal of the attack is to harm a platform or users using a large volume of content.  As in previous cases, there are overlaps with other threat models, but the distinction of spam use-cases is useful for understanding how attacks using machine generated text impact platforms when deployed at large scale.  3.5.2  Harassment.  Techniques similar to spamming may be used to cause distress to individuals or communities by targeting them with a large volume of messages.  An individual or group of motivated individuals may register social media accounts to be controlled by automation tools, or use a common bot to post from their own account, in order to generate a large volume of messages targeting a particular individual or community.  SMS and phone call automation tools may facilitate such approaches outside social media as well.  The motivations of threat actors engaging in such behaviour may range from personal grudges to political objectives.  Online communities formed around religion, racial identity, sexual orientation, or gender expression, may be at risk of brigading [7] from hate groups using such models to flood them with abuse.  Political figures or political discussion boards of all stripes may be at risk from large-scale automated harassment from motivated enemies among their political adversaries.  Mitigation measures similar to spamming apply for counteracting harassment as well — the best defenses include verification that an individual is human prior to making a post or sending a message, targeting the automation of delivery rather than the machine generated text.  3.5.4 Impacts of Attacks and Mitigation on Trust.  Similar to other attacks, spam and harassment harms the assumption in online communities that other users online represent real humans.  Even following the deactivation of the deployed GPT-4chan bot, discussion on 4chan continued to express concern that subsequent posts may be made by NLG models  [95].  The more frequently individuals knowingly encounter such models in social media, the less trust they will have in the integrity of online social spaces.  Mitigation of such attacks would incorporate increased verification of human posting activity.  Such restrictions would likely include limitations on usage of known proxies and VPNs; potentially requiring the provision of additional information on sign-up (e.g., emails, phone numbers, payment methods, government IDs); and an increased incidence of CAPTCHA challenges.  The overall result of this is a reduction of online privacy, and increased barriers to participation in online discussion — both of which may harm user trust in online platforms.  Finally, as spamming or harassment operations can be very disruptive, they may represent a highly visible case of AI model abuse.  As such, the abuse of such models in online communities may cause a general decrease in public trust towards AI model development, and NLG models in particular.  Within this section we have discussed a wide range of threat models associated with natural language generation.  We summarize our key findings as follows: • NLG models have significant potential for abuse in improving scaling and targeting of existing attacks • Platforms that receive text submissions of any kind are likely to face a growing influx of machine-generated text content, particularly as user-friendly tools continue to be developed  [84, 134] • Much of the research on NLG-enabled influence operations focuses on AI-generated news articles, while sociological data suggest that machine generated comments may pose a much greater threat •  While NLG models may make detection of automated coordinated inauthentic activity more difficult, abuse often still requires bypassing existing defenses such as IP reputation checks and CAPTCHA  [4] Future threat modeling and observed cyberattacks will certainly augment the threat models discussed in this section, but we have now provided sufficient motivation for exploring the defensive capabilities offered by machine generated text detection.  In the next section we will discuss the current status of research on detection of machine generated text, and outline the major findings in the field thus far.  DETECTION OF MACHINE GENERATED TEXT Analysis of threat models indicates that the detection of machine generated text, when utilized correctly, is a valuable tool for reducing the harms of NLG model abuse.  Detection of machine generated text is typically framed as a binary classification problem in which a classifier is trained to differentiate samples of machine generated text from human generated text [41, 106, 133, 169, 197], though there exists related research in attribution of machine generated text to the model that generated it  [132, 182] which we will discuss in §5.2.  In this section, we outline the methods used for detection of machine generated text.  In §4.1 we summarize feature-based approaches in machine generated text detection, while §4.2 covers detection approaches based around neural language models.  In §4.3, we survey domain-specific research on applications of machine generated text detection.  In §4.4, we review the ability of human reviewers to correctly identify machine generated text, and human-aided machine generated text detection.  In §4.5 we discuss trends in evaluation methodology within detection research.  Finally, in §4.6, we explain prompt injection: a method of shaping NLG model responses, which may be useful in facilitating detection.  Table 3 provides a summary of major detection methods and their evaluation in current research.  4.1 Feature-Based Approaches Machine generated text often differs from human text in ways that be identified using statistical techniques [41, 61, 133].  Feature-based approaches to machine generated text detection apply natural language processing to create feature vectors from input sequences, and classify these feature vectors using a downstream classification algorithm, such as a support-vector machines (SVM), random forest (RF), or neural network (NN)  [61, 133].  We provide a summary of the categories of features that have been used in prior art, with", "metadata": {"source_file": "2210.07321v4.pdf", "title": "Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods", "authors": ["Evan Crothers", "Nathalie Japkowicz", "Herna Viktor"], "year": "2023", "detected_language": "en", "page_count": 36, "origin_chunk_file": "2210.07321v4.chunks.json"}}
{"text": "Abstract—Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques.  In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.  Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.  This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.", "metadata": {"source_file": "2303.18223v16.pdf", "title": "L A Survey of Large Language Models  [cs.CL] 11 Mar 2025", "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li*", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Zhipeng Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "Jian-Yun Nie", "Ji-Rong Wen"], "year": "2025", "detected_language": "en", "page_count": 144, "origin_chunk_file": "2303.18223v16.chunks.json"}}
{"text": "Wayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen ANGUAGE is a prominent ability in human beings to express and communicate, which develops in early childhood and evolves over a lifetime [3, 4]. Machines, however, cannot naturally grasp the abilities of understanding and communicating in the form of human language, unless equipped with powerful artificial intelligence (AI) algorithms. It has been a longstanding research challenge to achieve this goal, to enable machines to read, write, and communicate like humans [5]. Technically, language modeling (LM) is one of the major approaches to advancing language intelligence of machines. In general, LM aims to model the generative likelihood of word sequences, so as to predict the probabilities of future (or missing) tokens. The research of LM has received extensive attention in the literature, which can be divided into four major development stages: • Statistical language models (SLM). SLMs [6–9] are developed based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, e.g., predicting the next word based on the most recent context. The SLMs with a fixed context length n are also called n-gram language models, e.g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval (IR) [10, 11] and natural language processing (NLP) [12–14]. However, they often suffer from the curse of dimensionality: it is difficult to accurately estimate high-order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies such as backoff estimation [15] and Good–Turing estimation [16] have been introduced to alleviate the data sparsity problem.  •  Neural language models (NLM).  NLMs [1, 17, 18] characterize the probability of word sequences by neural networks, e.g., multi-layer perceptron (MLP) and recurrent neural networks (RNNs).  As a remarkable contribution, the work in [1] introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features (i.e., the distributed word vectors).  By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end-to-end solution for Fig.  1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \"language model\" (since June 2018) and \"large language model\" (since October 2019), respectively.  The statistics are calculated using exact match by querying the keyphrases in title or abstract by months.  We set different x-axis ranges for the two keyphrases, because \"language models\" have been explored at an earlier time.  We label the points corresponding to important landmarks in the research progress of LLMs.  A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers that contain \"large language model\" in title or abstract goes from 0.40 per day to 8.58 per day (Figure 1(b)).  various NLP tasks [2].  Furthermore, word2vec [19, 20] was proposed to build a simplified shallow neural network for learning distributed word representations, which were demonstrated to be very effective across a variety of NLP tasks.  These studies have initiated the use of language models for representation learning (beyond word sequence modeling), having an important impact on the field of NLP.  • Pre-trained language models (PLM).  As an early attempt, ELMo [21] was proposed to capture context-aware word representations by first pre-training a bidirectional LSTM (biLSTM) network (instead of learning fixed word representations) and then fine-tuning the biLSTM network according to specific downstream tasks.  Furthermore, based on the highly parallelizable Transformer architecture [22] with self-attention mechanisms, BERT [23] was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled corpora.  These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks.  This study has inspired a large number of follow-up work, which sets the \"pre-training and fine-tuning\" learning paradigm.  Following this paradigm, a great number of studies on PLMs have been developed, introducing either different architectures  [24, 25] (e.g., GPT-2  [26] and BART  [24]) or improved pre-training strategies  [27–29].  In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks.  • Large language models (LLM).  Researchers find that scaling PLM (e.g., scaling model size or data size) often leads to an improved model capacity on downstream tasks (i.e., following the scaling law [30]).  A number of studies have explored the performance limit by training an ever larger PLM (e.g., the 175B-parameter GPT-3 and the 540Bparameter PaLM).  Although scaling is mainly conducted in model size (with similar architectures and pre-training tasks), these large-sized PLMs display different behaviors from smaller PLMs (e.g., 330M-parameter BERT and 1.5Bparameter GPT-2) and show surprising abilities (called emergent abilities [31]) in solving a series of complex tasks.  For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well.  Thus, the research community coins the term \"large language models (LLM)\"1 1.  Note that a LLM is not necessarily more capable than a small PLM, and emergent abilities may not occur in some LLMs.  2. conducts a literature review of the recent advances in LLMs from four major aspects, including pre-training (how to pretrain a capable LLM), adaptation (how to effectively adapt pre-trained LLMs for better use), utilization (how to use LLMs for solving various downstream tasks) and capability evaluation (how to evaluate the abilities of LLMs and existing empirical findings).  We thoroughly comb the literature and summarize the key findings, techniques, and methods of LLMs.  For this survey, we also create a GitHub project website by collecting the supporting resources for LLMs, at the link We are also aware of several related review articles on PLMs or LLMs  [32, 36, 38, 39, 43, 48–54].  These papers either discuss PLMs or some specific (or general) aspects of LLMs.  Compared with them, we focus on the techniques and methods to develop and use LLMs and provide a relatively comprehensive reference to important aspects of LLMs.  The remainder of this survey is organized as follows: Section 2 introduces the background for LLMs and the evolution of GPT-series models, followed by the summarization of available resources for developing LLMs in Section 3.  Sections 4, 5, 6, and 7 review and summarize the recent progress from the four aspects of pre-training, adaptation, utilization, and capacity evaluation, respectively.  Then, Section 8 discusses the practical guide for prompt design, and Section 9 reviews the applications of LLMs in several representative domains.  Finally, we conclude the survey in Section 10 by summarizing the major findings and discuss the remaining issues for future work.  In this section, we present an overview about the background of LLMs and then summarize the technical evolution of the GPT-series models.  Typically, large language models (LLMs) refer to Transformer language models that contain hundreds of billions (or more) of parameters4, which are trained on massive text data  [32], such as GPT-3  [55], PaLM [56], Galactica  [35], and LLaMA  [57].  LLMs exhibit strong capacities to understand natural language and solve complex tasks (via text generation).  To have a quick understanding of how LLMs work, this part introduces the basic background for LLMs, including scaling laws, emergent abilities and key techniques.  Formulation of Scaling Laws for LLMs.  Currently, LLMs are mainly built upon the Transformer architecture  [22], where multi-head attention layers are stacked in a very deep neural network.  Existing LLMs adopt similar Transformer architectures and pre-training objectives (e.g., language modeling) as small language models.  However, LLMs significantly extend the model size, data size, and total compute (orders of magnification).  Extensive research has 4.  In existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute.  In this survey, we take a slightly loose definition of LLMs, and mainly focus on discussing language models with a model size larger than 10B. shown that scaling can largely improve the model capacity of LLMs  [26, 55, 56].  Thus, it is useful to establish a quantitative approach to characterizing the scaling effect.  Next, we introduce two representative scaling laws for Transformer language models  [30, 34].  • KM scaling law5.  In 2020, Kaplan et al.  [30] (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N), dataset size (D), and the amount of training compute (C), for neural language models.  Given a compute budget c, they empirically presented three basic formulas for the scaling law6: where L(·) denotes the cross entropy loss in nats, and a follow-up study  [58] from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distribution) and reducible loss (an estimate of the KL divergence between the true and model distributions).  The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768 to 1.5B nonembedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors).  They showed that the model performance has a strong dependence relation on the three factors.  •  Chinchilla scaling law.  As another representative study, Hoffmann et al.  [34] (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the computeoptimal training for LLMs.  They conducted rigorous experiments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients as below [34]: where E = 1.69, A = 406.4, B = 410.7, α = 0.34 and β = 0.28.  By optimizing the loss L(N, D) under the constraint C ≈6ND, they showed that the optimal allocation of compute budget to model size and data size can be derived as follows: where a = α\nα+β , b = β\nα+β and G is a scaling coefficient that can be computed by A, B, α and β.  As analyzed in [34], given an increase in compute budget, the KM scaling law favors a larger budget allocation in model size than the data size, while the Chinchilla scaling law argues that the two sizes should be increased in equal scales, i.e., having similar values for a and b in Equation (3).  Discussion on Scaling Laws.  After introducing the formulations, we continue to discuss scaling law in the following two aspects, to enhance its understanding: • Predictable scaling.  In practice, scaling law can be used to instruct the training of LLMs, and it has been proven feasible to reliably estimate the performance of larger models based on that of smaller models, called predictable scaling [46].  The benefits of predictable scaling for training LLMs are mainly twofold.  Firstly, for large models, it is infeasible to rigorously examine various training tricks or variants, and it would be very helpful if experiences gained from small models could also apply to large models.  For instance, small proxy models can be trained to find the optimal schedule of the data mixture for large models [59].  Secondly, the training of large-scale models takes a long time, often suffering from issues such as training loss spike, and scaling law can be employed to monitor the training status of LLMs, e.g., identifying abnormal performance at an early time.  Despite that scaling law characterizes a smooth trend of performance increase (or loss decrease), it also indicates that diminishing returns7 might occur as model scaling.  An empirical study [58] from the OpenAI team has shown that representation quality or semantic content can still effectively improve even if approaching the point of diminishing returns (i.e., approaching the irreducible loss)  [58].  This finding suggests that training large models are promising for improving the performance of downstream tasks.  To further explore scaling effect, a potential issue is that the amount of available data for training LLMs is actually limited.  With the ever-increasing model scale, the public text data would be soon \"exhausted\" for LLMs  [60].  Thus, it will be meaningful to study how scaling laws apply to a data-constrained regime [61], where data repetition or augmentation might be useful to alleviate data scarcity.  • Task-level predictability.  Existing research of scaling laws are mostly conducted in terms of language modeling loss (e.g., per-token cross-entropy loss in nats [30]), while in practice we are more concerned about the performance of LLMs on actual tasks.  Thus, a basic problem is that how the decrease of language modeling loss translates into the improvement of task performance [58].  Intuitively, a model with a smaller language modeling loss tends to yield a better performance on downstream tasks, since language modeling loss can be considered as a general measure of the overall model capacity.  GPT-4  [46] has reported that some capabilities (e.g., coding ability) can be accurately predicted via scaling law.  Despite that, readers should be aware that a direct decrease in language modeling loss does not always indicate an improvement of model performance on downstream tasks.  Specially, the phenomenon of inverse scaling would occur for some tasks, where task performance surprisingly becomes worse as the language modeling loss decreases [62].  Overall, it is more difficult to explore and characterize task-level scaling laws, since it might be also dependent on task-related information (task metric, task difficulty, etc.).  Furthermore, some capacities (e.g., in-context learning [55]) are unpredictable according to the scaling law, which can be observed only when the model size exceeds a certain level (as discussed below).  Emergent Abilities of LLMs.  In the literature [31], emergent abilities of LLMs are formally defined as \"the abilities that are not present in small models but arise in large models\", which is one of the most prominent features that distinguish LLMs from previous PLMs.  It further introduces a notable characteristic when emergent abilities occur [31]: performance rises significantly above random when the scale reaches a certain level.  By analogy, such an emergent pattern has close connections with the phenomenon of phase transition in physics  [31, 63].  In principle, emergent abilities can be defined in relation to some complex tasks [31, 64], while we are more concerned with general abilities that can be applied to solve a variety of tasks.  Here, we briefly introduce three typical emergent abilities for LLMs and representative models that possess such an ability8.  •  In-context learning.  The in-context learning (ICL) ability is formally introduced by GPT-3  [55]: assuming that the language model has been provided with a natural language instruction and/or several task demonstrations, it can generate the expected output for the test instances by completing the word sequence of input text, without requiring additional training or gradient update9.  Among the GPTseries models, the 175B GPT-3 model exhibited a strong ICL ability in general, but not the GPT-1 and GPT-2 models.  Such an ability also depends on the specific downstream task.  For example, the ICL ability can emerge on the arithmetic tasks (e.g., the 3-digit addition and subtraction) for the 13B GPT-3, but 175B GPT-3 even cannot work well on the Persian QA task [31].  •  Instruction following.  By fine-tuning with a mixture of multi-task datasets formatted via natural language descriptions (called instruction tuning), LLMs are shown to perform well on unseen tasks that are also described in the form of instructions [28, 66, 67].  With instruction tuning, LLMs are enabled to follow the task instructions for new tasks without using explicit examples, thus having an improved generalization ability.  According to the experiments in [67], instruction-tuned LaMDA-PT [68] started to significantly outperform the untuned one on unseen tasks when the model size reached 68B, but not for 8B or smaller model sizes.  A recent study  [69] found that a model size of 62B is at least required for PaLM to perform well on various tasks in four evaluation benchmarks (i.e., MMLU, BBH, TyDiQA and MGSM), though a much smaller size might suffice for some specific tasks (e.g., MMLU).  • Step-by-step reasoning.  For small language models, it is usually difficult to solve complex tasks that involve 8.  It is difficult to accurately examine the critical size for emergent abilities of LLMs (i.e., the minimum size to possess an ability), since it might vary for different models or tasks.  Also, existing studies often test emergent abilities on very limited model sizes for a specific LLM.  For example, PaLM is often tested with three sizes of 8B, 62B and 540B.  It is unclear about the model performance of the untested sizes.  9.  In a recent study [65], it also shows that in-context learning implicitly performs meta-optimization through the attention mechanism.  multiple reasoning steps, e.g., mathematical word problems.  In contrast, with the chain-of-thought (CoT) prompting strategy  [33], LLMs can solve such tasks by utilizing the prompting mechanism that involves intermediate reasoning steps for deriving the final answer.  This ability is speculated to be potentially obtained by training on code  [33, 47].  An empirical study", "metadata": {"source_file": "2303.18223v16.pdf", "title": "L A Survey of Large Language Models  [cs.CL] 11 Mar 2025", "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li*", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Zhipeng Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "Jian-Yun Nie", "Ji-Rong Wen"], "year": "2025", "detected_language": "en", "page_count": 144, "origin_chunk_file": "2303.18223v16.chunks.json"}}
{"text": "[33] has shown that CoT prompting can bring performance gains (on arithmetic reasoning benchmarks) when applied to PaLM and LaMDA variants with a model size larger than 60B, while its advantage over the standard prompting becomes more evident when the model size exceeds 100B. Furthermore, the performance improvement with CoT prompting seems to be also varied for different tasks, e.g., GSM8K > MAWPS > SWAMP for PaLM [33]. How Emergent Abilities Relate to Scaling Laws. In existing literature [30, 31, 34], scaling laws and emergent abilities provide two perspectives to understand the advantage of large models over small models. In general, scaling law (often measured by language modeling loss) describes predictable performance relation with the potential effect of diminishing returns, while emergent abilities (often measured by task performance) are unpredictable but very profitable once such abilities actually emerge. Since the two perspectives reflect different performance trends (continuous improvement v.s. sharp performance leap), they might lead to misaligned findings or observations. There are also extensive debates on the rationality of emergent abilities. A popular speculation is that emergent abilities might be partially attributed to the evaluation setting for special tasks (e.g., the discontinuous evaluation metrics) [70, 71]: when evaluation metrics are altered accordingly, the sharpness of the emergent ability curve would disappear. However, the performance of LLMs on most tasks are perceived by users naturally in a discontinuous way. For instance, end users prefer a reliable code generated by LLMs that can successfully pass the test case, but are less interested in selecting a better code with fewer errors between two failed ones. More recently, a study [72] proposes a new evaluation setting that can enlarge the resolution of task metrics, making task performance more predictable. Despite these efforts, more fundamental research (e.g., grokking10) about the working mechanism of LLMs is still in need to understand the emergence of certain abilities.  The subtle relation between scaling law and emergent abilities can be explained by analogy with the ability acquisition of human11.  Take the speaking ability as an example.  For children, language development (especially infants) can be also considered as a multi-level process where \"emergent abilities\" occur.  Specially, the language ability would relatively stable within a time interval, but qualitative change only occurs when evolving into another ability level (e.g., from speaking simple words to speaking simple sentences).  Such a learning process is essentially not smooth and stable (i.e., language ability does not develop at a constant rate over time), though a child actually grows 10.  Grokking refers that \"a pattern in the data, improving generalization performance from random chance level to perfect generalization\", quoted from the original paper [73].  11.  This explanation is only for ease of understanding, and there is not direct evidence to connect the two points.  every day.  It is interesting that young parents would be often surprised by unexpected progress of the speaking ability exhibited by their babies.  Key Techniques for LLMs.  It has been a long way that LLMs evolve into the current state: general and capable learners.  In the development process, a number of important techniques are proposed, which largely improve the capacity of LLMs.  Here, we briefly list several important techniques that (potentially) lead to the success of LLMs, as follows.  • Scaling.  As discussed in previous parts, there exists an evident scaling effect in Transformer language models: larger model/data sizes and more training compute typically lead to an improved model capacity [30, 34].  As two representative models, GPT-3 and PaLM explored the scaling limits by increasing the model size to 175B and 540B, respectively.  Since compute budget is usually limited, scaling laws can be further employed to conduct a more compute-efficient allocation of the compute resources.  For example, Chinchilla (with more training tokens) outperforms its counterpart model Gopher (with a larger model size) by increasing the data scale with the same compute budget  [34].  In addition, data scaling should be with careful cleaning process, since the quality of pre-training data plays a key role in the model capacity.  •  Training.  Due to the huge model size, it is very challenging to successfully train a capable LLM.  Distributed training algorithms are needed to learn the network parameters of LLMs, in which various parallel strategies are often jointly utilized.  To support distributed training, several optimization frameworks have been released to facilitate the implementation and deployment of parallel algorithms, such as DeepSpeed [74] and Megatron-LM  [75–77].  Also, optimization tricks are also important for training stability and model performance, e.g., restart to overcome training loss spike [56] and mixed precision training [78].  More recently, GPT-4 [46] proposes to develop special infrastructure and optimization methods that reliably predict the performance of large models with much smaller models.  •  Ability eliciting.  After being pre-trained on large-scale corpora, LLMs are endowed with potential abilities as general-purpose task solvers.  These abilities might not be explicitly exhibited when LLMs perform some specific tasks.  As the technical approach, it is useful to design suitable task instructions or specific in-context learning strategies to elicit such abilities.  For instance, chain-of-thought prompting has been shown to be useful to solve complex reasoning tasks by including intermediate reasoning steps.  Furthermore, we can perform instruction tuning on LLMs with task descriptions expressed in natural language, for improving the generalizability of LLMs on unseen tasks.  These eliciting techniques mainly correspond to the emergent abilities of LLMs, which may not show the same effect on small language models.  •  Alignment tuning.  Since LLMs are trained to capture the data characteristics of pre-training corpora (including both high-quality and low-quality data), they are likely to generate toxic, biased, or even harmful content for humans.  It is necessary to align LLMs with human values, e.g., helpful, honest, and harmless.  For this purpose, InstructGPT [66] designs an effective tuning approach that enables LLMs to follow the expected instructions, which utilizes the technique of reinforcement learning with human feedback  [66, 79].  It incorporates human in the training loop with elaborately designed labeling strategies.  ChatGPT is indeed developed on a similar technique to InstructGPT, which shows a strong alignment capacity in producing high-quality, harmless responses, e.g., rejecting to answer insulting questions.  • Tools manipulation.  In essence, LLMs are trained as text generators over massive plain text corpora, thus performing less well on the tasks that are not best expressed in the form of text (e.g., numerical computation).  In addition, their capacities are also limited to the pre-training data, e.g., the inability to capture up-to-date information.  To tackle these issues, a recently proposed technique is to employ external tools to compensate for the deficiencies of LLMs  [80, 81].  For example, LLMs can utilize the calculator for accurate computation [80] and employ search engines to retrieve unknown information  [81].  More recently, ChatGPT has enabled the mechanism of using external plugins (existing or newly created apps)12, which are by analogy with the \"eyes and ears\" of LLMs.  Such a mechanism can broadly expand the scope of capacities for LLMs.  In addition, many other factors (e.g., the upgrade of hardware) also contribute to the success of LLMs.  Currently, we limit our discussion to the major technical approaches and key findings for developing LLMs.  Due to the excellent capacity in communicating with humans, ChatGPT has ignited the excitement of the AI community since its release.  ChatGPT is developed based on the powerful GPT model with specially optimized conversation capacities.  Considering the ever-growing interest in ChatGPT and GPT models, we add a special discussion about the technical evolution of the GPT-series models, to briefly summarize the progress how they have been developed in the past years.  Meanwhile, we drew a schematic diagram depicting the technological evolution of the GPT-series models in Figure 4.  The basic principle underlying GPT models is to compress the world knowledge into the decoder-only Transformer model by language modeling, such that it can recover (or memorize) the semantics of world knowledge and serve as a general-purpose task solver.  Two key points to the success are (I) training decoder-only Transformer language models that can accurately predict the next word and (II) scaling up the size of language models.  Overall, the research of OpenAI on LLMs can be roughly divided into the following stages13.  Early Explorations.  According to one interview with Ilya Sutskever14 (a co-founder and chief scientist of OpenAI), the idea of approaching intelligent systems with language models was already explored in the early days of OpenAI, while it was attempted with recurrent neural networks (RNN)  [121].  With the advent of Transformer, OpenAI developed two initial GPT models, namely GPT-1  [122] and GPT-2  [26], which can be considered as the foundation to more powerful models subsequently i.e., GPT-3 and GPT-4.  •  GPT-1.  In 2017, the Transformer model [22] was introduced by Google, and the OpenAI team quickly adapted their language modeling work to this new neural network architecture.  They released the first GPT model in 2018, i.e., GPT-1  [122], and coined the abbreviation term GPT as the model name, standing for Generative Pre-Training.  GPT-1 was developed based on a generative, decoder-only Transformer architecture, and adopted a hybrid approach of unsupervised pre-training and supervised fine-tuning.  GPT1 has set up the core architecture for the GPT-series models and established the underlying principle to model natural language text, i.e., predicting the next word.  • GPT-2.  Following a similar architecture of GPT-1, GPT-2  [26] increased the parameter scale to 1.5B, which was trained with a large webpage dataset WebText.  As claimed in the paper of GPT-2, it sought to perform tasks via unsupervised language modeling, without explicit fine-tuning using labeled data.  To motivate the approach, they introduced a probabilistic form for multi-task solving, i.e., p(output|input, task) (similar approaches have been adopted in [123]), which predicts the output conditioned on the input and task information.  To model this conditional probability, language text can be naturally employed as a unified way to format input, output and task information.  In this way, the process of solving a task can be cast as a word prediction problem for generating the solution text.  Further, they introduced a more formal claim for this idea: \"Since the (task-specific) supervised objective is the same as the unsupervised (language modeling) objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective (for various tasks)\"  [26]15.  A basic understanding of this claim is that each (NLP) task can be considered as the word prediction problem based on a subset of the world text.  Thus, unsupervised language modeling could be capable in solving various tasks, if it was trained to have sufficient capacity in recovering the world text.  These early discussion in GPT-2's paper echoed in the interview of Ilya Sutskever by Jensen Huang: \"What the neural network learns is some representation of the process that produced the text.  This text is actually a projection of the world...the more accurate you are in predicting the next word, the higher the fidelity, the more resolution you get in this process...\"16.  Capacity Leap.  Although GPT-2 is intended to be an \"unsupervised multitask learner\", it overall has an inferior performance compared with supervised fine-tuning stateof-the-art methods.  Because it has a relatively small model size, it has been widely fine-tuned in downstream tasks, especially the dialog tasks  [124, 125].  Based on GPT-2, GPT-3 Adaptation Evaluation Model Release Time Size (B) Base Model IT RLHF Pre-train Data Scale Latest Data Timestamp Hardware (GPUs / TPUs)  Training Time ICL CoT T5 [82] Oct-2019\n- 1T tokens Apr-2019 1024 TPU v3 -\n- 1T tokens -\nPanGu-α  [84] Apr-2021 13* -\n1.1TB -\n- 2.6TB -\nT5 ✓\n- CodeGen  [86] Mar-2022\n- 577B tokens -\nTk-Instruct [88] Apr-2022\nT5 ✓\n- 256 TPU v3 4 h ✓\n- 1T tokens Apr-2019 512 TPU v4 -\n- 180B tokens -\nCodeGeeX  [92] Sep-2022\n- 850B tokens -\n- 400B tokens -\nT5 ✓\n✓ BLOOM  [78] Nov-2022\n- 366B tokens -\nmT5 ✓\n- Galactica [35] Nov-2022\n- 106B tokens -\nBLOOMZ [94] Nov-2022\nBLOOM ✓\n- OPT-IML [95] Dec-2022\nOPT ✓\n- 1.4T tokens -\n- Pythia  [96] Apr-2023\n- 300B tokens -\n- 400B tokens -\nStarCoder  [98] May-2023 15.5 -\n1T tokens -\n✓ 2T tokens -\n✓ 2.6T tokens -\n✓ 3T tokens -\n- 311B tokens -\n- 300B tokens -\n- 1T tokens -\nGPT-3 -\n- 100B tokens May-2020 -\n- 375B tokens -\n- 300B tokens -\nHyperCLOVA  [108] Sep-2021\n- 300B tokens -\n- FLAN  [67] Sep-2021 137 LaMDA-PT ✓ -\n- 180B tokens -\nAnthropic [110] Dec-2021\n- 400B tokens -\nWebGPT  [81] Dec-2021\nGPT-3 -\n- Gopher  [64] Dec-2021\n- 300B tokens -\n- ERNIE 3.0 Titan [111] Dec-2021\n280B tokens -\n- LaMDA  [68] Jan-2022\n- 768B tokens -\n- 270B tokens -\nAlphaCode [114] Feb-2022\n- 967B tokens Jul-2021 -\n- InstructGPT [66] Mar-2022\nGPT-3 ✓\n- Chinchilla  [34] Mar-2022\n- 1.4T tokens -\n- 780B tokens -\n- 1.3T tokens -\n✓ Sparrow  [116] Sep-2022\n- 64 TPU v3 -\n- 300B tokens -\nPaLM -\n- 512 TPU v4 5 d ✓\n✓ Flan-PaLM  [69] Oct-2022\nPaLM ✓\n✓ Flan-U-PaLM [69] Oct-2022\nU-PaLM ✓\n✓ PanGu-Σ [119] Mar-2023 1085 PanGu-α -\n- 329B tokens -\n- Fig.  4: A brief illustration for the technical evolution of GPT-series models.  We plot this figure mainly based on the papers, blog articles and official APIs from OpenAI.  Here, solid lines denote that there exists an explicit evidence (e.g., the official statement that a new model is developed based on a base model) on the evolution path between two models, while dashed lines denote a relatively weaker evolution relation.  demonstrates a key capacity leap by scaling of the (nearly same) generative pre-training architecture.  • GPT-3.  GPT-3  [55] was released in 2020, which scaled the model parameters to an ever larger size of 175B. In the GPT-3's paper, it formally introduced the concept of in-context learning (ICL)17, which utilizes LLMs in a fewshot or zero-shot way.  ICL can teach (or instruct) LLMs to understand the tasks in the form of natural language text.  With ICL, the pre-training and utilization of LLMs converge to the same language modeling paradigm: pre-training predicts the following text sequence conditioned on the context, while ICL predicts the correct task solution, which can be also formatted as a text sequence, given the task description and demonstrations.  GPT-3 not only demonstrates very excellent performance in a variety of NLP tasks, but also on a number of specially designed tasks that require the abilities of reasoning or domain adaptation.  Although the GPT-3's paper does not explicitly discuss the emergent abilities of LLMs, we can observe large performance leap that might transcend the basic scaling law  [30], e.g., larger models have significantly stronger ICL ability (illustrated in the original Figure 1.2 of the GPT-3's paper  [55]).  Overall, GPT-3 can be viewed as a remarkable landmark in the journey evolving from PLMs to LLMs.  It has empirically proved that scaling the neural networks to a significant size can lead to a huge increase in model capacity.  Capacity Enhancement.  Due to the strong capacities, GPT3 has been the base model to develop even more capable LLMs for OpenAI.  Overall, OpenAI has explored two major approaches to further improving the GPT-3 model, i.e., training on code data and alignment with human preference, which are detailed as follows.  •  Training on code data.  A major limitation of the original GPT-3 model (pre-trained on plain text) lies in the lack of by OpenAI, namely ChatGPT [131] and GPT-4  [46], which have largely raised the capacity bar of existing AI systems.  • ChatGPT.  In November 2022, OpenAI released the conversation model ChatGPT, based on the GPT models (GPT-3.5 and GPT-4).  As the official blog article introduced [131], ChatGPT was trained in a similar way as InstructGPT (called \"a sibling model to InstructGPT\" in the original post), while specially optimized for dialogue.  They reported a difference between the training of ChatGPT and InstructGPT in the data collection setup: human-generated conversations (playing both the roles of user and AI) are combined with the InstructGPT dataset in a dialogue format for training ChatGPT.  ChatGPT exhibited superior capacities in communicating with humans: possessing a vast store of knowledge, skill at reasoning on mathematical problems, tracing the context accurately in multi-turn dialogues, and aligning well with human values for safe use.  Later on, the plugin mechanism has been supported in ChatGPT, which further extends the capacities of ChatGPT with existing tools or apps.  So far, it seems to be the ever most powerful chatbot in the AI history.  The launch of ChatGPT has a significant impact on the AI research in the future, which sheds light on the exploration of human-like AI systems.  •  GPT-4.  As another remarkable progress, GPT-4 [46] was released in March 2023, which extended the text input to multimodal signals.  Overall, GPT-4 has stronger capacities in solving complex tasks than GPT-3.5, showing a large performance improvement on many evaluation tasks.  A recent study [41] investigated the capacities of GPT4 by conducting qualitative tests with human-generated problems, spanning a diverse range of difficult tasks, and showed that GPT-4 can achieve more superior performance than prior GPT models.  Furthermore, GPT-4 responds more safely to malicious or provocative queries, due to a sixmonth iterative alignment (with an additional safety reward signal in the RLHF training).  In the technical report, OpenAI has emphasized how to safely develop GPT-4 and applied a number of intervention strategies to mitigate the possible issues of LLMs, such as hallucinations, privacy and overreliance.  For example, they introduced the mechanism called red teaming [132] to reduce the harm or toxic content generation.  As another important aspect, GPT-4 has been developed on a well-established deep learning infrastructure with improved optimization methods.  They introduced a new mechanism called predictable scaling that can accurately predict the final performance with a small proportion of compute during model training.  • GPT-4V, GPT-4 turbo, and beyond.  Based on the work done for GPT-4 [46], OpenAI further released GPT-4V in September 2023, which focused on the safe deployment of the vision capabilities of GPT-4.  In the GPT-4V's system card [133], it has extensively discussed the assessment and mitigation of risks related to visually augmented inputs.  Specially, GPT-4V exhibited strong vision capacities in various application scenarios, showing the great potential as a powerful multimodal learning system.  More recently, in November 2023, OpenAI released an upgraded generation of GPT-4 model at DevDay, named GPT-4 Turbo, with a series of technical improvements.  GPT-4 Turbo is featured by the improved model capacity (more capable than GPT4), the extended knowledge source (up to April 2023), long context window (up to 128k tokens), optimized model performance (cheaper price), and other useful functionality updates (function call, reproducible outputs, etc.).  At the same time, Assistants API was launched to ease the rapid development of agent-like assistants.  With this API, developers can easily create goal-oriented assistants within their applications, by leveraging specific instruction, extra knowledge and tool use.  Furthermore, multimodal capacities (see, hear, and speak) were also enhanced in this new release, supported by GPT-4 Turbo with vision, DALL·E 3, Text-to-speech (TTS), and Listen to voice samples.  These improvements have greatly extended the capacity scope and enhanced the task performance of GPT models.  More importantly, the application ecosystem will be greatly strengthened with the technology upgrade in improved models, APIs, and functionalities.  Despite the huge progress, there are still limitations with these superior LLMs, e.g., generating hallucinations with factual errors or potentially risky response within some specific context [46].  More limitations or issues of LLMs will be discussed in Section 7.  It poses long-standing research challenges to develop more capable, safer LLMs.  From the perspective of engineering, OpenAI has adopted an iterative deployment strategy  [134] to develop the models and products by following a five-stage development and deployment life-cycle, which aims to effectively reduce the potential risks of using the models.  In the following, we will dive into the technical details in order to have a specific understanding of how they have been developed.  It is by no means an easy job to develop or reproduce LLMs, considering the challenging technical issues and huge demands of computation resources.  A feasible way is to learn experiences from existing LLMs and reuse publicly available resources for incremental development or experimental study.  In this section, we briefly summarize the publicly available resources for developing LLMs, including model checkpoints (or APIs), corpora and libraries.  Given the huge cost of model pre-training, well-trained model checkpoints are critical to the study and development of LLMs for the research community.  Due to space limitation, we can only selectively discuss several representative LLMs.  In addition, for inference, we can directly employ public APIs to perform our tasks, without running the model locally.  Next, we introduce the publicly available model checkpoints and APIs.  Publicly Available Model Checkpoints.  To assist researchers in selecting a suitable model based on the resource budget and usage needs, we focus on discussing the model's parameter size, data and computational resources required for training, the relevant technologies employed by the model, and its performance evaluation in downstream tasks.  For more details of LLMs, see Table 1. • LLaMA.  The LLaMA series of models has gained immense popularity and widespread attention due to its openness and effectiveness.  From LLaMA [57], LLaMA-2  [99], Fig.  5: An evolutionary graph of the research work conducted on LLaMA.  Due to the huge number, we cannot include all the LLaMA variants in this figure, even much excellent work.  To support incremental update, we share the source file of this figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page.  and Baichuan-2 have two available parameter sizes (7B and 13B).  Baichuan supports both Chinese and English, with pre-training data reaching 1.2 trillion tokens.  Furthermore, Baichuan-2 expands its pre-training data to 2.6 trillion tokens.  Baichuan-2 surpasses Baichuan in all evaluation benchmarks, demonstrating excellent multilingual capabilities and showing potential for vertical applications in the domains such as law and healthcare (e.g., JEC-QA  [144] and MedQA  [145]).  LLaMA Model Family.  The collection of LLaMA models [57] were introduced by Meta AI in February, 2023, consisting of four sizes (7B, 13B, 30B and 65B).  Since released, LLaMA has attracted extensive attention from both research and industry communities.  LLaMA models have achieved very excellent performance on various open benchmarks, which have become the most popular open language models thus far.  A large number of researchers have extended LLaMA models by either instruction tuning or continual pre-training.  In particular, instruction tuning LLaMA has become a major approach to developing customized or specialized models, due to the relatively low computational costs.  To effectively adapt LLaMA models in non-English languages, it often needs to extend the original vocabulary (trained mainly on English corpus) or fine-tune it with instructions or data in the target language.  Among these extended models, Stanford Alpaca [146] is the first open instruct-following model fine-tuned based on LLaMA (7B).  It is trained by 52K instruction-following demonstrations generated via selfinstruct  [147] using text-davinci-003.  The instruction data, named Alpaca-52K, and training code have been extensively adopted in subsequent work, such as AlpacaLoRA [148] (a reproduction of Stanford Alpaca using LoRA [149]), Koala [150], and BELLE  [151].  In addition, Vicuna [152] is another popular LLaMA variant, trained upon user-shared conversations collected from ShareGPT  [153].  Due to the excellent performance and availability of the LLaMA model family, many multimodal models incorporate them as the base language models, to achieve strong language understanding and generation abilities.  Compared with other variants, Vicuna is more preferred in multimodal language models, which have led to the emergence of a variety of popular models, including LLaVA [154], MiniGPT4 [155], InstructBLIP  [156], and PandaGPT  [157].  The release of LLaMA has greatly advanced the research progress of LLMs.  To summarize the research work conducted on LLaMA, we present a brief evolutionary graph in Figure 5.  Public API of LLMs.  Instead of directly using the model copies, APIs provide a more convenient way for common users to use LLMs, without the need of running the model locally.  As a representative interface for using LLMs, the APIs for the GPT-series models [46, 55, 66, 105] have BookCorpus  [158] 5GB Books Dec-2015 Gutenberg [159] -\nBooks Dec-2021 C4  [82] 800GB CommonCrawl Apr-2019 CC-Stories-R [160] 31GB CommonCrawl Sep-2019 CC-NEWS  [27] 78GB CommonCrawl Feb-2019 REALNEWs  [161] 120GB CommonCrawl Apr-2019 OpenWebText  [162] 38GB Reddit links Mar-2023 Pushift.io  [163] 2TB Reddit links Mar-2023 Wikipedia  [164] 21GB Wikipedia Mar-2023 BigQuery [165] -\nCodes Mar-2023 the Pile  [166] 800GB Other Dec-2020 ROOTS  [167] 1.6TB Other Jun-2022  In contrast to earlier PLMs, LLMs which consist of a significantly larger number of parameters require a higher volume of training data that covers a broad range of content.  For this need, there are increasingly more accessible training datasets that have been released for research.  In this section, we will briefly summarize several widely used corpora for training LLMs.  Based on their content types, we categorize these corpora into five groups: web pages, books, Wikipedia, code, and others.  Web pages.  Web pages are a primary data source for training language models.  •  CommonCrawl.  CommonCrawl  [168] is one of the largest open-source web crawling databases, containing a Books & Academic Data.  Books and academic data contains a wealth of world knowledge and linguistic information, serving as a high-quality corpus for model learning.  •  Book Data.  BookCorpus  [158] is a commonly used dataset in previous small-scale models (e.g., GPT [122] and GPT-2  [26]), consisting of over 11,000 books covering a wide range of topics and genres (e.g., novels and biographies).  Another large-scale book corpus is Project Gutenberg  [159], consisting of over 70,000 literary books including novels, essays, poetry, drama, history, science, philosophy, and other types of works in the public domain.  It is currently one of the largest open-source book collections, which is used in training of MT-NLG [113] and LLaMA  [57].  As for Mixed Data.  In addition to the aforementioned specific types of datasets, different types of data have been combined to facilitate usage by researchers.  The Pile [166] is a large-scale, diverse, and open-source text dataset consisting of over 800GB of data from multiple sources, including books, websites, codes, scientific papers, and social media platforms.  It is constructed from 22 diverse high-quality subsets.  The Pile dataset is widely used in models with different parameter scales, such as GPT-J (6B)  [176], CodeGen (16B)  [86], and Megatron-Turing NLG (530B) [113].  ROOTS [167] is composed of various smaller datasets (totally 1.61 TB of text) and covers 59 different languages (containing natural languages and programming languages), which have been used for training BLOOM  [78].  Another mixture dataset is Dolma  [177], which includes web text from Common Crawl, academic papers from Semantic Scholar, GitHub code, books, social media from Reddit, and Wikipedia data.  Dolma consisting of 3T tokens of approximately 200TB of raw text and has been used to train OLMo [178].  In practice, it commonly requires a mixture of different data sources for pre-training LLMs (see Figure 6), instead of a single corpus.  Therefore, existing studies commonly mix several ready-made datasets (e.g., C4, OpenWebText, and the Pile), and then perform further processing to obtain the pre-training corpus.  Furthermore, to train the LLMs that are adaptive to specific applications, it is also important to extract data from relevant sources (e.g., Wikipedia and BigQuery) for enriching the corresponding information in pre-training data.  Nat.  Inst.  [179] Apr-2021 193K FLAN  [67] Sep-2021 4.4M P3  [180] Oct-2021 12.1M Super Nat.  Inst.  [88] Apr-2022 5M MVPCorpus  [181] Jun-2022 41M xP3  [94] Nov-2022 81M OIG[182]  Mar-2023 43M HH-RLHF [183] Apr-2022 160K HC3 [184] Jan-2023 87K ShareGPT  [153] Mar-2023 90K  Dolly [185] Apr-2023 15K OpenAssistant  [186] Apr-2023 161K Self-Instruct [147] Dec-2022 82K Alpaca  [187] Mar-2023 52K Guanaco [188] Mar-2023 535K Baize  [189] Apr-2023 158K BELLE  [190] Apr-2023 1.5M Summarize from Feedback  [129] Sep-2020 193K SHP  [191] Oct-2021 385K WebGPT Comparisons  [81] Dec-2021 19K Stack Exchange P", "metadata": {"source_file": "2303.18223v16.pdf", "title": "L A Survey of Large Language Models  [cs.CL] 11 Mar 2025", "authors": ["Wayne Xin Zhao", "Kun Zhou", "Junyi Li*", "Tianyi Tang", "Xiaolei Wang", "Yupeng Hou", "Yingqian Min", "Beichen Zhang", "Junjie Zhang", "Zican Dong", "Yifan Du", "Chen Yang", "Yushuo Chen", "Zhipeng Chen", "Jinhao Jiang", "Ruiyang Ren", "Yifan Li", "Xinyu Tang", "Zikang Liu", "Peiyu Liu", "Jian-Yun Nie", "Ji-Rong Wen"], "year": "2025", "detected_language": "en", "page_count": 144, "origin_chunk_file": "2303.18223v16.chunks.json"}}
{"text": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the 'where' and 'how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:\nINTRODUCTION Understanding the essence of intelligence and establishing whether a machine embodies it poses a compelling question for scientists. It is generally agreed upon that authentic intelligence equips us with reasoning capabilities, enables us to test hypotheses, and prepares for future eventualities [92]. In particular, Artificial Intelligence (AI) researchers focus on the development of machine-based intelligence, as opposed to biologically based intellect [136].  Proper measurement helps to understand intelligence.  For instance, measures for general intelligence in human individuals often encompass IQ tests [12].  Within the scope of AI, the Turing Test [193], a widely recognized test for assessing intelligence by discerning if responses are of human or machine origin, has been a longstanding objective in AI evolution.  It is generally believed among researchers that a computing machine that successfully passes the Turing Test can be considered as intelligent.  Consequently, when viewed from a wider lens, the chronicle of AI can be depicted as the timeline of creation and evaluation of intelligent models and algorithms.  With each emergence of a novel AI model or algorithm, researchers invariably scrutinize its capabilities in real-world scenarios through evaluation using specific and challenging tasks.  For instance, the Perceptron algorithm [49], touted as an Artificial General Intelligence (AGI) approach in the 1950s, was later revealed as inadequate due to its inability to resolve the XOR problem.  The subsequent rise and application of Support Vector Machines (SVMs)  [28] and deep learning [104] have marked both progress and setbacks in the AI landscape.  A significant takeaway from previous attempts is the paramount importance of AI evaluation, which serves as a critical tool to identify current system limitations and inform the design of more powerful models.  Recently, large language models (LLMs) have incited substantial interest across both academic and industrial domains  [11, 219, 255].  As demonstrated by existing work  [15], the great performance of LLMs has raised promise that they could be AGI in this era.  LLMs possess the capabilities to solve diverse tasks, contrasting with prior models confined to solving specific tasks.  Due to its great performance in handling different applications such as general natural language tasks and domain-specific ones, LLMs are increasingly used by individuals with critical information needs, such as students or patients.  Evaluation is of paramount prominence to the success of LLMs due to several reasons.  First, evaluating LLMs helps us better understand the strengths and weakness of LLMs.  For instance, the PromptBench [262] benchmark illustrates that current LLMs are sensitive to adversarial prompts, thus a careful prompt engineering is necessary for better performance.  Second, better evaluations can provide better guidance for human-LLMs interaction, which could inspire future interaction design and implementation.  Third, the broad applicability of LLMs underscores the paramount importance of ensuring their safety and reliability, particularly in safety-sensitive sectors such as financial institutions and healthcare facilities.  Finally, as LLMs are becoming larger with more emergent abilities, existing evaluation protocols may not be enough to evaluate their capabilities and potential risks.  Therefore, we aim to raise awareness in the community of the importance to LLMs evaluations by reviewing the current evaluation protocols and most importantly, shed light on future research about designing new LLMs evaluation protocols.  With the introduction of ChatGPT [145] and GPT-4 [146], there have been a number of research efforts aiming at evaluating ChatGPT and other LLMs from different aspects (Figure 2), encompassing a range of factors such as natural language tasks, reasoning, robustness, trustworthiness, medical applications, and ethical considerations.  Despite these efforts, a comprehensive overview capturing the entire gamut of evaluations is still lacking.  Furthermore, the ongoing evolution of LLMs has also presented novel aspects for evaluation, thereby challenging existing evaluation protocols and reinforcing the need for thorough, multifaceted evaluation techniques.  While existing research such as Bubeck et al.  [15] claimed that GPT-4 can be seen as sparks of AGI, others contest this claim due to the human-crafted nature of its evaluation approach.  This paper serves as the first comprehensive survey on the evaluation of large language models.  As depicted in Figure 1, we explore existing work in three dimensions: 1) What to evaluate, 2) Where to evaluate, and 3) How to evaluate.  Specifically, \"what to evaluate\" encapsulates existing evaluation tasks for LLMs, \"where to evaluate\" involves selecting appropriate datasets and benchmarks for evaluation, while \"how to evaluate\" is concerned with the evaluation process given appropriate tasks and datasets.  These three dimensions are integral to the evaluation of LLMs.  We subsequently discuss potential future challenges in the realm of LLMs evaluation.  uate, where to evaluate, and how to evaluate.  Our categorization is general and encompasses the entire life cycle of LLMs evaluation.  (2) Regarding what to evaluate, we summarize existing tasks in various areas and obtain insightful conclusions on the success and failure case of LLMs (Section 6), providing experience for future research.  (3) As for where to evaluate, we summarize evaluation metrics, datasets, and benchmarks to provide a profound understanding of current LLMs evaluations.  In terms of how to evaluate, we explore current protocols and summarize novel evaluation approaches.  (4) We further discuss future challenges in evaluating LLMs.  We open-source and maintain the The paper is organized as follows.  In Section 2, we provide the basic information of LLMs and AI model evaluation.  Then, Section 3 reviews existing work from the aspects of \"what to evaluate\".  After that, Section 4 is the \"where to evaluate\" part, which summarizes existing datasets and benchmarks.  Section 5 discusses how to perform the evaluation.  In Section 6, we summarize the key findings of this paper.  We discuss grand future challenges in Section 7 and Section 8 concludes the paper.  2.1 Large Language Models Language models (LMs)  [36, 51, 96] are computational models that have the capability to understand and generate human language.  LMs have the transformative ability to predict the likelihood of word sequences or generate new text based on a given input.  N-gram models  [13], the most common type of LM, estimate word probabilities based on the preceding context.  However, LMs also face challenges, such as the issue of rare or unseen words, the problem of overfitting, and the difficulty in capturing complex linguistic phenomena.  Researchers are continuously working on improving LM architectures and training methods to address these challenges.  Large Language Models (LLMs)  [19, 91, 255] are advanced language models with massive parameter sizes and exceptional learning capabilities.  The core module behind many LLMs such as GPT-3 [43], InstructGPT  [149], and GPT-4 [146] is the self-attention module in Transformer  [197] that serves as the fundamental building block for language modeling tasks.  Transformers have revolutionized the field of NLP with their ability to handle sequential data efficiently, allowing for parallelization and capturing long-range dependencies in text.  One key feature of LLMs is in-context learning [14], where the model is trained to generate text based on a given context or prompt.  This enables LLMs to generate more coherent and contextually relevant responses, making them suitable for interactive and conversational applications.  Reinforcement Learning from Human Feedback (RLHF)  [25, 266] is another crucial aspect of LLMs.  This technique involves fine-tuning the model using human-generated responses as rewards, allowing the model to learn from its mistakes and improve its performance over time.  In an autoregressive language model, such as GPT-3 and PaLM [24], given a context sequence X, the LM tasks aim to predict the next token y.  The model is trained by maximizing the probability of the given token sequence conditioned on the context, i.e., P(y|X) = P(y|x1,x2, . . .  ,xt−1), where x1,x2, . .  .  ,xt−1 are the tokens in the context sequence, and t is the current position.  By using the chain rule, the conditional probability can be decomposed into a product of probabilities at each position: where T is sequence length.  In this way, the model predicts each token at each position in an autoregressive manner, generating a complete text sequence.  One common approach to interacting with LLMs is prompt engineering  [26, 221, 261], where users design and provide specific prompt texts to guide LLMs in generating desired responses or completing specific tasks.  This is widely adopted in existing evaluation efforts.  People can also engage in question-and-answer interactions  [83], where they pose questions to the model and receive answers, or engage in dialogue interactions, having natural language conversations with LLMs.  In conclusion, LLMs, with their Transformer architecture, in-context learning, and RLHF capabilities, have revolutionized NLP and hold promise in various applications.  Table 1 provides a brief comparison of traditional ML, deep learning, and LLMs.  Comparison Traditional ML Deep Learning LLMs Training Data Size Large Large Very large Feature Engineering Manual Automatic Automatic Model Complexity Limited Complex Very Complex Interpretability Good Poor Poorer Performance Moderate High Highest Hardware Requirements Low High Very High AI model evaluation is an essential step in assessing the performance of a model.  There are some standard model evaluation protocols, including k-fold cross-validation, holdout validation, leave one out cross-validation (LOOCV), bootstrap, and reduced set [8, 95].  For instance, k-fold cross-validation divides the dataset into k parts, with one part used as a test set and the rest as training sets, which can reduce training data loss and obtain relatively more accurate model performance evaluation", "metadata": {"source_file": "3641289.pdf", "title": "A Survey on Evaluation of Large Language Models", "authors": ["XU WANG", "LINYI YANG", "YIDONG WANG", "WEI YE", "PHILIP S. YU", "XING XIE", "Y. Chang", "X. Wang", "Y. Wu", "J. Wang", "X. Yi", "X. Xie", "L. Yang", "C. Wang", "Y. Zhang", "K. Zhu", "H. Chen", "Y. Wang", "W. Ye", "P. S. Yu", "Q. Yang"], "year": "2024", "detected_language": "en", "page_count": 45, "origin_chunk_file": "3641289.chunks.json"}}
{"text": "[48]; Holdout validation divides the dataset into training and test sets, with a smaller calculation amount but potentially more significant bias; LOOCV is a unique kfold cross-validation method where only one data point is used as the test set [222]; Reduced set trains the model with one dataset and tests it with the remaining data, which is computationally simple, but the applicability is limited. The appropriate evaluation method should be chosen according to the specific problem and data characteristics for more reliable performance indicators. Figure 3 illustrates the evaluation process of AI models, including LLMs. Some evaluation protocols may not be feasible to evaluate deep learning models due to the extensive training size. Thus, evaluation on a static validation set has long been the standard choice for deep learning models. For instance, computer vision models leverage static test sets such as ImageNet [33] and MS COCO [120] for evaluation. LLMs also use GLUE [200] or SuperGLUE [199] as the common test sets. As LLMs are becoming more popular with even poorer interpretability, existing evaluation protocols may not be enough to evaluate the true capabilities of LLMs thoroughly. We will introduce recent evaluations of LLMs in Section 5.\nWHAT TO EVALUATE What tasks should we evaluate on LLMs to show their performance? On what tasks can we claim the strengths and weaknesses of LLMs? In this section, we divide existing tasks into the following categories: natural language processing, robustness, ethics, biases and trustworthiness, social sciences, natural science and engineering, medical applications, agent applications (using LLMs as agents), and other applications.1 1Note that LLMs are evaluated in various tasks and the categorization in this paper is only one possible way for classification of these works. There are certainly other taxonomies. The initial objective behind the development of language models, particularly large language models, was to enhance performance on natural language processing tasks, encompassing both understanding and generation.  Consequently, the majority of evaluation research has been primarily focused on natural language tasks.  Table 2 summarizes the evaluation aspects of existing research, and we mainly highlight their conclusions in the following.2 3.1.1 Natural Language Understanding.  Natural language understanding represents a wide spectrum of tasks that aims to obtain a better understanding of the input sequence.  We summarize recent efforts in LLMs evaluation from several aspects.  Sentiment analysis is a task that analyzes and interprets the text to determine the emotional inclination.  It is typically a binary (positive and negative) or triple (positive, neutral, and negative) class classification problem.  Evaluating sentiment analysis tasks is a popular direction.  Liang et al.  [114] and Zeng et al.  [242] showed that the performance of the models on this task is usually high.  ChatGPT's sentiment analysis prediction performance is superior to traditional sentiment analysis methods [129] and comes close to that of GPT-3.5  [159].  In fine-grained sentiment and emotion cause analysis, ChatGPT also exhibits exceptional performance [218].  In low-resource learning environments, LLMs exhibit significant advantages over small language models [249], but the ability of ChatGPT to understand low-resource languages is limited [6].  In conclusion, LLMs have demonstrated commendable performance in sentiment analysis tasks.  Future work should focus on enhancing their capability to understand emotions in under-resourced languages.  Text classification and sentiment analysis are related fields; text classification not only focuses on sentiment, but also includes the processing of all texts and tasks.  The work of Liang et al.  [114] showed that GLM-130B was the best-performed model, with an overall accuracy of 85.8% for miscellaneous text classification.  Yang and Menczer [232] found that ChatGPT can produce credibility ratings for a wide range of news outlets, and these ratings have a moderate correlation with those from human experts.  Furthermore, ChatGPT achieves acceptable accuracy in a binary classification scenario (AUC=0.89).  Peña et al.  [154] discussed the problem of topic classification for public affairs documents and showed that using an LLM backbone in combination with SVM classifiers is a useful strategy to conduct the multi-label topic classification task in the domain of public affairs with accuracies over 85%.  Overall, LLMs perform well on text classification and can even handle text classification tasks in unconventional problem settings as well.  Natural language inference (NLI) is the task of determining whether the given \"hypothesis\" logically follows from the \"premise\".  Qin et al.  [159] showed that ChatGPT outperforms GPT-3.5 for NLI tasks.  They also found that ChatGPT excels in handling factual input that could be attributed to its RLHF training process in favoring human feedback.  However, Lee et al.  [105] observed LLMs perform poorly in the scope of NLI and further fail in representing human disagreement, which indicates that LLMs still have large room for improvement in this field.  Semantic understanding refers to the meaning or understanding of language and its associated concepts.  It involves the interpretation and comprehension of words, phrases, sentences, and the relationships between them.  Semantic processing goes beyond the surface level and focuses on understanding the underlying meaning and intent.  Tao et al.  [184] comprehensively evaluated the event semantic processing abilities of LLMs covering understanding, reasoning, and prediction about the event semantics.  Results indicated that LLMs possess an understanding of individual events, but their capacity to perceive the semantic similarity among events is constrained.  In reasoning tasks, LLMs exhibit robust reasoning abilities in causal and intentional relations, yet their performance in other relation types is comparatively weaker.  In prediction tasks, LLMs exhibit enhanced predictive capabilities for future events with increased contextual information.  Riccardi and Desai  [166] explored the semantic proficiency of LLMs and showed that these models perform poorly in evaluating basic phrases.  Furthermore, GPT-3.5 and Bard cannot distinguish between ChatGPT exhibits a strong capability for arithmetic reasoning by outperforming GPT-3.5 in the majority of tasks [159].  However, its proficiency in mathematical reasoning still requires improvement  [6, 45, 263].  On symbolic reasoning tasks, ChatGPT is mostly worse than GPT-3.5, which may be because ChatGPT is prone to uncertain responses, leading to poor performance [6].  Through the poor performance of LLMs on task variants of counterfactual conditions, Wu et al.  [226] showed that the current LLMs have certain limitations in abstract reasoning ability.  On abstract reasoning, Gendron et al.  [56] found that existing LLMs have very limited ability.  In logical reasoning, Liu et al.  [124] indicated that ChatGPT and GPT-4 outperform traditional fine-tuning methods on most benchmarks, demonstrating their superiority in logical reasoning.  However, both models face challenges when handling new and out-of-distribution data.  ChatGPT does not perform as well as other LLMs, including GPT-3.5 and BARD  [159, 228].  This is because ChatGPT is designed explicitly for chatting, so it does an excellent job of maintaining rationality.  FLAN-T5, LLaMA, GPT-3.5, and PaLM perform well in general deductive reasoning tasks [170].  GPT-3.5 is not good at keeping oriented for reasoning in the inductive setting [228].  For multi-step reasoning, Fu et al.  [47] showed PaLM and Claude2 are the only two model families that achieve similar performance (but still worse than the GPT model family).  Moreover, LLaMA-65B is the most robust open-source LLMs to date, which performs closely to code-davinci-002.  Some papers separately evaluate the performance of ChatGPT on some reasoning tasks: ChatGPT generally performs poorly on commonsense reasoning tasks, but relatively better than non-text semantic reasoning [6].  Meanwhile, ChatGPT also lacks spatial reasoning ability, but exhibits better temporal reasoning.  Finally, while the performance of ChatGPT is acceptable on causal and analogical reasoning, it performs poorly on multi-hop reasoning ability, which is similar to the weakness of other LLMs on complex reasoning [148].  In professional domain reasoning tasks, zero-shot InstructGPT and Codex are capable of complex medical reasoning tasks, but still need to be further improved [117].  In terms of language insight issues, Orrù et al.  [147] demonstrated the potential of ChatGPT for solving verbal insight problems, as ChatGPT's performance was comparable to that of human participants.  It should be noted that most of the above conclusions are obtained for specific data sets.  In contrast, more complex tasks have become the mainstream benchmarks for assessing the capabilities of LLMs.  These include tasks such as mathematical reasoning [225, 236, 243] and structured meaningful and nonsense phrases, consistently classifying highly nonsense phrases as meaningful.  GPT-4 shows significant improvements, but its performance is still significantly lower than that of humans.  In summary, the performance of LLMs in semantic understanding tasks is poor.  In the future, we can start from this aspect and focus on improving its performance on this application.  In social knowledge understanding, Choi et al.  [23] evaluated how well models perform at learning and recognizing concepts of social knowledge and the results revealed that despite being much smaller in the number of parameters, finetuning supervised models such as BERT lead to much better performance than zero-shot models using state-of-the-art LLMs, such as GPT  [162], GPT-J-6B  [202] and so on.  This statement demonstrates that supervised models significantly outperform zero-shot models in terms of performance, highlighting that an increase in parameters does not necessarily guarantee a higher level of social knowledge in this particular scenario.  3.1.2  Reasoning.  The task of reasoning poses significant challenges for an intelligent AI model.  To effectively tackle reasoning tasks, the models need to not only comprehend the provided information but also utilize reasoning and inference to deduce answers when explicit responses are absent.  Table 2 reveals that there is a growing interest in evaluating the reasoning ability of LLMs, as evidenced by the increasing number of articles focusing on exploring this aspect.  Currently, the evaluation of reasoning tasks can be broadly categorized into mathematical reasoning, commonsense reasoning, logical reasoning, and domain-specific reasoning.  Summarization is a generation task that aims to learn a concise abstract for the given sentence.  In this evaluation, Liang et al.  [114] found that TNLG v2 (530B)  [179] achieved the highest score in both scenarios, followed by OPT (175B)  [245] in second place.  The fine-tuned Bart [106] is still better than zero-shot ChatGPT.  Specifically, ChatGPT demonstrates comparable zero-shot performance to the text-davinci-002 [6], but performs worse than GPT-3.5  [159].  These findings indicate that LLMs, particularly ChatGPT, have a general performance in summarization tasks.  data inference [86, 151].  Overall, LLMs show great potential in reasoning and show a continuous improvement trend, but still face many challenges and limitations, requiring more in-depth research and optimization.  3.1.3 Natural Language Generation.  NLG evaluates the capabilities of LLMs in generating specific texts, which consists of several tasks, including summarization, dialogue generation, machine translation, question answering, and other open-ended generation tasks.  Evaluating the performance of LLMs on dialogue tasks is crucial to the development of dialogue systems and improving human-computer interaction.  Through such evaluation, the natural language processing ability, context understanding ability and generation ability of the model can be improved, so as to realize a more intelligent and more natural dialogue system.  Both Claude and ChatGPT generally achieve better performance across all dimensions when compared to GPT-3.5  [121, 159].  When comparing the Claude and ChatGPT models, both models demonstrate competitive performance across different evaluation dimensions, with Claude slightly outperforming ChatGPT in specific configurations.  Research by Bang et al.  [6] underscores that fully fine-tuned models tailored for specific tasks surpass ChatGPT in both task-oriented and knowledge-based dialogue contexts.  Additionally, Zheng et al.  [257] have curated a comprehensive LLMs conversation dataset, LMSYS-Chat-1M, encompassing up to one million samples.  This dataset serves as a valuable resource for evaluating and advancing dialogue systems.  While LLMs are not explicitly trained for translation tasks, they can still demonstrate strong performance.  Wang et al.  [208] demonstrated that ChatGPT and GPT-4 exhibit superior performance in comparison to commercial machine translation (MT) systems, as evaluated by humans.  Additionally, they outperform most document-level NMT methods in terms of sacreBLEU scores.  During contrastive testing, ChatGPT shows lower accuracy in comparison to traditional translation models.  However, GPT-4 demonstrates a robust capability in explaining discourse knowledge, even though it may occasionally select incorrect translation candidates.  The findings from Bang et al.  [6] indicated that ChatGPT performs X →Eng translation well, but it still lacks the ability to perform Eng →X translation.  Lyu et al.  [130] investigated several research directions in MT utilizing LLMs.  This study significantly contributes to the advancement of MT research and highlights the potential of LLMs in enhancing translation capabilities.  In summary, while LLMs perform satisfactorily in several translation tasks, there is still room for improvement, e.g., enhancing the translation capability from English to non-English languages.  Question answering is a crucial technology in the field of human-computer interaction, and it has found wide application in scenarios like search engines, intelligent customer service, and QA systems.  The measurement of accuracy and efficiency in QA models will have significant implications for these applications.  According to Liang et al.  [114], among all the evaluated models, InstructGPT davinci v2 (175B) exhibited the highest performance in terms of accuracy, robustness, and fairness across the 9 QA scenarios.  Both GPT-3.5 and ChatGPT demonstrate significant advancements compared to GPT-3 in their ability to answer general knowledge questions.  In most domains, ChatGPT surpasses GPT-3.5 by more than 2% in terms of performance [9, 159].  However, ChatGPT performs slightly weaker than GPT-3.5 on the CommonsenseQA and Social IQA benchmarks.  This can be attributed to ChatGPT's cautious nature, as it tends to decline to provide an answer when there is insufficient information available.  Fine-tuned models, such as Vícuna and ChatGPT, exhibit exceptional performance with near-perfect scores, surpassing models that lack supervised fine-tuning by a significant margin [5, 6].  Laskar et al.  [102] evaluated the effectiveness of ChatGPT on a range of academic datasets, including various tasks such as answering questions, summarizing text, generating code, reasoning with commonsense, solving math problems, translating languages, detecting bias, and addressing ethical issues.  Overall, LLMs showcase flawless performance on QA tasks and hold the potential for further enhancing their proficiency in social, event, and temporal commonsense knowledge in the future.  There are also other generation tasks to explore.  In the field of sentence style transfer, Pu and Demberg [158] demonstrated that ChatGPT surpasses the previous SOTA supervised model through training on the same subset for few-shot learning, as evident from the higher BLEU score.  However, when it comes to controlling the formality of sentence style, ChatGPT's performance still differs significantly from human behavior.  In writing tasks, Chia et al.  [22] discovered that LLMs exhibit consistent performance across various categories such as informative, professional, argumentative, and creative writing.  This finding implies that LLMs possess a general proficiency in writing capabilities.  In text generation quality, Chen et al.  [20] revealed that ChatGPT excels in assessing text quality from multiple angles, even in the absence of reference texts, surpassing the performance of most existing automated metrics.  Employing ChatGPT to generate numerical scores for text quality emerged as the most reliable and effective approach among the various testing methods studied.  3.1.4 Multilingual Tasks.  While English is the predominant language, many LLMs are trained on mixed-language training data.  The combination of multilingual data indeed helps LLMs gain the ability to process inputs and generate responses in different languages, making them widely adopted and accepted across the globe.  However, due to the relatively recent emergence of this technology, LLMs are primarily evaluated on English data, leading to a potential oversight of evaluating their multilingual performance.  To address this, several articles have provided comprehensive, open, and independent evaluations of LLMs' performance on various NLP tasks in different non-English languages.  These evaluations offer valuable insights for future research and applications.  Abdelali et al.  [1] evaluated the performance of ChatGPT in standard Arabic NLP tasks and observed that ChatGPT exhibits lower performance compared to SOTA models in the zero-shot setting for most tasks.  Ahuja et al.  [2], Bang et al.  [6], Lai et al.  [100], Zhang et al.  [248] utilized a greater number of languages across multiple datasets, encompassing a wider range of tasks, and conducted a more comprehensive evaluation of LLMs, including BLOOM, Vicuna, Claude, ChatGPT, and GPT-4.  The results indicated that these LLMs perform poorly when it came to non-Latin languages and languages with limited resources.  Despite translating the input to English and using it as the query, generative LLMs still displays subpar performance across tasks and languages compared to SOTA models  [2].  Furthermore, Bang et al.  [6] highlighted that ChatGPT still faces a limitation in translating sentences written in non-Latin script languages with rich linguistic resources.  The aforementioned demonstrates that there are numerous challenges and ample opportunities for enhancement in multilingual tasks for LLMs.  Future research should prioritize achieving multilingual balance and addressing the challenges faced by non-Latin languages and low-resource languages, with the aim of better supporting users worldwide.  At the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications.  3.1.5 Factuality.  Factuality in the context of LLMs refers to the extent to which the information or answers provided by the model align with real-world truths and verifiable facts.  Factuality in LLMs significantly impacts a variety of tasks and downstream applications, such as QA systems, information extraction, text summarization, dialogue systems, and automated fact-checking, where incorrect or inconsistent information could lead to substantial misunderstandings and misinterpretations.  Evaluating factuality is of great importance in order to trust and efficiently use these models.  This includes the ability of these models to maintain consistency with known facts, avoid generating misleading or false information (known as \"factual hallucination\"), and effectively learn and recall factual knowledge.  A range of methodologies have been proposed to measure and improve the factuality of LLMs.  Wang et al.  [204] assessed the internal knowledge capabilities of several large models, namely InstructGPT, ChatGPT-3.5, GPT-4, and BingChat  [137], by examining their ability to answer open questions based on the Natural Questions [98] and TriviaQA  [88] datasets.  The evaluation process involved human assessment.  The results of the study indicated that while GPT-4 and BingChat can provide correct answers for more than 80% of the questions, there is still a remaining gap of over 15% to achieve complete accuracy.  In the work of Honovich et al.  [74], they conducted a review of current factual consistency evaluation methods and highlighted the absence of a unified comparison framework and the limited reference value of related scores compared to binary labels.  To address this, they transformed existing fact consistency tasks into binary labels, specifically considering only whether there is a factual conflict with the input text, without factoring in external knowledge.  The research discovered that fact evaluation methods founded on natural language inference and question generation answering exhibit superior performance and can complement each other.  Pezeshkpour [156] proposed a novel metric, based on information theory, to assess the inclusion of specific knowledge in LLMs.  The metric utilized the concept of uncertainty in knowledge to measure factualness, calculated by LLMs filling in prompts and examining the probability distribution of the answer.  The paper discussed two methods for injecting knowledge into LLMs: explicit inclusion of knowledge in the prompts and implicit fine-tuning of the LLMs using knowledge-related data.  The study demonstrated that this approach surpasses traditional ranking methods by achieving an accuracy improvement of over 30%.  Gekhman et al.  [55] improved the method for evaluating fact consistency in summarization tasks.  It proposed a novel approach that involved training student NLI models using summaries generated by multiple models and annotated by LLMs to ensure fact consistency.  The trained student model was then used for summarization fact consistency evaluation.  Manakul et al.  [133] operated on two hypotheses regarding how LLMs generate factual or hallucinated responses.  It proposed the use of three formulas (BERTScore [247], MQAG  [134] and n-gram) to evaluate factuality and employed alternative LLMs to gather token probabilities for black-box language models.  The study discovered that simply computing sentence likelihood or entropy helped validate the factuality of the responses.  Min et al.  [138] broke down text generated by LLMs into individual \"atomic\" facts, which were then evaluated for their correctness.  The FActScore is used to measure the performance of estimators through the calculation of F1 scores.  The paper tested various estimators and revealed that current estimators still have some way to go in effectively addressing the task.  Lin et al.  [119] introduced the TruthfulQA dataset, designed to cause models to make mistakes.  Multiple language models were tested by providing factual answers.  The findings from these experiments suggest that simply scaling up model sizes may not necessarily improve their truthfulness, and recommendations are provided for the training approach.  This dataset has become widely used for evaluating the factuality of LLMs  [89, 146, 192, 219].  3.2 Robustness, Ethics, Bias, and Trustworthiness  The evaluation encompasses crucial aspects of robustness, ethics, biases, and trustworthiness.  These factors have gained increasing importance in assessing the performance of LLMs comprehensively.  Table 3 shows a summary of the research.  Reference Robustness Ethics and biases Trustworthiness Cao et al.  [16] ✓\nDhamala et al.  [37] ✓\nDeshpande et al.  [35] ✓\nFerrara  [42] ✓\nGehman et al.  [53] ✓\nHartmann et al.  [65] ✓\nHendrycks et al.  [69] ✓\nHagendorff and Fabi  [62] ✓\nLi et al.  [111] ✓\nLiu et al.  [123] ✓\nLiu et al.  [123] ✓\nLi et al.  [113] ✓\nParrish et al.  [153] ✓\nRutinowski et al.  [167] ✓\nRawte et al.  [163] ✓\nSheng et al.  [175] ✓\nSimmons  [176] ✓\nWang et al.  [207] ✓\nWang et al.  [206] ✓\nWang et al.  [201] ✓\nWang et al.  [209] ✓\nXie et al.  [227] ✓\nYang et al.  [233] ✓\nZhao et al.  [256] ✓\nZhuo et al.  [265] ✓\nZhu et al.  [262] ✓\nZhuo et al.  [264] ✓\nZhang et al.  [251] ✓\nFor adversarial robustness, Zhu et al.  [262] evaluated the robustness of LLMs to prompts by proposing a unified benchmark called PromptBench.  They comprehensively evaluated adversarial text attacks at multiple levels (character, word, sentence, and semantics).  The results showed that contemporary LLMs are vulnerable to adversarial prompts, highlighting the importance of the models' robustness when facing adversarial inputs.  As for new adversarial datasets, Wang et al.  [201] introduced AdvGLUE++ benchmark data for assessing adversarial robustness and implemented a new evaluation protocol to scrutinize machine ethics via jailbreaking system prompts.  3.2.2 Ethics and Bias.  LLMs have been found to internalize, spread, and potentially magnify harmful information existing in the crawled training corpora, usually, toxic languages, like offensiveness, hate speech, and insults  [53], as well as social biases like stereotypes towards people with a particular demographic identity (e.g., gender, race, religion, occupation, and ideology)  [175].  More recently, Zhuo et al.  [264] used conventional testing sets and metrics  [37, 53, 153] to perform a systematic evaluation of ChatGPT's toxicity and social bias, finding that it still exhibits noxious content to some extend.  Taking a further step, Deshpande et al.  [35] introduced role-playing into the model and observed an increase in generated toxicity up to 6x.  Furthermore, such role-playing also caused biased toxicity towards specific entities.  Different from simply measuring social biases, Ferrara  [42] investigated the sources, underlying mechanisms, and corresponding ethical consequences of these biases potentially produced by ChatGPT.  Beyond social biases, LLMs have also been assessed by political tendency and personality traits  [65, 167] based questionnaires like the Political Compass Test and MBTI test, demonstrating a propensity for progressive views and an ENFJ personality type.  In addition, LLMs like GPT-3 were found to have moral biases [176] in terms of the Moral Foundation theory [58]; The study conducted by [69] reveals that existing LMs have potential in ethical judgment, but still need improvement.  [254] proposes a Chinese conversational bias evaluation dataset CHBias, discovers bias risks in pre-trained models, and explores debiasing methods.  Moreover, in the assessment of GPT-4 alignment, [209] discovered a systematic bias.  ChatGPT is also observed to exhibit somewhat bias on cultural values [16].  Wang et al.  [201] also incorporated an evaluation dataset specifically aimed at gauging stereotype bias, using both targeted and untargeted system prompts.  All these ethical issues might elicit serious risks, impeding the deployment of LLMs and having a profound negative impact on society.  3.2.3 Trustworthiness.  Some work focuses on other trustworthiness problems in addition to robustness and ethics.3  In their 2023 study, DecodingTrust, Wang et al.  [201] offered a multifaceted exploration of trustworthiness vulnerabilities in the GPT models, especially GPT-3.5 and GPT-4.  Their evaluation expanded beyond the typical trustworthiness concerns to include eight critical aspects: toxicity, stereotype bias, adversarial and out-of-distribution robustness, robustness to adversarial demonstrations, privacy, machine ethics, and fairness.  DecodingTrust's investigation employs an array of newly constructed scenarios, tasks, and metrics.  They revealed that while GPT-4 often showcases improved trustworthiness over GPT-3.5 in standard evaluations, it is simultaneously more susceptible to attacks.  In another study by Hagendorff and Fabi [62], LLMs with enhanced cognitive abilities were evaluated.  They found that these models can avoid common human intuitions and cognitive errors, demonstrating super-rational performance.  By utilizing cognitive reflection tests and semantic illusion experiments, the researchers gained insights into the psychological aspects of LLMs.  This method offers new perspectives for evaluating model biases and ethical issues that may not have been previously identified.  Furthermore, a study by [227] brings attention to a significant concern: the consistency of judgment in LLMs diminishes notably when faced with disruptions such as questioning, negation, or misleading cues, even if their initial judgments were accurate.  The research delves into various prompting methods designed to mitigate this issue and successfully demonstrates their efficacy.  LLMs are capable of generating coherent and seemingly factual text.  However, the information generated can include factual inaccuracies or statements ungrounded in reality, a phenomenon known as hallucination  [163, 251].  Evaluating these issues helps improve the training methods of LLMs to reduce the occurrence of hallucinations.  For the evaluation of illusions in large-scale visual models, Liu et al.  [123] introduced a comprehensive and robust large-scale visual instruction dataset: LRV-Instruction.  Through the GAVIE method, they fine-tuned the evaluation visual instructions, and experimental results demonstrated that LRV-Instruction effectively alleviates illusions in LLMs.  In addition, Li et al.  [113] conducted an assessment of illusions in large-scale visual language models, revealing through experiments that the distribution of objects in visual instructions significantly impacts object illusions in LVLMs.  To enhance the assessment of object illusions in LVLMs, they introduced a polling-based query method, known as POPE.  This method provides an improved evaluation of object illusions in LVLMs.  3.3 Social Science Social science involves the study of human society and individual behavior, including economics, sociology, political science, law, and other disciplines.  Evaluating the performance of LLMs in social science is important for academic research, policy formulation, and social problem-solving.  Such evaluations can help improve the applicability and quality of models in the social sciences, increasing understanding of human societies and promoting social progress.  Wu et al.  [223] evaluated the potential use of LLMs in addressing scaling and measurement issues in social science and found that LLMs can generate meaningful responses regarding political ideology and significantly improve text-as-data methods in social science.  In computational social science (CSS) tasks, Ziems et al.  [267] presented a comprehensive evaluation of LLMs on several CSS tasks.  During classification tasks, LLMs exhibit the lowest absolute performance on event argument extraction, character tropes, implicit hate, and empathy classification, achieving accuracy below 40%.  These tasks either involve complex structures (event arguments) or subjective expert taxonomies with semantics that differ from those learned during LLM pretraining.  Conversely, LLMs achieve the best performance on misinformation, stance, and emotion classification.  When it comes to generation tasks, LLMs often produce explanations that surpass the quality of gold", "metadata": {"source_file": "3641289.pdf", "title": "A Survey on Evaluation of Large Language Models", "authors": ["XU WANG", "LINYI YANG", "YIDONG WANG", "WEI YE", "PHILIP S. YU", "XING XIE", "Y. Chang", "X. Wang", "Y. Wu", "J. Wang", "X. Yi", "X. Xie", "L. Yang", "C. Wang", "Y. Zhang", "K. Zhu", "H. Chen", "Y. Wang", "W. Ye", "P. S. Yu", "Q. Yang"], "year": "2024", "detected_language": "en", "page_count": 45, "origin_chunk_file": "3641289.chunks.json"}}
{"text": "ABSTRACT In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in realworld scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software. to make these insights more accessible and understandable. For more complex tasks, such as statistical inference and predictive analysis, statistical and machine learning models are often necessary. This involves data processing, feature engineering, modeling, evaluation, and more. Upon completing the analysis, a final report is usually drafted to summarize the findings and insights. However, for individuals without expertise in statistics, data science, and programming, data analysis remains a highbarrier task. The barriers to data analysis primarily exist in the following areas: •\nLack of systematic statistical training: Individuals without a background in statistics may find it challenging to understand which types of analysis are feasible, even when data is presented to them. As data and models become increasingly complex, gaining a solid understanding of current statistical techniques typically requires at least a Master's level of statistical training.  •\nSoftware limitation: Simple data analysis tools like Excel are inadequate for complex scenarios, such as predictive analysis or analyzing data from enterprise databases.  Conversely, advanced programming languages for data analysis, such as Python and R, require prior programming knowledge, which can be a barrier for many users.  •\nChallenges in domain-specific problems: In specialized fields like protein or genetic data analysis, general data scientists may find it difficult to perform effective analysis due to a lack of domain-specific knowledge.  •\nDifficulty in integrating domain knowledge: Corresponding to the last point, domain experts often lack the data science and programming skills needed to quickly incorporate their expertise into data analysis tools.  For example, PSAAM (Steffensen, Dufault-Thompson, and Zhang 2016) is software designed for the curation and analysis of metabolic models, yet a biologist researching metabolism might find it challenging to integrate this analytical method into common data analysis tools like Excel or R. With the rise of generative AI, new opportunities have emerged in statistics and data science.  LLM-based data agents are gradually addressing existing challenges while introducing a new paradigm for approaching data analysis tasks.  An \"AI agent\" (or LLM agent) refers to an autonomous or semi-autonomous software system powered by AI models such as LLMs.  These agents can interpret natural language instructions, plan and execute tasks, and interact with users or other systems to complete complex workflows (Cheng et al. 2024).  Specifically, we define an LLM-based data agent as an autonomous or semi-autonomous software system powered by LLMs, capable of understanding natural language instructions, planning and executing data-centric tasks, and interacting with users or external tools to accomplish complex objectivesfrom exploratory data analysis to machine learning model development.  In this article, the terms \"LLM-based data science agent,\" \"LLM-based data agent,\" and \"data science agent\" are collectively referred to as \"data agent\" for simplicity.  This survey explores recent advancements in data agents and highlights data analysis performed by various agents through a series of case studies.  In Section 2, we briefly discuss the opportunities introduced by recent developments in generative AI.  Section 3 reviews and categorizes recent work on data science agents.  We then present several case studies in Section 4.  Section 5 examines the challenges and future directions in this field, followed by our discussion in Section 6.  Finally, we present our conclusions in Section 7.  The rise and potential of generative AI, particularly Large Language Models (LLMs) or vision language models (VLMs) in the field of data science and analysis have gained increasing recognition in recent years.  In addition to understand text, LLMs are also trained to understand tabular data, allowing them to effectively extract insights, identify patterns, and draw meaningful conclusions from tables (Dong and Wang 2024).  Consequently, LLMs have emerged as powerful tools capable of significantly enhancing and transforming a variety of data-driven applications and workflows (Nejjar et al. 2023;  Tu et al. 2023; Cheng, Li, and Bing 2023).  Recent research has focused on designing LLMbased data science agents (data agents) to automatically address data science tasks through natural language, as demonstrated by tools like ChatGPT-Advanced Data Analysis (ChatGPT-ADA) (OpenAI 2023), LAMBDA (Sun et al. 2024) and Colab Data Science Agent (Google 2025).  The emergence of data agents offers a potential solution to the previously mentioned challenges, as they lower the entry barrier for users who lack programming or statistical knowledge.  By providing an intuitive interface that harnesses the capabilities of LLMs, users can request analyses using natural language, and the data agents can interpret these instructions, access relevant data, and autonomously apply appropriate analytical techniques.  For example, a user might request, \"Calculate the sales growth in different regions from 2021 to 2028, generate a bar chart to visualize the results, and provide key insights.\"  With this simplified instruction, data agents can automatically extract, analyze, visualize, and report data, reducing the requirement for technical expertise and fostering a more efficient workflow.  This significantly lowers the entry barriers for individuals unfamiliar with traditional data analysis tools and methods.  Furthermore, by embedding specialized knowledge into LLMs, data agents can potentially overcome challenges faced by data scientists in fields like genomics, where domain expertise is crucial (Cao 2017).  Simultaneously, domain experts who may lack data science or programming skills can rely on data agents to seamlessly integrate their expertise into data analysis workflows.  This ability to bridge the gap between domain expertise and data science has the potential to advance interdisciplinary research and decision-making in complex scenarios (Figure 1).  LLM-based data agents leverage the powerful natural language understanding and generation capabilities of LLMs to autonomously tackle complex data analysis tasks.  Figure 3 illustrates a commonly used framework for these agents.  In this framework, the LLM serves as the core of the entire system, driving its performance and reliability.  As such, the capabilities of the LLM are critical to the system's effectiveness, with advanced models like GPT-4 often being used.  Data analysis typically involves multiple steps, especially when addressing complex tasks.  Techniques such as Planning, Reasoning, and Reflection help ensure that the LLM processes these tasks with greater logical coherence and makes optimal use of its knowledge.  In the architecture, the LLM generates the code for a given data analysis task, executes it, and retrieves the corresponding results.  This requires an execution environment, represented by the Sandbox, which safely isolates the code execution process.  The Sandbox allows users to run programs and access files without risking the underlying system or platform.  It includes pre-installed programming environments and software, such as Python, R, Jupyter, and SQL Server.  A user-friendly interface is also essential to improving usability.  An intuitive interface not only attracts users but also enables them to quickly engage with and use the system effectively.  Research on data agents began gaining momentum in 2023.  Chandel et al. (2022) trained and evaluated a model within a Jupyter Notebook to predict code based on given commands and results.  Soon after, it was discovered that LLMs, such as GPT, could generate accurate code for basic data analysis.  With the rise of the LLM-based agent, researchers began designing special data agents for automating data science and analysis tasks by human language.  Figure 2 shows some selected works from 2023, while Table 1 illustrates some key characteristics.  The user interface is crucial for attracting users at first glance.  Current research on user interface design can be broadly categorized into four types: Integrated Development Environmentbased (IDE-based), Independent System, Command line-based (Command-based), and Operation System-based (OS-based).  IDE-based.  Integrated Development Environments (IDEs) such as Jupyter provide convenient tools for data science and analysis.  Recent efforts, including Colab Data Science Agent (Google 2025), Jupyter-AI (jupyterlab 2023), Chapyter (chapyter 2023), and MLCopilot (Zhang et al. 2023a), have incorporated LLMs into Jupyter environments.  For example, Colab Data Science Agent enables planning, automatic code cell generation, execution, and result presentation in the notebook.  This approach is particularly popular because it allows users to review, edit, and run code directly.  Independent System.  Some works have focused on developing independent systems equipped with user interfaces.  For example, ChatGPT introduced a streamlined, intuitive conversational system-a model of interaction that has been widely adopted in subsequent projects.  In the context of data analysis tasks, beyond Figure 3.  An architecture of an LLM-based data agent.  The diagram illustrates the interaction between LLMs and a sandbox environment.  On the left, key components of LLMs are highlighted, including User Interface, Planning, Reasoning, Reflection, and Error Handling.  The sandbox, positioned centrally, serves as a controlled environment for executing task codes and generating results.  On the right, various tools and software that can be pre-installed in the sandbox, such as Python, SQL, Jupyter, and R, indicate the diverse ecosystems where LLM-powered agents can operate.  ChatGPT-ADA (OpenAI 2023)  Conversational System Linear ✗\nData Copilot (Zhang et al. 2023b)  End-to-end System Linear ✗\nJupyter AI (jupyterlab 2023)  Conversational IDE-based Basic IO ✔\nMLCopilot (Zhang et al. 2023a)  Conversational IDE-based Basic IO ✔\nChapyter (chapyter 2023)  Conversational IDE-based Basic IO ✔\nOpenagents (Xie et al. 2023)  Conversational System Linear ✗\nJarviX (Chen et al. 2024)  End-to-end –\nDS-Agent (Guo et al. 2024)  End-to-end CLI Linear ✗\nSpider2-V (Cao et al. 2024)  End-to-end OS-Based –\n– ChatGLM-DA (GLM 2024)  Conversational System Linear ✗\nTaskWeaver (Qiao et al. 2023)  End-to-end CLI & System Linear ✗\nData Interpreter (Hong et al. 2024)  End-to-end CLI Hierarchical ✔\nLAMBDA (Sun et al. 2024)  Conversational System Basic IO ✔\nData Formulator 2 (Wang et al. 2024a)  Conversational System Basic IO ✗\nAutoM3L (Luo et al. 2024)  End-to-end –\nSELA (Chi et al. 2024)  End-to-end CLI Hierarchical ✗\nAIDE (Jiang et al. 2024)  End-to-end CLI Hierarchical ✗\nAutoKagle (Li et al. 2024)  End-to-end CLI Linear ✔\nAutoML-Agent (Trirat, Jeong, and Hwang 2024)  End-to-end –\nLinear –\nAgent K v1.0 (Grosnit et al. 2024)  End-to-end –\nLinear –\nGPT-4o (OpenAI 2024) End-to-end System –\n✔ AutoGen Studio (Wu et al. 2023)  End-to-end System Linear ✗\nColab Data Science Agent (Google 2025)  End-to-end IDE-based Linear ✔\nNOTE: Methods can be categorized into Conversational and End-to-End approaches.  Conversational methods support interactive dialogue with iterative user feedback, whereas End-to-End approaches rely on a single prompt, with the agent autonomously planning and solving the problem.  The user interface can be categorized into IDEbased, Systems, CLI, and OS-based.  The term \"Human-in-the-Loop\"indicates that humans can intervene in the data agent's workflow, such as modifying code in situations whereautomaticprocessesareinadequate.\"Self-Correcting\"referstotheagent'sabilitytoautomaticallyidentifyandcorrecterrorswithintheworkflowthroughreflection.  Finally, \"Expandable\"denotes the data agent's capacity to incorporate customized tools or knowledge.  \"–\"indicates that the attribute is either not mentioned in the article or could not be observed from the provided resources.  basic text-based input and output, several systems have introduced specialized features, such as visualization, report generation, and file download options, to simplify user interactions.  For instance, LAMBDA (Sun et al. 2024) facilitates easy data review by enabling intuitive data display after users upload their data.  Data Formulator 2 (Wang et al. 2024a) further enhances the iterative process of creating data visualizations through a multimodal interface, combining graphical user interface (GUI) elements with natural language inputs, allowing users to specify their visualization intentions with both precision and flexibility.  WaitGPT (Xie et al. 2024) addresses the challenge of understanding and verifying LLM-generated code by transforming raw code into an interactive, step-by-step visual representation.  This allows users to comprehend, validate, and adjust specific data operations, actively guiding and refining the analysis process.  Command Line-based.  Works like Data Interpreter (Hong et al. 2024) and TaskWeaver (Qiao et al. 2023) using command-line interfaces (CLI) in their works.  For researchers and experienced users, it provides greater flexibility and control over the system, allowing users to execute a wide range of functions in the command line and customize their actions.  Besides, commandbased interfaces often require less computational overhead compared to graphical user interfaces, making them more efficient.  OS-based.  OS-based agents, such as UFO (Zhang et al. 2024), are designed to operate directly within an operating system environment, allowing them to control a wide range of system tasks and resources.  Similarly, Spider2-V (Cao et al. 2024) simulates the typical workflow of a data scientist by mimicking actions such as clicking, typing, and writing code, providing an OSlevel interactive experience that closely resembles how humans manage data science tasks.  However, while OS-based agents like Spider2-V lay a solid foundation for user interaction, achieving full automation of the data science workflow remains an ongoing challenge (Cao et al. 2024).  Figure 4.  Commonly used planning and reasoning strategies in LLM-based data agents for organizing tasks or solving problems.  Each node represents a sub-task in the roadmap.  Planning, Reasoning, and Reflection often play crucial roles in guiding the actions of data agents.  In particular, planning and reasoning emphasize the generation of a logically structured sequence or roadmap of actions and thought processes to systematically address problems step by step (Huang et al. 2024b; Hong et al. 2024).  Complex tasks often require a step-bystep approach to ensure effective resolution, while simpler tasks can be handled without such detailed breakdowns.  Recently, GPT-4o (OpenAI 2024) introduces a planning architecture that integrates external tools and decomposes complex tasks into structured sub-tasks, enabling more accurate and controllable multi-step reasoning.  Some approaches focus on building conversational data agents (Zhang et al. 2023a, 2023b; Sun et al. 2024), where users interact with the agent over multiple rounds to complete a task.  In these cases, under human supervision, complex planning is not necessary, as guidance can simplify decision-making and adjust the workflow dynamically.  Some of these works operate in a Basic I/O mode.  On the other hand, End-to-end data agents (Qiao et al. 2023; Guo et al. 2024; Hong et al. 2024; Chi et al. 2024; Jiang et al. 2024; Li et al. 2024; Trirat, Jeong, and Hwang 2024; Grosnit et al. 2024) are designed to allow users to issue a single prompt that encompasses all requirements.  In these cases, the agent employs planning, reasoning, and reflection to iteratively complete all tasks autonomously.  Recent research in planning has introduced two main approaches: Linear Structure Planning (or Single Path Planning/Reasoning) and Hierarchical Structure Planning (or Multiple Path Planning/Reasoning).  Figure 4 illustrates some recent planning methodologies like Chain-of-Thought (CoT) (Wei et al. 2022), ReAct (Yao et al. 2022), Tree-of-Thoughts (ToT) (Yao et al. 2024), and Graph-of-Thoughts (GoT) (Besta et al. 2024).  Linear Structure Planning.  In linear structure planning, a task is decomposed into a sequential, step-by-step process.  For example, DS-Agent (Guo et al. 2024) uses Case-Based Reasoning to retrieve and adapt relevant insights from a knowledge base of past successful Kaggle solutions.  This approach allows the agent to learn from previous experiences and continuously improve its performance.  Similarly, AutoML-Agent (Trirat, Jeong, and Hwang 2024) adopts a retrieval-augmented planning (RAP) strategy to generate diverse plans for AutoML tasks.  By leveraging the knowledge embedded in LLMs, information retrieved from external APIs, and user requirements, RAP allows the agent to explore a wider range of potential solutions, leading to more optimal plans.  Hierarchical Structure Planning.  Simple linear planning is often insufficient for complex tasks.  Such tasks may require hierarchical and dynamic, adaptable plans that can account for unexpected issues or errors in execution (Hong et al. 2024).  For instance, Hong et al. (2024) uses a hierarchical graph modeling approach that breaks down intricate data science problems into manageable sub-problems, represented as nodes in a graph, with their dependencies as edges.  This structured representation enables dynamic task management and allows for real-time adjustments to evolving data and requirements.  Additionally, they further introduce \"Programmable Node Generation,\" to automate the generation, refinement, and verification of nodes within the graph, ensuring accurate and robust code generation.  AIDE (Jiang et al. 2024) employs Solution Space Tree Search to iteratively improve solutions through generation, evaluation, and selection components.  Similarly, SELA (Chi et al. 2024) combines LLMs with Monte Carlo Tree Search (MCTS) to enhance AutoML performance.  It starts by using LLMs to generate insights for various machine learning stages, creating a search space for solutions.  MCTS then explores this space by iteratively selecting, simulating, and back-propagating feedback, enabling the discovery of optimal pipelines.  Agent K v1.0 (Grosnit et al. 2024) employs a structured reasoning framework with memory modules, operating through multiple phases.  The first phase, automation, handles data preparation and task setup, generating actions through structured reasoning.  The second phase, optimization, involves solving tasks and enhancing performance using techniques such as Late-Fusion Model Generation and Bayesian optimization.  The final phase, generalization, uses a memory-driven system for adaptive task selection.  Reflection.  Reflection enables an agent to evaluate past actions and decisions, adjust strategies, and improve future task performance.  This process is essential for self-correction and debugging during task execution.  For example, Wang et al. (2024b) employs trajectory filtering to train agents that can learn from interactions and enhance their self-debugging capabilities.  This technique involves selecting trajectories in which the model initially makes errors but successfully corrects them through self-reflection in subsequent interactions.  Similarly, Data-copilot (Zhang et al. 2023b) and LAMBDA (Sun et al. 2024) use self-reflection based on code execution feedback to address errors.  If a compilation error occurs, the agents repeatedly attempt to revise the code until it runs successfully or a maximum retry limit is reached.  This iterative process helps ensure code correctness and usability.  Multi-agent System (MAS) enable task decomposition through role assignment.  In this setup, agents communicate, negotiate, and share information to optimize their collective performance (Xi et al. 2023).  It offers several advantages over single-agent setups.  First, they reduce redundant and complex context accumulation by isolating responsibilities across agents.  Second, each agent instance can be powered by a different language model, opening opportunities to specialize models for domain-specific expertise.  For example, in LAMBDA (Sun et al. 2024), a dedicated Programmer Agent is responsible for code generation, while noisy error outputs are handled separately by an Inspector Agent.  This separation helps the Programmer Agent avoid context overload, simplifies historical trace management, and ultimately improves response accuracy.  AutoGen introduces a programming framework specifically designed for constructing MAS (Wu et al. 2023).  Furthermore, AutoML-Agent (Trirat, Jeong, and Hwang 2024) involves the Agent Manager, Prompt Agent, Operation Agent, Data Agent, and Model Agent-that together cover the entire pipeline, from data retrieval to model deployment.  OpenAgents (Xie et al. 2023) consisted of agents such as the Data Agent, Plugins Agent, and Web Agent.  Similarly, AutoKaggle (Li et al. 2024) employs agents like Reader, Planner, Developer, Reviewer, and Summarizer to manage each phase of the process, ensuring comprehensive analysis, effective planning, coding, quality assurance, and detailed reporting.  These collaborating mode help decentralized the complicated task, allowing each agent to focus on its specific role, thereby enhancing the overall efficiency and effectiveness of the data analysis process.  Integrating domain-specific knowledge into data agents presents a challenge (Dash et al. 2022; Sun et al. 2024).  For example, when a domain expert has specialized knowledge, such as specific protein analysis code, the agent system are expected able to incorporate and apply this knowledge effectively.  One approach is tool-based, where the expert's analysis code is treated as a tool that is recognizable by the LLM (Xie et al. 2023).  When the agent encounters a relevant problem, it can call upon the appropriate tool from its library to execute the specialized analysis.  Another method involves the Retrieval-Augmented Generation (RAG) technique (Lewis et al. 2020), where relevant code is first retrieved and then embedded within the context to facilitate incontext learning.  LLM-based agents can also access and interact with external knowledge sources, such as databases or knowledge graphs, to augment their reasoning capabilities (Wang et al. 2024b).  Sun et al. (2024) proposes a Knowledge Integration method that builds on this concept.  In LAMBDA, analysis codes are parsed into two parts: descriptions and executable code.  These are then stored in a knowledge base.  When the agent receives a task, it retrieves the relevant knowledge based on the similarity between the task description and the descriptions stored in the knowledge base.  The corresponding code is then used for incontext learning (ICL) or back-end execution, depending on the configuration.  This approach enables agents to effectively leverage domain-specific knowledge in relevant scenarios.  Evaluating the performance of data agents is crucial for understanding their effectiveness and reliability.  Current benchmarks primarily rely on deterministic output comparisons, where an LLM processes a task, generates code, and is evaluated based on the final execution results.  For example, DS-1000 (Lai et al. 2023) provides a large-scale benchmark of 1000 realistic problems spanning seven core Python data science libraries, with execution-based multi-criteria evaluation and mechanisms to reduce memorization bias.  MLAgentBench (Huang et al. 2024a) introduces a benchmark focused on machine learning research workflows by constructing an LLM-agent pipeline.  Furthermore, InfiAgent-DABench (Hu et al. 2024) presents a end-to-end benchmark for evaluating the capabilities of data agents, the tasks require agents to end-to-end solving complex tasks by interacting with an execution environment.  However, for tasks such as data visualization, the outputs are often difficult to compare directly.  Designing effective evaluation strategies for data visualizations remains an open and important question.  Recent advancements in interactive data science systems highlight a variety of approaches in system design, with LLMs and structured frameworks significantly enhancing the user experience across key areas such as data visualization, task specification, predictive modeling, and data exploration.  Notable systems like VIDS (Hassan, Knipper, and Santu 2023), Data-Copilot (Zhang et al. 2023b), InsightPilot (Ma et al. 2023), and JarviX (Liu et al. 2023) exemplify diverse design principles tailored to these specific functions.  For instance, Data-Copilot adopts a code-centric approach, generating intermediate code to process data and subsequently transforming it into visual outputs, such as charts, tables, and summaries (Zhang et al. 2023b).  Other frameworks emphasize workflow automation.  InsightPilot integrates an \"insight engine\" that guides data exploration, reducing LLM hallucinations and enhancing the accuracy of exploratory tasks (Ma et al. 2023).  JarviX, in combination with MLCopilot (Zhang et al. 2023a), contributes to automated machine learning by merging LLM-driven insights with AutoML pipelines.  Additionally, in the domain of database management, systems like LLMDB (Zhou, Zhao, and Li 2024) improve efficiency and reduce hallucinations and computational costs during tasks such as query rewriting, database diagnosis, and data analytics.  In terms of data visualization, MatPlotAgent (Yang et al. 2024) transforms raw data into clear, informative visualizations by leveraging both code-based and multi-modal LLMs.  Moreover, Data Formulator 2 (Wang et al. 2024a) organizes user interactions into \"data threads\" to provide context and facilitate the exploration and revision of prior steps.  A similar approach is seen in WaitGPT (Xie et al. 2024), which transforms Figure 5.  Partial dialogue from the ChatGPT-Advanced Data Analysis in Case Study 1.  Items 1–4 list the work done by ChatGPT in each step.  raw code into an interactive visual representation.  This provides a step-by-step visualization of LLM-generated code in real-time, allowing users to understand, verify, and modify individual data operations.  SEED (Chen et al. 2024) combines LLMs with methods like code generation and small models to produce domainspecific data curation solutions.  HuggingGPT (Shen et al. 2024), on the other hand, uses LLMs to coordinate a variety of expert models from platforms such as Hugging Face, solving a broader range of AI tasks across multiple modalities.  Lastly, in terms of industry applications, lots of companies have used agents in the business analysis.  For example FUTU use AI to analyze the stock market and provide investment advice (FUTU 2024).  Julius (Julius 2025) facilitates data science education by building a bridge that allowing professors to create interactive workflows for lessons, which can be shared with students for a seamless teaching experience through natural language interaction.  In this section, we present a series of case studies conducted by a diverse range of agents, each illustrating the new data analysis paradigm facilitated through natural language interaction.  These case studies demonstrate how this approach enables users to engage with data more intuitively and effectively, breaking down traditional barriers to data accessibility and understanding.  By leveraging natural language processing, these agents can interpret and respond to complex queries, providing insights that are both comprehensive and easily digestible.  Through these examples, we aim to highlight the transformative potential of natural language interaction in data analysis.  In this case study, we used ChatGPT and LAMBDA to demonstrate exploratory data analysis (EDA) and a simple model building process.  Specifically, we first used ChatGPT to explore the effect of alcohol content on the quality of different types of wine, focusing on both red and white varieties.  Then, we used LAMBDA to illustrate an interactive modeling process and automatically generate analysis reports.  We used the Wine Quality dataset, a tabular dataset with dimension 4898 × 11.  The goal is to examine how 10 covariates in this dataset affect the wine quality rating.  We employed ChatGPT-ADA to conduct EDA and visualize the influence of alcohol content on wine quality ratings.  Figure 5 illustrates the detailed planning and problem-solving process.  GPT-ADA first analyzed the problems and then outlined a step-by-step plan to solve the tasks.  The entire workflow proceeded smoothly, with the code running efficiently to load the data, check for missing values, and generate visualizations, with each step delivering accurate results.  Its ability to interpret data and provide insights significantly streamlined the analytical process.  Finally, it provided insights into the relationship between quality scores and alcohol content.  Next, we train a set of models to predict wine quality using LAMBDA.  LAMBDA facilitates an interactive analysis process, enabling us to perform tasks such as data processing, feature engineering, model training, parameter tuning, and evaluation through a series of guided conversations.  Finally, we used LAMBDA's built-in report generation feature to compile a analysis report, which includes details of the tasks completed in the conversation history.  The analysis process, including the conversation and the generated report, is presented in Figure 6.  As beginner-level users, we first asked LAMBDA to recommend some models, and it suggested advanced options like XGBoost.  Next, we tasked LAMBDA with basic data preprocessing, which it handled correctly.  We then trained and evaluated the recommended models using 5-fold crossvalidation, a task LAMBDA performed exceptionally well, even providing download links for the resulting models.  Finally, we used LAMBDA's report generation feature to create a structured and comprehensive report that effectively captured the key insights.  This example demonstrates the effectiveness of conversational data agents like ChatGPT and LAMBDA in streamlining the data visualization and machine learning workflow, particularly for users without programming experience.  To examine the ability of LLM-based data agents to perform statistically rigorous regression diagnostics, we prompted LAMBDA and GPT-4o to conduct a linear regression analysis using the Auto MPG dataset, a tabular data with dimension of 398 times 7.  The goal was to predict mpg (miles per gallon) based on vehicle characteristics, notably horsepower and weight.  The prompt and response of LAMBDA are detailed in the Figure 7.  LAMBDA correctly loaded the dataset, performed appropriate preprocessing (e.g., handling non-numeric entries), and fit a linear model using statsmodels.  It then computed and visualized residuals, followed by executing the Breusch-Pagan test for heteroscedasticity.  The test output included the LM statistic and associated p-value, indicating a strong violation of the homoscedasticity assumption.  The residual plot visually confirmed increasing residual variance with larger fitted values.  LAMBDA also summarized next steps, suggesting robust standard errors or model transformation to address heteroscedasticity.  This example demonstrates LAMBDA's ability to execute, interpret, and communicate statistically meaningful diagnostics in a flexible code-first environment.  Besides, GPT-4o was also able to complete the same task successfully; further details In this case study, we assessed whether LLM-based data agents can perform nonparametric inference through bootstrap resampling.  Using the Wine Quality dataset, the task was to estimate the average alcohol content for red wine and construct a 95% confidence interval using 1000 bootstrap resamples.  Figure 7 shows the interaction with LAMBDA for completing this task.  LAMBDA successfully filtered the dataset to isolate red wines, extracted the alcohol variable, and implemented the bootstrap routine by repeatedly sampling with replacement.  It then computed the empirical 2.5th and 97.5th percentiles of the bootstrapped means to form the confidence interval.  The agent also produced a histogram showing the bootstrap distribution, overlaid with the CI bounds and sample mean.  This case illustrates that LAMBDA is capable of performing robust uncertainty quantification and generating high-quality visual explanations without relying on strict parametric assumptions.  GPT-4o also successfully completed this task; its outputs and detailed interactions are included in the supplementary materials.  We found that different prompting may lead to differences in implementation details, such as the choice of hyperparameters or types of plots.  In many situations, we encounter tasks that cannot be handled effectively using LLMs because their training data do not include the necessary knowledge for such tasks.  In these cases, if a data agent is designed to be extensible, manual tool expansion or knowledge integration can address this limitation.  In this case study, we demonstrate how both the Data Interpreter and LAMBDA leverage integration mechanisms to incorporate additional packages or domain-specific knowledge.  Tools Integration in Data Interpreter.  In this example, our objective is to extract submission deadlines for AI conferences from a public website1 and save the results.  We prompted the agent with the target URL and the desired output format.  The agent successfully identified relevant information such as conference names and deadlines and generated structured output.  The complete workflow, including prompt, execution, and results, is shown in Figure 8.  In this example, the Data Interpreter began with an initial plan.  For each sub-task, it recommended relevant tools with a score indicating their suitability.  The system then decided whether to use the suggested tool.  For instance, it used scrape_web_playwright for a web-scraping task.  This iterative recommendation and tool selection process continued until all sub-tasks were completed, addressing limitations in LLMs' built-in abilities and knowledge.  Knowledge Integration in LAMBDA.  In this example, we consider the problem of training a Fixed Point Non-Negative Neural Network (FPNNN), which is defined as a neural network that maps nonnegative vectors to nonnegative vectors.  We train a FPNNN with MNIST data.  First, we integrated the code into the knowledge base.  Then, we defined the model as Core and delineated the Core function, which directly accepts parameters, and the Runnable function, which was defined and executed separately.  Figure 6 presents the configuration, prompt, and problem-solving process.  LAMBDA first retrieved the relevant code from the knowledge base, and then its Core function was presented in the context.  By modifying the core code, LAMBDA generated the correct code and completed the task successfully (Figure 9).  In this section, we highlight some challenges and suggest future directions in using LLMs or LLM-based data agents for statistical analysis.  LLMs function as the \"brain\" of a data agent, interpreting user intent and generating structured plans to carry out data analysis tasks.  For a data agent to be effective, it must possess advanced knowledge in statistics, data science, and programming, enabling it to support users throughout the analytical process.  Advanced Models.  Current state-of-the-art models like GPT4 show strong performance on undergraduate-level mathematics and statistics problems, yet struggle with more advanced, graduate-level tasks (Frieder et al. 2023).  Additionally, the success rate of fully automating complete data workflows with current agents remains low (Cao et al. 2024).  This suggests that enhancements in LLMs, particularly in knowledge of statistics and data analysis, are still needed.  Multi-Modality and Reasoning.  A key challenge for current LLMs lies in processing multi-modal inputs, including charts, tables, and code, which are essential to data analysis workflows (Inala et al. 2024).  Future advancements may improve the ability to perform reasoning across mixed modalities, such as generating visualizations by replicating the style of an input visualization.  Intelligent Statistical Analysis Software.  While established tools such as SPSS and R are highly mature, data agents have the potential to transform statistical analysis through intelligent assistance.  To realize this vision, agents must support flexible package integration, facilitate contributions from domain experts, and remain aligned with evolving programming ecosystems.  Such a collaborative framework could accelerate innovation in the field.  Furthermore, by guiding users and recommending appropriate methods, data agents can enhance research efficiency and expand access to advanced statistical techniques.  Incorporating Other Large Models into Statistical Analysis.  Statistical analysis of complex data is increasingly leveraging representations generated by large models for research purposes.  For example, in predicting the tertiary structure of proteins, LLMs can use representations of primary and secondary structures-capabilities that traditional statistical software such as Matlab and R currently lack.  Similarly, in the analysis of electronic health records, LLMs are being used to construct meaningful representations that facilitate downstream analysis.  If data agents can effectively harness domain-specific knowledge models, they have the potential to significantly advance statistical and data science research, enabling more sophisticated analyses and fostering deeper insights across scientific disciplines.  Although the data agents have shown great potential in improving the accessibility of data analysis, there are still several challenges that need to be addressed for real-world adoption.  Tradeoff Between Hardware and Privacy.  First, deploying large language models often requires high-performance computing resources.  Running these models on CPU-only machines results in slow inference.  API-based solutions also raise concerns about data privacy and security, as sensitive information may be transmitted to external servers.  This is especially critical in fields such as healthcare and finance, where data confidentiality is paramount.  Therefore, developing lightweight, expert-level data science models that can run efficiently on local machines without compromising performance is essential.  High-concurrency System.  High-concurrency environments pose significant scalability issues.  In client-server architectures where each user session is associated with an isolated sandbox for secure code execution, the server may experience substantial resource strain under heavy load.  Maintaining a large number of concurrent sandboxes can overwhelm system resources, leading to degraded performance or system instability.  Therefore, the design of efficient scheduling algorithms to manage limited computational resources across multiple sandbox instances becomes critical.  Integration with Existing Workflows.  While data agents excel in lowering the barrier to entry for non-programmers, they currently lack the flexibility and debugging capabilities of traditional IDEs.  This makes them less suitable for complex, customized workflows that require iterative development and finegrained control.  A promising direction is to support the seamless export of an agent's actions (Sun et al. 2024), such as executed code, into IDEs like Jupyter Notebooks, which can serve as a bridge for smoother integration with conventional tools and workflows.  While data agents are generally robust to variations in prompt phrasing and can reliably complete the intended analytical tasks, we observed notable differences in their reasoning processes and implementation details.  For example, when prompted to perform regression diagnostics, different phrasings such as \"analyze residuals\" versus \"check model assumptions\" resulted in the same core analysis but with different statistical tests or plotting choices.  Similarly, in visualization tasks, one prompt might produce a bar chart while another yields a pie chart, depending on how the goal is described.  Even for model training, default hyperparameters, such as learning rate or number of iterations, could vary slightly across prompts, leading to differences in performance metrics.  These variations do not typically prevent task completion but can impact result interpretability, especially in rigorous statistical workflows where consistency across runs is critical.  This survey has explored the recent progress of LLM-based data science agents.  These agents have shown great potential in making data analysis more accessible to a wider range of users, even those with limited technical skills.  By leveraging the capabilities of LLMs, they are able to handle various data analysis tasks, from data visualization to machine learning, through natural language interaction.  However, as discussed, they also face several challenges.  In terms of model capabilities, improvements are needed in domain-specific knowledge and multi-modal handling.  For intelligent statistical analysis software, seamless package management and community building are crucial.  Additionally, effectively integrating other large models into statistical analysis and addressing data infrastructure and evaluation issues remain important areas for future development.  Overall, while LLM-based data science agents have made significant strides, continuous research and innovation are required to overcome the existing challenges and fully realize their potential in revolutionizing the field of data analysis.", "metadata": {"source_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf", "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science", "authors": ["Maojun Sun", "Binyan Jiang", "Houduo Qi", "Defeng Sun"], "year": null, "detected_language": "en", "page_count": 15, "origin_chunk_file": "A Survey on Large Language Model-based Agents for Statistics and Data Science.chunks.json"}}
{"text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes Matrix Factorization Methods. Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus.  The particular type of information captured by such matrices varies by application.  In LSA, the matrices are of \"term-document\" type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus.  In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of \"term-term\" type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word.  A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness.  A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropy- or correlation-based normalization.  An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval.  A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation.  More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations.  Shallow Window-Based Methods.  Another approach is to learn word representations that aid in making predictions within local context windows.  For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling.  Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models.  Recently, the importance of the full neural network structure for learning useful word representations has been called into question.  The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors.  Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric.  In the skip-gram and ivLBL models, the objective is to predict a word's context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context.  Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors.  Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus.  Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.  The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning.  In this section, we shed some light on this question.  We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model.  First we establish some notation.  Let the matrix of word-word co-occurrence counts be denoted by X, whose entries Xi j tabulate the number of times word j occurs in the context of word i. Let Xi = P\nk Xik be the number of times any word appears in the context of word i.  Finally, let Pi j = P(j|i)  = Xi j/Xi be the probability that word j appear in the Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus.  Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam.  context of word i. We begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities.  Consider two words i and j that exhibit a particular aspect of interest; for concreteness, suppose we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam.  The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, k.  For words k related to ice but not steam, say k = solid, we expect the ratio Pik/Pjk will be large.  Similarly, for words k related to steam but not ice, say k = gas, the ratio should be small.  For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one.  Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations.  Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.  The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves.  Noting that the ratio Pik/Pjk depends on three words i, j, and k, the most general model takes the form, are separate context word vectors whose role will be discussed in Section 4.2.  In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters.  The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice.  First, we would like F to encode the information present the ratio Pik/Pjk in the word vector space.  Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences.  With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words, modifying Eqn.  (1) to, Next, we note that the arguments of F in Eqn.  (2) are vectors while the right-hand side is a scalar.  While F could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture.  To avoid this issue, we can first take the dot product of the arguments, which prevents F from mixing the vector dimensions in undesirable ways.  Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles.  To do so consistently, we must not only exchange w ↔˜w but also X ↔XT.  Our final model should be invariant under this relabeling, but Eqn. (3) is not.  However, the symmetry can be restored in two steps.  First, we require that F be a homomorphism between the groups (R,+) and (R>0, ×), i.e., Next, we note that Eqn.  (6) would exhibit the exchange symmetry if not for the log(Xi) on the right-hand side.  However, this term is independent of k  so it can be absorbed into a bias bi for wi.  Finally, adding an additional bias ˜bk for ˜wk restores the symmetry, Eqn. (7) is a drastic simplification over Eqn.  (1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero.  One resolution to this issue is to include an additive shift in the logarithm, log(Xik) →log(1 + Xik), which maintains the sparsity of X while avoiding the divergences.  The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments.  A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never.  Such rare cooccurrences are noisy and carry less information than the more frequent ones — yet even just the zero entries account for 75–95% of the data in X, depending on the vocabulary size and corpus.  We propose a new weighted least squares regression model that addresses these problems.  Casting Eqn.  (7) as a least squares problem and introducing a weighting function f (Xi j) into the cost function gives us the model i, j=1 f\nXi j wT i ˜wj + bi + ˜bj −log Xi j 2 , 1. f  (0) = 0.  If f is viewed as a continuous function, it should vanish as x →0 fast enough that the limx→0 f (x) log2 x is finite.  3. f (x) should be relatively small for large values of x, so that frequent co-occurrences are not overweighted.  Of course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be commonalities between the models.  Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL.  Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn.  (8).  The starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that word j appears in the context of word i. For concreteness, let us assume that Qi j is a softmax, Most of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a context window scans over the corpus.  Training proceeds in an on-line, stochastic fashion, but the implied global objective function can be written as, Evaluating the normalization factor of the softmax for each term in this sum is costly.  To allow for efficient training, the skip-gram and ivLBL models introduce approximations to Qi j. However, the sum in Eqn.  (11) can be evaluated much where we have used the fact that the number of like terms is given by the co-occurrence matrix X. Recalling our notation for Xi = P k Xik and Pi j = Xi j/Xi, we can rewrite J as, (13) where H(Pi,Qi) is the cross entropy of the distributions Pi and Qi, which we define in analogy to Xi.  As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn.  (8).  In fact, it is possible to optimize Eqn.  (13) directly as opposed to the on-line training methods used in the skip-gram and ivLBL models.  One could interpret this objective as a \"global skip-gram\" model, and it might be interesting to investigate further.  On the other hand, Eqn. (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors.  To begin, cross entropy error is just one among many possible distance measures between probability distributions, and it has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events.  Furthermore, for the measure to be bounded it requires that the model distribution Q be properly normalized.  This presents a computational bottleneck owing to the sum over the whole vocabulary in Eqn.  (10), and it would be desirable to consider a different distance measure that did not require this property of Q. A natural choice would be a least squares objective in which normalization factors in Q and P are discarded,  where ˆPi j = Xi j and ˆQi j = exp(wT i ˜wj) are the unnormalized distributions.  At this stage another problem emerges, namely that Xi j often takes very large values, which can complicate the optimization.  An effective remedy is to minimize the Finally, we observe that while the weighting factor Xi is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal.  In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words.  With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well.  The result is, As can be seen from Eqn.  (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero elements in the matrix X. As this number is always less than the total number of entries of the matrix, the model scales no worse than O(|V |2).  At first glance this might seem like a substantial improvement over the shallow windowbased approaches, which scale with the corpus size, |C|.  However, typical vocabularies have hundreds of thousands of words, so that |V |2 can be in the hundreds of billions, which is actually much larger than most corpora.  For this reason it is important to determine whether a tighter bound can be placed on the number of nonzero elements of X.  In order to make any concrete statements about the number of nonzero elements in X, it is necessary to make some assumptions about the distribution of word co-occurrences.", "metadata": {"source_file": "D14-1162.pdf", "title": "Glove: Global Vectors for Word Representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "year": "2014", "detected_language": "en", "page_count": 12, "origin_chunk_file": "D14-1162.chunks.json"}}
{"text": "In particular, we will assume that the number of co-occurrences of word i with word j, Xi j, can be modeled as a power-law function of the frequency rank of that word pair, ri j: The total number of words in the corpus is proportional to the sum over all elements of the cooccurrence matrix X, where we have rewritten the last sum in terms of the generalized harmonic number Hn,m. The upper limit of the sum, |X|, is the maximum frequency rank, which coincides with the number of nonzero elements in the matrix X. This number is also equal to the maximum value of r in Eqn. (17) such that Xi j ≥1, i.e., |X| = k1/α. Therefore we can write Eqn. (18) as, We are interested in how |X| is related to |C| when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X|. For this purpose we use the expansion of generalized harmonic numbers (Apostol, 1976), where ζ(s) is the Riemann zeta function. In the limit that X is large, only one of the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α > 1, For the corpora studied in this article, we observe that Xi j is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X| = O(|C|0.8). Therefore we conclude that the complexity of the model is much better than the worst case O(V 2), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG† Model Dim. Size Sem. Syn. Tot. ivLBL\nCBOW\n42B 81.9 69.3 75.0 dataset for NER (Tjong Kim Sang and De Meulder, 2003).  Word analogies.  The word analogy task consists of questions like, \"a is to b as c is to ?\"  The dataset contains 19,544 such questions, divided into a semantic subset and a syntactic subset.  The semantic questions are typically analogies about people or places, like \"Athens is to Greece as Berlin is to ?\".  The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example \"dance is to dancing as fly is to ?\".  To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match.  We answer the question \"a is to b as c is to ?\" by finding the word d whose representation wd is closest to wb −wa + wc according to the cosine similarity.4 Figure 2: Accuracy on the analogy task as function of vector size and window size/type.  All models are trained on the 6 billion token corpus.  In (a), the window size is 10.  In (b) and (c), the vector size is 100.  Word similarity.  While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3.  These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013).  Named entity recognition.  The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous.  We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set.  We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013).  We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005).  A total of 437,905 discrete features were generated for the CoNLL2003 training dataset.  In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features.  With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013).  We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which the analogy task.  This number is evaluated on a subset of the dataset so it is not included in Table 2.  3COSMUL performed worse than cosine similarity in almost all of our experiments.  has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl5.  We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of cooccurrence counts X. In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context.  We explore the effect of these choices below.  In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count.  This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words' relationship to one another.  For all our experiments, we set xmax  = 100, α = 3/4, and train the model using AdaGrad (Duchi et al., 2011), stochastically sampling nonzero elements from X, with initial learning rate of 0.05.  We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate).  Unless otherwise noted, we use a context of ten words to the left and ten words to the right.  The model generates two sets of word vectors, W and ˜W. When X is symmetric, W and ˜W are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently.  On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al., 2012).  With this in mind, we choose to use 5To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.  6For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words.  We present results on the word analogy task in Table 2.  The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora.  Our results using the word2vec tool are somewhat better than most of the previously published results.  This is due to a number of factors, including our choice to use negative sampling (which typically works better than the hierarchical softmax), the number of negative samples, and the choice of the corpus.  We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost.  We note that increasing the corpus size does not guarantee improved results for other models, as can be seen by the decreased performance of the SVD7We also investigated several other weighting schemes for transforming X; what we report here performed best.  Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies.  With smaller vocabularies, these information-theoretic transformations do indeed work well on word similarity measures, but they perform very poorly on the word analogy task.  Table 3: Spearman rank correlation on word similarity tasks.  All vectors are 300-dimensional.  The CBOW∗vectors are from the word2vec website and differ in that they contain phrase vectors.  Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25.6 SVD-S 6B 56.5 71.5 71.0 53.6 34.7 SVD-L 6B 65.7 72.7 75.1 56.5 37.0 CBOW† 6B 57.2 65.6 68.2 57.0 32.5 SG† 6B 62.8 65.2 69.7 58.1 37.2 GloVe 6B 65.8 72.7 77.8 53.9 38.1 SVD-L 42B 74.0 76.4 74.1 58.3 39.9 GloVe 42B 75.9 83.6 82.9 59.6 47.8 CBOW∗100B 68.4 79.6 75.4 59.4 45.5 L model on this larger corpus.  The fact that this basic SVD model does not scale well to large corpora lends further evidence to the necessity of the type of weighting scheme proposed in our model.  Table 3 shows results on five different word similarity datasets.  A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculating the cosine similarity.  We compute Spearman's rank correlation coefficient between this score and the human judgments.  CBOW∗denotes the vectors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data.  GloVe outperforms it while using a corpus less than half the size.  Table 4 shows results on the NER task with the CRF-based model.  The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations.  Otherwise all configurations are identical to those used by Wang and Manning (2013).  The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features.  In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW).  We trained the CBOW model using the word2vec tool8.  The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better.  We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10.  Table 4: F1 score on NER task with 50d vectors.  Discrete is the baseline without word vectors.  We use publicly-available vectors for HPCA, HSMN, and CW.  See text for details.  Model Dev Test ACE MUC7 Discrete 91.0 85.4 77.4 73.4 SVD 90.8 85.7 77.3 73.7 SVD-S 91.0 85.5 77.6 74.3 SVD-L 90.5 84.8 73.6 71.5 HPCA 92.6 88.7 81.7 80.7 HSMN 90.5 85.7 78.7 74.7 CW 92.2 87.4 81.7 80.2 CBOW 93.1 88.2 82.2 81.1 GloVe 93.2 88.3 82.9 82.2 In Fig.  2, we show the results of experiments that vary vector length and context window.  A context window that extends to the left and right of a target word will be called symmetric, and one which extends only to the left will be called asymmetric.  In (a), we observe diminishing returns for vectors larger than about 200 dimensions.  In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context windows.  Performance is better on the syntactic subtask for small and asymmetric context windows, which aligns with the intuition that syntactic information is mostly drawn from the immediate context and can depend strongly on word order.  Semantic information, on the other hand, is more frequently non-local, and more of it is captured with larger window sizes.  In Fig. 3, we show performance on the word analogy task for 300-dimensional vectors trained on different corpora.  On the syntactic subtask, there is a monotonic increase in performance as the corpus size increases.  This is to be expected since larger corpora typically produce better statistics.  Interestingly, the same trend is not true for the semantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus.  This is likely due to the large number of city- and countrybased analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations.  Moreover, Wikipedia's entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information.  The total run-time is split between populating X and training the model.  The former depends on many factors, including window size, vocabulary size, and corpus size.  Though we did not do so, this step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert (2014) for some benchmarks).  Using a single thread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes.  Given X, the time it takes to train the model depends on the vector size and the number of iterations.  For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes.  See Fig. 4 for a plot of the learning curve.  A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on performance.  We control for the main sources of variation that we identified in Sections 4.4 and 4.5 by setting the vector length, context window size, corpus, and vocabulary size to the configuration mentioned in the previous subsection.  The most important remaining variable to control for is training time.  For GloVe, the relevant parameter is the number of training iterations.  For word2vec, the obvious choice would be the number of training epochs.  Unfortunately, the code is currently designed for only a single epoch: Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram (b).  In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.  it specifies a learning schedule specific to a single pass through the data, making a modification for multiple passes a non-trivial task.  Another choice is to vary the number of negative samples.  Adding negative samples effectively increases the number of training words seen by the model, so in some ways it is analogous to extra epochs.  We set any unspecified parameters to their default values, assuming that they are close to optimal, though we acknowledge that this simplification should be relaxed in a more thorough analysis.  In Fig. 4, we plot the overall performance on the analogy task as a function of training time.  The two x-axes at the bottom indicate the corresponding number of training iterations for GloVe and negative samples for word2vec.  We note that word2vec's performance actually decreases if the number of negative samples increases beyond about 10.  Presumably this is because the negative sampling method does not approximate the target probability distribution well.9 For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec.  It achieves better results faster, and also obtains the best results irrespective of speed.  9In contrast, noise-contrastive estimation is an approximation which improves with more negative samples.  In Table 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples.  methods or from prediction-based methods.  Currently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks.  In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous.  We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec.  The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks.  Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.  2014.  Don't count, predict!  A\nsystematic comparison of context-counting vs. context-predicting semantic vectors.  In ACL.  John A. Bullinaria and Joseph P. Levy.  2007.  Extracting semantic representations from word cooccurrence statistics: A computational study.  Behavior Research Methods, 39(3):510–526.  Dan C. Ciresan, Alessandro Giusti, Luca M. Gambardella, and J¨urgen Schmidhuber.  2012.  Deep neural networks segment neuronal membranes in electron microscopy images.  In NIPS, pages 2852–2860.  Ronan Collobert and Jason Weston.  2008.  A unified architecture for natural language processing: deep neural networks with multitask learning.  In Proceedings of ICML, pages 160–167.  Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.  2011.  Natural Language Processing (Almost) from Scratch.  JMLR, 12:2493–2537.  Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman.  1990.  Indexing by latent semantic analysis.  Journal of the American Society for Information Science, 41.  John Duchi, Elad Hazan, and Yoram Singer.  2011.  Adaptive subgradient methods for online learning and stochastic optimization.  JMLR, 12.  Lev Finkelstein, Evgenly Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.  2001.  Placing search in context: The concept revisited.  In Proceedings of the 10th international conference on World Wide Web, pages 406–414. ACM.  Kevin Lund and Curt Burgess.  1996.  Producing high-dimensional semantic spaces from lexical co-occurrence.  Behavior Research Methods, Instrumentation, and Computers, 28:203–208.  Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013.  Better word representations with recursive neural networks for morphology.  CoNLL-2013.  Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a.  Efficient Estimation of Word Representations in Vector Space.  In ICLR Workshop Papers.  Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b.  Distributed representations of words and phrases and their compositionality.  In NIPS, pages 3111–3119.  Tomas Mikolov, Wen tau  Yih, and Geoffrey Zweig. 2013c.  Linguistic regularities in continuous space word representations.  In HLTNAACL.  Douglas L. T. Rohde, Laura M. Gonnerman, and David C. Plaut.  2006.  An improved model of semantic similarity based on lexical co-occurence.  Communications of the ACM, 8:627–633.  Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013.  Parsing With Compositional Vector Grammars.  In ACL.  Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton.  2003.  Quantitative evaluation of passage retrieval algorithms for question answering.  In Proceedings of the SIGIR Conference on Research and Development in Informaion Retrieval.  Erik F. Tjong Kim Sang and Fien De Meulder.  2003.  Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.  In CoNLL-2003.  Joseph Turian, Lev Ratinov, and Yoshua Bengio.  2010.  Word representations: a simple and general method for semi-supervised learning.  In Proceedings of ACL, pages 384–394.  Mengqiu Wang and Christopher D. Manning.  2013.  Effect of non-linear deep architecture in sequence labeling.  In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP).", "metadata": {"source_file": "D14-1162.pdf", "title": "Glove: Global Vectors for Word Representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "year": "2014", "detected_language": "en", "page_count": 12, "origin_chunk_file": "D14-1162.chunks.json"}}
{"text": "Expert systems (ES) are knowledge-based systems that were one of the earlier research fields in Artificial Intelligence (AI) and can be defined as knowledgeintensive software that can perform some tasks normally requiring human expertise. Expert systems are used to solve specific domain problems and each step of reasoning for a specific problem is determined by the human expert professionally. So, they behave as an artificial advisory system for a particular problem domain. Although AI is used in various commercial applications today, an expert system application is sometimes regarded as \"AI\" too. After expert systems have moved out of research laboratories during early 1980s, they became more popular and found several application fields such as engineering, chemistry, medicine, industry, and many others. The construction process of expert systems with specialized domain knowledge is defined as knowledge engineering. Knowledge-based expert systems contain knowledge acquired from periodicals, books, or from domain interviews with human experts. Expert systems are mostly preferred as they produce reasonable solutions for even some ill-structured problems that have no efficient algorithmic solution (1). In addition to classical expert systems, there are hybrid expert systems today using techniques such as artificial neural networks and genetic algorithms. Broader information for expert systems is given in Section 2. Then, historical development and current applications of expert systems will be mentioned in Sections 3 and 4, respectively. 2.1. General Concepts. The first expert systems were built by interviewing an expert and attempting to capture the knowledge, hence the term \"expert systems.\" An ES is a computer program, which is constructed by utilizing the experience of a domain expert. It performs functions like asking questions and explaining its reasoning. The user interface of this kind of system proceeds with the question answer manner by the end user. The kernel of an expert system has two main components, namely the knowledge base and the inference engine.  The knowledge base contains knowledge about the expert's domain.  It may be represented by simple facts, or by more complex representations like frames.  There are also rules that explicitly represent the expert's skills or knowledge about the domain under consideration.  The expert system uses this knowledge by exploiting the second main component, that is the inference engine that has several roles including determining how the system reasons using the IF–THEN rules in the knowledge base.  Once the knowledge base is built, the ES can begin making inferences.  The most common forms of inferencing are forward and backward chaining.  The process of moving forward from known facts to conclusions that follow them is called forward chaining.  Alternatively, the process of working backward from a hypothesis to known facts that support it, is called backward chaining.  The general architecture of an expert system is presented in Figure 1 and its components are defined as follows (1): • User interface—the mechanism by which the user and the expert system communicate.  • Explanation facility—explains the reasoning of the system to a user.  • Working memory—a database of facts used by the rules.  • Inference engine—makes inferences by deciding which rules are satisfied by facts or objects, prioritizes the satisfied rules, and executes the rule with the highest priority.  •  Agenda—a prioritized list of rules created by the inference engine, whose patterns are satisfied by facts or objects in working memory.  •  Knowledge acquisition facility—an automatic way for the user to enter knowledge in the system rather than by having the knowledge engineer explicitly code the knowledge.  Knowledge Elicitation.  The first phase of building an ES involves obtaining expert's knowledge.  Domain-specific knowledge is extracted from a number of sources using one or more of the following knowledge elicitation techniques: Structured interviews—specific probing questions are asked (scope of the questions are usually identified in a preliminary interview with the expert).  Unstructured interviews—expert is asked to provide all information regarding the domain without specific elaboration of detail.  Unstructured interviews can be used as a preliminary interview to the structured approach.  In the protocol analysis, the expert is asked to \"think aloud.\"  This is then recorded in some way.  There are two forms of protocol analysis: The two protocol techniques when applied together may provide precise information concerning how a problem is tackled.  Multidimensional scaling is technique to elicit experience and relationships between objects from the view of an expert.  Primarily used when there are a number of closely related concepts and no specialized vocabulary to express subtle distinctions and relationships.  The technique involves visually representing the psychological similarities between objects or experiences as points on a scatter graph.  Objects, which are psychologically dissimilar, are shown far apart; the distance between them can be analyzed to interpret the underlying dimensions as to why these objects have been judged relative to one another.  Finally, card sorting techniques provide means of achieving a more focused or systematic understanding of the classifications and relationships in the expert's domain.  It is easy to implement and involves writing the names of objects, experiences, or rules in the expert's domain onto individual cards.  Usually implemented either as group separation tasks or group creation tasks.  In group separation tasks, the expert is asked to group cards into two, which are then named.  The cards are shuffled and then the whole procedure is applied for three, four, and more groupings.  Whereas, in the group creation groups are not made smaller but \"built-up.\"  The expert is asked to find a pair of cards from the set of cards that are most similar than any other pair.  Rule-Based ES—Knowledge Representation and Inferencing.  Rulebased expert systems consist of set of IF–THEN rules as represented in the following (2): IF  the substance x is copper AND the temperature of x is room temperature THEN the phase of x is solid.  The IF part is defined as the antecedent that contain fact(s) and the THEN part is defined as the conclusion that contain goal(s).  The antecedent part in which facts are executed first is called as data-driven, or forward chaining and the conclusion part in which goal parts of IF–THEN rules are executed first is called as goal-driven or backward chaining.  The forward chaining approach is generally preferred in the case of a few and expensive data collection and backward chaining approach is preferred when a specific result is required with respect to large quantity of data (3).  The two main tasks related to reasoning issues addressed by the inference engine are the following: 1.  After obtaining the input from the user, the inference engine decides where to start the reasoning process by going through the rules and facts that reside in the static knowledge base.  2.  The inference engine resolves conflicts that occur when more than one rule has a consequent matching the current goal.  When the system reaches to a point where there are more than a few rules ready to be executed the inference engine decides which rule to examine next.  This is called conflict resolution and there are many different strategies to handle this sort of situation.  To summaries, the inference process is carried out in three stages as shown in Figure 2.  During the match stage, the contents of working memory are compared to facts and rules contained in the knowledge base.  When consistent matches are found, the corresponding rules are placed in a conflict set.  Once all the matched rules have been added to the conflict set during a cycle, one of the rules is selected for execution.  Development of an ES.  Basically, the expert system development process consists of construction of rules that are derived from problem-solving interviews with a human expert.  The knowledge engineer then organizes this knowledge collected from the expert in a form that can be effectively represented in an expert system.  Building an ES is an iterative process involving the creation of a prototype system and then over a number of cycles of testing, repair, and extension, incrementally improving the system so that it eventually performs in a way that is satisfactory and beneficial to the users.  The problem identification, conceptualization, formalization, construction, and testing are the stages of ES development process (4).  Once a system has been built and debugged it is a simple matter to extend its coverage of cases at that level of expertise.  Limitations of Expert Systems.  The main limitations are the following: ∗ES work well only in a narrow domain.  ∗Knowledge transfer from the domain expert is subject to perceptual and judgmental biases.  ES constructed with the help of a single expert can differ in its conclusions if checked independently by another expert in the same area.  ∗Contradictory information obtained from domain experts can yield faulty conclusions.  interpretation, monitoring, planning, prognosis, remedy, and control.  For example, early well-known expert systems are DENDRAL, CRYSALIS, TQMSTUNE, CLONER, MOLGEN, SECS, SPEX, and SYNCHEM2 in chemistry (2,7–9).  ACE, EURISKO, SOPHIE, PALLADIO, REDESIGN, TALIB, and CADHELP are in electronics, whereas PUFF, SPE, VM, CADUCEOUS, BLUE BOX, ONCOCIN, GUIDON, MYCIN, are ABEL are earlier examples of medical expert systems (1,10).  Connectionist Expert Systems Based on Artificial Neural Networks.  A\nneural network can, in fact, serve as the knowledge base for an expert system that does classification tasks.  For a neural network expert system, knowledge representation consists of a network, connection weights, and semantic interpretations attached to cells and activations.  The main advantage of this approach is that the underlying learning algorithm, such as Backpropagation, can take training examples and generate expert systems automatically.  This procedure is illustrated in Figure 3.  Typical applications include management and administration (cost estimation, scheduling), industrial (process control, manufacturing quality control, fault diagnosis), medical (medical diagnosis in specialized domains, bacteria identification), banking (credit and loan decisions), and other fields (forecasting).  If training data is available then any domain becomes a candidate for a connectionist expert system.  The neuron model developed by McCulloch and Pitts in 1943 can be regarded as the starting point for the connectionist expert systems.  Also, a learning process of neurons is defined by Hebb in 1949, and the efficiency of this Hebbian neuron learning model is determined by the emulation and transferring each impulse of one neuron to another neuron successfully and this process is defined as firing rules between each neuron.  Inductive learning is used in the neural network expert systems or connectionist expert systems, and these types of expert systems are advantageous when there is much empirical data and also it is used to prevent knowledge acquisition bottleneck (13–15).  The expert system rule application to define training and test patterns is represented in the following medical expert system example.  The knowledge base of this expert system consist of various IF–THEN rules that are related with diagnosing illnesses to achieve the more appropriate treatment and this system can be implemented as a three layered-neural network as in the following example (13).  The neural network consists of cells that correspond to symptoms in the input layer, the diseases are presented in the intermediate or hidden layer and neural cells (nodes) for the treatments defined in the output layer.  Training patterns consist of 0's (lack of knowledge about presence or absence of the disease), 1's (presence of the disease), and 1's (absence of the disease).  A sample rule from Hepar, a medical expert system for the diagnosis of liver and biliary tract diseases is (16)  The general topology for this connectionist ES consists of a two-layer neural network and is shown in Figure 4.  In this structure, the diagnosis of the related disease is activated only certain symptomatic situations are identified as true (\"1\").  At the same time, this system is trained and tested by one of the supervised learning algorithm such as backpropagation algorithm.  However, we should note that not all expert system problems are suitable for a neural network approach.  The most suitable problems are those that seek to classify inputs into a small number of groups.  Induction-Based Systems.  The major bottleneck of building expert systems lie in knowledge elicitation from domain experts.  There are several difficulties with acquiring knowledge from an expert.  1.  Knowledge mismatch, the difference between the way expert's own knowledge is structured and the way it is represented in the program.  2. Inability of humans to express knowledge they possess and the inherent nature of knowledge (subconscious, approximate, incomplete, inconsistent, etc).  3. Problem of verification and validation.  1.  It might be more competent than humans for acquiring or fine-tuning certain kinds of knowledge.  2. It might significantly reduce the high cost in human resources involved in construing the system.  4.1. Current Applications.  Current applications of expert systems and AI are mentioned in the bibliographic studies for expert systems (10,18–21).  Expert system implementation methods can be classified as classical expert systems, neural expert systems, fuzzy expert systems, rough set-based expert Expert systems have now reached the maturity stage in their development.  The early research has established the viability of this approach.  Many commercial systems have been developed since then to show their use in real environments.  These days we find them either as hybrid expert systems or embedded on web pages in the Internet (29–33).  The basic limitation currently, is to build expert systems with heuristic and empirical knowledge rather than deep knowledge, which include models of the functional and causal relations that underlie a problem.  In the future, more systems might be developed using functional and causal models using a variety of representations.  Using multiple sources of knowledge (i.e., domain experts) in a cooperative manner is still a difficult problem waiting to be tackled.  Knowledge-based expert systems will continue to increase individual and social potential by preserving know-how, distributing knowledge more effectively, and improving performance of tasks that require expertise.  24. I. M. Dokas and A. Alapetite, A Development Process Meta-Model for Web Based Expert Systems: The Web Engineering Point of View, Risø National Laboratory, Denmark, 2006.  25.  R. Islam and K. Andersson, Mohammad Shahadat Hossain, A Web Based Belief Rule Based Expert System to Predict Flood, WAS2015, Brussels, Belgium, Dec. 11–13, 2015.  26.  O. Verhodubs and J. Grundspenkis, Towards the Semantic Web Expert System.  Appl.  Comput.  Syst Q3 .  44 (1), 116–123 (2012).  27.  M. Nofal and K. M. Fouad, Int.  J. Comput.  Sci.  Issues 11, 1 (1), 103–110 (2014).  28.  F. H. Grupe, The Internet and Higher Education: A Quarterly Review of Innovations in Post-Secondary Education, Jai Press, Greenwich, Conn., 5 (4), 2002, pp.  333–345 29.  S. Wang and R. A. Noe, Hum.  Resource Manage.  Rev. 20, 115–131 (2010).  30.  J. K. Nurminen, O. Karonen, and K. Hätönen, Expert Syst.  Appl. 24, 199–211 (2003).  31.  J. T. Ball and R. F. Moody, The Future of Expert System Development Tools?  ORION International Technologies, Inc., The Fifteenth Annual Ideas in Science & Electronic Exposition and Symposium'93, 1993.  32.  C. F. Tan and co-workers, ARPN J. Eng.  Appl.  Sci. 11 (4), 2448–2453 (2016).  33. C. Angeli, Appl.  Res. 1, 50–73 (2010).  in the online version.  The abstract and keywords are included in the print version only for your review.  Expert systems have emerged around mid-1970s under the umbrella of Artificial Intelligence and as soon as convincing success was attained, the field was transformed into an established branch of computer science.  The potential of expert systems that emulate human knowledge and skill has also encouraged the development of many applications in various areas.  Expert systems contain specialized knowledge elicited from a domain expert.  Various expert system building tools or shells exist to greatly facilitate and speed up the development of expert systems.  Some of the new generation tools also allow an expert system to be delivered over the Internet.  This article introduces expert system and knowledge engineering concepts and discusses issues related to expert system design and development in various areas.", "metadata": {"source_file": "Expert_Systems.pdf", "title": "Expert Systems 6 38,340", "authors": ["Seda Şahin", "Kasim Oztoprak"], "year": "2016", "detected_language": "en", "page_count": 15, "origin_chunk_file": "Expert_Systems.chunks.json"}}
{"text": "This publication is a Technical report by the Joint Research Centre (JRC), the European Commission's science and knowledge service. It aims to provide evidence-based scientific support to the European policymaking process. The scientific output expressed does not imply a policy position of the European Commission. Neither the European Commission nor any person acting on behalf of the Commission is responsible for the use that might be made of this publication. Contact information Blagoj DELIPETREV European Commission, Joint Research Centre, TP262, Via Fermi, 21027 Ispra (VA), ITALY blagoj..eu Tel.: + 39 0332 786 352 All content © European Union, 2020, except: - Table 1: Some definitions of AI, organized in four categories (Russell and Norvig 2016); - Figure 8: Adversarial attacks (OpenAI.com). Abstract ...................................................................................................................................................................4 This report is published in the context of AI Watch, the European Commission knowledge service to monitor the development, uptake and impact of Artificial Intelligence (AI) for Europe, launched in December 2018. AI has become an area of strategic importance with potential to be a key driver of economic development. AI also has a wide range of potential social implications. As part of its Digital Single Market Strategy, the European Commission put forward in April 2018 a European strategy on AI in its Communication \"Artificial Intelligence for Europe\" COM (2018)237. The aims of the European AI strategy announced in the communication are: •\nTo boost the EU's technological and industrial capacity and AI uptake across the economy, both by the private and public sectors Subsequently, in December 2018, the European Commission and the Member States published a \"Coordinated Plan on Artificial Intelligence\", COM (2018)795, on the development of AI in the EU. The Coordinated Plan mentions the role of AI Watch to monitor its implementation.  AI Watch monitors European Union's industrial, technological and research capacity in AI; AI-related policy initiatives in the Member States; uptake and technical developments of AI; and AI impact.  AI Watch has a European focus within the global landscape.  In the context of AI Watch, the Commission works in coordination with Member States.  AI Watch results and analyses are published on the AI Watch Portal (\nFrom AI Watch in-depth analyses, we will be able to understand better European Union's areas of strength and areas where investment is needed.  AI Watch will provide an independent assessment of the impacts and benefits of AI on growth, jobs, education, and society.  This report addresses the following objectives of AI WATCH:  Analysis of the evolution of AI technologies.  As part of this objective this report particularly aims to analyse the historical evolution of AI and the similarities and differences among the different phases of this evolution.  We are thankful to our JRC colleagues: Emilia Gómez, Massimo Craglia, Paul Desruelle, Giuditta De Prato, Montserrat López Cobo, Marius Miron and Alessandro Annoni, for their valuable comments and suggestions.  Today, AI has become a mature technology and an increasingly important part of the modern life fabric, but its future is uncertain, with chances of both a higher growth and a decline.  Reflecting on the history of the domain may provide us with insights on its future.  Starting with fundamental definitions and building on the historical context, this report summarizes the evolution of AI, it presents the periods of AI development and describes the current rise of interest in AI.  It concludes with a comparison of the current and past AI development contexts, trying to get insights on the uncertain future of AI.  1.  AI fundamentals, including several AI definitions, main approaches and methods.  2.  The AI development periods (\"seasons\" - i.e. winters for the decline and springs for the growth), shown in Figure 1, including: \nThe spring of the 1950s, with the establishment of the foundation of most of the AI algorithms, followed by the winter of the 1970s.   The spring of the 1970s, with the paradigm shift in symbolic algorithms and building of expert systems, followed by the winter of the 1990s.    The development of machine learning in the 1990s, which led to the development of deep learning and the spring of the 2010s.  3.  An analysis of the AI paradigm shifts accompanied by bold predictions, followed by massive investments, and major issues.  Some of the major issues in the first two springs have led to the two AI winters.  The analysis is more focused on the current machine learning and deep learning period.  4.  A comparison of the current and past AI development contexts to get insights on the uncertain future of AI.  The context parameters examined include, among others, the main AI investors, the entities performing basic research, the paradigm shifts, the computing capacity, the availability of relevant regulation, etc.  The future of AI is uncertain, with chances of another AI winter or of an even greater AI summer.  Given this uncertainty, it is particularly important to have in place an unbiased initiative such as AI Watch to monitor the evolution and assess its impacts over the coming years which one way or the other will see very significant changes in our digitally transformed society.  Artificial intelligence (AI) can have a major impact on the way modern societies respond to the hard challenges they face.  Properly harnessed, AI can create a more fair, healthy, and inclusive society.  Today, AI has become a mature technology and an increasingly important part of the modern life fabric.  AI is already deployed in different application domains, e.g. recommendation systems, spam filters, image recognition, voice recognition, virtual assistants, etc.  It spans across many sectors, from medicine to transportation, and across decades, since the term was introduced in the 1950s.  The approaches also evolved, from the foundational AI algorithms of the 1950s, to the paradigm shift in symbolic algorithms and expert system development in the 1970s, the introduction of machine learning in the 1990s and the deep learning algorithms of the 2010s.", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "Starting with the fundamental definitions and building on the historical context, this report summarizes the evolution of AI, it introduces the \"seasons\" of AI development (i.e. winters for the decline and springs for the growth), describes the current rise of interest in AI, and concludes with the uncertainty on the future of AI, with chances of another AI winter or of an even greater AI spring. AI is defined as machine intelligence or intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. The term AI is often used to describe machines that mimic human cognitive functions such as learning, understanding, reasoning or problem-solving (Russell and Norvig 2016). AI has two main dimensions, as shown in Table 2 below. The AI definitions on top of the table relate to processes and reasoning, whereas the ones at the bottom side address behaviour. The definitions on the left side of the table measure success in terms of fidelity to human performance, whereas the ones on the right-side measure against an ideal concept of intelligence and rationality. \"The exciting new effort to make computers think ... machines with minds, in the full and literal sense\" (Haugeland 1985) The High-Level Expert Group on Artificial Intelligence (HLEG 2019) produced an AI definition that has also been adopted by AI Watch (Samoili, Lopez Cobo, et al. 2020): \"Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans that, given a complex goal, act in the physical or digital dimension by perceiving their environment through data acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or processing the information, derived from this data and deciding the best action(s) to take to achieve the given goal. AI systems can either use symbolic rules or learn a numeric model, and they can also adapt their behaviour by analysing how the environment is affected by their previous actions.", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "As a scientific discipline, AI includes several approaches and techniques, such as machine learning (of which deep learning and reinforcement learning are specific examples), machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search, and optimization), and robotics (which includes control, perception, sensors, and actuators, as well as the integration of all other techniques into cyber-physical systems).\" Artificial Narrow Intelligence (ANI), often referred to as \"Weak\" AI is the type of AI that mostly exists today. ANI systems can perform one or a few specific tasks and operate within a predefined environment, e.g., those exploited by personal assistants Siri, Alexa, language translations, recommendation systems, image recognition systems, face identification, etc. ANI can process data at lightning speed and boost the overall productivity and efficiency in many practical applications, e.g., translate between 100+ languages simultaneously, identify faces and objects in billions of images with high accuracy, assist users in many data-driven decisions in a quicker way. ANI can perform routine, repetitive, and mundane tasks that humans would prefer to avoid. While ANI is superior in specialized domains, it is incapable of generalization, i.e. to re-use learned knowledge across domains, e.g., the ANI capable of image recognition cannot transfer its knowledge in the domain of speech recognition. The generalization problem is still an open question (Hernández-Orallo 2017). Artificial General Intelligence (AGI) or \"Strong\" AI refers to machines that exhibit human intelligence. In other words, AGI aims to perform any intellectual task that a human being can. AGI is often illustrated in science fiction movies with situations where humans interact with machines that are conscious, sentient, and driven by emotion and self-awareness. At this moment, there is nothing like an AGI. Artificial Superintelligence (ASI) is defined as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\" (Bostrom 2016).  ASI is supposed to surpass human intelligence in all aspects — such as creativity, general wisdom, and problem-solving.  ASI is supposed to be capable of exhibiting intelligence that we have not seen in the brightest thinkers amongst us.  Many thinkers are worried about ASI.  At this moment, ASI belongs to science fiction.  If we ever succeed in creating an AI that is capable of generalizing, understanding causality, making a model of the world, it is highly likely that it will be closer to ASI than AGI.  AI excels in numerical calculations, and there is no logical explanation as to why AI would downgrade its abilities to simulate humans.  AI's quest ultimately leads to ASI.  ML is an AI subfield.  Machine learning (ML) is the scientific study of algorithms that computer systems that learn through experience.  ML algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.  ML can be divided into these categories: \nSupervised learning algorithms map input to output values based on labelled examples of input-output pairs.  For example, we want to predict whether the image contains a cat, then the algorithm is shown a lot of labelled images with cats and without cats.  Afterwards, the ML algorithm learns to recognize cats in unseen images.  Supervised learning needs considerable amounts of labelled data, which is often done by humans.    Unsupervised learning algorithms help finding previously unknown patterns in datasets without preexisting labels.  The objective is to discover the underlying data structure, for example, by grouping similar items to form \"clusters.\"  Unsupervised learning does not require labelled data, but instead tries to learn by itself.  \nSemi-Supervised learning algorithms can be considered a category between supervised and unsupervised learning, where the data contains both labelled and unlabelled data.  \nReinforcement learning (RL) explores how agents take actions in an environment in order to maximize a reward.  An example is when the RL agent plays Go against itself, learns the game, and acquires above human intelligence in Go.  In 1950, Alan Turing published the milestone paper \"Computing machinery and intelligence\" (Turing 1950), considering the fundamental question \"Can machines think?\"  Turing proposed an imitation game, known as the Turing test afterwards, where if a machine could carry on a conversation indistinguishable from a conversation with a human being, then it is reasonable to say that the machine is intelligent.  The Turing test was the first experiment proposed to measure machine intelligence.  The first \"AI period\" began with the Dartmouth conference in 1956, where AI got its name and mission.  McCarthy coined the term \"artificial intelligence,\" which became the name of the scientific field.  The primary conference assertion was, \"Every aspect of any other feature of learning or intelligence should be accurately described so that the machine can simulate it\" (Russell and Norvig 2016).  Among the conference attendees were Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Herbert A. Simon, and Allen Newell, all of whom became key figures in the Ai field People were excited because for the first-time computers were solving problems like humans and seemed intelligent.  The wider AI research community shared an initial optimism making bold claims and boosting popularity.  Examples, where AI solved problems included algebraic application problems, language translation, geometric theorem proving, etc.  A list of several important AI breakthroughs of that period are:\nCheckers was the first program to demonstrate that computers can learn and not just perform what they are programmed to do.  Checkers attracted media attention and learned to play at a level high enough to challenge a decent amateur human player (Samuel 1960).  The Logic Theorist had proven 38 theorems from Principia Mathematica and introduced critical concepts in artificial intelligence, like heuristics, list processing, 'reasoning as search,' etc.  (Newell et al. 1962).  Inspired by the human brain, Rosenblatt discovered the perceptron.  The perceptron was predicted to be \"the embryo of an electronic computer that will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"  The perceptron was the birth of connectionism, the foundation of Neural Networks (NN) and Deep Learning (Rosenblatt 1961).  Machine Educable Nougats And Crosses Engine (MENACE) was one of the first programs capable of learning to play a perfect game of Tic-Tac-Toe (Michie 1963).  ELIZA was a natural language processing system that imitated a doctor.  ELIZA responded to questions like a psychotherapist.  Some users believed they are interacting with another human being until it reached its limitations, and the conversation became nonsense (Weizenbaum 1966).  Shakey the Robot was the first general-purpose mobile robot capable of reasoning its actions.  This project integrated research in robotics with computer vision and natural language processing, thus being the first project that combined logical reasoning and physical action (Bertram 1972).  \"It is not my aim to surprise or shock you, but the simplest way I can summarize is to say that there are now in the world machines that think, that learn, and that create.  Moreover, their ability to do these things is going to increase rapidly until – in a visible future – the range of problems they can handle will be coextensive with the range to which the mind has been applied.\"  The first AI winter started in the 1970s, due to unfulfilled promises, vast expectations, and financial difficulties.  At the same time, AI encountered impossible-to-overcome technological barriers, mainly on the limitations of computing power, memory, and processing speed.  In the mid-1960s, researchers discovered a distinction between polynomial and exponential growth in problem complexity.  The exponential growth in complexity prohibits solving moderately large instance problems at any reasonable time.  This led to the most important concept of complexity, NP-completeness, and its most fundamental question, whether P = NP, where \"P\" is a general class of questions for which some algorithm can provide an answer in polynomial time.  Many combinatorial and logical problems are NP-complete requiring exponential processing time and unfeasible-tobe-effective systems.  The first AI Winter has been marked by a dramatic decrease in the AI activities in both industry and academia.  The AI shortcomings were explained in two reports: a) the Automatic Language Processing Advisory Committee (ALPAC) report by the US Government (ALPAC 1966) and b) the Lighthill report (Lighthill 1973) by the British government.  These predictions established massive funding in machine translation by the US agencies since 1956.  Progress was slow, and the ALPAC committee was assigned to report on the reasons.  The ALPAC report focused on the economic return of machine translation without considering its scientific value.  The report concluded that machine translation did not provide adequate improvements in quality, speed, and cost; on the contrary, machine translation was of poor quality.  It noted that enough translators were available (4,000 were currently under contract, but only 300 of them on average worked each month).  Government officials were disappointed, emphasizing the little benefits gained from the massive investments.  The report was reflecting on AI disappointment in both the public and the scientific community.  The report highlighted that the conventional engineering approach with radio waves was performing better than AI methods of automatic landing systems for airplanes.  AI experiments worked in the labs and small domains, but were inadequate in large-scale real-world problems, often because of the \"Combinatorial Explosion.\"  In the 1980s, the AI paradigm shifted to symbolic AI and the so-called \"expert systems\" or \"knowledge-based systems.\"  The underlying idea was to get human expert knowledge in a computer form and spread it as a program to many Personal Computers (PCs).  Expert systems had two components:  1. the knowledge base – a collection of facts, rules, and relationships on a specific domain; and  2. the inference engine that described how to manipulate and combine these symbols.  The facts and rules had explicit representation and were modifiable.  Lisp and Prolog were the main symbolic programming languages.  Companies started producing expert systems in the 1990s.  These companies offered software packages called \"inference engines\" and related knowledge services to customers.  However, these tools and frameworks lacked sufficient expressive power to capture the breadth of the expert knowledge and behaviour required to achieve satisfactory performance.  Several important AI breakthroughs of that period are:\nMYCIN was an expert system specialized in the diagnosis of blood diseases and prescription of drugs.  It adopted a calculus of uncertainty that seemed to fit well with the doctors' assessment on the diagnosis (Shortliffe et al. 1975).  SHRDLU was a natural language computer program that allowed the user to carry on a conversation with the computer, name collections, and query the state of a simplified \"blocks world\" (Winograd 1971).  \"Hopfield net\" was a form of neural network that learned and processed information in a new way (Hopfield 1982).  The \"Hopfield net\" and \"backpropagation\" (Rumelhart et al. 1985) revived the AI field of connectionism.  ID3 is an algorithm that generates a decision tree from a dataset.  ID3 is the precursor to the C4.5 algorithm used in machine learning and natural language processing (Quinlan 1986).  \"Suppose my projections are correct, and the hardware requirements for human equivalence are available in 10 years for about the current price of a medium-large computer.  Suppose further that software development keeps pace (and it should be increasingly easy, because big computers are great programming aids), and machines able to think as well as humans begin to appear in 10 years.\"  \"I'm still a realist: If we work really hard - and smart - we can have something like a HAL in between four and four hundred years.  I suppose if we're lucky, then, we can make it by 2001!\"  The expert system called XCON saved <CUR>40 million a year to the Digital Equipment Corporation in <PHONE>.  The leading hardware companies were Symbolics, Lisp Machines, and software companies IntelliCorp and Aion.  By 1985, the US investments in AI expert systems reached over <CUR>1 billion.  The UK started the <CUR>350 million Alvey project.  In 1981, the Japanese Ministry of Economy, Trade, and Industry allocated <CUR>850 million to support the fifthgeneration computer project.  The goal was to create machines that could talk to people, translate languages, interpret images, and reason like humans.  The European Strategic Program on Research in Information Technology (ESPRIT) was a series of integrated programmes of information technology research and development projects and industrial technology transfer measures.  It was a European Union initiative managed by the Directorate General for Industry (DG III) of the European Commission.  ESPRIT had a substantial budget comparable to the USA and Japan.  After the 1990s, the term \"expert system\" dropped from the IT lexicon, and it was referred to as the second AI winter.  One of the main problems in expert systems was knowledge acquisition.  Knowledge acquisition captures expert knowledge and represents it in a symbolic language.  Obtaining domain expert time and expertise was difficult since they are in constant need by their organizations.  Therefore, expert system research focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by the experts.  The symbolic programming languages were Lisp and Prolog, and the hardware platforms were the Lisp machines and PCs.  The expert system development environment could not match the compiled language efficiency (such as C).  The PC performance of Apple and IBM continued to increase, and by 1987, they had surpassed the expensive specialized Lisp machines produced by Symbolics and other manufacturers, collapsing a <CUR>500 million industry.  However, while funding dried up, and researchers avoided the term AI, many of them continued working under other discipline-specific names: cognitive systems, intelligent systems, knowledge representation, and reasoning.  The Business Rules Management System (BRMS) is a legacy of the symbolic era.  BRMSs are still used across many industries.  In the 1990s–2010s, AI had addressed complex problems, providing solutions that were found to be useful in different application domains including data mining, industrial robotics, logistics, business intelligence, banking software, medical diagnosis, recommendation systems, and search engines.  AI researchers began to develop and use more sophisticated mathematical tools.  There was a widespread realization that many AI problems have already been worked on by researchers in fields like mathematics, economics, or operations research.  The shared mathematical language allowed a higher level of collaboration with established fields and made AI a more rigorous scientific discipline.  Many AI researchers in the 1990s deliberately called their work by other names, such as informatics, knowledgebased systems, cognitive systems, optimization algorithms, or computational intelligence.  The new names helped to procure funding.  The failed promises of the AI Winter continued to haunt AI research in the commercial world.  The annual competition ImageNet Large Scale Visual Recognition Challenge (ILSVRC) saw dramatic progress in the last decade.  The average ILSVRC classification error rate was around 25% in 2011.  In 2012, a deep convolutional neural net called AlexNet achieved a 16% classification error rate (Krizhevsky et al. 2012), and in the next couple of years, error rates fell to a few percent.  These breakthroughs made the AI paradigm shift to deep learning (DL).  DL is a subfield of ML and AI, as shown in Figure 3.  DL introduced a multi-layer neural network architecture that learns data representations with levels of abstraction (LeCun et al. 2015).  DL neural network architectures include deep neural networks, deep belief networks, recurrent neural networks, and convolutional neural networks (CNN).  The terms AI or ML often replace DL, especially in the news and media.  The classical programming paradigm consists of defining explicit instructions in different programming languages such as Java or C. The developer specifies the rules, which, combined with the data, produce the answers.  In the ML paradigm, inputs include data and answers, and ML produces rules from inputs, as shown in Figure 4.  The ML system is trained, rather than explicitly programmed.  ML programming was an established paradigm, but got a new meaning when rules are saved as weights in the neural network.  Typical DL networks can have millions or billions of weight parameters.  For example, let us explain how DL and other classical algorithms would solve the problem of recognizing cats in an image.  The classical algorithms would process the image using different methods to identify the two eyes, nose, legs, etc. and many other parts of a cat body.  These methods required complex and long programs compared to DL, where the process of image recognition is almost completely automatic after selecting the DL architectural model (which, in this case, would probably be convolutional neural networks).  DL automatically learns to recognize cats after observing many examples of images with and without cats.  Transfer learning is a technique whereby a neural network model is first trained on a large general problem, e.g., recognizing objects in an image.  These pre-trained neural network models are then fine-tuned on our problem-specific image dataset.  For example, pre-trained image recognition models such as VGG, InceptionV3 and ResNet5 can be fine-tuned for many image recognition problems, such as cat recognition.  Transfer learning significantly speeds up the DL training stage, saves computational resources, and produces quality models.  MOOC courses offered by Coursera2 since 2012 have had more than a million enrolments.  These factors contributed to the broader and faster AI proliferation and adoption.  Reading handwritten digits using convolutional neural networks.  The system processed about 10-20% of handwritten cashed checks and zip codes in the United States between the late 90s and early 2000s (LeCun et al. 1989).  \"Learning from Delayed Rewards\" introduced the concept of Q-learning, which greatly improves the practicality and feasibility of reinforcement learning.  The Q-learning algorithm learned optimal control without modelling the transition probabilities or expected rewards of the Markov Decision Process (Watkins 1989).  German computer scientist Schmidhuber solved a \"very deep learning\" task in 1993 that required over 1,000 layers in the recurrent neural network (Schmidhuber 1993).  \"No Hands Across America3.\"  A semi-autonomous car drove 4,501 km coast-to-coast across the United States with computer-controlled steering.  A human driver-controlled throttle and brakes.  IBM Deep Blue won the game of chess against Garry Kasparov, the best human player in the world.  Deep Blue did not use DL or any of the latest AI techniques.  The system had \"learned\" all possible played games of chess and could evaluate the present and suggest the next move.  Long short-term memory (LSTM) architecture improved both the efficiency and practicality of RNN by eliminating the long-term dependency problem (Hochreiter and Schmidhuber 1997).  Gradient-Based Learning was improved by combining the stochastic gradient descent algorithm with the backpropagation algorithm (LeCun et al. 1998).  TD-Gammon achieved performance as the best human players in the game of Backgammon.  TD-Gammon was a major achievement of combining neural nets and Reinforcement Learning (RL) with the self-play method (Tesauro 2002).  The Stanford robot won the DARPA Grand Challenge.  The Stanford robot drove autonomously for 131 miles along an unrehearsed desert track (Thrun et al. 2006).  IBM Watson won a game of Jeopardy against Ken Jennings and Brad Rutter.  Ken Jennings and Brad Rutter were among the most successful contestants on the Jeopardy show.  Watson is a question-answering computing system with advanced natural language processing, information retrieval, knowledge representation, automated reasoning, and question answering.  It was a truly remarkable AI system combining many state-of-the-art components in speech recognition, voice synthesis, and information retrieval, among others (Ferrucci 2012).  The Cat experiment has learned to identify and recognize cats from 10,000,000 unlabelled images randomly taken from YouTube.  The program performed nearly 70% better than previous attempts of unsupervised learning (Le 2013).  Generative Adversarial Networks (GANs) are deep neural net architectures composed of two nets, pitting one against the other (thus the \"adversarial\").  GANs can learn to mimic any distribution of data and can generate content like any domain: images, music, speech, etc.  (Goodfellow et al. 2014).  DeepRL has mastered a diverse range of Atari 2600 games to superhuman level with only the raw pixels and scores as inputs.  Atari games became standard benchmarks for AI algorithms.  AI has excelled human players in most of them (Mnih et al. 2015).  AlphaGo defeated Lee Sedol, the world's number one Go player (Silver et al. 2016).  Because of its complexity, the game of Go was considered out of AI reach for at least another decade.  AlphaGo was upgraded a year later into a generalized and more powerful algorithm AlphaZero (Silver et al. 2017).  AlphaZero learned how to play master-level chess in just four hours and defeated Stockfish (the top AI chess player) in a 100-game match — without losing a single game.  The Transformer is a novel DL architecture based on a self-attention mechanism.  Transformers are core algorithms in language modelling, machine translation, and question answering (Vaswani et al. 2017).  Libratus has decisively beaten four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL).  Over nearly three weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent.  Poker is an imperfect information game (Brown and Sandholm, 2018).  OpenAI Five has defeated an amateur human team at Dota 2, exceeding human intelligence in a complex video game.  Relative to previous successes like Chess or Go, complex video games capture the messiness and continuous nature of the real world (Pachocki et a. 2018).  AlphaStar has defeated a top professional player in StarCraft II.  AlphaStar decisively beat Team Liquid's Grzegorz \"MaNa\" Komincz, one of the world's strongest professional StarCraft players with 5-0.  The matches took place under professional match conditions on a competitive ladder map and without any game restrictions (Vinyals et al. 2019).  OpenAI has trained Dactyl4, a human-like robot hand to manipulate physical objects with unprecedented dexterity.  Dactyl is trained entirely in simulation and transfers its knowledge to reality, adapting to real-world physics.  Dactyl learns from scratch using the same generalpurpose RL algorithm.  GPT-25 is a large-scale unsupervised language model that generates coherent paragraphs of text, achieves state-of-the-art performance in many language modelling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.  AI methods have been exploited in various domains, including computer vision, speech recognition, natural language understanding, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection, games, etc.  The AI field is expanding in many domains, making it difficult and complicated to track its evolution.  Papers with code6 is tracking the AI progress providing a free and open repository of papers, code, and evaluation tables.  The AI Collaboratory7 aims to develop a collaborative initiative for the analysis, evaluation, comparison, and classification of AI algorithms.  AI Collaboratory has become part of AI WATCH, European Commission's service for AI technology monitoring, and analysis.  We provide here some figures from the AI Collaboratory.  We can observe the DL advancement in image recognition on the ImageNet dataset as illustrated in the top image in Figure 5.  The y-axis shows model accuracy.  Another historically famous dataset is CIFAR-100, with 80 million of tiny images in 100 classes containing 600 images (Figure 5, bottom image), showing a similar rate of progress in AI.  Figure 5: State-of-the-art table for Image Classification on ImageNet (top) and CIFAR 100 (bottom).  Coloured dashed lines model the present and future (potential) trends using different Machine translation performance is advancing with the continuous improvement of the Bilingual Evaluation Understudy Score (BLEU).  BLEU is a metric for evaluating a generated sentence compared to a reference sentence.  The SOTA table for Machine Translation on WMT2014 for English to French (top) and English to German are shown in Figure 6.  The DL language models made major advancements in 2018 with the introduction of transformers implemented in BERT and GPT-2 (Radford et al. 2019).  AI has transcended human intelligence in the games of Checkers 1952, Chess 1996, Jeopardy 2011, Go 2016, No-limit Poker 2017, Dota-2 2018.  Checkers, Chess and Go are perfect information games as each player can observe all the pieces on the board.  Card games where each player's cards are hidden from other players such as No-limit poker are examples of games with imperfect information.  Dota 2 and StarCraft are complex and dynamic environments of multiplayer games.  AI has succeeded to win human opponents in all of them.  With more board configurations than atoms in the observable universe, the ancient Chinese game of Go has long been considered a grand challenge for AI.  On March 9, 2016, in South Korea, the top world player Lee Sedol collided with AlphaGo AI for an extraordinary best-of-five-game competition.  Hundreds of millions of people watched as a legendary Go master took on an unproven AI challenger for the first time in history.  The five games were astonishing, especially the two exceptional moves 1) 37 in game two by AlphaGo and 2) 78 in game four by Lee Sedol.  Both moves at first seemed unreasonable but were decisive to win the game.  In the end, AlphaGo won the series 4:1.  AlphaGo learned by self-play, instead of studying the games based on human players.  Some consider this competition as a turning point in China's AI strategy8.  In 2018, Yoshua Bengio, Geoffrey Hinton, and Yann LeCun received the Turing Award for conceptual and engineering breakthroughs that have made DL a critical component of computing.  They developed theoretical foundations for the field, identified surprising phenomena through experiments, and contributed engineering advances that demonstrated the practical advantages of deep neural networks.  The statements below from scientists, researchers, investors, politicians, and historians, exhibit a wide variety of opinions and ideas, as well as predicted risks and gains of AI in the future.  \"The development of full artificial intelligence could spell the end of the human race… It would take off on its own, and re-design itself at an ever-increasing rate.  Humans, who are limited by slow biological evolution, couldn't compete, and would be superseded.\"  \"I'm increasingly inclined to think that there should be some regulatory oversight, maybe at the national and international level, just to make sure that we don't do something very foolish.  I mean with artificial intelligence we're summoning the demon.\"  \"Artificial intelligence will reach human levels by around 2029.  Follow that out further to, say, 2045, we will have multiplied the intelligence, the human biological machine intelligence of our civilization a billion-fold.\"  \"Some people call this artificial intelligence, but the reality is this technology will enhance us.  So instead of artificial intelligence, I think we'll augment our intelligence.\"  \"We've been seeing specialized AI in every aspect of our lives, from medicine and transportation to how electricity is distributed, and it promises to create a vastly more productive and efficient economy …  But it also has some downsides that we're gonna have to figure out in terms of not eliminating jobs.  It could increase inequality.  It could suppress wages.\"  \"Artificial intelligence is the future, not only for Russian but for all of humankind.  It comes with colossal opportunities, but also threats that are difficult to predict.  Whoever becomes the leader in this sphere will become the ruler of the world.\"  \"Just as electricity transformed almost everything 100 years ago, today I actually have a hard time thinking of an industry that I don't think AI (Artificial Intelligence) will transform in the next several years.\"  \"I have always been convinced that the only way to get artificial intelligence to work is to do the computation in a way similar to the human brain.  That is the goal I have been pursuing.  We are making progress, though we still have lots to learn about how the brain actually works. \"  Many bold claims are about the emergence of AI superintelligence (ASI) with predictions of Elon Musk, Stephen Hawking, Yuval Noah Harari, Ray Kurzweil, and others.  The leading scientists, including Yann LeCun, Fei-Fei Li, and Andrew Ng consider that AI will trigger a fourth industrial revolution becoming part of every aspect of human lives and extension on our capabilities.  The CEOs of the leading IT companies, Larry Page of Alphabet, the parent company of Google, and Ginni Rometty of IBM, and many others are mainly focused on expanding their business and developing new products and services based on AI.  Companies avoid debate on AGI and ASI or long-term AI development.  Political leaders, including Barack Obama and Vladimir Putin, consider AI as a revolutionary technology that will bring vast opportunities and influence world politics.  Scientists and thinkers are debating, what else is needed to achieve AGI or ASI?  Will DL be superseded by a different AI paradigm?  Gary Markus as one of the DL sceptics is in favour of hybrid models that would incorporate not just supervised forms of DL, but other techniques as well, such as symbol-manipulation, and unsupervised learning9.  Judea Pearl, a pioneering figure in AI, expects causal reasoning to provide machines with human-level intelligence (Pearl & Mackenzie 2018).  Instead of the mere ability to correlate fever and malaria, machines need the capacity to reason that malaria causes fever.  Once the causal framework is in place, it becomes possible for machines to ask counterfactual questions — to inquire how the causal relationships would change given intervention — which Pearl views as the cornerstone of scientific thought.  Kahneman makes ground-breaking discoveries and explains the two cognitive systems named System 1 (Fast) and System 2 (Slow).  System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical.  Kahneman exposes the extraordinary capabilities—and the faults and biases—of fast thinking and reveals the pervasive influence of intuitive impressions on our thoughts and behaviour (Kahneman 2011).  Stuart J. Russell asserts the risk to humanity from advanced AI despite the uncertainty surrounding future progress in AI (Russell 2019).  Rodney Brooks demystifies the AI hype10 and makes relevant claims why predicting the AI future is difficult, especially with over and underestimating and cites the famous Amara law: \"We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.\"  Amara law presents a pattern found in many emerging technologies.  A big promise up front, disappointment, and then slowly growing confidence beyond where the original expectations were aimed.  Many technologies are overestimated in the short run, while gains are accumulated in the long run.  Amara law resembles the hype cycles11 introduced by Gartner.  The Hype Cycle is a graphical depiction of a common pattern that arises with each new technology or other innovation.  Although many of Hype Cycles focus on specific technologies or innovations, the same pattern of hype and disillusionment applies to higher-level concepts such as IT methodologies and management disciplines.  In many cases, the Hype Cycles also position higher-level trends and ideas, such as strategies, standards, management concepts, competencies and capabilities.  Older scientists who remember the last AI winter(s) first-hand, understand the risk of history repeating due to overpromises.  Many of them are actively working in demystifying AI without provoking additional hype and framing the progress in reasonable terms.  Private companies are primary investors in the current AI period.  Major internet companies invested substantial funds and hired the leading scientists, e.g. Geoffrey Hinton in Google, Yann LeCun in Facebook.  Microsoft invested <CUR>1 billion in OpenAI12.  Google acquired DeepMind for <CUR>500 million in 2014.  DeepMind received investments of <CUR>570 million in 2018, <CUR>341 million in 2017, and <CUR>154 million in 201613.  Other companies, e.g.,\nApple, Amazon, Uber, Tesla, etc. have followed similar investments and hiring strategies.  From 2013 to 2017, the amount of annual funding by Venture Capital firms increased by 350%, while all VC funding increased by 100% across all funding stages (Perrault et al. 2019).  China's ICT ecosystem is also strong, with prime leaders being Tencent, Baidu, and Alibaba, followed by Huawei.  Like their USA counterparts, most Chinese companies have announced AI strategic plans with substantial investment plans.  The Chinese government State Council plans for AI outline an ambitious three-stage process towards achieving China's goals on AI (Ding 2018): \nBy 2020, China plans to make the AI industry \"inline\" with the most advanced countries.  China's core AI gross industry output to exceed RMB 150 billion (EUR 20 billion) and AI output to exceed RMB 1 trillion (EUR 135 billion).  \nBy 2025, China aims to reach a \"world-leading\" level in some AI fields.  The AI industry gross output to exceed RMB 400 billion (EUR 54 billion) and AI output to exceed RMB 5 trillion (EUR 680 billion).  \nBy 2030, China seeks to become the world's \"primary\" AI innovation centre.  The AI industry gross output to exceed RMB 1 trillion (EUR 135 billion) and AI production to exceed RMB 10 trillion (EUR 1.35 trillion).  The European Commission (EC), in coordination with the EU Member States, has dedicated substantial AI investment funds.  Under the next multiannual financial framework, the EC has proposed to dedicate at least EUR 1 billion per year from Horizon Europe and the Digital Europe Programme to AI.  EU AI investment levels are low and fragmented, relative to other parts of the world, such as the US and China.  The goal is to increase investments progressively to EUR 20 billion per year in the next decade.  The Digital Europe program, with an overall budget of <CUR>9.2 billion, will support the digital transformation of Europe's society and economy (COM (2018) 795).  Some of the EU member states have allocated significant AI national budgets, e.g., France, with <CUR>1.5 billion14.  The European Commission plans to spend approximately <CUR>5.4 billion between 2018 and 2022, as shown in Table 3.  The Member States will match this amount with over <CUR>2.6 billion.  The key role of AI Watch is to monitor EU member states' AI strategies and investments and assess their impacts in terms of adoption and use of AI.  Note: To allow cross-country comparability, national currencies are converted into euro Purchasing Power Standard (PPS), a unit based on current euros, to account for the effect of differences in price levels across countries and of movements in exchange rates.  One of the key early reports of AI Watch has been to analyse the landscape of Ai players worldwide.  In this study, 34,000 AI players are identified worldwide in the period 2009-2018, with the US leading the landscape in number of players, mainly due to their strong industrial ecosystem (96% of US players are firms) (Samoili, Righi et al., 2020).  The EU is among the geographical areas with the highest number of players active in AI, together with the United States and China (Fig. 7).  The US is specialised in the areas of AI Services and in Robotics and Automation and has a leading or strong presence in all other AI thematic areas.  China is the first geographical area regarding number of players filing patent applications.  China's specialisation reflects its leadership in Computer vision, as well as in Machine learning and Connected and Automated vehicles.  The EU hosts a very strong research network and is second by number of research institutions active in AI worldwide and first by number of research institutions with AI publications.  Robotics and Automation, and AI Services are the EU's main areas of specialisation where it has a strong revealed comparative advantage.  Other countries important in the AI landscape are India, South Korea, Canada, Japan, Israel, Russia and Singapore.  The MIT Technology Review Insights performed a survey among senior executives covering 2,357 cases and a wide range of industries worldwide (MIT insights 2018).  The main conclusions are: The vast majority (over 85%) of respondents consider data as the primary resource for optimal business decisions, delivering better results to customers, and growing their business.  European Commission made a survey of 9640 enterprises in January – March 2020 and measured five KPIs: AI awareness, AI adoption, AI sourcing, external and internal obstacles to AI adoption (Kazakova et al. 2020).  The main conclusions are: \nAwareness of AI is high across the EU (78%).  Four in ten (42%) enterprises have adopted at least one AI technology, 25% have adopted at least two.  While 18% have plans to adopt AI in the next two years, 40% have neither adopted AI nor plan to do so.  Adoption at the level of each technology is still relatively low: from 3% for sentiment analysis to 13% for anomaly detection and process/equipment optimisation.  The most common sourcing strategy is external, as 59% of EU enterprises that use AI purchase software or ready-to-use systems.  \nThree key internal barriers to AI adoption are difficulties in hiring new staff with the right skills (57%), the cost of adoption (52%) and the cost of adapting operational processes (49%).  Reducing uncertainty can be beneficial, as enterprises find liability for potential damages (33%), data standardisation (33%) and regulatory obstacles (29%) to be major external challenges to AI adoption.  Deep learning has multiple issues that need to be addressed including adversarial attacks, generation of deepfake content, fairness, accountability, transparency and other ethical considerations.  Here we will just briefly mention several issues, their impact and possible solutions.  Adversarial machine learning is a technique that tries to fool models by supplying deceptive input.  This is probably the most common cause of a malfunction in an ML model.  Most ML techniques were designed to work on specific problem sets in which the training and test data are generated from the same statistical distribution.  In the real world, adversaries may supply data that violates that statistical assumption.  Attackers can arrange the data to exploit specific vulnerabilities and produce massive fail in ML model.  DL is vulnerable to adversarial examples that are malicious inputs modified to yield erroneous machine model outputs.  Figure 8 shows a famous adversarial example where adding a small perturbation to a panda image makes an image recognition system fail.  Adversarial attacks can be very dangerous e.g., attackers can target autonomous vehicles with stickers or paint that will create an adversarial stop sign (Papernot et al. 2016).  AI is reducing the cost of generating fake content and waging disinformation campaigns.  The public will need to become more sceptical about online content, including text, speech, images, and videos.  AI can: Deepfake videos and images can depict a person saying things or performing actions that never occurred.  Malicious actors with fake accounts have already begun to target the online community.  For instance, it is reported that Mona Lisa can speak with only one image (Zakharov et al. 2019), or that criminals have impersonated CEOs' voice and stole millions (Kaveh et al. 2019).  Over the last years, researchers and practitioners have expressed their concerns on the ethical implications of AI systems in real-world scenarios.  The European Commission appointed the High-Level Expert Group (HLEG) on AI in 2018 and one of their tasks has been to define ethical guidelines for trustworthy AI.  This section is taken from the \"Ethics Guidelines for Trustworthy AI\" (HLEG 2019).  For an AI system to be trustworthy, it should ensure the following three components through the system's entire life cycle: The four ethical principles which are rooted in fundamental rights that must be respected to ensure that AI systems are developed, deployed, and used in a trustworthy manner are:  Many of these are already reflected in existing legal requirements for which mandatory compliance is required and hence fall within the scope of lawful AI, which is the first component of Trustworthy AI.  As set out above, while many legal obligations reflect ethical principles, adherence to ethical principles goes beyond formal compliance with the existing laws.  They are specified as ethical imperatives, such that AI practitioners should always strive to adhere to.", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "Different groups of stakeholders have different roles to play in ensuring that the AI requirements are met: (a) Developers should implement and apply the requirements to the design and development processes; (b) Deployers should ensure that the systems they use, and the products and services they offer, meet the requirements; and (c) End-users and the broader society should be informed about these requirements and be able to request that they are upheld. While all requirements are of equal importance, context, and potential tensions between them will need to be considered when applying them across different domains and industries. Implementation of these requirements should occur throughout an AI system's entire life cycle and depends on the specific application. While most requirements apply to all AI systems, special attention is given to those directly or indirectly affecting individuals. Therefore, for some applications (for instance, in industrial settings), they may be of less relevance. The discussion of AI ethical principles is ongoing with the EU having a lead in the world. Following the EU, China has released its own AI principles named Beijing AI principles15 on AI research, development, use, governance, and long-term planning. OECD produced council recommendations on the AI16. Google AI principles17 focus on building socially beneficial AI, avoid creating or reinforcing unfair bias, safe, accountable to people, privacy by design, and uphold high standards of scientific excellence. Google commits not to pursue technology that will cause or likely cause harm, including AI weapons, surveillance that violates internationally accepted norms, and those that are contrary to widely accepted international laws and human rights. Asilomar AI principles18 in 2017 were proclaimed by leading researchers from academia and industry, and thought leaders in Economics, Law, Ethics, and Philosophy and were then embraced by an impressive list of 1273 AI/Robotics researchers and 2541 others.", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "The Asilomar AI principles: \nEthics and values consider safety, transparency, judicial transparency, responsibility, human values, personal privacy, shared benefits, human control, AI arms race, etc. \nAI systems should be designed in a way that respects the rule of law, human rights, democratic values and diversity, and they should include appropriate safeguards – for example, enabling human intervention where necessary – to ensure a fair and just society. \nThere should be transparency and responsible disclosure around AI systems to ensure that people understand AI-based outcomes and can challenge them. \nAI systems must function in a robust, secure and safe way throughout their life cycles and potential risks should be continually assessed and managed. \nOrganisations and individuals developing, deploying or operating AI systems should be held accountable for their proper functioning in line with the above principles. Ethical concerns have risen some related research, such as the one driven by the FAT-ML21 (Fairness, Accountability, and Transparency in Machine Learning) community and research advances are making it possible to develop evaluation frameworks for fairness, transparency, and accountability. Computers can perform trillions of calculations per second and execute AI tasks beyond the human abilities. The tasks where AI exceeds human performance include object recognition in billions of images in a few seconds, instantaneous translation of 100+ languages, identification of spam from billions of emails, playing chess, etc. Despite these impressive performances, even the most advanced AI lacks the capabilities of a fouryear-old child to move and talk, understand the world, causality, and discuss abstract concepts. Decades of AI advancements are still not a substitute for a billion years of evolution. Our study reflects on the similarities and differences between AI periods. The first two AI periods share a similar pattern, as they start with a scientific breakthrough, a research paradigm shift, followed by bold predictions, vast media attention, massive investments, disappointments, unfulfilled promises, and the start of an AI winter.  The similarities and differences between the three main AI periods are illustrated Table 3.  Government investments were dominant in the first two periods, while industry is nowadays investing greatly in AI developments.  Academia was leading basic research in the first two periods, while today some technological companies are also leading AI basic research.  These companies have been attracting leading AI scientists with cutting-edge computing resources, large-scale datasets and good working conditions, often not available in academia.  Governments which have recognized the potential are investing significant resources in AI.  European Union (EU) and its member states have announced an ambitious AI strategy and allocated substantial funds.  The first paradigm shift laid the foundation for most well-known AI methods and algorithms.  The second paradigm shift was related to symbolic algorithms and expert systems, also known as knowledge-based systems.  The increased availability of digital data and computing power shifted the last paradigm to machine learning and deep learning.  We might distinguish the second AI period from the third one with the former being model-driven and the latter being data-driven.  Clearly, AI also has negative aspects and its use can create new risks.  Several AI deficiencies, including data quality and representativeness, can lead to biased, non-transparent, and low-performance AI.  Malicious actors can use AI to make fake news, deep fakes, perform highly automated cyber-attacks, etc.  AI risks need to be addressed.  Many AI ethical committees are working on AI fairness, accountability, transparency, explainability and related issues, paving the ground for government regulation.  Many bold statements often accompany AI.  Some of the bold statements will possibly fail to deliver on their promises causing disappointments and a sense of disillusion to stakeholders, venture capital companies, employees, and the general public.  It is therefore important to have a balanced view of both opportunities and limitations of this emerging technology.  At the present time, what we can observe is a significant increase in AI publications, conferences, patents, investment, education, national strategies, etc.  Considering the current trends, it is safe to foresee in the short-term future that AI will continue to grow and spread across domains although other global emergent topics e.g. COVID-19 may impact AI expansion and change public priorities.  Complementary to AI ethical issues, data sovereignty and technological sovereignty are becoming important.  The European Commission made a strategy for policy measures and investments to enable the data economy for the coming five years23.  This data strategy is presented at the same time as the Commission's Communication on \"Shaping Europe's digital future24\" and a White Paper on artificial intelligence25 that indicates how the Commission will support and promote the development and uptake of artificial intelligence across the EU.  The future of AI is uncertain, with chances of another AI winter or of an even greater AI summer.  Given this uncertainty, it is particularly important to have in place an unbiased initiative such as AI Watch to monitor the evolution and assess its impacts over the coming years which one way or the other will see very significant changes in our digitally transformed society.  ALPAC, L. (1966).  Machines: Computers in Translation and Linguistics.  Report by the Automatic Language Processing Advisory Committee, Division of Behavioral Sciences, 1416.", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "Craglia M., (Ed.), Annoni A., Benczur P., Bertoldi P., Delipetrev B., De Prato G., Feijoo C., Fernandez Macias E., Gomez E., Iglesias M., Junklewitz H, López Cobo M., Martens B., Nascimento S., Nativi S., Polvora A., Sanche. I., Tolan S., Tuomi I., Vesnic Alujevic L., Artificial Intelligence - A European Perspective., EUR 29425 EN, Publications Office, Luxembourg, 2018, ISBN 978-92-79-97218-5, dos:10.<PHONE>, JRC113826 Dean, J., Patterson, D., & Young, C. (2018). A new golden age in computer architecture: Empowering the machinelearning revolution. IEEE Micro, 38(2), 21-29. Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (pp. 248-255). IEEE. Domingos P., (2015). \"The master algorithm: How the quest for the ultimate learning machine will remake our world\" Basic Books. EC. (2018). \"Coordinated Plan on Artificial Intelligence.\" Text. Digital Single Market - European Commission. December 7, 2018. (COM(2018) 795 final) EC. (2020). \"COMMUNICATION FROM THE COMMISSION TO THE EUROPEAN PARLIAMENT, THE COUNCIL, THE EUROPEAN ECONOMIC AND SOCIAL COMMITTEE AND THE COMMITTEE OF THE REGIONS: A European strategy for data\" European Commission, Brussels, 19.2.2020 COM(2020) 66 final EC. (2020). \"WHITE PAPER On Artificial Intelligence - A European approach to excellence and trust\" European Commission, Brussels, 19.2.2020 COM(2020) 65 final Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680). Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8), 2554-2558. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp.  <PHONE>).  LeCun, Y. (2019).  1.1 Deep Learning Hardware: Past, Present, and Future.  In 2019 IEEE International Solid-State Circuits Conference-(ISSCC) (pp. 12-19).  IEEE.  LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989).  Backpropagation applied to handwritten zip code recognition.  Neural computation, 1(4), 541-551.  LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998).  Gradient-based learning applied to document recognition.  Proceedings of the IEEE, 86(11), 2278-2324.  Le, Q. V. (2013).  Building high-level features using large scale unsupervised learning.  In 2013 IEEE international conference on acoustics, speech and signal processing (pp. 8595-8598).  IEEE.  Michie, D. (1963).  Experiments on the mechanization of game-learning Part I. Characterization of the model and its parameters.  The Computer Journal, 6(3), 232-236.  Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015).  Human-level control through deep reinforcement learning.  Nature, 518(7540), 529-533.  Pachocki J., Brockman G., Raiman J., Zhang S., Pondé H., Tang J., Wolski.  F., \"OpenAI Five, 2018.\" URL openai.com/openai-five.  Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).  Language models are unsupervised multitask learners.  OpenAI Blog, 1(8), 9.  Rosenblatt, F. (1961).  Principles of neurodynamics.  perceptrons and the theory of brain mechanisms (No. VG1196-G-8).  Cornell Aeronautical Lab Inc Buffalo NY.  Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1985).  Learning internal representations by error propagation (No. ICS-8506).  California Univ San Diego La Jolla Inst for Cognitive Science.  Schmidhuber, J. (1993).  Habilitation thesis: System modeling and optimization.  ff demonstrates credit assignment across the equivalent of 1,200 layers in an unfolded RNN.  Shortliffe, E. H., Davis, R., Axline, S. G., Buchanan, B. G., Green, C. C., & Cohen, S. N. (1975).  Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system.  Computers and biomedical research, 8(4), 303-320.  Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., ... & Dieleman, S. (2016).  Mastering the game of Go with deep neural networks and tree search.  Nature, 529(7587), 484.  Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... & Chen, Y. (2017).  Mastering the game of go without human knowledge.  Nature, 550(7676), 354-359.  Nativi, S., Kotsev A., Petra, S.,, Pogorzelska, K., Vakalis, I., Dalla Benetta, A., Perego., A. \"IoT 2.0 and the internet of transformation (web of things and digital twins)\", JRC press, in print.  Thrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D., Aron, A., Diebel, J., ... & Lau, K. (2006).  Stanley: The robot that won the DARPA Grand Challenge.  Journal of field Robotics, 23(9), 661-692.  Turing, A. M. (1950).  Computing machinery and intelligence.  The Essential Turing: The Ideas that Gave Birth to the Computer Age.  Ed.  B. Jack Copeland.  Oxford: Oxford UP, 433-64.  Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017).  Attention is all you need.  In Advances in neural information processing systems (pp.  5998-6008).  Vinyals O., Babuschkin I., Chung J., Mathieu M., Jaderberg M., Czarnecki W., Dudzik A., (2019).  AlphaStar:  Mastering the Real-Time Strategy Game StarCraft II. strategy-game-starcraft-ii/ Weizenbaum, J. (1966).  ELIZA—a computer program for the study of natural language communication between man and machine.  Communications of the ACM, 9(1), 36-45.  Winograd, T. (1971).  Procedures as a representation for data in a computer program for understanding natural language (No. MAC-TR-84).  MASSACHUSETTS INST OF TECH CAMBRIDGE PROJECT MAC.  Zakharov, E., Shysheya, A., Burkov, E., & Lempitsky, V. (2019).  Few-shot adversarial learning of realistic neural talking head models.  In Proceedings of the IEEE International Conference on Computer Vision (pp. 9459-9468).  Figure 5: State-of-the-art table for Image Classification on ImageNet (top) and CIFAR 100 (bottom).  Coloured dashed lines model the present and future (potential) trends using different linear and polynomial smoothing methods ................................................................................................................15 Figure 6: State-of-the-art table for Machine Translation on WMT2014 for English to French (top) and English to German (bottom).  Coloured dashed lines model the present and future (potential) trends using different linear and polynomial smoothing methods .................................................................................16 GETTING IN TOUCH WITH THE EU  In person All over the European Union there are hundreds of Europe Direct information centres.  You can find the address of the centre nearest you at: On the phone or by email Europe Direct is a service that answers your questions about the European Union.  You can contact this service: - by freephone: 00", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "800 6 7 8 9 10 11 (certain operators may charge for these calls), - at the following standard number: +32 22999696, or - by electronic mail via: FINDING INFORMATION ABOUT THE EU Online Information about the European Union in all the official languages of the EU is available on the Europa website at:\nEU publications You can download or order free and priced EU publications from EU Bookshop at: Multiple copies of free publications may be obtained by contacting Europe Direct or your local information centre (see", "metadata": {"source_file": "jrc120469_historical_evolution_of_ai-v1.1.pdf", "title": "AI Watch Historical Evolution of Artificial Intelligence Analysis of the three main paradigm shifts in AI", "authors": ["Micaela Ballario"], "year": "2020", "detected_language": "en", "page_count": 36, "origin_chunk_file": "jrc120469_historical_evolution_of_ai-v1.1.chunks.json"}}
{"text": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. Pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component in many neural language understanding models. However, learning high quality representations can be challenging. They should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). In this paper, we introduce a new type of deep contextualized word representation that directly addresses both challenges, can be easily integrated into existing models, and significantly improves the state of the art in every considered case across a range of challenging language understanding problems. Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus. For this reason, we call them ELMo (Embeddings from Language Models) representations. Unlike previous approaches for learning contextualized word vectors (Peters et al., 2017; McCann et al., 2017), ELMo representations are deep, in the sense that they are a function of all of the internal layers of the biLM.  More specifically, we learn a linear combination of the vectors stacked above each input word for each end task, which markedly improves performance over just using the top LSTM layer.  Combining the internal states in this manner allows for very rich word representations.  Using intrinsic evaluations, we show that the higher-level LSTM states capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lowerlevel states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).  Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of semi-supervision that are most useful for each end task.  Extensive experiments demonstrate that ELMo representations work extremely well in practice.  We first show that they can be easily added to existing models for six diverse and challenging language understanding problems, including textual entailment, question answering and sentiment analysis.  The addition of ELMo representations alone significantly improves the state of the art in every case, including up to 20% relative error reductions.  For tasks where direct comparisons are possible, ELMo outperforms CoVe (McCann et al., 2017), which computes contextualized representations using a neural machine translation encoder.  Finally, an analysis of both ELMo and CoVe reveals that deep representations outperform those derived from just the top layer of an LSTM.", "metadata": {"source_file": "N18-1202.pdf", "title": "Deep Contextualized Word Representations", "authors": ["Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "year": "2018", "detected_language": "en", "page_count": 11, "origin_chunk_file": "N18-1202.chunks.json"}}
{"text": "Our trained models and code are publicly available, and we expect that ELMo will provide similar gains for many other NLP problems.1 Due to their ability to capture syntactic and semantic information of words from large scale unlabeled text, pretrained word vectors (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) are a standard component of most state-ofthe-art NLP architectures, including for question answering (Liu et al., 2017), textual entailment (Chen et al., 2017) and semantic role labeling (He et al., 2017). However, these approaches for learning word vectors only allow a single contextindependent representation for each word. Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information (e.g., Wieting et al., 2016; Bojanowski et al., 2017) or learning separate vectors for each word sense (e.g., Neelakantan et al., 2014). Our approach also benefits from subword units through the use of character convolutions, and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes. Other recent work has also focused on learning context-dependent representations. context2vec (Melamud et al., 2016) uses a bidirectional Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word. Other approaches for learning contextual embeddings include the pivot word itself in the representation and are computed with the encoder of either a supervised neural machine translation (MT) system (CoVe; McCann et al., 2017) or an unsupervised language model (Peters et al., 2017). Both of these approaches benefit from large datasets, although the MT approach is limited by the size of parallel corpora. In this paper, we take full advantage of access to plentiful monolingual data, and train our biLM on a corpus with approximately 30 million sentences (Chelba et al., 2014). We also generalize these approaches to deep contextual representations, which we show work well across a broad range of diverse NLP tasks.  Previous work has also shown that different layers of deep biRNNs encode different types of information.  For example, introducing multi-task syntactic supervision (e.g., part-of-speech tags) at the lower levels of a deep LSTM can improve overall performance of higher level tasks such as dependency parsing (Hashimoto et al., 2017) or CCG super tagging (Søgaard and Goldberg, 2016).  In an RNN-based encoder-decoder machine translation system, Belinkov et al. (2017) showed that the representations learned at the first layer in a 2layer LSTM encoder are better at predicting POS tags then second layer.  Finally, the top layer of an LSTM for encoding word context (Melamud et al., 2016) has been shown to learn representations of word sense.  We show that similar signals are also induced by the modified language model objective of our ELMo representations, and it can be very beneficial to learn models for downstream tasks that mix these different types of semi-supervision.  Dai and Le (2015) and Ramachandran et al. (2017) pretrain encoder-decoder pairs using language models and sequence autoencoders and then fine tune with task specific supervision.  In contrast, after pretraining the biLM with unlabeled data, we fix the weights and add additional taskspecific model capacity, allowing us to leverage large, rich and universal biLM representations for cases where downstream training data size dictates a smaller supervised model.  Unlike most widely used word embeddings (Pennington et al., 2014), ELMo word representations are functions of the entire input sentence, as described in this section.  They are computed on top of two-layer biLMs with character convolutions (Sec. 3.1), as a linear function of the internal network states (Sec. 3.2).  This setup allows us to do semi-supervised learning, where the biLM is pretrained at a large scale (Sec. 3.4) and easily incorporated into a wide range of existing neural NLP architectures (Sec. 3.3).  Given a sequence of N tokens, (t1, t2, ..., tN), a forward language model computes the probability of the sequence by modeling the probability of toRecent state-of-the-art neural language models (J´ozefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017) compute a context-independent token representation xLM  k\n(via token embeddings or a CNN over characters) then pass it through L layers of forward LSTMs.  At each position k, each LSTM layer outputs a context-dependent representation −!  h LM  A backward LM is similar to a forward LM, except it runs over the sequence in reverse, predicting the previous token given the future context: It can be implemented in an analogous way to a forward LM, with each backward LSTM layer j in a L layer deep model producing representations\n− h LM k,j of tk given (tk+1, . . .  , tN).  A biLM combines both a forward and backward LM.  Our formulation jointly maximizes the log likelihood of the forward and backward directions: We tie the parameters for both the token representation (⇥x) and Softmax layer (⇥s) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction.  Overall, this formulation is similar to the approach of Peters et al. (2017), with the exception that we share some weights between directions instead of using completely independent parameters.  In the next section, we depart from previous work by introducing a new approach for learning word representations that are a linear combination of the biLM layers.  k,j ], for each biLSTM layer.  For inclusion in a downstream model, ELMo collapses all layers in R into a single vector, ELMok = E(Rk; ⇥e).  In the simplest case, ELMo just selects the top layer, E(Rk)  = hLM k,L , as in TagLM (Peters et al., 2017) and CoVe (McCann et al., 2017).  More generally, we compute a task specific weighting of all biLM layers: (1) In (1), stask are softmax-normalized weights and the scalar parameter γtask allows the task model to scale the entire ELMo vector.  γ is of practical importance to aid the optimization process (see supplemental material for details).  Considering that the activations of each biLM layer have a different distribution, in some cases it also helped to apply layer normalization (Ba et al., 2016) to each biLM layer before weighting.  Given a pre-trained biLM and a supervised architecture for a target NLP task, it is a simple process to use the biLM to improve the task model.  We simply run the biLM and record all of the layer representations for each word.  Then, we let the end task model learn a linear combination of these representations, as described below.  First consider the lowest layers of the supervised model without the biLM.  Most supervised NLP models share a common architecture at the lowest layers, allowing us to add ELMo in a consistent, unified manner.  Given a sequence of tokens (t1, . . . , tN), it is standard to form a context-independent token representation xk for each token position using pre-trained word embeddings and optionally character-based representations.  Then, the model forms a context-sensitive representation hk, typically using either bidirectional RNNs, CNNs, or feed forward networks.  k\n] into the task RNN.  For some tasks (e.g., SNLI, SQuAD), we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing hk with [hk; ELMotask k\n].  As the remainder of the supervised model remains unchanged, these additions can happen within the context of more complex neural models.  For example, see the SNLI experiments in Sec. 4 where a bi-attention layer follows the biLSTMs, or the coreference resolution experiments where a clustering model is layered on top of the biLSTMs.  Finally, we found it beneficial to add a moderate amount of dropout to ELMo (Srivastava et al., 2014) and in some cases to regularize the ELMo weights by adding λkwk2 2 to the loss.  This imposes an inductive bias on the ELMo weights to stay close to an average of all biLM layers.  The pre-trained biLMs in this paper are similar to the architectures in J´ozefowicz et al. (2016) and Kim et al. (2015), but modified to support joint training of both directions and add a residual connection between LSTM layers.  We focus on large scale biLMs in this work, as Peters et al.  (2017) highlighted the importance of using biLMs over forward-only LMs and large scale training.  To balance overall language model perplexity with model size and computational requirements for downstream tasks while maintaining a purely character-based input representation, we halved all embedding and hidden dimensions from the single best model CNN-BIG-LSTM in J´ozefowicz et al. (2016).  The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer.  The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.  As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely character input.  In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary.  After training for 10 epochs on the 1B Word Benchmark (Chelba et al., 2014), the average forward and backward perplexities is 39.7, compared to 30.0 for the forward CNN-BIG-LSTM.  Generally, we found the forward and backward perplexities to be approximately equal, with the backward value slightly lower.  Once pretrained, the biLM can compute representations for any task.  In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance.  This can be seen as a type of domain transfer for the biLM.  As a result, in most cases we used a fine-tuned biLM in the downstream task.  See supplemental material for details.  Table 1 shows the performance of ELMo across a diverse set of six benchmark NLP tasks.  In every task considered, simply adding ELMo establishes a new state-of-the-art result, with relative error reductions ranging from 6 - 20% over strong base models.  This is a very general result across a diverse set model architectures and language understanding tasks.  In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details.  RELATIVE) SQuAD Liu et al. (2017) 84.4 81.1 85.8 4.7 / 24.9% SNLI Chen et al. (2017) 88.6 88.0 88.7 ± 0.17 0.7 / 5.8% SRL  He et al. (2017) 81.7 81.4 84.6 3.2 / 17.2% Coref Lee et al. (2017) 67.2 67.2 70.4 3.2 / 9.8% NER Peters et al. (2017) 91.93 ± 0.19 90.15 92.22 ± 0.10 2.06 / 21% SST-5 McCann et al. (2017) 53.7 51.4 54.7 ± 0.5 3.3 / 6.8% Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across six benchmark NLP tasks.  The performance metric varies across tasks – accuracy for SNLI and SST-5; F1 for SQuAD, SRL and NER; average F1 for Coref.  Due to the small test sizes for NER and SST-5, we report the mean and standard deviation across five runs with different random seeds.  The \"increase\" column lists both the absolute and relative improvements over our baseline.  Textual entailment Textual entailment is the task of determining whether a \"hypothesis\" is true, given a \"premise\".  The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs.  Our baseline, the ESIM sequence model from Chen et al. (2017), uses a biLSTM to encode the premise and hypothesis, followed by a matrix attention layer, a local inference layer, another biLSTM inference composition layer, and finally a pooling operation before the output layer.  Overall, adding ELMo to the ESIM model improves accuracy by an average of 0.7% across five random seeds.  A five member ensemble pushes the overall accuracy to 89.3%, exceeding the previous ensemble best of 88.9% (Gong et al., 2018).  Semantic role labeling A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering \"Who did what to whom\".  He et al. (2017) modeled SRL as a BIO tagging problem and used an 8-layer deep biLSTM with forward and backward directions interleaved, following Zhou and Xu (2015).  As shown in Table 1, when adding ELMo to a re-implementation of He et al. (2017) the single model test set F1 jumped 3.2% from 81.4% to 84.6% – a new state-of-the-art on the OntoNotes benchmark (Pradhan et al., 2013), even improving over the previous best ensemble result by 1.2%.  Coreference resolution Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities.  Our baseline model is the end-to-end span-based neural model of Lee et al. (2017).  It uses a biLSTM and attention mechanism to first compute span representations and then applies a softmax mention ranking model to find coreference chains.  In our experiments with the OntoNotes coreference annotations from the CoNLL 2012 shared task (Pradhan et al., 2012), adding ELMo improved the average F1 by 3.2% from 67.2 to 70.4, establishing a new state of the art, again improving over the previous best ensemble result by 1.6% F1.  Named entity extraction The CoNLL 2003 NER task (Sang and Meulder, 2003) consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC).  Following recent state-of-the-art systems (Lample et al., 2016; Peters et al., 2017), the baseline model uses pre-trained word embeddings, a character-based CNN representation, two biLSTM layers and a conditional random field (CRF) loss (Lafferty et al., 2001), similar to Collobert et al. (2011).  As shown in Table 1, our ELMo enhanced biLSTM-CRF achieves 92.22% F1 averaged over five runs.  The key difference between our system and the previous state of the art from Peters et al. (2017) is that we allowed the task model to learn a weighted average of all biLM layers, whereas Peters et al. (2017) only use the top biLM layer.  As shown in Sec. 5.1, using all layers instead of just the last layer improves performance across multiple tasks.  Sentiment analysis The fine-grained sentiment classification task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review.  The sentences contain diverse linguistic phenomena such as idioms and complex syntacTask Baseline Last Only All layers λ=1 λ=0.001 SQuAD 80.8 84.7 85.0 85.2 SNLI 88.1 89.1 89.3 89.5 SRL 81.6 84.1 84.6 84.8 Table 2: Development set performance for SQuAD, SNLI and SRL comparing using all layers of the biLM (with different choices of regularization strength λ) to just the top layer.  Table 3  : Development set performance for SQuAD, SNLI and SRL when including ELMo at different locations in the supervised model.  tic constructions such as negations that are difficult for models to learn.  Our baseline model is the biattentive classification network (BCN) from McCann et al. (2017), which also held the prior state-of-the-art result when augmented with CoVe embeddings.  Replacing CoVe with ELMo in the BCN model results in a 1.0% absolute accuracy improvement over the state of the art.  This section provides an ablation analysis to validate our chief claims and to elucidate some interesting aspects of ELMo representations.  Sec. 5.1 shows that using deep contextual representations in downstream tasks improves performance over previous work that uses just the top layer, regardless of whether they are produced from a biLM or MT encoder, and that ELMo representations provide the best overall performance.  Sec. 5.3 explores the different types of contextual information captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders.  It also shows that our biLM consistently provides richer representations then CoVe.  Additionally, we analyze the sensitivity to where ELMo is included in the task model (Sec. 5.2), training set size (Sec. 5.4), and visualize the ELMo learned weights across the tasks (Sec. 5.5).  There are many alternatives to Equation 1 for combining the biLM layers.  Previous work on contextual representations used only the last layer, whether it be from a biLM (Peters et al., 2017) or an MT encoder (CoVe; McCann et al., 2017).  The choice of the regularization parameter λ is also important, as large values such as λ = 1 effectively reduce the weighting function to a simple average over the layers, while smaller values (e.g., λ = 0.001) allow the layer weights to vary.  Table 2 compares these alternatives for SQuAD, SNLI and SRL.  Including representations from all layers improves overall performance over just using the last layer, and including contextual representations from the last layer improves performance over the baseline.  For example, in the case of SQuAD, using just the last biLM layer improves development F1 by 3.9% over the baseline.  Averaging all biLM layers instead of using just the last layer improves F1 another 0.3% (comparing \"Last Only\" to λ=1 columns), and allowing the task model to learn individual layer weights improves F1 another 0.2% (λ=1 vs. λ=0.001).  A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ (not shown).  The overall trend is similar with CoVe but with smaller increases over the baseline.  For SNLI, averaging all layers with λ=1 improves development accuracy from 88.2 to 88.7% over using just the last layer.  SRL F1 increased a marginal 0.1% to 82.2 for the λ=1 case compared to using the last layer only.  All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN.  However, we find that including ELMo at the output of the biRNN in task-specific architectures improves overall results for some tasks.  As shown in Table 3, including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and coreference resolution, not shown) performance is highest when it is included at just the input layer.  One possible explanation for this result is that both the SNLI and SQuAD architectures use attention layers after the biRNN, so introducing ELMo at this layer allows the model to attend directly to the biLM's internal representations.  In the SRL case, Kieffer , the only junior in the group , was commended for his ability to hit in the clutch , as well as his all-round excellent play .  Olivia De Havilland signed to do a Broadway play for Garson {. .. } {. ..}  they were actors who had been handed fat roles in a successful play , and had talent enough to fill the roles competently , with nice understatement .  Model F1 WordNet 1st Sense Baseline 65.9 Raganato et al. (2017a) 69.9 Iacobacci et al. (2016) 70.1 CoVe, First Layer 59.4 CoVe, Second Layer 64.7 biLM, First layer 67.4 biLM, Second layer 69.0 Table 5: All-words fine grained WSD F1.  For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.  Since adding ELMo improves task performance over word vectors alone, the biLM's contextual representations must encode information generally useful for NLP tasks that is not captured in word vectors.  Intuitively, the biLM must be disambiguating the meaning of words using their context.  Consider \"play\", a highly polysemous word.  The top of Table 4 lists nearest neighbors to \"play\" using GloVe vectors.  They are spread across several parts of speech (e.g., \"played\", \"playing\" as verbs, and \"player\", \"game\" as nouns) but concentrated in the sportsrelated senses of \"play\".  In contrast, the bottom two rows show nearest neighbor sentences from the SemCor dataset (see below) using the biLM's context representation of \"play\" in the source sentence.  In these cases, the biLM is able to disambiguate both the part of speech and word sense in the source sentence.  Model Acc.  Collobert et al. (2011) 97.3 Ma and Hovy (2016) 97.6  Ling et al. (2015) 97.8 CoVe, First Layer 93.3 CoVe, Second Layer 92.8 biLM, First Layer 97.3 biLM, Second Layer 96.8 Table 6  : Test set POS tagging accuracies for PTB.  For CoVe and the biLM, we report scores for both the first and second layer biLSTMs.  intrinsic evaluation of the contextual representations similar to Belinkov et al. (2017).  To isolate the information encoded by the biLM, the representations are used to directly make predictions for a fine grained word sense disambiguation (WSD) task and a POS tagging task.  Using this approach, it is also possible to compare to CoVe, and across each of the individual layers.  Word sense disambiguation Given a sentence, we can use the biLM representations to predict the sense of a target word using a simple 1nearest neighbor approach, similar to Melamud et al. (2016).  To do so, we first use the biLM to compute representations for all words in SemCor 3.0, our training corpus (Miller et al., 1994), and then take the average representation for each sense.  At test time, we again use the biLM to compute representations for a given target word and take the nearest neighbor sense from the training set, falling back to the first sense from WordNet for lemmas not observed during training.  Table 5 compares WSD results using the evaluation framework from Raganato et al. (2017b) across the same suite of four test sets in Raganato et al. (2017a).  Overall, the biLM top layer representations have F1 of 69.0 and are better at WSD then the first layer.  This is competitive with a state-of-the-art WSD-specific supervised model using hand crafted features (Iacobacci et al., 2016) and a task specific biLSTM that is also trained with auxiliary coarse-grained semantic labels and POS tags (Raganato et al., 2017a).  The CoVe biLSTM layers follow a similar pattern to those from the biLM (higher overall performance at the second layer compared to the first); however, our biLM outperforms the CoVe biLSTM, which trails the WordNet first sense baseline.  POS tagging To examine whether the biLM captures basic syntax, we used the context representations as input to a linear classifier that predicts POS tags with the Wall Street Journal portion of the Penn Treebank (PTB) (Marcus et al., 1993).  As the linear classifier adds only a small amount of model capacity, this is direct test of the biLM's representations.  Similar to WSD, the biLM representations are competitive with carefully tuned, task specific biLSTMs (Ling et al., 2015; Ma and Hovy, 2016).  However, unlike WSD, accuracies using the first biLM layer are higher than the top layer, consistent with results from deep biLSTMs in multi-task training (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) and MT (Belinkov et al., 2017).  CoVe POS tagging accuracies follow the same pattern as those from the biLM, and just like for WSD, the biLM achieves higher accuracies than the CoVe encoder.  Implications for supervised tasks Taken together, these experiments confirm different layers in the biLM represent different types of information and explain why including all biLM layers is important for the highest performance in downstream tasks.  In addition, the biLM's representations are more transferable to WSD and POS tagging than those in CoVe, helping to illustrate why ELMo outperforms CoVe in downstream tasks.  Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size.  For example, the SRL model reaches a maximum development F1 after 486 epochs of training without ELMo.  After adding ELMo, the model exceeds the baseline maximum at epoch 10, a 98% relative decrease in the number of updates needed to reach Figure 1: Comparison of baseline vs. ELMo performance for SNLI and SRL as the training set size is varied from 0.1% to 100%.  Figure 2: Visualization of softmax normalized biLM layer weights across tasks and ELMo locations.  Normalized weights less then 1/3 are hatched with horizontal lines and those greater then 2/3 are speckled.  In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo.  Figure 1 compares the performance of baselines models with and without ELMo as the percentage of the full training set is varied from 0.1% to 100%.  Improvements with ELMo are largest for smaller training sets and significantly reduce the amount of training data needed to reach a given level of performance.  In the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set.  Figure\nvisualizes the softmax-normalized learned layer weights.  At the input layer, the task model favors the first biLSTM layer.  For coreference and SQuAD, the this is strongly favored, but the distribution is less peaked for the other tasks.  The output layer weights are relatively balanced, with a slight preference for the lower layers.  Task GloVe ELMo ELMo ELMo + type GloVe SQuAD 80.8 81.4 85.3 85.6 SNLI 88.1 88.5 89.1 89.5 SRL 81.6 81.7 84.5 84.7 Table 7: Development set ablation analysis for SQuAD, SNLI and SRL comparing different choices for the context-independent type representation and contextual representation.  From left to right, the table compares systems with only GloVe vectors; only the ELMo context-independent type representation without the ELMo biLSTM layers; full ELMo representations without GloVe; both GloVe and ELMo.  In addition to the contextual information captured in the biLM's biLSTM layers, ELMo representations also contain sub-word information in the fully character based context insensitive type layer, xLM  k\n.  To analyze the relative contribution of the contextual information compared to the sub-word information, we ran an additional ablation that replaced the GloVe vectors with just the biLM character based xLM  k\nlayer without the biLM biLSTM layers.  Table 7 summarizes the results for SQuAD, SNLI and SNLI.  Replacing the GloVe vectors with the biLM character layer gives a slight improvement for all tasks (e.g. from 80.8 to 81.4 F1 for SQuAD), but overall the improvements are small compared to the full ELMo model.  From this, we conclude that most of the gains in the downstream tasks are due to the contextual information and not the sub-word information.  All of the results presented in Sec.4 include pretrained word vectors in addition to ELMo representations.  However, it is natural to ask whether pre-trained vectors are still necessary with high quality contextualized representations.  As shown in the two right hand columns of Table 7, adding GloVe to models with ELMo generally provides a marginal improvement over ELMo only models (e.g. 0.2% F1 improvement for SRL from 84.5 to 84.7).  We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks.  Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about wordsin-context, and that using all layers improves overall task performance.  san Sajjad, and James R. Glass. 2017.  What do neural machine translation models learn about morphology?  In ACL.  and Christopher D. Manning. 2015.  A large annotated corpus for learning natural language inference.  In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).  Association for Computational Linguistics.  Thorsten Brants, Phillipp Koehn, and Tony Robinson.  2014.  One billion word benchmark for measuring progress in statistical language modeling.  In INTERSPEECH.  ruoka, and Richard Socher.  2017.  A joint many-task model: Growing a neural network for multiple nlp tasks.  In EMNLP 2017.  Iyyer, Ishaan Gulrajani James Bradbury, Victor Zhong, Romain Paulus, and Richard Socher.  2016.  Ask me anything: Dynamic memory networks for natural language processing.  In ICML.  coso, Ramon Fermandez, Silvio Amir, Lu´ıs Marujo, and Tiago Lu´ıs. 2015.  Finding function in form: Compositional character models for open vocabulary word representation.  In EMNLP.  Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong.  2013.  Towards robust linguistic analysis using ontonotes.  In CoNLL.  Olga Uryupina, and Yuchen Zhang.  2012.  Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes.  In EMNLPCoNLL Shared Task.  Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts.  2013.  Recursive deep models for semantic compositionality over a sentiment treebank.  In EMNLP.  Anders Søgaard and Yoav Goldberg.  2016.  Deep multi-task learning with low level tasks supervised at lower layers.  In ACL 2016.  Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.  2014.  Dropout: a simple way to prevent neural networks from overfitting.  Journal of Machine Learning Research 15:1929–1958.  Hongyun Bao, and Bo Xu. 2016.  Text classification improved by integrating bidirectional lstm with twodimensional max pooling.  In COLING.", "metadata": {"source_file": "N18-1202.pdf", "title": "Deep Contextualized Word Representations", "authors": ["Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "year": "2018", "detected_language": "en", "page_count": 11, "origin_chunk_file": "N18-1202.chunks.json"}}
{"text": "Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representa tion, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. achine-learning technology powers many aspects of modern society: from web searches to content filtering on social net works to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users' interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning.  Conventional machine-learning techniques were limited in their ability to process natural data in their raw form.", "metadata": {"source_file": "NatureDeepReview.pdf", "title": "M REVIEW Deep learning", "authors": ["Pavillon André-Aisenstadt"], "year": "2015", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NatureDeepReview.chunks.json"}}
{"text": "For decades, con structing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a fea ture extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence commu nity for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applica ble to many domains of science, business and government. In addition to beating records in image recognition1–4 and speech recognition5–7, it has beaten other machine-learning techniques at predicting the activ ity of potential drug molecules8, analysing particle accelerator data9,10, reconstructing brain circuits11, and predicting the effects of mutations in non-coding DNA on gene expression and disease12,13. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding14, particularly topic classification, sentiment analysis, question answering15 and lan guage translation16,17. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available com putation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only acceler ate this progress. Supervised learning The most common form of machine learning, deep or not, is super vised learning. Imagine that we want to build a system that can classify images as containing, say, a house, a car, a person or a pet. We first collect a large data set of images of houses, cars, people and pets, each labelled with its category.  During training, the machine is shown an image and produces an output in the form of a vector of scores, one for each category.  We want the desired category to have the highest score of all categories, but this is unlikely to happen before training.  We compute an objective function that measures the error (or dis tance) between the output scores and the desired pattern of scores.  The machine then modifies its internal adjustable parameters to reduce this error.  These adjustable parameters, often called weights, are real numbers that can be seen as 'knobs' that define the input–output func tion of the machine.  In a typical deep-learning system, there may be hundreds of millions of these adjustable weights, and hundreds of millions of labelled examples with which to train the machine.  To properly adjust the weight vector, the learning algorithm com putes a gradient vector that, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount.  The weight vector is then adjusted in the opposite direc tion to the gradient vector.  Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction.  These methods have dramatically improved the state-of-the-art in speech rec ognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.  Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer.  Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.  4 3 6 | N  A T U R E | V O L 5 2 1 | 2 8 M A Y 2 0 1 5 be seen as a kind of hilly landscape in the high-dimensional space of weight values.  The negative gradient vector indicates the direction of steepest descent in this landscape, taking it closer to a minimum, where the output error is low on average.  In practice, most practitioners use a procedure called stochastic gradient descent (SGD).  This consists of showing the input vector for a few examples, computing the outputs and the errors, computing the average gradient for those examples, and adjusting the weights accordingly.  The process is repeated for many small sets of examples from the training set until the average of the objective function stops decreasing.  It is called stochastic because each small set of examples gives a noisy estimate of the average gradient over all examples.  This simple procedure usually finds a good set of weights surprisingly quickly when compared with far more elaborate optimization tech niques18.  After training, the performance of the system is measured on a different set of examples called a test set.  This serves to test the generalization ability of the machine — its ability to produce sensible answers on new inputs that it has never seen during training.  Many of the current practical applications of machine learning use linear classifiers on top of hand-engineered features.  A two-class linear classifier computes a weighted sum of the feature vector components.  If the weighted sum is above a threshold, the input is classified as belonging to a particular category.  Since the 1960s we have known that linear classifiers can only carve their input space into very simple regions, namely half-spaces sepa rated by a hyperplane19.", "metadata": {"source_file": "NatureDeepReview.pdf", "title": "M REVIEW Deep learning", "authors": ["Pavillon André-Aisenstadt"], "year": "2015", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NatureDeepReview.chunks.json"}}
{"text": "But problems such as image and speech recog nition require the input–output function to be insensitive to irrelevant variations of the input, such as variations in position, orientation or illumination of an object, or variations in the pitch or accent of speech, while being very sensitive to particular minute variations (for example, the difference between a white wolf and a breed of wolf-like white dog called a Samoyed). At the pixel level, images of two Samoyeds in different poses and in different environments may be very different from each other, whereas two images of a Samoyed and a wolf in the same position and on similar backgrounds may be very similar to each other. A linear classifier, or any other 'shallow' classifier operating on Figure 1 | Multilayer neural networks and backpropagation. a, A multilayer neural network (shown by the connected dots) can distort the input space to make the classes of data (examples of which are on the red and blue lines) linearly separable. Note how a regular grid (shown on the left) in input space is also transformed (shown in the middle panel) by hidden units. This is an illustrative example with only two input units, two hidden units and one output unit, but the networks used for object recognition or natural language processing contain tens or hundreds of thousands of units. Reproduced with permission from C. Olah ( b, The chain rule of derivatives tells us how two small effects (that of a small change of x on y, and that of y on z) are composed. A small change Δx in x gets transformed first into a small change Δy in y by getting multiplied by ∂y/∂x (that is, the definition of partial derivative). Similarly, the change Δy creates a change Δz in z. Substituting one equation into the other gives the chain rule of derivatives — how Δx gets turned into Δz through multiplication by the product of ∂y/∂x and ∂z/∂x. It also works when x, y and z are vectors (and the derivatives are Jacobian matrices). c, The equations used for computing the forward pass in a neural net with two hidden layers and one output layer, each constituting a module through which one can backpropagate gradients.  At each layer, we first compute the total input z to each unit, which is a weighted sum of the outputs of the units in the layer below.  Then a non-linear function f(.) is applied to z to get the output of the unit.  For simplicity, we have omitted bias terms.  The non-linear functions used in neural networks include the rectified linear unit (ReLU) f(z) = max(0,z), commonly used in recent years, as well as the more conventional sigmoids, such as the hyberbolic tangent, f(z)  = (exp(z) − exp(−z))/(exp(z) + exp(−z)) and logistic function logistic, f(z) = 1/(1 + exp(−z)).  d, The equations used for computing the backward pass.  At each hidden layer we compute the error derivative with respect to the output of each unit, which is a weighted sum of the error derivatives with respect to the total inputs to the units in the layer above.  We then convert the error derivative with respect to the output into the error derivative with respect to the input by multiplying it by the gradient of f(z).  At the output layer, the error derivative with respect to the output of a unit is computed by differentiating the cost function.  This gives yl − tl if the cost function for unit l is 0.5(yl − tl)2, where tl is the target value.  Once the ∂E/∂zk is known, the error-derivative for the weight wjk on the connection from unit j in the layer below is just yj ∂E/∂zk.  2 8 M A Y 2 0 1 5 | V O L 5 2 1 | N  A T U R E | 4 3 7 raw pixels could not possibly distinguish the latter two, while putting the former two in the same category.  This is why shallow classifiers require a good feature extractor that solves the selectivity–invariance dilemma — one that produces representations that are selective to the aspects of the image that are important for discrimination, but that are invariant to irrelevant aspects such as the pose of the animal.  To make classifiers more powerful, one can use generic non-linear features, as with kernel methods20, but generic features such as those arising with the Gaussian kernel do not allow the learner to general ize well far from the training examples21.  The conventional option is to hand design good feature extractors, which requires a consider able amount of engineering skill and domain expertise.  But this can all be avoided if good features can be learned automatically using a general-purpose learning procedure.  This is the key advantage of deep learning.  A deep-learning architecture is a multilayer stack of simple mod ules, all (or most) of which are subject to learning, and many of which compute non-linear input–output mappings.  Each module in the stack transforms its input to increase both the selectivity and the invariance of the representation.  With multiple non-linear layers, say a depth of 5 to 20, a system can implement extremely intricate func tions of its inputs that are simultaneously sensitive to minute details — distinguishing Samoyeds from white wolves — and insensitive to large irrelevant variations such as the background, pose, lighting and surrounding objects.  Backpropagation to train multilayer architectures From the earliest days of pattern recognition22,23, the aim of research ers has been to replace hand-engineered features with trainable multilayer networks, but despite its simplicity, the solution was not widely understood until the mid 1980s.  As it turns out, multilayer architectures can be trained by simple stochastic gradient descent.  As long as the modules are relatively smooth functions of their inputs and of their internal weights, one can compute gradients using the backpropagation procedure.  The idea that this could be done, and that it worked, was discovered independently by several different groups during the 1970s and 1980s24–27.  The backpropagation procedure to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules is nothing more than a practical application of the chain rule for derivatives.  The key insight is that the derivative (or gradi ent) of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module (or the input of the subsequent module) (Fig. 1).  The backpropagation equation can be applied repeatedly to propagate gradients through all modules, starting from the output at the top (where the network produces its prediction) all the way to the bottom (where the external input is fed).  Once these gradients have been computed, it is straightforward to compute the gradients with respect to the weights of each module.  Many applications of deep learning use feedforward neural net work architectures (Fig. 1), which learn to map a fixed-size input (for example, an image) to a fixed-size output (for example, a prob ability for each of several categories).  To go from one layer to the next, a set of units compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function.  At present, the most popular non-linear function is the rectified linear unit (ReLU), which is simply the half-wave rectifier f(z) = max(z, 0).  In past decades, neural nets used smoother non-linearities, such as tanh(z) or 1/(1 + exp(−z)), but the ReLU typically learns much faster in networks with many layers, allowing training of a deep supervised network without unsupervised pre-training28.  Units that are not in the input or output layer are conventionally called hidden units.  The hidden layers can be seen as distorting the input in a non-linear way so that categories become linearly separable by the last layer (Fig. 1).  In the late 1990s, neural nets and backpropagation were largely forsaken by the machine-learning community and ignored by the computer-vision and speech-recognition communities.  It was widely thought that learning useful, multistage, feature extractors with lit tle prior knowledge was infeasible.  In particular, it was commonly thought that simple gradient descent would get trapped in poor local minima — weight configurations for which no small change would reduce the average error.  In practice, poor local minima are rarely a problem with large net works.  Regardless of the initial conditions, the system nearly always reaches solutions of very similar quality.  Recent theoretical and empirical results strongly suggest that local minima are not a serious issue in general.  Instead, the landscape is packed with a combinato rially large number of saddle points where the gradient is zero, and the surface curves up in most dimensions and curves down in the Figure 2 | Inside a convolutional network.  The outputs (not the filters) of each layer (horizontally) of a typical convolutional network architecture applied to the image of a Samoyed dog (bottom left; and RGB (red, green, blue) inputs, bottom right).  Each rectangular image is a feature map corresponding to the output for one of the learned features, detected at each of the image positions.  Information flows bottom up, with lower-level features acting as oriented edge detectors, and a score is computed for each image class in output.  ReLU, rectified linear unit.  4 3 8 | N  A T U R E | V O L 5 2 1 | 2 8 M A Y 2 0 1 5 remainder29,30.  The analysis seems to show that saddle points with only a few downward curving directions are present in very large numbers, but almost all of them have very similar values of the objec tive function.  Hence, it does not much matter which of these saddle points the algorithm gets stuck at.  There was, however, one particular type of deep, feedforward net work that was much easier to train and generalized much better than networks with full connectivity between adjacent layers.  This was the convolutional neural network (ConvNet)41,42.  It achieved many practical successes during the period when neural networks were out of favour and it has recently been widely adopted by the computervision community.  Convolutional neural networks ConvNets are designed to process data that come in the form of multiple arrays, for example a colour image composed of three 2D arrays containing pixel intensities in the three colour channels.  Many data modalities are in the form of multiple arrays: 1D for signals and sequences, including language; 2D for images or audio spectrograms; and 3D for video or volumetric images.  There are four key ideas behind ConvNets that take advantage of the properties of natural signals: local connections, shared weights, pooling and the use of many layers.  The architecture of a typical ConvNet (Fig. 2) is structured as a series of stages.  The first few stages are composed of two types of layers: convolutional layers and pooling layers.  Units in a convolu tional layer are organized in feature maps, within which each unit is connected to local patches in the feature maps of the previous layer through a set of weights called a filter bank.  The result of this local weighted sum is then passed through a non-linearity such as a ReLU.  All units in a feature map share the same filter bank.  Differ ent feature maps in a layer use different filter banks.  The reason for this architecture is twofold.  First, in array data such as images, local groups of values are often highly correlated, forming distinctive local motifs that are easily detected.  Second, the local statistics of images and other signals are invariant to location.  In other words, if a motif can appear in one part of the image, it could appear anywhere, hence the idea of units at different locations sharing the same weights and detecting the same pattern in different parts of the array.  Mathemati cally, the filtering operation performed by a feature map is a discrete convolution, hence the name.  Although the role of the convolutional layer is to detect local con junctions of features from the previous layer, the role of the pooling layer is to merge semantically similar features into one.  Because the relative positions of the features forming a motif can vary somewhat, reliably detecting the motif can be done by coarse-graining the posi tion of each feature.  A typical pooling unit computes the maximum of a local patch of units in one feature map (or in a few feature maps).  Neighbouring pooling units take input from patches that are shifted by more than one row or column, thereby reducing the dimension of the representation and creating an invariance to small shifts and dis tortions.  Two or three stages of convolution, non-linearity and pool ing are stacked, followed by more convolutional and fully-connected layers.  Backpropagating gradients through a ConvNet is as simple as through a regular deep network, allowing all the weights in all the filter banks to be trained.  Deep neural networks exploit the property that many natural sig nals are compositional hierarchies, in which higher-level features are obtained by composing lower-level ones.  In images, local combi nations of edges form motifs, motifs assemble into parts, and parts form objects.  Similar hierarchies exist in speech and text from sounds to phones, phonemes, syllables, words and sentences.  The pooling allows representations to vary very little when elements in the previ ous layer vary in position and appearance.  The convolutional and pooling layers in ConvNets are directly inspired by the classic notions of simple cells and complex cells in visual neuroscience43, and the overall architecture is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral path way44.  When ConvNet models and monkeys are shown the same pic ture, the activations of high-level units in the ConvNet explains half of the variance of random sets of 160 neurons in the monkey's infer otemporal cortex45.  ConvNets have their roots in the neocognitron46, the architecture of which was somewhat similar, but did not have an end-to-end supervised-learning algorithm such as backpropagation.  A primitive 1D ConvNet called a time-delay neural net was used for the recognition of phonemes and simple words47,48.  There have been numerous applications of convolutional net works going back to the early 1990s, starting with time-delay neu ral networks for speech recognition47 and document reading42.  The document reading system used a ConvNet trained jointly with a probabilistic model that implemented language constraints.  By the late 1990s this system was reading over 10% of all the cheques in the United States.  A number of ConvNet-based optical character recog nition and handwriting recognition systems were later deployed by Microsoft49.  ConvNets were also experimented with in the early 1990s for object detection in natural images, including faces and hands50,51, and for face recognition52.  Image understanding with deep convolutional networks Since the early 2000s, ConvNets have been applied with great success to the detection, segmentation and recognition of objects and regions in images.  These were all tasks in which labelled data was relatively abun dant, such as traffic sign recognition53, the segmentation of biological images54 particularly for connectomics55, and the detection of faces, text, pedestrians and human bodies in natural images36,50,51,56–58.  A major recent practical success of ConvNets is face recognition59.  Importantly, images can be labelled at the pixel level, which will have applications in technology, including autonomous mobile robots and 2 8 M A Y 2 0 1 5 | V O L 5 2 1 | N  A T U R E | 4 3 9 self-driving cars60,61.  Companies such as Mobileye and NVIDIA are using such ConvNet-based methods in their upcoming vision sys tems for cars.  Other applications gaining importance involve natural language understanding14 and speech recognition7.  Despite these successes, ConvNets were largely forsaken by the mainstream computer-vision and machine-learning communities until the ImageNet competition in 2012.  When deep convolutional networks were applied to a data set of about a million images from the web that contained 1,000 different classes, they achieved spec tacular results, almost halving the error rates of the best compet ing approaches1.  This success came from the efficient use of GPUs, ReLUs, a new regularization technique called dropout62, and tech niques to generate more training examples by deforming the existing ones.  This success has brought about a revolution in computer vision; ConvNets are now the dominant approach for almost all recognition and detection tasks4,58,59,63–65 and approach human performance on some tasks.  A recent stunning demonstration combines ConvNets and recurrent net modules for the generation of image captions (Fig. 3).  Recent ConvNet architectures have 10 to 20 layers of ReLUs, hun dreds of millions of weights, and billions of connections between units.  Whereas training such large networks could have taken weeks only two years ago, progress in hardware, software and algorithm parallelization have reduced training times to a few hours.  Microsoft, IBM, Yahoo!, Twitter and Adobe, as well as a quickly growing number of start-ups to initiate research and development projects and to deploy ConvNet-based image understanding products and services.  ConvNets are easily amenable to efficient hardware implemen tations in chips or field-programmable gate arrays66,67.  A number of companies such as NVIDIA, Mobileye, Intel, Qualcomm and Samsung are developing ConvNet chips to enable real-time vision applications in smartphones, cameras, robots and self-driving cars.  Distributed representations and language processing Deep-learning theory shows that deep nets have two different expo nential advantages over classic learning algorithms that do not use distributed representations21.  Both of these advantages arise from the power of composition and depend on the underlying data-generating distribution having an appropriate componential structure40.  First, learning distributed representations enable generalization to new combinations of the values of learned features beyond those seen during training (for example, 2n combinations are possible with n binary features)68,69.  Second, composing layers of representation in a deep net brings the potential for another exponential advantage70 (exponential in the depth).  The hidden layers of a multilayer neural network learn to repre sent the network's inputs in a way that makes it easy to predict the target outputs.  This is nicely demonstrated by training a multilayer neural network to predict the next word in a sequence from a local Figure 3 | From image to text.  Captions generated by a recurrent neural network (RNN) taking, as extra input, the representation extracted by a deep convolution neural network (CNN) from a test image, with the RNN trained to 'translate' high-level representations of images into captions (top).  Reproduced with permission from ref.  102.  When the RNN is given the ability to focus its attention on a different location in the input image (middle and bottom; the lighter patches were given more attention) as it generates each word (bold), we found86 that it exploits this to achieve better 'translation' of images into captions.  A little girl sitting on a bed with a teddy bear.  A group of people sitting on a boat in the water.  A giraffe standing in a forest with 4 4 0 | N  A T U R E | V O L 5 2 1 | 2 8 M A Y 2 0 1 5 context of earlier words71.  Each word in the context is presented to the network as a one-of-N vector, that is, one component has a value of 1 and the rest are 0.  In the first layer, each word creates a different pattern of activations, or word vectors (Fig. 4).  In a language model, the other layers of the network learn to convert the input word vec tors into an output word vector for the predicted next word, which can be used to predict the probability for any word in the vocabulary to appear as the next word.  The network learns word vectors that contain many active components each of which can be interpreted as a separate feature of the word, as was first demonstrated27 in the context of learning distributed representations for symbols.  These semantic features were not explicitly present in the input.  They were discovered by the learning procedure as a good way of factorizing the structured relationships between the input and output symbols into multiple 'micro-rules'.  Learning word vectors turned out to also work very well when the word sequences come from a large corpus of real text and the individual micro-rules are unreliable71.  When trained to predict the next word in a news story, for example, the learned word vectors for Tuesday and Wednesday are very similar, as are the word vectors for Sweden and Norway.  Such representations are called distributed representations because their elements (the features) are not mutually exclusive and their many configurations correspond to the variations seen in the observed data.  These word vectors are composed of learned features that were not determined ahead of time by experts, but automatically discovered by the neural network.  Vector representations of words learned from text are now very widely used in natural language applications14,17,72–76.  The issue of representation lies at the heart of the debate between the logic-inspired and the neural-network-inspired paradigms for cognition.  In the logic-inspired paradigm, an instance of a symbol is something for which the only property is that it is either identical or non-identical to other symbol instances.  It has no internal structure that is relevant to its use; and to reason with symbols, they must be bound to the variables in judiciously chosen rules of inference.  By contrast, neural networks just use big activity vectors, big weight matrices and scalar non-linearities to perform the type of fast 'intui tive' inference that underpins effortless commonsense reasoning.  Before the introduction of neural language models71, the standard approach to statistical modelling of language did not exploit distrib uted representations: it was based on counting frequencies of occur rences of short symbol sequences of length up to N (called N-grams).  The number of possible N-grams is on the order of VN, where V is the vocabulary size, so taking into account a context of more than a handful of words would require very large training corpora.  N-grams treat each word as an atomic unit, so they cannot generalize across semantically related sequences of words, whereas neural language models can because they associate each word with a vector of real valued features, and semantically related words end up close to each other in that vector space (Fig. 4).  Recurrent neural networks When backpropagation was first introduced, its most exciting use was for training recurrent neural networks (RNNs).  For tasks that involve sequential inputs, such as speech and language, it is often better to use RNNs (Fig. 5).  RNNs process an input sequence one element at a time, maintaining in their hidden units a 'state vector' that implicitly contains information about the history of all the past elements of the sequence.  When we consider the outputs of the hidden units at different discrete time steps as if they were the outputs of different neurons in a deep multilayer network (Fig. 5, right), it becomes clear how we can apply backpropagation to train RNNs.  RNNs are very powerful dynamic systems, but training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish77,78.  Thanks to advances in their architecture79,80 and ways of training them81,82, RNNs have been found to be very good at predicting the next character in the text83 or the next word in a sequence75, but they can also be used for more complex tasks.  For example, after reading an English sentence one word at a time, an English 'encoder' network can be trained so that the final state vector of its hidden units is a good representation of the thought expressed by the sentence.  This thought vector can then be used as the initial hidden state of (or as extra input to) a jointly trained French 'decoder' network, which outputs a prob ability distribution for the first word of the French translation.  If a particular first word is chosen from this distribution and provided as input to the decoder network it will then output a probability dis tribution for the second word of the translation and so on until a full stop is chosen17,72,76.  Overall, this process generates sequences of French words according to a probability distribution that depends on the English sentence.  This rather naive way of performing machine translation has quickly become competitive with the state-of-the-art, and this raises serious doubts about whether understanding a sen tence requires anything like the internal symbolic expressions that are manipulated by using inference rules.  It is more compatible with the view that everyday reasoning involves many simultaneous analogies Figure 4 | Visualizing the learned word vectors.  On the left is an illustration of word representations learned for modelling language, non-linearly projected to 2D for visualization using the t-SNE algorithm103.  On the right is a 2D representation of phrases learned by an English-to-French encoder–decoder recurrent neural network75.  One can observe that semantically similar words or sequences of words are mapped to nearby representations.  The distributed representations of words are obtained by using backpropagation to jointly learn a representation for each word and a function that predicts a target quantity such as the next word in a sequence (for language modelling) or a whole sequence of translated words (for machine translation)18,75.  2 8 M A Y 2 0 1 5 | V O L 5 2 1 | N  A T U R E | 4 4 1 Instead of translating the meaning of a French sentence into an English sentence, one can learn to 'translate' the meaning of an image into an English sentence (Fig. 3).  The encoder here is a deep Con vNet that converts the pixels into an activity vector in its last hidden layer.  The decoder is an RNN similar to the ones used for machine translation and neural language modelling.  There has been a surge of interest in such systems recently (see examples mentioned in ref. 86).  RNNs, once unfolded in time (Fig. 5), can be seen as very deep feedforward networks in which all the layers share the same weights.  Although their main purpose is to learn long-term dependencies, theoretical and empirical evidence shows that it is difficult to learn to store information for very long78.  To correct for that, one idea is to augment the network with an explicit memory.  The first proposal of this kind is the long short-term memory (LSTM) networks that use special hidden units, the natural behaviour of which is to remember inputs for a long time79.  A special unit called the memory cell acts like an accumulator or a gated leaky neuron:  it has a connection to itself at the next time step that has a weight of one, so it copies its own real-valued state and accumulates the external signal, but this self-connection is multiplicatively gated by another unit that learns to decide when to clear the content of the memory.  LSTM networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step87, enabling an entire speech recognition system that goes all the way from acoustics to the sequence of characters in the transcription.  LSTM networks or related forms of gated units are also currently used for the encoder and decoder networks that perform so well at machine translation17,72,76.  Beyond simple memorization, neural Turing machines and mem ory networks are being used for tasks that would normally require reasoning and symbol manipulation.  Neural Turing machines can be taught 'algorithms'.  Among other things, they can learn to output The future of deep learning Unsupervised learning91–98 had a catalytic effect in reviving interest in deep learning, but has since been overshadowed by the successes of purely supervised learning.  Although we have not focused on it in this Review, we expect unsupervised learning to become far more important in the longer term.  Human and animal learning is largely unsupervised: we discover the structure of the world by observing it, not by being told the name of every object.  Human vision is an active process that sequentially samples the optic array in an intelligent, task-specific way using a small, high-resolution fovea with a large, low-resolution surround.  We expect much of the future progress in vision to come from systems that are trained end-toend and combine ConvNets with RNNs that use reinforcement learning to decide where to look.  Systems combining deep learning and rein forcement learning are in their infancy, but they already outperform passive vision systems99 at classification tasks and produce impressive results in learning to play many different video games100.  Natural language understanding is another area in which deep learn ing is poised to make a large impact over the next few years.  We expect systems that use RNNs to understand sentences or whole documents will become much better when they learn strategies for selectively attending to one part at a time76,86.  Ultimately, major progress in artificial intelligence will come about through systems that combine representation learning with complex reasoning.  Although deep learning and simple reasoning have been used for speech and handwriting recognition for a long time, new paradigms are needed to replace rule-based manipulation of symbolic expressions by operations on large vectors101.  ■ Figure 5 |  A recurrent neural network and the unfolding in time of the computation involved in its forward computation.  The artificial neurons (for example, hidden units grouped under node s with values st at time t) get inputs from other neurons at previous time steps (this is represented with the black square, representing a delay of one time step, on the left).  In this way, a recurrent neural network can map an input sequence with elements xt into an output sequence with elements ot, with each ot depending on all the previous xtʹ (for tʹ ≤ t).  The same parameters (matrices U,V,W ) are used at each time step.  Many other architectures are possible, including a variant in which the network can generate a sequence of outputs (for example, words), each of which is used as inputs for the next time step.  The backpropagation algorithm (Fig. 1) can be directly applied to the computational graph of the unfolded network on the right, to compute the derivative of a total error (for example, the log-probability of generating the right sequence of outputs) with respect to all the states st and all the parameters.  4 4 2 | N  A T U R E | V O L 5 2 1 | 2 8 M A Y 2 0 1 5 regulated splicing code.  Bioinformatics 30, i121–i129 (2014).  13.  Xiong, H. Y. et al.  The human splicing code reveals new insights into the genetic determinants of disease.  Science 347, 6218 (2015).  14.  Collobert, R., et al.  Natural language processing (almost) from scratch.  J. Mach. networks.  In Proc.  Advances in Neural Information Processing Systems 27 3104–3112 (2014).  This paper showed state-of-the-art machine translation results with the architecture introduced in ref.  72, with a recurrent network trained to read a sentence in one language, produce a semantic representation of its meaning, and generate a translation in another language.  18.  Bottou, L. & Bousquet, O.  The tradeoffs of large scale learning.  In Proc.  Advances in Neural Information Processing Systems 20 161–168 (2007).  19.  Duda, R. O. & Hart, P. E. Pattern Classification and Scene Analysis (Wiley, 1973).  20.  Schölkopf, B. & Smola, A. Learning with Kernels (MIT Press, 2002).  21.  Bengio, Y., Delalleau, O. & Le Roux, N.  The curse of highly variable functions for local kernel machines.  In Proc.  Advances in Neural Information Processing Systems 18 107–114 (2005).  22.  Selfridge, O. G. Pandemonium: a paradigm for learning in mechanisation of thought processes.  In Proc. Symposium on Mechanisation of Thought Processes 513–526 (1958).  23.  Rosenblatt, F.  The Perceptron — A Perceiving and Recognizing Automaton.  Tech.  Behavioral Sciences.  PhD thesis, Harvard Univ.  (1974).  25.  Parker, D. B. Learning Logic Report TR–47 (MIT Press, 1985).  26.  LeCun, Y. Une procédure d'apprentissage pour Réseau à seuil assymétrique in Cognitiva 85: a la Frontière de l'Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences  [in French] 599–604 (1985).  27.  Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors.  Nature 323, 533–536 (1986).  28.  Glorot, X., Bordes, A. & Bengio.  Y. Deep sparse rectifier neural networks.  In Proc. 14th  International Conference on Artificial Intelligence and Statistics 315–323 (2011).  This paper showed that supervised training of very deep neural networks is much faster if the hidden layers are composed of ReLU.  29.  Dauphin, Y. et al.  Identifying and attacking the saddle point problem in highdimensional non-convex optimization.  In Proc.  Advances in Neural Information Processing Systems 27 2933–2941 (2014).  30.  Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. & LeCun, Y.  The loss International Joint Conference on Artificial intelligence 1765–1775 (2005).  32.  Hinton, G. E., Osindero, S. & Teh, Y.-W. A fast learning algorithm for deep belief nets.  Neural Comp. 18, 1527–1554 (2006).  This paper introduced a novel and effective way of training very deep neural networks by pre-training one hidden layer at a time using the unsupervised learning procedure for restricted Boltzmann machines.  33.  Bengio, Y., Lamblin, P., Popovici, D. & Larochelle, H. Greedy layer-wise training of deep networks.  In Proc.  Advances in Neural Information Processing Systems 19 153–160 (2006).  This report demonstrated that the unsupervised pre-training method introduced in ref.  32 significantly improves performance on test data and generalizes the method to other unsupervised representation-learning techniques, such as auto-encoders.  34.  Ranzato, M., Poultney, C., Chopra, S. & LeCun, Y. Efficient learning of sparse representations with an energy-based model.  In Proc.  Advances in Neural Information Processing Systems 19 1137–1144 (2006).  35.  Hinton, G. E. & Salakhutdinov, R. Reducing the dimensionality of data with using graphics processors.  In Proc.  26th Annual International Conference on Machine Learning 873–880 (2009).  38.  Mohamed, A.-R., Dahl, G. E. & Hinton, G. Acoustic modeling using deep belief networks.  IEEE Trans.  Audio Speech Lang.  Process.  20, 14–22 (2012).  39.  Dahl, G. E., Yu, D., Deng, L. & Acero, A. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition.  IEEE Trans.  Audio Speech Lang.  Process.  20, 33–42 (2012).  40.  Bengio, Y., Courville, A. & Vincent, P. Representation learning: a review and new perspectives.  IEEE Trans.  Pattern Anal.  Machine Intell. 35, 1798–1828 (2013).  41.  LeCun, Y. et al.  Handwritten digit recognition with a back-propagation network.  In Proc.  Advances in Neural Information Processing Systems 396–404 (1990).  This is the first paper on convolutional networks trained by backpropagation for the task of classifying low-resolution images of handwritten digits.  42.  LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learning applied to document recognition.  Proc.  IEEE 86, 2278–2324 (1998).  This overview paper on the principles of end-to-end training of modular systems such as deep neural networks using gradient-based optimization showed how neural networks (and in particular convolutional nets) can be combined with search or inference mechanisms to model complex outputs that are interdependent, such as sequences of characters associated with the content of a document.  43.  Hubel, D. H. & Wiesel, T. N. Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex.  J. Physiol.  160, 106–154 (1962).  44.  Felleman, D. J. & Essen, D. C. V. Distributed hierarchical processing in the primate cerebral cortex.  Cereb.  Cortex 1, 1–47 (1991).  45.  Cadieu, C. F. et al.  Deep neural networks rival the representation of primate it cortex for core visual object recognition.  PLoS  Comp.  Biol.  10, e1003963 (2014).  46.  Fukushima, K. & Miyake, S. Neocognitron: a new algorithm for pattern recognition tolerant of deformations and shifts in position.  Pattern Recognition 15, 455–469 (1982).  47.  Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K. & Lang, K. Phoneme recognition using time-delay neural networks.  IEEE Trans.  Acoustics Speech Signal Process.  37, 328–339 (1989).  48.  Bottou, L., Fogelman-Soulié, F., Blanchet, P. & Lienard, J. Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition.  In Proc.  EuroSpeech 89 537–540 (1989).  49.  Simard, D., Steinkraus, P. Y. & Platt, J. C. Best practices for convolutional neural networks.  In Proc.  Document Analysis and Recognition 958–963 (2003).  50.  Vaillant, R., Monrocq, C. & LeCun, Y. Original approach for the localisation of objects in images.  In Proc.  Vision, Image, and Signal Processing 141, 245–250 (1994).  51.  Nowlan, S. & Platt, J. in Neural Information Processing Systems 901–908 (1995).  52.  Lawrence, S., Giles, C. L., Tsoi, A. C. & Back, A. D. Face recognition: a convolutional neural-network approach.  IEEE Trans.  Neural Networks 8, 98–113 (1997).  53.  Ciresan, D., Meier, U. Masci, J. & Schmidhuber, J. Multi-column deep neural network for traffic sign classification.  Neural Networks 32, 333–338 (2012).  54.  Ning, F. et al.  Toward automatic phenotyping of developing embryos from videos.  IEEE Trans. Image Process.  14, 1360–1371 (2005).  55.  Turaga, S. C. et al.  Convolutional networks can learn to generate affinity graphs for image segmentation.  Neural Comput.  22, 511–538 (2010).  56.  Garcia, C. & Delakis, M. Convolutional face finder: a neural architecture for fast and robust face detection.  IEEE Trans.  Pattern Anal.  Machine Intell.  26, 1408–1423 (2004).  57.  Osadchy, M., LeCun, Y. & Miller, M. Synergistic face detection and pose estimation with energy-based models.  J. Mach.  Learn.  Res. 8, 1197–1215 (2007).  58.  Tompson, J., Goroshin, R. R., Jain, A., LeCun, Y. Y. & Bregler, C. C. Efficient object human-level performance in face verification.  In Proc. Conference on Computer Vision and Pattern Recognition 1701–1708 (2014).  60.  Hadsell, R. et al.  Learning long-range vision for autonomous off-road driving.  J. Field Robot.  26, 120–144 (2009).  61.  Farabet, C., Couprie, C., Najman, L. & LeCun, Y. Scene parsing with multiscale Dropout: a simple way to prevent neural networks from overfitting.  J. Machine Learning Res. 15, 1929–1958 (2014).  63.  Sermanet, P. et al.  Overfeat: integrated recognition, localization and detection accurate object detection and semantic segmentation.  In Proc. Conference on Computer Vision and Pattern Recognition 580–587 (2014).  65.  Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale network processor with programmable topology.  J. Solid State Circuits 26, 2017–2025 (1991).  67.  Farabet, C. et al.  Large-scale FPGA-based convolutional networks.  In Scaling up Machine Learning: Parallel and Distributed Approaches (eds Bekkerman, R., Bilenko, M. & Langford, J.) 399–419 (Cambridge Univ.  Press, 2011).  68.  Bengio, Y. Learning Deep Architectures for AI (Now, 2009).  69.  Montufar, G. & Morton, J. When does a mixture of products contain a product of mixtures?  J. Discrete Math. 29, 321–347 (2014).  70.  Montufar, G. F., Pascanu, R., Cho, K. & Bengio, Y. On the number of linear regions of deep neural networks.  In Proc.  Advances in Neural Information Processing Systems 27 2924–2932 (2014).  71.  Bengio, Y., Ducharme, R. & Vincent, P. A neural probabilistic language model.  In Proc.  Advances in Neural Information Processing Systems 13 932–938 (2001).  This paper introduced neural language models, which learn to convert a word symbol into a word vector or word embedding composed of learned semantic features in order to predict the next word in a sequence.  72.  Cho, K. et al.  Learning phrase representations using RNN encoder-decoder 2 8 M A Y 2 0 1 5 | V O L 5 2 1 | N  A T U R E | 4 4 3 for statistical machine translation.  In Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734 (2014).  73.  Schwenk, H. Continuous space language models.  Computer Speech Lang. 21, natural language with recursive neural networks.  In Proc.  International Conference on Machine Learning 129–136 (2011).  75.  Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J. Distributed representations of words and phrases and their compositionality.  In Proc.  Advances in Neural Information Processing Systems 26 3111–3119 (2013).  76.  Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly gradient descent is difficult.  IEEE Trans.  Neural Networks 5, 157–166 (1994).  79.  Hochreiter, S. & Schmidhuber, J. Long short-term memory.  Neural Comput.  9, 1735–1780 (1997).  This paper introduced LSTM recurrent networks, which have become a crucial ingredient in recent advances with recurrent networks because they are good at learning long-range dependencies.  80.  ElHihi, S. & Bengio, Y. Hierarchical recurrent neural networks for long-term dependencies.  In Proc.  Advances in Neural Information Processing Systems 8\nlong-term-dependencies (1995).  81.  Sutskever, I. Training Recurrent Neural Networks.  PhD thesis, Univ.  Toronto networks.  In Proc. 30th International Conference on Machine Learning 1310– 1318 (2013).  83.  Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural networks.  In Proc. 28th International Conference on Machine Learning 1017– 1024 (2011).  84.  Lakoff, G. & Johnson, M. Metaphors  We Live By (Univ.  Chicago Press, 2008).  85.  Rogers, T. T. & McClelland, J. L. Semantic Cognition: A Parallel Distributed Processing Approach (MIT Press, 2004).  86.  Xu, K. et al.  Show, attend and tell: Neural image caption generation with visual Conference on Artificial Intelligence and Statistics 448–455 (2009).  93.  Vincent, P., Larochelle, H., Bengio, Y. & Manzagol, P.-A. Extracting and composing robust features with denoising autoencoders.  In Proc. 25th International Conference on Machine Learning 1096–1103 (2008).  94.  Kavukcuoglu, K. et al.  Learning convolutional feature hierarchies for visual recognition.  In Proc.  Advances in Neural Information Processing Systems 23 1090–1098 (2010).  95.  Gregor, K. & LeCun, Y. Learning fast approximations of sparse coding.  In Proc.  International Conference on Machine Learning 399–406 (2010).  96.  Ranzato, M., Mnih, V., Susskind, J. M. & Hinton, G. E. Modeling natural images using gated MRFs.  IEEE Trans.  Pattern Anal.  Machine Intell. 35, 2206–2222 (2013).  97.  Bengio, Y., Thibodeau-Laufer, E., Alain, G. & Yosinski, J. Deep generative stochastic networks trainable by backprop.  In Proc. 31st International Conference on Machine Learning 226–234 (2014).  98.  Kingma, D., Rezende, D., Mohamed, S. & Welling, M. Semi-supervised learning with deep generative models.  In Proc.  Advances in Neural Information Processing Systems 27 3581–3589 (2014).  99.  Ba, J., Mnih, V. & Kavukcuoglu, K. Multiple object recognition with visual 4 4 4 | N  A T U R E | V O L 5 2 1 | 2 8 M A Y 2 0 1 5", "metadata": {"source_file": "NatureDeepReview.pdf", "title": "M REVIEW Deep learning", "authors": ["Pavillon André-Aisenstadt"], "year": "2015", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NatureDeepReview.chunks.json"}}
{"text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. Current approaches to object recognition make essential use of machine learning methods. To improve their performance, we can collect larger datasets, learn more powerful models, and use better techniques for preventing overfitting. Until recently, datasets of labeled images were relatively small — on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and CIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size, especially if they are augmented with label-preserving transformations. For example, the currentbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4]. But objects in realistic settings exhibit considerable variability, so to learn to recognize them it is necessary to use much larger training sets. And indeed, the shortcomings of small image datasets have been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to collect labeled datasets with millions of images.  The new larger datasets include LabelMe  [23], which consists of hundreds of thousands of fully-segmented images, and ImageNet  [6], which consists of over 15 million labeled high-resolution images in over 22,000 categories.  To learn about thousands of objects from millions of images, we need a model with a large learning capacity.  However, the immense complexity of the object recognition task means that this problem cannot be specified even by a dataset as large as ImageNet, so our model should also have lots of prior knowledge to compensate for all the data we don't have.  Convolutional neural networks (CNNs) constitute one such class of models  [16, 11, 13, 18, 15, 22, 26].  Their capacity can be controlled by varying their depth and breadth, and they also make strong and mostly correct assumptions about the nature of images (namely, stationarity of statistics and locality of pixel dependencies).  Thus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have much fewer connections and parameters and so they are easier to train, while their theoretically-best performance is likely to be only slightly worse.  Despite the attractive qualities of CNNs, and despite the relative efficiency of their local architecture, they have still been prohibitively expensive to apply in large scale to high-resolution images.  Luckily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful enough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet contain enough labeled examples to train such models without severe overfitting.  The specific contributions of this paper are as follows: we trained one of the largest convolutional neural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012 competitions [2] and achieved by far the best results ever reported on these datasets.  We wrote a highly-optimized GPU implementation of 2D convolution and all the other operations inherent in training convolutional neural networks, which we make available publicly1.  Our network contains a number of new and unusual features which improve its performance and reduce its training time, which are detailed in Section 3.  The size of our network made overfitting a significant problem, even with 1.2 million labeled training examples, so we used several effective techniques for preventing overfitting, which are described in Section 4.  Our final network contains five convolutional and three fully-connected layers, and this depth seems to be important: we found that removing any convolutional layer (each of which contains no more than 1% of the model's parameters) resulted in inferior performance.  In the end, the network's size is limited mainly by the amount of memory available on current GPUs and by the amount of training time that we are willing to tolerate.  Our network takes between five and six days to train on two GTX 580 3GB GPUs.  All of our experiments suggest that our results can be improved simply by waiting for faster GPUs and bigger datasets to become available.  ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories.  The images were collected from the web and labeled by human labelers using Amazon's Mechanical Turk crowd-sourcing tool.  Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held.  ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories.  In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.  ImageNet consists of variable-resolution images, while our system requires a constant input dimensionality.  Therefore, we down-sampled the images to a fixed resolution of 256 × 256.  Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256×256 patch from the resulting image.  We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel.  So we trained our network on the (centered) raw RGB values of the pixels.  The architecture of our network is summarized in Figure 2.  It contains eight learned layers — five convolutional and three fully-connected.  Below, we describe some of the novel or unusual features of our network's architecture.  Sections 3.1-3.4 are sorted according to our estimation of their importance, with the most important first.  Figure 1: A four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line).  The learning rates for each network were chosen independently to make training as fast as possible.  No regularization of any kind was employed.  The magnitude of the effect demonstrated here varies with network architecture, but networks with ReLUs consistently learn several times faster than equivalents with saturating neurons.  The standard way to model a neuron's output f as a function of its input x is with f(x)  = tanh(x) or f(x)  = (1 + e−x)−1.  In terms of training time with gradient descent, these saturating nonlinearities are much slower than the non-saturating nonlinearity f(x)  = max(0, x).  Following Nair and Hinton [20], we refer to neurons with this nonlinearity as Rectified Linear Units (ReLUs).  Deep convolutional neural networks with ReLUs train several times faster than their equivalents with tanh units.  This is demonstrated in Figure 1, which shows the number of iterations required to reach 25% training error on the CIFAR-10 dataset for a particular four-layer convolutional network.  This plot shows that we would not have been able to experiment with such large neural networks for this work if we had used traditional saturating neuron models.  We are not the first to consider alternatives to traditional neuron models in CNNs.  For example, Jarrett et al.  [11] claim that the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset.  However, on this dataset the primary concern is preventing overfitting, so the effect they are observing is different from the accelerated ability to fit the training set which we report when using ReLUs.  Faster learning has a great influence on the performance of large models trained on large datasets.  A single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks that can be trained on it.  It turns out that 1.2 million training examples are enough to train networks which are too big to fit on one GPU.  Therefore we spread the net across two GPUs.  Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one another's memory directly, without going through host machine memory.  The parallelization scheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers.  This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2.  However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU.  Choosing the pattern of connectivity is a problem for cross-validation, but this allows us to precisely tune the amount of communication until it is an acceptable fraction of the amount of computation.  The resultant architecture is somewhat similar to that of the \"columnar\" CNN employed by Cire¸san et al.  [5], except that our columns are not independent (see Figure 2).  This scheme reduces our top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU.  The two-GPU net takes slightly less time to train than the one-GPU net2.  2The one-GPU net actually has the same number of kernels as the two-GPU net in the final convolutional layer.  This is because most of the net's parameters are in the first fully-connected layer, which takes the last convolutional layer as input.  So to make the two nets have approximately the same number of parameters, we did not halve the size of the final convolutional layer (nor the fully-conneced layers which follow).  Therefore this comparison is biased in favor of the one-GPU net, since it is bigger than \"half the size\" of the two-GPU net.  ReLUs have the desirable property that they do not require input normalization to prevent them from saturating.  If at least some training examples produce a positive input to a ReLU, learning will happen in that neuron.  However, we still find that the following local normalization scheme aids generalization.  Denoting by ai x,y the activity of a neuron computed by applying kernel i at position (x, y) and then applying the ReLU nonlinearity, the response-normalized activity bi x,y is given by the expression where the sum runs over n \"adjacent\" kernel maps at the same spatial position, and N is the total number of kernels in the layer.  The ordering of the kernel maps is of course arbitrary and determined before training begins.  This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels.  The constants k, n, α, and β are hyper-parameters whose values are determined using a validation set; we used k = 2, n = 5, α = 10−4, and β = 0.75.  We applied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).  This scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al.  [11], but ours would be more correctly termed \"brightness normalization\", since we do not subtract the mean activity.  Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively.  We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization3.  Pooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel map.  Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g., [17, 11, 4]).  To be more precise, a pooling layer can be thought of as consisting of a grid of pooling units spaced s pixels apart, each summarizing a neighborhood of size z  × z centered at the location of the pooling unit.  If we set s = z, we obtain traditional local pooling as commonly employed in CNNs.  If we set s < z, we obtain overlapping pooling.  This is what we use throughout our network, with s = 2 and z = 3.  This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme s = 2, z = 2, which produces output of equivalent dimensions.  We generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.  Now we are ready to describe the overall architecture of our CNN.  As depicted in Figure 2, the net contains eight layers with weights; the first five are convolutional and the remaining three are fullyconnected.  The output of the last fully-connected layer is fed to a 1000-way softmax which produces a distribution over the 1000 class labels.  Our network maximizes the multinomial logistic regression objective, which is equivalent to maximizing the average across training cases of the log-probability of the correct label under the prediction distribution.  The kernels of the second, fourth, and fifth convolutional layers are connected only to those kernel maps in the previous layer which reside on the same GPU (see Figure 2).  The kernels of the third convolutional layer are connected to all kernel maps in the second layer.  The neurons in the fullyconnected layers are connected to all neurons in the previous layer.  Response-normalization layers follow the first and second convolutional layers.  Max-pooling layers, of the kind described in Section 3.4, follow both response-normalization layers as well as the fifth convolutional layer.  The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.", "metadata": {"source_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Geoffrey E. Hinton"], "year": "2014", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.chunks.json"}}
{"text": "The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring 3We cannot describe this network in detail due to space constraints, but it is specified precisely by the code and parameter files provided here: Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities between the two GPUs. One GPU runs the layer-parts at the top of the figure while the other runs the layer-parts at the bottom. The GPUs communicate only at certain layers. The network's input is 150,528-dimensional, and the number of neurons in the network's remaining layers is given by 253,440–186,624–64,896–64,896–43,264– 4096–4096–1000. neurons in a kernel map). The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48. The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each. Our neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, this turns out to be insufficient to learn so many parameters without considerable overfitting. Below, we describe the two primary ways in which we combat overfitting. The easiest and most common method to reduce overfitting on image data is to artificially enlarge the dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms of data augmentation, both of which allow transformed images to be produced from the original images with very little computation, so the transformed images do not need to be stored on disk.  In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images.  So these data augmentation schemes are, in effect, computationally free.  The first form of data augmentation consists of generating image translations and horizontal reflections.  We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches4.  This increases the size of our training set by a factor of 2048, though the resulting training examples are, of course, highly interdependent.  Without this scheme, our network suffers from substantial overfitting, which would have forced us to use much smaller networks.  At test time, the network makes a prediction by extracting five 224 × 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the network's softmax layer on the ten patches.  The second form of data augmentation consists of altering the intensities of the RGB channels in training images.  Specifically, we perform PCA on the set of RGB pixel values throughout the ImageNet training set.  To each training image, we add multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian with mean zero and standard deviation 0.1.  Therefore to each RGB image pixel Ixy = [IR xy, IG xy, IB xy]T we add the following quantity: where pi and λi are ith eigenvector and eigenvalue of the 3 × 3 covariance matrix of RGB pixel values, respectively, and αi is the aforementioned random variable.  Each αi is drawn only once for all the pixels of a particular training image until that image is used for training again, at which point it is re-drawn.  This scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination.  This scheme reduces the top-1 error rate by over 1%.  We use dropout in the first two fully-connected layers of Figure 2.  Without dropout, our network exhibits substantial overfitting.  Dropout roughly doubles the number of iterations required to converge.  Figure 3: 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer on the 224×224×3 input images.  The top 48 kernels were learned on GPU 1 while the bottom 48 kernels were learned on GPU 2.  See Section 6.1 for details.  We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005.  We found that this small amount of weight decay was important for the model to learn.  In other words, weight decay here is not merely a regularizer: it reduces the model's training error.  The update rule for weight w was where i is the iteration index, v is the momentum variable, ϵ is the learning rate, and D\n∂L ∂w\nwi the average over the ith batch Di of the derivative of the objective with respect to w, evaluated at wi.  We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01.  We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1.  This initialization accelerates the early stages of learning by providing the ReLUs with positive inputs.  We initialized the neuron biases in the remaining layers with the constant 0.  We used an equal learning rate for all layers, which we adjusted manually throughout training.  The heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate.  The learning rate was initialized at 0.01 and reduced three times prior to termination.  We trained the network for roughly 90 cycles through the training set of 1.2 million images, which took five to six days on two NVIDIA GTX 580 3GB GPUs.  Our results on ILSVRC-2010 are summarized in Table 1.  Our network achieves top-1 and top-5 test set error rates of 37.5% and 17.0%5.", "metadata": {"source_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Geoffrey E. Hinton"], "year": "2014", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.chunks.json"}}
{"text": "The best performance achieved during the ILSVRC2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced from six sparse-coding models trained on different features [2], and since then the best published results are 45.7% and 25.7% with an approach that averages the predictions of two classifiers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24]. We also entered our model in the ILSVRC-2012 competition and report our results in Table 2. Since the ILSVRC-2012 test set labels are not publicly available, we cannot report test error rates for all the models that we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1% (see Table 2). The CNN described in this paper achieves a top-5 error rate of 18.2%. Averaging the predictions of five similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth convolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release (15M images, 22K categories), and then \"fine-tuning\" it on ILSVRC-2012 gives an error rate of 16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 release with the aforementioned five CNNs gives an error rate of 15.3%. The second-best contest entry achieved an error rate of 26.2% with an approach that averages the predictions of several classifiers trained on FVs computed from different types of densely-sampled features [7]. Model Top-1 (val) Top-5 (val) Top-5 (test) SIFT + FVs [7] —\n7 CNNs* 36.7% 15.4% 15.3% Table 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were \"pre-trained\" to classify the entire ImageNet 2011 Fall release. See Section 6 for details. Figure 3 shows the convolutional kernels learned by the network's two data-connected layers. The network has learned a variety of frequency- and orientation-selective kernels, as well as various colored blobs.  Notice the specialization exhibited by the two GPUs, a result of the restricted connectivity described in Section 3.5.  The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific.  This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo a renumbering of the GPUs).  Figure 4: (Left) Eight ILSVRC-2010 test images and the five labels considered most probable by our model.  The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).  (Right) Five ILSVRC-2010 test images in the first column.  The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.  In the left panel of Figure 4 we qualitatively assess what the network has learned by computing its top-5 predictions on eight test images.  Notice that even off-center objects, such as the mite in the top-left, can be recognized by the net.  Most of the top-5 labels appear reasonable.  For example, only other types of cat are considered plausible labels for the leopard.  In some cases (grille, cherry) there is genuine ambiguity about the intended focus of the photograph.  Another way to probe the network's visual knowledge is to consider the feature activations induced by an image at the last, 4096-dimensional hidden layer.  If two images produce feature activation vectors with a small Euclidean separation, we can say that the higher levels of the neural network consider them to be similar.  Figure 4 shows five images from the test set and the six images from the training set that are most similar to each of them according to this measure.  Notice that at the pixel level, the retrieved training images are generally not close in L2 to the query images in the first column.  For example, the retrieved dogs and elephants appear in a variety of poses.  We present the results for many more test images in the supplementary material.  Computing similarity by using Euclidean distance between two 4096-dimensional, real-valued vectors is inefficient, but it could be made efficient by training an auto-encoder to compress these vectors to short binary codes.  This should produce a much better image retrieval method than applying autoencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency to retrieve images with similar patterns of edges, whether or not they are semantically similar.  Our results show that a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning.  It is notable that our network's performance degrades if a single convolutional layer is removed.  For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network.  So the depth really is important for achieving our results.  To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data.  Thus far, our results have improved as we have made our network larger and trained it longer but we still have many orders of magnitude to go in order to match the infero-temporal pathway of the human visual system.  Ultimately we would like to use very large and deep convolutional nets on video sequences where the temporal structure provides very helpful information that is missing or far less obvious in static images.", "metadata": {"source_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["Alex Krizhevsky", "Geoffrey E. Hinton"], "year": "2014", "detected_language": "en", "page_count": 9, "origin_chunk_file": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.chunks.json"}}
{"text": "CRC Press 6000 Broken Sound Parkway, NW Suite 300, Boca Raton, FL 33487 270 Madison Avenue New York, NY 1001 6 2 Park Square, Milton Park Abingdon, Oxon OX14 4RN, UK The publisher offers discounts on this book when ordered in bulk quantities. For more information, write to Special Sales/Professional Marketing at the address below. Neither this book nor any part may be reproduced or transmitted hi any form or by any means, electronic or mechanical, including photocopying, microfilming, and recording, or by any information storage and retrieval system, without permission in writing from the publisher. Expert Systems are one of the first commercial successes of AI and their use in industry, business, education, science, law and medicine has increased greatly over the past few years. As demonstrated by the huge number of successfully deployed expert systems around the world, using expert systems can result in decreased costs and immense savings, reduced downtime and increased quality and throughput. An expert or knowledge based system uses knowledge gained through reasoning and heuristics to solve with a high degree of reliability intractable, complex real world problems. Expert systems are usually used to solve those problems for which algorithmic, polynomial time solutions are not available. In addition to being able to solve knowledge intensive problems that are not easily addressed by conventional software, expert systems provide other highly desirable features which include their declarative, nonprocedural nature, and the ability to provide explanations for their decisions and to work with incomplete or uncertain data. Development in the field has moved beyond first generation expert systems. In second generation expert systems, the knowledge engineering process is viewed more as a modeling activity that entails the construction of a knowledge level model of the system and instantiation of the model with domain specific knowledge. Second generation expert systems are distinguished from their first generation counterparts in that they organize their knowledge base in modules and multilevel structures.", "metadata": {"source_file": "preview-9781000064971_A38593932.pdf", "title": null, "authors": ["Chris Nikolopoulos"], "year": "2021", "detected_language": null, "page_count": 36, "origin_chunk_file": "preview-9781000064971_A38593932.chunks.json"}}
{"text": "For example, they separate control from domain knowledge, employ hybrid representations by combining multiple representation schemes, supply sophisticated semi-automated and automated learning tools to help with knowledge acquisition, and can provide more satisfactory and comprehensive explanations of their actions as a by-product of treating the knowledge engineering process as a modeling activity. Another very important trend in expert systems development is the merging of multiple paradigms, including the connectionist and evolutionary paradigm, with the traditional logic based approach. Multiparadigm hybrid expert systems draw on the strengths of each individual approach to render more powerful systems. The integration of paradigms enables the system to solve certain subtasks that are not amenable to solution by the traditional \"elicit knowledge-represent knowledge\" approach and endows the system with learning capabilities. These new developments in the field warrant that learners and aspiring expert system developers be aware of the options available and be knowledgeable about not only the symbolic approach but also the knowledge level and modeling approach as well as various areas of machine learning. This book provides an introduction to the field of expert/knowledge based systems and covers current and emerging trends and research areas. In addition to the traditional areas of reasoning, knowledge acquisition, verification, validation, knowledge representation and uncertainty, this book covers second generation expert systems, hybrid expert systems and machine learning, including the connectionist and evolutionary paradigms. The book is intended for use in undergraduate or first year graduate level classes on knowledge based systems, as well as for use by computer professionals or engineers who wish to acquire a working knowledge of the field and an understanding of its foundational principles. It is self-contained as much as possible, and the only requirement for thorough understanding is some knowledge of basic probability theory and statistics, boolean logic, and set theory.  There are basically two alternatives for developing an expert system: one can use an expert system shell or a programming environment.  This book caters to both approaches.  The distinction between shells and programming environments is no longer as clear as it was a few years back.  There are many expert system shells that provide sophisticated control languages and interfaces to programming language environments.  This gives the developer the option of bypassing the built-in knowledge representation and inference mechanisms of the shell and the ability to custom-make the system to fit the application, instead of trying to fit the application to the shell.  An introduction to one of the commercially available expert system shells, EXSYS, is included in", "metadata": {"source_file": "preview-9781000064971_A38593932.pdf", "title": null, "authors": ["Chris Nikolopoulos"], "year": "2021", "detected_language": null, "page_count": 36, "origin_chunk_file": "preview-9781000064971_A38593932.chunks.json"}}
{"text": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors. Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data including labeled or unlabeled images (Hinton et al., 2006), sequences of mel-cepstral coefficients that represent speech (Mohamed & Hinton, 2010), bags of words that represent documents (Salakhutdinov & Hinton, 2009), and user ratings of movies (Salakhutdinov et al., 2007). In their conditional form they can be used to model highdimensional temporal sequences such as video or motion capture data (Taylor et al., 2006). Their most important use is as learning modules that are composed to form deep belief nets (Hinton et al., 2006). Images composed of binary pixels can be modeled by an RBM that uses a layer of binary hidden units (feature detectors) to model the higher-order correlations between pixels. If there are no direct interactions between the hidden units and no direct interactions between the visible units that represent the pixels, there is a simple and efficient method called \"Contrastive Divergence\" to learn a good set of feature detectors from a set of training images (Hinton, 2002). We start with small, random weights on the symmetric connections between each pixel i and each feature detector j. Then we repeatedly update each weight, wij, using the difference between two measured, pairwise correlations where ǫ is a learning rate, data is the frequency with which visible unit i and hidden unit j are on together when the feature detectors are being driven by images from the training set and recon is the corresponding frequency when the hidden units are being driven by reconstructed images.  A similar learning rule can be used for the biases.  where bj is the bias of j and vi is the binary state of pixel i.", "metadata": {"source_file": "reluICML.pdf", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "year": "2010", "detected_language": "en", "page_count": 8, "origin_chunk_file": "reluICML.chunks.json"}}
{"text": "Once binary states have been chosen for the hidden units we produce a \"reconstruction\" of the training image by setting the state of each pixel to be 1 with probability The learned weights and biases implicitly define a probability distribution over all possible binary images via the energy, E(v, h), of a joint configuration of the RBMs were originally developed using binary stochastic units for both the visible and hidden layers (Hinton, 2002). To deal with real-valued data such as the pixel intensities in natural images, (Hinton & Salakhutdinov, 2006) replaced the binary visible units by linear units with independent Gaussian noise as first suggested by (Freund & Haussler, 1994). The energy function then becomes: It is possible to learn the variance of the noise for each visible unit but this is difficult using binary hidden units. In many applications, it is much easier to first normalise each component of the data to have zero mean and unit variance and then to use noise-free reconstructions, with the variance in equation 6 set to 1. The reconstructed value of a Gaussian visible unit is then equal to its top-down input from the binary hidden units plus its bias. We use this type of noise-free visible unit for the models of object and face images described later. To allow each unit to express more information, (Teh & Hinton, 2001) introduced binomial units which can be viewed as N separate copies of a binary unit that all share the same bias and weights. A nice sideeffect of using weight-sharing to synthesize a new type of unit out of binary units is that the mathematics underlying learning in binary-binary RBM's remains unchanged. Since all N copies receive the same total input, they all have the same probability, p, of turning on and this only has to be computed once. The expected number that are on is Np and the variance in this number is Np(1 −p). For small p, this acts like a Poisson unit, but as p approaches 1 the variance becomes small again which may not be desireable.  Also, for small values of p the growth in p is exponential in the total input.  This makes learning much less stable Figure 1.  A comparison of three different ways to model rectified linear units.  The red curve shows the expected value of the sum of an infinite number of binary units with each having a bias one less than the previous one.  The blue curve is the approximation log(1+exp(x)).  The green curve is the expected value of a rectified linear unit with added Gaussian noise as described in section 2.  The red and blue curves are virtually indistinguishable.  A small modification to binomial units makes them far more interesting as models of real neurons and also more useful for practical applications.  We make an infinite number of copies that all have the same learned weight vector w and the same learned bias, b, but each copy has a different, fixed offset to the bias.  If the offsets are −0.5, −1.5, −2.5, ... the sum of the probabilities of the copies is extremely close to having a closed form (figure 1): A drawback of giving each copy a bias that differs by a fixed offset is that the logistic sigmoid function needs to be used many times to get the probabilities required for sampling an integer value correctly.  It is possible, however, to use a fast approximation in which the sampled value of the rectified linear unit is not constrained to be an integer.  Instead it is given by max(0, x+N(0, σ(x)) where N(0, V ) is Gaussian noise with zero mean and variance V .  We call a unit that uses this approximation a N oisyRectified Linear U nit (NReLU) and this paper shows that NReLUs work better than binary hidden units for several different tasks.  (Jarrett et al., 2009) have explored various rectified nonlinearities (including the max(0, x) nonlinearity, which they refer to as \"positive part\") in the context of convolutional networks and have found them to improve discriminative performance.  Our empirical results in sections 5 and 6 further support this observation.  We also give an approximate probabilistic interpretation for the max(0, x) nonlinearity, further justifying their use.  NReLU's have some interesting mathematical properties (Hahnloser et al., 2003), one of which is very useful for object recognition.  A major consideration when designing an object recognition system is how to make the output invariant to properties of the input such as location, scale, orientation, lighting etc.  Convolutional neural networks are often said to achieve translation invariance but in their pure form they actually achieve something quite different.  If an object is translated in the input image, its representation in a pool of local filters that have shared weights is also translated.  So if it can be represented well by a pattern of feature activities when it is in one location, it can also be represented equally well by a translated pattern of feature activities when it is another location.  We call this translation equivariance: the representation varies in the same way as the image.  In a deep convolutional net, translation invaraince is achieved by using subsampling to introduce a small amount of translation invariance after each layer of filters.  Binary hidden units do not exhibit intensity equivariance, but rectified linear units do, provided they have zero biases and are noise-free.  Scaling up all of the intensities in an image by α > 0 cannot change whether a zero-bias unit receives a total input above or below zero.  So all of the \"off\" units remain offand the remainder all increase their activities by a factor of α.  This stays true for many layers of rectified linear units.  When deciding whether two face images come from the same person, we make use of this nice property of rectified linear units by basing the decision on the cosine of the angle between the activities of the feature detectors in the last hidden layer.  The feature vectors are intensity equivariant and the cosine is intensity invariant.  The type of intensity invariance that is important for recognition cannot be achieved by simply dividing all the pixel intensities by their sum.  This would cause a big change in the activities of feature detectors that attend to the parts of a face when there is a bright spot in the background.  The stereo-pair images are subsampled from their original resolution of 108 × 108 × 2 to 32 × 32 × 2 to speed up experiments.  They are normalized to be zero-mean and divided by the average standard deviation of all the pixels in all the training images.  There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).  We hold out 58,320 cases from the training set and use them as a validation set for selecting the model architecture (number of hidden units and number of layers) and for early stopping.  The validation set is created by taking all 9,720 training images of a single (randomly selected) object instance from each of the five object classes, and an equal number of randomly selected images from the \"None\" class.  To train a classifier we use a similar approach to (Larochelle et al., 2007).  We first greedily pre-train two layers of features, each as an RBM using CD.  Then we use multinomial regression at the top-most hidden layer to predict the label and discriminatively fine-tune the parameters in all layers of the classifier (see figure 3).  We have tried 1000, 2000 and 4000 units for the first hidden layer, and 1000 and 2000 units for the second one.  Using more units always gave better classification results, so the architecture with the best results have 4000 units in the first layer and 2000 in the second.  We suspect that the results will be even better with more hidden units.  In all cases, the pixels are represented by Gaussian units (Hinton & Salakhutdinov, 2006) and the hidden units are either NReLUs or stochastic binary units.  Pretraining is done for 300 epochs (in both layers), using mini-batches of 100 training examples with a learning rate of 10−3 applied to the average per-case CD update, along with momentum (Hinton et al., 2006).  Figure 4 shows a subset of features learned by the firstlevel RBM with 4000 NReLUs in the hidden layer.  Many of these are Gabor-like filters, so NReLUs seem capable of learning qualitatively sensible features from images.  The classification results (section 5.2) show that they are quantitatively sensible as well.  Figure 5 Figure 3.  Network architecture used for the JitteredCluttered NORB classification task.  We greedily pre-train two hidden layers of NReLUs as RBMs.  The class label is represented as a K-dimensional binary vector with 1-of-K activation, where K is the number of classes.  The classifier computes the probability of the K classes from the second layer hidden activities h2 using the softmax function.  Fraction of training images on which a hidden unit is active Figure 5.  Histogram of NReLUs binned according to how often they are \"active\" (i.e. has a value above zero) on training images, as computed on Jittered-Cluttered NORB for an RBM with 4000 NReLUs in the hidden layer.  is a histogram that shows how often the hidden units in this RBM have values above zero on the training set.  If a unit is always \"active\", then it would end up in the rightmost bin of the histogram.  Note that an always-active unit is purely linear since it never gets rectified.  As the histgroam shows, there are no such units in this model.  There is some variety in how often the units are active.  We have looked at the features that correspond to each of the three peaks in the histogram: the peak near 0.2 on the x-axis are Gabor-like filters, while the peak near 0.6 are point filters.  The smaller peak between 0.4 and 0.5 corresponds mostly to more global filters.  Table 1 lists the test set error rates of classifiers with a single hidden layer, using either binary units or NReLUs, with and without pre-training.  NReLUs  outFigure 4.  A subset of the features learned by an RBM on images from Jittered-Cluttered NORB.  The RBM has 4000 NReLUs in the hidden layer, and those shown are the 216 features with the highest L2 norm.  Sorting by L2 norm tends to pick features with well-defined Gabor-like weight patterns.  Only about 25% of the features are Gabor-like.  The rest consist of filters with more global weight patterns, as well as \"point\" filters that copy pixels to the hidden units.  perform binary units, both when randomly initialized and when pre-trained.  Pre-training helps improve the performance of both unit types.  But NReLUs without pre-training are better than binary units with pretraining.  Table 2 lists the results for classifiers with two hidden layers.  Just as for single hidden layer classifiers, NReLUs outperform binary units regardless of whether greedy pre-training is used only in the first layer, in both layers, or not at all.  Pre-training improves the results: pre-training only the first layer and randomly initializing the second layer is better than randomly initialized both.  Pre-training both layers gives further improvement for NReLUs but not for binary units.  For comparison, the error rates of some other models are: multinomial regression on pixels 49.9%, Gaussian kernel SVM 43.3%, convolutional net 7.2%, convolutional net with an SVM at the top-most hidden layer 5.9%.  The last three results are from (Bengio & LeCun, 2007).  Our results are worse than that of convolutional nets, but 1) our models use heavily subsampled images, and 2) convolutional nets have knowledge of image topology and approximate translation invariance hard-wired into their architecture.  Table 1.  Test error rates for classifiers with 4000 hidden units trained on 32 × 32 × 2 Jittered-Cluttered NORB images.  Table 2.  Test error rates for classifiers with two hidden layers (4000 units in the first, 2000 in the second), trained on 32 × 32 × 2 Jittered-Cluttered NORB images.  Layer 1 Layer 2 NReLU Binary pre-trained?  pre-trained?  No  No 17.6% 23.6%  Yes  No 16.5% 18.8%  Yes Yes 15.2% 18.8% The prediction task for the Labeled Faces in the Wild (LFW) dataset is as follows: given two face images as input, predict whether the identities of the faces are the same or different.  The dataset contains colour faces of public figures collected from the web using a frontal-face detector.  The bounding box computed by the face detector is used to approximately normalize the face's position and scale within the image.  Some examples from the dataset are shown in figure 6.  For details see (Huang et al., 2007).  The task requires a binary classifier with two sets of inputs (the two faces).  If we stitch the two inputs together and treat the result as one extended input vector, the classifier's output will depend on the order in which the inputs are stitched.  To make the classifier symmetric with respect to the inputs, we use a siamese architecture (Chopra et al., 2005).  The idea is to learn a function that takes a single face as input and computes some feature vector from it.  Given a pair of faces, this function is applied to both faces separately, and the two corresponding feature vectors are combined using a fixed, symmetric function into a single representation which is invariant to input order.  The probability of the two faces being the same person is computed as output from this representation.  The entire system, including the feature extractor replicated over the two faces, can be learned jointly.  Here we choose the feature extractor to be a fullyconnected feedforward layer of NReLUs, pre-trained as an RBM.  We use cosine distance as the symmetric function that combines the two feature vectors.  Cosine distance is invariant to rescaling of its inputs, which when combined with the equivariance of NReLUs makes the entire model analytically invariant to rescaling of the pixels by a positive scalar.  The invariance holds regardless of the number of layers of NReLUs, so it is possible to train deep architectures with this property.  In order to make the feature extractor exactly equivariant, we do not use biases into the hidden units.  Figure 7 shows the architecture of our face verification model.  LFW images are of size 250×250 (×3 colour channels) with the face in the centre and a lot of background surrounding it.  Recently it has been found that humans are able to get 94.27% accuracy on the LFW task even when the centre of the image is masked (Kumar et al., 2009).  To prevent background information from artificially inflating the results, we only use a 144 × 144 window from the centre.  The images are then rotated and scaled such that the coordinates of the eyes are the same across all images.  We further subsample this window to 32×32 (×3 channels).  The same image normalization procedure used for Jittered-Cluttered NORB is applied here as well.  LFW contains 13,233 images of 5,749 people.  For the purposes of reporting results, the designers of LFW have pre-defined 10 splits of the dataset for 10-fold cross validation, each containing 5,400 training pairs and 600 test pairs.  The number of \"same\" and \"differFigure 7.  Siamese network used for the Labeled Faces in the Wild task.  The feature extractor FW contains one hidden layer of NReLUs pre-trained as an RBM (on single faces) with parameters W. FW is applied to the face images IA and IB, and the cosine distance d between the resulting feature vectors FW(IA) and FW(IB) is computed.  The probability of the two faces having the same identity is then computed as Pr(\"Same\")  =\n1+exp(−(wd+b)) where w and b are scalar learnable parameters.  ent\" cases are always equal, both for training and test sets.  The identities of the people in the training and test sets are always kept disjoint, so at test time the model must predict on unseen identities.  We first pretrain a layer of features as an RBM using all 10,800 single faces in the training set of each split, then plug it into the siamese architecture in figure 7 and discriminatively fine-tune the parameters on pairs of faces.  As before, during pre-training pixels are Gaussian units, and the hidden units are either NReLUs or stochastic binary units.  Figure 8 shows 100 of the 4000 features learned by an RBM on 32 × 32 colour images with NReLUs in the hidden layer.  Like the NORB model in section 5.1, this model is also pre-trained for 300 epochs on mini-batches of size 100 with a learning rate of 10−3 and momentum.  The model has learned detectors for parts of faces like eyes, nose, mouth, eye brows etc.  Some features detect the boundary of the face.  There is a mix of localized filters and more global ones that detect more than just a single part of the face.  The histogram in figure 9 shows how often the units in this RBM turn on for the faces in LFW.  Unlike the NORB model, here the histogram has only one peak.  In particular, there are almost no point filters in this model.  During discriminative fine-tuning, we use a subset of the Pubfig face dataset (Kumar et al., 2009) as a validation set for selecting the model architecture and for Figure 8.  A subset of the features learned by an RBM on 32×32 colour images from LFW.  The RBM has 4000 NReLUs in the hidden layer, and shown above are the 100 features with the highest L2 norm.  Fraction of training images on which a hidden unit is active Figure 9.  Histogram of NReLUs binned according to how often they have a value above zero on single face images in LFW for an RBM with 4000 NReLUs in the hidden layer.  early stopping.  This subset, called the \"development set\" by the creators of Pubfig, do not contain any identities that are in LFW.  As explained before, after pre-training a single layer of features as an RBM, we insert it into the siamese architecture in figure 7 and discriminatively fine-tune the parameters.  We have tried models with 1000, 2000, 4000 and 8000 units in the hidden layer.  The difference in accuracy is small between 4000 and 8000 units – all the results in this section are for 4000 units.  The rules of LFW specify that a model's accuracy must be computed using ten-fold cross validation using the ten pre-specified splits of the dataset.  To speed up experiments, we merge two splits into one and perform five-fold cross validation.  Table 3 lists the average accuracy of various models, along with the standard deviations.  Models using NReLUs seem to be more accurate, but the standard deviations are too large to draw firm conclusions.  The two current best LFW results are 0.8683 ± 0.0034  (Wolf et al., 2009), and 0.8529±0.0123 (Kumar et al., 2009).  The former uses a commercial automatic face alignment system to normalize the faces, while the latter uses additional labels (collected manually) that describe the face, such as ethnicity, sex, age etc.  Such enhancements can be applied to our model as well, and they are likely to increase accuracy significantly.  These results may also be benefiting from the background pixels around the face, which we have (mostly) removed here.  We have shown that NReLUs work well for discrimination, but they are also an interesting way of modeling the density of real-valued, high-dimensional data.  A standard way to do this is to use a mixture of diagonal Gaussians.  Alternatively we can use a mixture of factor analysers.  Both of these models are exponentially inefficient if the data contains componential structure.  Consider, for example, images of pairs of independent digits.  If a mixture model for single digit images needs N components, a single mixture model of pairs of digits needs N 2 components.  Fortunately, this exponential growth in the number of components in the mixture can be achieved with only linear growth in the number of latent variables and quadratic growth in the number of parameters if we use rectified linear hidden units.  Consider using rectified linear units with zero bias to model data that lies on the surface of a unit hypersphere.  Each rectified linear unit corresponds to a plane through the centre of the hypersphere.  It has an activity of 0 for one half of the hypersphere and for the other half its activity increases linearly with distance from that plane.  N units can create 2N regions on the surface of the hypersphere3.  As we move around within each of these regions the subset of units that are non-zero does not change so we have a linear model, but it is a different linear model in every region.  The mixing proportions of the exponentially many linear models are defined implicitly by the same parameters as are used to define p(v|h) and, unlike a directed model, the mixing proportions are hard to compute explicitly (Nair & Hinton, 2008).  This is a much better way of implementing an exponentially large mixture of linear models with shared latent variables than the method described in (Hinton et al., 1999) which uses directed linear models as the components of the mixture and a separate sigmoid belief net to decide which hidden units should be part of the current linear model.  In that model, it is hard to infer the values of the binary latent variables and there can be jumps in density at the boundary between two linear regions.  A big advantage of switching between linear models at the point where a hidden unit receives an input of exactly zero is that it avoids discontinuities in the modeled probability density.  We showed how to create a more powerful type of hidden unit for an RBM by tying the weights and biases of an infinite set of binary units.  We then approximated these stepped sigmoid units with noisy rectified linear units and showed that they work better than binary hidden units for recognizing objects and comparing faces.  We also showed that they can deal with large intensity variations much more naturally than binary units.  Finally we showed that they implement mixtures of undirected linear models (Marks & Movellan, 2001) with a huge number of components using a modest number of parameters.  Chopra, S., Hadsell, R., and LeCun, Y. Learning a similarity metric discriminatively, with application to face verification.  In CVPR, pp.  539–546, Washington, DC, USA, 2005.  IEEE Computer Society.  Freund, Y. and Haussler, D. Unsupervised learning of distributions on binary vectors using two layer networks.  Technical report, Santa Cruz, CA, USA, 1994.  Hahnloser, Richard H. R., Seung, H. Sebastian, and Slotine, Jean-Jacques.  Permitted and forbidden sets in symmetric threshold-linear networks.  Neural Computation, 15(3):621–638, 2003.  ISSN <PHONE>.  Hinton, G. E., Osindero, S., and Teh, Y. A fast learning algorithm for deep belief nets.  Neural Computation, 18: 1527–1554, 2006.  Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. What is the best multi-stage architecture for object recognition?  In Proc.  International Conference on Computer Vision (ICCV'09).  IEEE, 2009.  Kumar, N., Berg, A. C., Belhumeur, P. N., and Nayar, S. K. Attribute and simile classifiers for face verification.  In International Conference on Computer Vision, 2009.  Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio., Y. An empirical evaluation of deep architectures on problems with many factors of variation.  In ICML, pp.  473–480, 2007.  LeCun, Y., Huang, F. J., and Bottou., L. Learning methods for generic object recognition with invariance to pose and lighting.  In CVPR, Washington, D.C., 2004.  Marks, T. K. and Movellan, J. R. Diffusion networks, products of experts, and factor analysis.  Technical Report UCSD MPLab TR 2001.02, 2001.  Salakhutdinov, R. and Hinton, G. E. Replicated softmax: an undirected topic model.  In Advances in Neural Information Processing Systems 22, 2009.  Salakhutdinov, R., Mnih, A., and Hinton, G. E. Restricted Boltzmann machines for collaborative filtering.  In Proceedings of the International Conference on Machine Learning, volume 24, pp.  791–798, 2007.  Taylor, G. W., Hinton, G. E., and Roweis, S. Modeling human motion using binary latent variables.  In Advances in Neural Information Processing Systems 19, Cambridge, MA, 2006.  MIT Press.  Teh, Y.W. and Hinton, G. E. Rate-coded restricted boltzmann machines for face recognition.  In Advances in Neural Information Processing Systems, volume 13, 2001.  Wolf, L., Hassner, T., and Taigman, Y. Similarity scores based on background samples.  In Asian Conference on Computer Vision, 2009.", "metadata": {"source_file": "reluICML.pdf", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "authors": ["Vinod Nair", "Geoffrey E. Hinton"], "year": "2010", "detected_language": "en", "page_count": 8, "origin_chunk_file": "reluICML.chunks.json"}}
{"text": "• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain\n• You may freely distribute the URL identifying the publication in the public portal. If the publication is distributed under the terms of Article 25fa of the Dutch Copyright Act, indicated by the \"Taverne\" license above, please follow below link for the End User Agreement: <URL> Take down policy If you believe that this document breaches copyright please contact us at:\nproviding details and we will investigate your claim. THE APPLICATION OF EXPERT SYSTEM: A REVIEW OF RESEARCH AND APPLICATIONS\nC. F. Tan1, L. S. Wahidin1, S. N. Khalil1, N. Tamaldin1, J. Hu2 and G.W. M. Rauterberg2 1Integrated Design Research Group, Centre for Advanced Research on Energy, Universiti Teknikal Malaysia Melaka, Durian Tunggal, Melaka, Malaysia 2Designed Intelligence Group, Department of Industrial Design, Eindhoven", "metadata": {"source_file": "tanappli2016.pdf", "title": null, "authors": ["Tan", "C. F.", "S. N.", "Tamaldin", "N.", "Hu", "G. W. M.", "Download"], "year": "2025", "detected_language": "en", "page_count": 7, "origin_chunk_file": "tanappli2016.chunks.json"}}
{"text": "Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e. Since Valiant's pioneering paper (1984), interest has flourished in the so-called distribution- free orprobably approximately correct (PAC) model of learning. In this model, the learner tries to identify an unknown concept based on randomly chosen examples of the concept. Examples are chosen according to a fixed but unknown and arbitrary distribution on the space of instances. The learner's task is to find an hypothesis or prediction rule of his own that correctly classifies new instances as positive or negative examples of the concept. With high probability, the hypothesis must be correct for all but an arbitrarily small fraction of the instances. Often, the inference task includes a requirement that the output hypothesis be of a specified form. In this paper, however, we will instead be concerned with a representation-independent model of learning in which the learner may output any hypothesis that can be used to classify instances in polynomial time.  A class of concepts is learnable (or strongly learnable) if there exists a polynomial-time algorithm that achieves low error with high confidence for all concepts in the class.  A weaker model of learnability, called weak learnability, drops the requirement that the learner be able to achieve arbitrarily high accuracy; a weak learning algorithm need only output an hypothesis that performs slightly better (by an inverse polynomial) than random guessing.  The notion of weak learnability was introduced by Kearns and Valiant (1988; 1989) who left open the question of whether the notions of strong and weak learnability are equivalent.  This question was termed the hypothesis boosting problem since showing the notions are equivalent requires a method for boosting the low accuracy of a weak learning algorithm's hypotheses.  Kearns (1988), considering the hypothesis boosting problem, gives a convincing argument discrediting the natural approach of trying to boost the accuracy of a weak learning algorithm by running the procedure many times and taking the \"majority vote\" of the output hypoth- eses.  Also, Kearns and Valiant (1989) show that, under a uniform distribution on the instance space, monotone Boolean functions are weakly, but not strongly, learnable.  This shows that strong and weak learnability are not equivalent when certain restrictions are placed on the instance space distribution.  Thus, it did not seem implausible that the strong and weak learning models would prove to be inequivalent for unrestricted distributions as well.  Nevertheless, in this paper, the hypothesis boosting question is answered in the affirma- tive.  The main result is a proof of the perhaps surprising equivalence of strong and weak learnability.  This result may have significant applications as a tool for proving that a concept class is learnable since, in the future, it will suffice to find an algorithm correct on only, say, 51% of the instances (for all distributions).  Alternatively, in its negative contmpositive form, the result says that if a concept class cannot be learned with accuracy 99.9 %, then we cannot hope to do even slightly better than guessing on the class (for some distribution).  The proof presented here is constructive; an explicit method is described for directly converting a weak learning algorithm into one that achieves arbitrary accuracy.  The con- struction uses filtering to modify the distribution of examples in such a way as to force the weak learning algorithm to focus on the harder-to-learn parts of the distribution.  Thus, the distribution-free nature of the learning model is fully exploited.  An immediate corollary of the main result is the equivalence of strong and group learna- bility.  A group-learning algorithm need only output an hypothesis capable of classifying large groups of instances, all of which are either positive or negative.  The notion of group learnability was considered by Kearns, Li, Pitt and Valiant (1987), and was shown to be equivalent to weak learnability by Kearns and Valiant (1989).  The result also extends those of Haussler, Kearns, Littlestone and Warmuth (1988) which prove the equivalence of numer- ous relaxations and variations on the basic PAC-learning model; both weak and group learna- bility are added to this class of equivalent learning models.  The relevance of the main result to a number of other learning models is also considered in this paper.  An interesting and unexpected consequence of the construction is a proof that any strong learning algorithm outputting hypotheses whose length (and thus whose time to evaluate) depends on the allowed error e can be modified to output hypotheses of length only poly- nomial in log(I/e).  Thus, any learning algorithm can be converted into one whose ouptut hypotheses do not become significantly more complex as the error tolerance is lowered.  Put in other terms, this bound implies that a sequence of labeled examples of a learnable concept can, in a sense, be efficiently compressed into a far more compact form--that is, into a rule or hypothesis consistent with the labels of the examples.  In particular, it is shown that a sample of size m can be compressed into a rule of size only poly-logarithmic in m.  In fact, in the discrete case, the size of the output hypothesis is entirely independent of m. This provides a partial converse to Occam's Razor, a result of Blumer, Ehrenfeucht, Haussler and Warmuth (1987) stating that the existence of such a compression algorithm implies the learnability of the concept class.  This also complements the results of Board and Pitt (1990) who also provide a partial converse to Occam's Razor, but of a somewhat different flavor.  Finally, this result yields a strong bound on the sample size needed to learn a discrete concept class.  This bound on the size of the output hypothesis also implies the hardness of learning any concept class not evaluatable by a family of small circuits.  For example, this shows that pattern languages--a class of languages considered previously by Angluin (1980) and others--are unlearnable assuming only that NP/poly ;~ P/poly.  This is the first representation- independent hardness result not based on cryptographic assumptions.  The bound also shows that, for any function not computable by polynomial-size circuits, there exists a distribution on the function's domain over which the function cannot be even roughly approximated by a family of small circuits.  In addition to the bound on hypothesis size, the construction implies a set of general bounds on the dependence on e of the time, sample and space complexity needed to efficiently learn any learnable concept class.  Most surprising is a proof that there exists for every learnable concept class an efficient algorithm requiring space only poly-logarithmic in 1/e.  Because the size of the sample needed to learn with this accuracy is in general f~(1/e), this means, for example, that far less space is required to learn than would be necessary to store the entire sample.  Since most of the known learning algorithms work in exactly this manner--that is, by storing a large sample and finding an hypothesis consistent with it-- this implies a dramatic savings of memory for a whole class of algorithms (though possibly at the cost of requiring a larger sample).  Such general complexity bounds have implications for the on-line learning model as well.  In this model, the learner is presented one instance at a time in a series of trials.  As each is received, the learner tries to predict the true classification of the new instance, attempting to minimize the number of mistakes, or prediction errors.  Translating the bounds described above into the on-line model, it is shown that, for every learnable concept class, there exists an on-line algorithm whose space requirements are quite modest in comparison to the number of examples seen so far.  In particular, the space needed on the first m trials is only poly-logarithmic in m. Such space efficient on-line algo- rithms are of particular interest because they capture the notion of an incremental algorithm forced by its limited memory to explicitly generalize or abstract from the data observed.  Also, these results on the space-efficiency of batch and on-line algorithms extend the work of others interested in this problem, including Boucheron and Sallantin (1988), Floyd (1989), and Haussler (1988).  In particular, these results solve an open problem proposed by Haussler, Littlestone and Warmuth (1987).  An interesting bound is also derived on the expected number of mistakes made on the first m trials.  It is shown that, if a concept class is learnable, then there exists an on-line algorithm for the class for which this expectation is bounded by a polynomial in log m. Thus, for large m, we expect an extremely small fraction of the first m predictions to be incorrect.  This result answers another open question given by Haussler, Littlestone and Warmuth (1987), and significantly improves a similar bound given in their paper (as well as their paper with Kearns (1988)) of m s for some constant ot _lCn, and all the concepts in Cn have a common domain Xn.  We assume each instance in Xn has encoded length bounded by a polynomial in n, and we let X = On_>lXn.  Also, we associate with each concept c its size s, typically a measure of the length of c's representation under some encoding scheme on the concepts in C. For example, the concept class C might consist of all functions computed by Boolean formulas.  In this case, Cn is the set of all functions computed by a Boolean formula on n variables, Xn is the set {0, 1} n of all assignments to the n variables, and the size of a concept c in C is the length of the shortest Boolean formula that computes the function c.  The learner is assumed to have access to a source EX of examples.  Each time oracle EX is called, one instance is randomly and independently chosen from X n according to some fixed but unknown and arbitrary distribution D.  The oracle returns the chosen instance v, along with a label indicating the value c(v) of the instance under the unknown target con- cept c ~Cn.  Such a labeled instance is called an example.  We assume EX runs in unit time.  Given access to EX, the learning algorithm runs for a time and finally outputs an hypothesis h, a prediction rule on X,.  In this paper, we make no restrictions on h other than that there exist a (possibly probabilistic) polynomial time algorithm that, given h and an instance v, computes h(v), h's prediction on v. We write Prv~D[Tr(v)] to indicate the probability of predicate a- holding on instances v drawn from Xn according to distribution D.  To accommodate probabilistic hypotheses, we will find it useful to regard a-(v) as a Bernoulli random variable.  For example, Pr[h(v) ;~ c(v)] is the chance that hypothesis h (which may be randomized) will misclassify some particular instance v.  In contrast, the quantity PrveD[h(v) ~ c(v)] is the probability that h will misclassify an instance v chosen at random according to distribution D. Note that this last probability is taken over both the random choice of v, and any random bits used by h.  In general, assuming independence, we have Kearns and Valiant (1989) introduced a weaker form of learnability in which the error e cannot necessarily be made arbitrarily small.", "metadata": {"source_file": "the-strength-of-weak-learnability-3kx3bkg7w2.pdf", "title": "The Strength of Weak Learnability", "authors": ["Robert E. Schapire"], "year": null, "detected_language": "en", "page_count": 31, "origin_chunk_file": "the-strength-of-weak-learnability-3kx3bkg7w2.chunks.json"}}
{"text": "A concept class C is weakly learnable if there exists a polynomial p and an algorithm A such that for all n -> 1, for all target con- cepts c E Cn, for all distributions D on Xn, and for all 0 _ 1/2 - 1/p(n, s) then an assumed weak learning algorithm can be used to find the desired hypothesis; otherwise, an c-close hypoth- esis is computed recursively by calling the subroutine with e set to g-l(e). Unfortunately, this scheme by itself does not quite work due to a technical difficulty: because of the way EX2 and EX3 are constructed, examples may be required from a very small portion of the original distribution. If this happens, the time spent waiting for an example to be chosen from this region may be great. Nevertheless, we will see that this EX1 e- EX hi +-- Learn(a, ~/5, EX1) rl *- e/3 let &a be an estimate of el = PrveD[hl(v) ~ c(v)]: choose a sample sufficiently large that lal - gql - 1 - 5/5 if ~x _ 1 - 5/5 if <CUR> < e- r2 then return h2 defun EX30 { return the first instance v from EX for which hi(v) ~ h2(v) } h3 e-- Learn(a, ~/5, EX3) { ~1 ~- hi(v), b~ ~ h~(,,) if bx = b2 then return bl else return h3(v) } return h difficulty can be overcome by explicitly checking that the errors of hypotheses h~ and h 2 on D are not too small. Figure 2 shows a detailed sketch of the resulting strong learning algorithm Lea rn. The procedure takes an error parameter e and a confidence parameter 6, and is also provided with an examples oracle EX. The procedure is required to return an hypothesis whose error is at most e with probability at least 1 - & In the figure, p is a polynomial and Weak Lea r n(6, EX) is an assumed weak learning procedure that outputs an hypothesis (1/2 is the function 3ix 2 -- 2c¢ 3, and the variable c~ is set to the value g-l(e). Also, the quantities al and ~ are estimates of the errors of hi and h 2 under the given distribution D. These estimates are made with error tolerances 7-1 and 7- 2 (defined in the figure), and are computed in the obvious manner based on samples drawn from EX; the required size of these samples can be determined, for instance, using Chernoff bounds.  The parameters s and n are assumed to be known globally.  Note that Lea r n is a procedure taking as one of its inputs a function (EX) and returning as output another function (h, a hypothesis, which is treated like a procedure).  Furthermore, to simulate new example oracles, Lea rn must have a means of dynamically defining new procedures (as is allowed, for instance, by most Lisp-like languages).  Therefore, in the figure, we have used the somewhat nonstandard keyword defun to denote the definition of a new function; its syntax calls for a name for the procedure, followed by a parenthesized list of arguments, and the body indented in braces.  Static scoping is assumed.  Lea r n works by recursively boosting the accuracy of its hypotheses.  Lea r n typically calls itself three times using the three simulated example oracles described in the preceding section.  On each recursive call, the required error bound of the constructed hypotheses comes closer to 1/2; when this bound reaches 1/2 - 1/p(n, s), the weak learning algorithm WeakLea rn can be used.  The procedure takes measures to limit the run time of the simulated oracles it provides on recursive calls.  When Lea r n calls itself a second time to find h2, the expected number of iterations of EX2 to find an example depends on the error of hi, which is estimated by ~1.  If hi already has the desired accuracy 1 - e, then there is no need to find h 2 and ha since h~ is a sufficiently good hypothesis; otherwise, if al = fl(e), then it can be shown that EX 2 will not loop too long to find an instance.  Similarly, when Lea rn calls itself to find ha, the expected number of iterations of EX3 depends on how often hi and h E disagree, which we will see is in turn a function of the error of h 2 on the original distribution D.  If this error e (which is estimated by ~ is small, then h E is a good hypothesis and is returned by Learn.  Otherwise, it will be shown that EX3 also will not run for too long.  THEOREM 2.  For 0 < e < 1/2 and for 0 < 6 --< 1, the hypothesis returned by calling Lea rn(e, 6, EX) is e-close to the target concept with probability at least 1 - 6.  Proof.  In proving this theorem, we will find it useful to assume that nothing \"goes wrong\" throughout the execution of Lea r n. More specifically, we will say that Lea r n has a good run if every hypothesis returned by WeakLea rn is indeed (1/2 - 1/p(n, s))-close to the target concept, and if every statistical estimate (i.e., of the quantities al and e) is obtained with the required accuracy.  We will then argue inductively on the depth of the recursion that if Lea rn has a good run then the output hypothesis is e-close to the target concept, and furthermore, that the probability of a good run is at least 1 - & Together, these facts clearly imply the theorem's statement.  Weakkearn.  In the general case, by inductive hypothesis, each of the three (or fewer) recursive calls to Lea rn are good runs with probability at least 1 - 6/5.  Moreover, each of the estimates al and ~ has the desired accuracy with probability at least 1 - 6/5.  Thus, the chance of a good run is at least the chance that all five of these events occur, which is at least 1 - 6.  It remains then only to show that on a good run the output hypothesis has error at most e. An easy special case is that fil or <CUR> is found to be smaller than e - r~ or e - r2, respec- tively.  In either case, it follows immediately, due to the accuracy with which a~ and e are assumed to have been estimated, that the returned hypothesis is e-close to the target concept.  Otherwise, in the general case, all three sub-hypotheses must be found and combined.  Let a i be the error of h i under D i. Here, D is the distribution of the provided oracle EX, and D i is the distribution induced by oracle EX i on the ith recursive call (i = 1, 2, 3).  By inductive hypothesis, each a i <_ or.  In the special case that all hypotheses are deterministic, the distributions D1 and D2 can be depicted schematically as shown in Figure 3.  The figure shows the portion of each distribu- tion on which the hypotheses h~ and h2 agree with the target concept c.  For each distribu- tion, the top crosshatched bar represents the relative fraction of the instance space on which hi agrees with c; the bottom striped bar represents those instances on which h2 agrees with c.  Although only valid for deterministic hypotheses, this figure may be helpful for motivating one's intuition in what follows.  Let pi(v)  = Pr[hi(v ) # c(v)] be the chance that some fixed instance v is misclassified by h  i .  (Recall that hypotheses may be randomized, and therefore it is necessary to consider the probability that a particular fixed instance is misclassified.)  Similarly, let q(v)  = Pr[h~(v)  # h2(v)] be the chance that v is classified differently by hx and h2.  Also define w, x, y, and z as follows: Equation (3) is trivial.  To see that Equation (4) holds, note that the chance that the initial coin flip comes up tails is 1/2, and the chance that instance v is the first instance misclassified by hi is D(v)p~(v)/a~.  The case that the coin comes up heads is handled in a similar fashion, as is the derivation of Equation (5).  From Equation (4), we have that (Note that Equation (6) could also have been derived from Figure 3 in the case of deter- ministic hypotheses: ifB is as shown in the figure, then it is not hard to see that y = 2alB and x = 2(1 - al)(1 - a2 - /3).  These imply Equation (6).)  Combining Equations (1), (2) and (6), we see that the values of w and z can be solved for and written explicitly in terms of y, al and a2: Using these values and Equation (5), we are finally ready to compute the error of the out- put hypothesis h: as desired.  The inequalities here follow from the facts that each ai - o~ < 1/2, and that, by Equation (2), y _< a l-  This completes the proof.  •  In this section, we argue that Lea rn runs in polynomial time.  Here and throughout this section, unless stated otherwise, polynomial refers to polynomial in n, s, 1/e and 1/6.  Our approach will be first to derive a bound on the expected running time of the procedure, and then to use a part of the confidence 6 to bound with high-probability the actual running time of the algorithm.  Thus, we will have shown that the procedure is probably fast and correct, completing the proof of .  (Although technically we only show that Lea rn halts probabilistically, using techniques described by Haussler, Kearns, Littlestone and Warmuth (1988), the procedure can easily be converted into a learning algorithm that halts deterministically in polynomial time.)  We will be interested in bounding several quantities.  First, we are of course interested in bounding the expected running time T(e, 5) of Lea rn(e, 5, EX).  This running time in turn depends on the time U(e, 5) to evaluate an hypothesis returned by Lea rn, and on the expected number of examples M(e, t5) needed by Lea rn.  In addition, let t(6), u(6) and m(6) be analogous quantities for Wea kLea r n(6, EX).  By assumption, t, u and m are poly- nomially bounded.  Also, all of these functions depend implicitly on n and s.  As a technical point, we note that the expectations denoted by T and M are taken only over good runs of Lea r n. That is, the expectations are computed given the assumption that every sub-hypothesis and every estimator is successfully computed with the desired accuracy.  By , Learn will have a good run with probability at least 1 - 6.  It is also important to point out that T (respectively, 0 is the expected running time of Lea rn (WeakLea rn) when called with an oracle EX that provides examples in unit time.  Our analysis will take into account the fact that the simulated oracles supplied to Lea rn or WeakLea rn at lower levels of the recursion do not in general run in unit time.  We will see that T, U and M are all exponential in the depth of the recursion induced by calling Lea r n. We therefore begin by bounding this depth.  Let B(e, p) be the smallest integer i for which gi(1/2 - lip) (11/8)(1/2 - x).  This implies that 1/2 - gi(x) >_ (11/8)i(1/2 - x), assuming that x, g(x) .....  gi-l(x) are all at least 1/4.  Thus, gb(ll2 -- lip(n, s)) <- 1/4 if b = Flog11/8(P(n, s)/4)7 .  •  For the remainder of this analysis, we let p = p(n, s) and, where clear from context, letB = B(e,p).  Note that B(g-l(e), p)  = B- 1 fore 1/2 - lip, then Learn returns an hypothesis computed by WeakLearn.  In this case, U(e, 6) = u(6).  Otherwise, the hypothesis returned by Lea rn involves the com- putation of at most three sub-hypotheses.  Thus, When an example is requested of a simulated oracle on one of Lea r n's recursive calls, that oracle must itself draw several examples from its own oracle EX.  For instance, on the third recursive call, the simulated oracle must draw instances until it finds one on which ha and h2 disagree.  Naturally, the running time of Lea rn depends on how many examples must be drawn in this manner by the simulated oracle.  The next lemma bounds this quantity.  LEMMA 3.  Let r be the expected number of examples drawn from EX by any oracle EX i simulated by Lea rn on a good run when asked to provide a single example.  Then r e/4.  Note that the error e of h2 on the original distribution D is w + z.  Thus, using this fact and Equations (1), (2) and (6), we can solve explicitly for w and y in terms of e, aa and a2, and so find that w + y = aa + e - 4aaaz(1 - al) > aa + e - 4aac~(1 - aa) 1 - 2aa 1 - 2aa (7) Regarding e and ot min(f(0), f(o0).  We can assume that e _ • - 2z2 = (3/4 + od2)e; otherwise, if e were smaller than this quantity, then Lea rn would have returned h: rather than going on to compute h3.  Thus, f(0)  = e _> 3e/4, and, using our bound for e and the fact that e = 3c¢ z - 2a 3, Since 4 - 7~ + 2or 2 ~ 1 for u __ a/4 _ e/4.  We conclude w + y _>  e/4, completing the proof.  •  To bound the number of examples needed to estimate al and e, we will make use of the following bounds on the tails of a binomial distribution (Angluin and Valiant, 1979; Hoeffding, 1963).  LEMMA 4.  (Chernoff Bounds) Consider a sequence of m independent Bernoulli trials, each succeeding with probability p. Let S be the random variable describing the total number of successes.  Then for 0 _ 3\"m] (1 + 3\")mp] 1/2 - 1/p, Learn simply calls WeakLearn, SO we have M(e, 6) = m(6).  Otherwise, on each of the recursive calls, the simulated oracle is required to provide M(g-l(e), 6/5) examples.  To provide one such example, the simulated oracle must itself draw at most an average of 4/e examples from EX.  Thus, each recursive call demands at most (4/e) • M(g-l(e), 6/5) examples on average.  In addition, Lea rn requires some examples for making its estimates t]~ and <CUR>.  Using the first bound of , it follows that a sample of size O(log(1/6)/~) suffices for each esti- mate.  Note that 1/p <_ 1/2 - e = 1/2 - g(ot)  = (1/2 - or)(1 + 2c~ - 2ct 2) ___ (3/2)(1/2 - or).  Thus, by our choice of r~ and r2, both estimates can be made using O(p 2 log(1/6)/e 2) examples.  We thus arrive at the recurrent inequality: < 12 ~.36 B-I. m(6/5 B) + c(36 B-1 - 1)p 2 1og(5B/6)_ '] + cp 2 log(l/6) M(e, 6) -- e\n~ (g-l(e))2 ..J 6 2 which clearly implies Equation (9).  The last inequality here follows from the fact that e _ 3(g-l(e)) 2 since g(ot) _<  3or 2 for a _ 0.  •\nProof As in the previous lemmas, the base case that e _ 1/2 - Up is easily handled.  In this case, T(e, 6) = t(6).  Otherwise, Lea rn takes time 3 • T(g-l(e), 6/5) on its three recursive calls.  In addition, Lea rn spends time drawing examples to make the estimates t~ and <CUR>, and overhead time is also spent by the simulated examples oracles passed on the three recursive calls.  A typical example that is drawn from Learn's oracle EX is evaluated on zero, one or two of the previously computed sub-hypotheses.  For instance, an example drawn for the purpose of estimating t~ is evaluated once by hi; an example drawn for the simulated oracle EX3 is evaluated by both h~ and h2.  Thus, Lea rn's overhead time is proportional to the product of the total number of examples needed by t,ea rn and the time it takes to evaluate a sub- hypothesis on one of these examples.  Therefore, the following recurrence holds: T(e, 5) 1/2 - 1/p, the bounds are trivial.  To bound Q, note that the hypothesis returned by t,ea rn is a composite of three (or fewer) hypotheses.  Thus, Finally, to bound S, note that the space required by Lea rn is dominated by the storage of the sub-hypotheses, by their recursive computation, and by the space needed to evaluate them.  Since the sub-hypotheses are computed one at a time, we have: In this section, we describe a modification to the construction of Section 3 that significantly improves Lea r n's time and sample complexity.  In particular, we will improve these com- plexity measures by roughly a factor of 1/e, giving bounds that are linear in 1/e (ignoring log factors).  These improved bounds will have some interesting consequences, described in later sections.  In the original construction of Lea rn, much time and many examples are squandered by the simulated oracles EXi waiting for a desirable instance to be drawn.  showed that the expected time spent waiting is O(1/e).  The modification described below will reduce this to O(1/ot) = O(1/n/ee).  (Here, a = g-l(e) as before.)  Recall that the running time of oracle EX2 depends on the error al of the first sub- hypothesis ha.  In the original construction, we ensured that a~ not be too small by estimating its value, and, if smaller than e, returning ha instead of continuing the normal execution of the subroutine.  Since this approach only guarantees that a~ _ i2(e), there does not seem to be any way of ensuring that EX2 run for o(1/e) time.  To improve EX~s running time then, we will instead modify h~ by deliberately increasing its error.  Ironically, this intentional injection of error will have the effect of improving Lea rn's worst case running time by limiting the time spent by either EX2 or EX3 waiting for a suitable instance.  Specifically, here is how Lea rn is modified.  Call the new procedure Lea rn '.  Following the recursive computation of ha, Lea rn' estimates the error al of hi, although less accurately than Learn.  Let aa be this estimate, and choose a sample large enough that lal - hal pa(v).  This implies that the error of h is at most the error of h', which is bounded by e. Next, we show that Lea rn' runs faster using fewer examples than Lea rn.  We use essen-  tially the same analysis as in Section 3.4.  The following three lemmas are modified ver- sions of Lemmas 3, 5 and 6.  The proofs of the other lemmas apply immediately to Learn' with little or no modification, and so are ommitted.  LEMMA 8.  Let r be the expected number of examples drawn from EX by any oracle EXi simulated by Learn' on a good run when asked to provide a single example.  Then r cd2, r is at most 2/or in this case.  Finally, to bound the number of iterations of EX3, we will shown that w + y >_ a/4 using Equation (7) as in the original proof.  To lower bound w + y, we find the minimum of the last formulafof Equation (7) (with al replaced by a; of course) on the interval [od2, a].  As noted previously, the functionfmust achieve its minimum at one endpoint of the inter- val.  We assume as in the original proof that e _> (3/4 + a/2)e.  It was previously shown thatf(a) _ od4, and, by a similar argument, we can showf(a/2) _> od2 + oP + a2/4 (1 - or) ___ od2.  This completes the proof.  •\nLEMMA 9.  On a good run, the expected number of examples M(e, 6) needed by Lea rn' (e, 6, EX) is Proof The proof is nearly the same as for .  In addition to incorporating the superior bound given by on the number of examples needed by the simulated oracles, we must also consider the number of examples needed to estimate al and e.  The first, al, can be estimated using a sample of size O(log(1/6)/a z) = O(log(1/6)/e); this can be derived from the first bound of , and by noting that e = g(a) ___ 3a 2 for c~ > 0.  By estimating e in a slightly different manner, we can also achieve a better bound on the sam- ple size needed.  Specifically, we can choose a sample large enough that, with probability\n- 6/5,~ _ e - 72ife _>  e. Such an estimate has all of the properties needed by Lea rn ', but only requires a sample of size O(p 2 log(1/6)/e) as can be derived using the second and third bound of .  (See Haussler, Keams, Littlestone and Warmuth (1988) for a detailed example of this sort of calculation.)  Thus, we arrive at the recurrence T(e, 6) = 0  ~3 B • t(6/5 s) + 108 B- u(6/5 B) • (p2 log(5S/6) + m(6/5s))~ .  An immediate consequence of concerns group learnability.  In the group learn- ing model, the learner produces a hypothesis that need only correctly classify large groups of instances, all of which are either positive or negative examples.  Kearns and Valiant (1989) prove the equivalence of group learning and weak learning.  Thus, by , group learning is also equivalent to strong learning.  Much of the PAC-learning research has been concerned with the form or representation of the hypotheses output by the learning algorithm.  Clearly, the construction described in Section 3 does not in general preserve the form of the hypotheses used by the weak learning algorithm.  It is natural to ask whether there exists any construction preserving this form.  That is, if concept class C is weakly learnable by an algorithm using hypotheses from a class H of representations, does there then exist a strong learning algorithm for C that also only outputs hypotheses from H?.  In general, the answer to this question is no (modulo some relatively weak complexity assumptions).  As a simple example, consider the problem of learning k-term DNF formulas using only hypotheses represented by k-term DNF.  (A formula in disjunctive normal form (DNF) is one written as a disjunction of terms, each of which is a conjunction of literals, a literal being either a variable or its complement.)  Pitt and Valiant (1988) show that this learning problem is infeasible if RP ~ NP for k as small as 2.  Nevertheless, the weak learning problem is solved by the algorithm sketched below.  (A similar algorithm is given by Kearns (1988).)  First, choose a \"large\" sample.  If significantly more than half of the examples in the sample are negative (positive), then output the \"always predict negative (positive)\" hypothesis, and halt.  Otherwise, we can assume that the distribu- tion is roughly evenly split between positive and negative examples.  Select and output the disjunction of k or fewer literals that misclassifies none of the positive examples, and the fewest of the negative examples.  We briefly argue that this hypothesis is, with high probability, (1/2 - fl(1/ne))-close to the target concept.  First, note that the target k-term DNF formula is equivalent to some k-CNF formula (Pitt and Valiant, 1988).  (A formula in conjunctive normal form (CNF) is one written as the conjunction of clauses, each clause a disjunction of literals.  If each clause consists of only k literals, then the formula is in k-CNE)  Next, we observe that every clause is satisfied by every assignment that satisfies the entire k-CNF formula.  More- over, since the formula has at most O(n ~) clauses, by an averaging argument, there must be one clause not satisfied by fl(1/n k) of the assignments (as weighted by the target distribu- tion) that do not satisfy the entire formula.  Thus, there exists some disjunction of k literals that is correct for nearly all of the positive examples and for at least flO/n k) of the negative examples.  In particular, the output hypothesis has this property.  Since the distribution is roughly evenly divided between positive and negative examples, this implies that the output hypothesis is roughly (1/2 - f~(1/nk))-close to the target formula.  A number of researchers have considered learning scenarios in which the learner is not only able to passively observe randomly selected examples, but is also able to ask a \"teacher\" various sorts of questions or queries about the target concept.  For instance, the learner might be allowed to ask if some particular instance is a positive or negative example.  Angluin (1988) describes several kinds of query that might be useful to the learner.  The purpose of this section is simply to point out that the construction of Section 3 is applicable even in the presence of most kinds of query.  That is, a weak learning algorithm that depends on the availability of certain kinds of query can be converted, using the same construction, into a strong learning algorithm using the same query types.  In this paper, we have only considered Boolean valued concepts, that is, concepts that classify every instance as either a positive or a negative example.  Of course, in the \"real world,\" most learning tasks require classification into one of several categories (for instance, character recognition).  How does the result generalize to handle many-valued concepts?  First of all, for learning a k-valued concept, it is not immediately clear how to define the notion of weak learnability.  An hypothesis that guesses randomly on every instance will be correct only 1/k of the time, so one natural definition would require only that the weak learning algorithm classify instances correctly slightly more than 1/k of the time.  Unfortunately, under this definition, strong and weak learnability are inequivalent for k as small as three.  As an informal example, consider learning a concept taking the values 0, 1 and 2, and suppose that it is \"easy\" to predict when the concept has the value 2, but \"hard\" to predict whether the concept's value is 0 or 1.  Then to weakly learn such a con- cept, it suffices to fmd an hypothesis that is correct whenever the concept is 2, and that guesses randomly otherwise.  For any distribution, this hypothesis will be correct half of the time, achieving the weak learning criterion of accuracy significantly better than 1/3.  However, boosting the accuracy further is clearly infeasible.  Thus, a better definition of weak learnability is one requiring that the hypothesis be correct on slightly more than half of the distribution, regardless of k. Using this definition, the construction of Section 3 is easily modified to handle many-valued concepts.  The construction derived in Sections 3 and 4 yields some unexpected relationships between the allowed error e and various complexity measures that might be applied to a strong learn- ing algorithm.  One of the more surprising of these is a proof that, for every learnable con- cept class, there exists an efficient algorithm whose output hypotheses can be evaluated in time polynomial in log(l/e).  Furthermore, such an algorithm's space requirements are also only poly-logarithmic in I/e--far less, for instance, than would be needed to store the entire sample.  In addition, its time and sample size requirements grow only linearly in 1/e (disregarding log factors).  • uses space p3(n, s, log0/e), log0/b)), and • outputs hypotheses of size p4(n, s, log0/e)), evaluatable in time ps(n, s, log0/e))  Proof Given a strong learning algorithm A for C, \"hard-wire\" e = 1/4, thus converting A into a weak learning algorithm A' that outputs hypotheses 1/4-close to the target concept.  Now let A\" be the procedure obtained by applying the construction of Lea rn' with A' plugged in for Weakl_ea rn.  As remarked previously, we can assume without loss of gener- ality that A\" halts deterministically in polynomial time.  Note, by the lemmas of Sections 3 and 4 that A\" \"almost\" achieves the resource bounds given in the theorem, the only problem being that the bounds attained are polynomial in 1/6 rather than log(1/~) as desired.  This problem is alleviated by applying the construction of Haussler, Kearns, Littlestone and Warmuth (1988) for converting any learning algorithm B into one running in time poly- nomial in log0/6).  Essentially, this construction works as follows: Given inputs n, s, e and 6, first simulate B O(logl/6)) times, each time setting B's accuracy parameter to e/4 and B's confidence parameter to 1/2.  Save all of the computed hypotheses.  Next, draw a sample of O(log(1/di)/e) examples, and output the one that misclassifies the fewest examples in the sample.  Haussler, et al. argue that the resulting procedure outputs an e-close hypothesis with probability 1 - 6.  Applying this construction to A\", we obtain a final procedure that one can verify achieves all of the stated bounds.  •  These bounds can be applied immediately to a number of existing learning algorithms, yielding improvements in time and/or space complexity (at least in terms of e).  For instance, the computation time of Blumer, Ehrenfeucht, Haussler and Warmuth's (1989) algorithm for learning half-spaces of R n, which involves the solution of a linear programming prob- lem of size proportional to the sample, can be improved by a polynomial factor of 1/e.  The same is also true of Baum's (1989) algorithm for learning unions of half-spaces, which involves finding the convex hull of a significant fraction of the sample.  There are many more algorithms for which the theorem implies improved space effi- ciency.  This is especially true of the many known PAC algorithms that work by choosing a large sample and then finding an hypothesis consistent with it.  For instance, this is how Rivest's (1987) decision list algorithm works, as do most of the algorithms described by Blumer, et al., as well as Helmbold, Sloan and Warmuth's (1990) construction for learning nested differences of learnable concepts.  Since the entire sample must be stored, these algorithms are not terribly space efficient, and so can be dramatically improved by applying .  Of course, these improvements typically come at the cost of requiring a some- what larger sample (by a polynomial factor of log(l/e)).  Thus, there appears to be a trade- off between space and sample size (or time) complexity.  Blumer, Ehrenfeucht, Haussler and Warmuth (1987; 1989) have considered the relationship between learning and data compression.  They have shown that, if any sample can be com- pressed--that is, represented by a prediction rule significantly smaller than the original sample--then this compression algorithm can be converted into a PAC-learning algorithm.  In some sense, the bound given in on the size of the output hypothesis implies the converse.  In particular, suppose Cn is a learnable concept class and that we have been given m examples (vl, C(Vl)), (v2, c(v2)) .....  (Vm, c(vm))  where each 12 i E X n and c is a concept in Cn of size s.  These examples need not have been chosen at random.  The data compression problem is to find a small representation for the data, that is, an hypothesis h that is significantly smaller than the original data set with the property that h(vi) = c(vi) for each v i. An hypothesis with this last property is said to be consistent with the sample.  implies the existence of an efficient algorithm that outputs consistent hypotheses only poly-logarithmic in the size m of the sample.  This is proved by the following theorem: THEOREM 5.  Let C be a learnable concept class.  Then there exists an efficient algorithm that, given 0 < 8 _ 1 and m (distinct) examples of a concept c ~ Cn of size s, outputs with probability at least 1 - ~ a deterministic hypothesis consistent with the sample and of size polynomial in n, s and log m. Proof Pitt and Valiant (1988) show how to convert any learning algorithm into one that finds hypotheses consistent with a set of data points.  The idea is to choose e < 1/m and to run the learning algorithm on a (simulated) uniform distribution over the data set.  Since e is less than the weight placed on any element of the sample, the output hypothesis must have error zero.  Applying this technique to a learning algorithm  A satisfying the conditions of , we see that the output hypothesis has size only polynomial in n, s and log m, and so is far smaller than the original sample for large m. Technically, this technique requires that the learning algorithm output deterministic hypoth- eses.  However, probabilistic hypotheses can also be handled by choosing a somewhat smaller value for e, and by \"hard-wiring\" the computed probabilistic hypothesis with a sequence of random bits.  More precisely, set e = 1/2m, and run A over the same distribution as before.  Assume A has a good run.  Note that the output hypothesis h can be regarded as a deter- ministic function of an instance v and a sequence of random bits r. Let p be the chance that, for a randomly chosen sequence r, h(', r) misclassifies one or more of the instances in the sample.  For such an r, the chance is certainly at least 1/m that an instance v is chosen (according to the simulated uniform distribution on the sample) for which h(v, r) ~ c(v).  Thus, the error of h is at least p/m.  By our choice of e, this implies that p - 1/2, or, in other words, that the probability that a random sequence r is chosen for which h(', r) correctly classifies all of the m examples is at least 1/2.  Thus, choosing and testing random sequences r, we can quickly find one for which the deterministic hypothesis h(., r) is con- sistent with the sample.  Finally, note that the size of this output hard-wired hypothesis is bounded by Ih[ + [rl, and that Irl is bounded by the time it takes to evaluate h, which is poly-logarithmic in m. •\nNaturally, the notion of size in the preceding theorem depends on the underlying model of computation, which we have left unspecified.  However, the theorem has some immediate corollaries when the learning problem is discrete, that is, when every instance in the domain Xn is encoded using a finite alphabet by a string of legnth bounded by a polynomial in n, and every concept in C of size s is also encoded using a finite alphabet by a string of length bounded by a polynomial in s. COROLLARY 1.  Let C be a learnable discrete concept class.  Then there exists an efficient algorithm that, given 0 < 6 ___ 1 and a sample as in , outputs with probability at least 1 - 6 a deterministic consistent hypothesis of size polynomial in n and s, and independent of m. Proof Since we assume (without loss of generality) that all the points of the sample are distinct, its size m cananot exceed [Xn].  Since log Ix.  I is bounded by a polynomial in n, the corollary follows immediately.  •\nApplying Occam's Razor of Blumer, et al. (1987), this implies the following strong general bound on the sample size needed to efficiently learn C. Although the bound is better than that given by (at least in terms of e), it should be pointed out that this improve- ment requires the sacrifice of space efficiency since the entire sample must be stored.  THEOREM 6.  Let C be a learnable discrete concept class.  Then there exists an efficient learn- ing algorithm for C requiring a sample of size 's bound on the size of the output hypothesis also implies that any hard-to-evaluate concept class is unlearnable.  Although this result does not sound surprising, it was previously unclear how it might be proved: since a learning algorithm's hypotheses are technically permitted to grow polynomially in l/e, the learnability of such classes did not seem out of the question.  This result yields the first representation-independent hardness results not based on cryp- tographic assumptions.  For instance, assuming P/poly ~ NP/poly, the class of polynomial- size, nondeterministic Boolean circuits is not learnable.  (The set P/poly (NP/POly) consists of those languages accepted by a family of polynomial-size deterministic (nondeterministic) circuits.)  Furthermore, since learning pattern languages was recently shown (Schapire, 1989) to be as hard as learning NP/poly, this result shows that pattern languages are also unlearn- able under this relatively weak structural assumption.  THEOREM 7.  Suppose C is learnable, and assume that X~ = {0, 1}L  Then there exists a polynomial p such that for all concepts c E C~ of size s, there exists a circuit of size p(n, s) exactly computing c. Proof.  Consider the set of 2 n pairs {(v, c(v))  I v ~ Xn}.  By Corollary 1, there exists an algorithm that, with positive probability, will output an hypothesis consistent with this set of elements of size only polynomial in n and s. Since this hypothesis is polynomially eval- uatable, it can be converted using standard techniques into a circuit of the required size.  By a similar argument, the bound on hypothesis size implies that any function not com- putable by small circuits cannot even be weakly approximated by a family of small circuits, for some distribution on the inputs.  Letfbe a Boolean function on {0, 1}*, D a distribution on {0, 1} n and C a circuit on n variables.  Then Cis said to 13-approximatefunder D if the probability is at most/5 that C(v) ;~ f(v) on an assignment v chosen randomly from {0, 1} ~ according to D. THEOREM 8.  Suppose some function f cannot be computed by any family of polynomial- size circuits.  Then there exists a family of distributions D~, D2 ..... where D~ is over the set {0, 1} n, such that for all polynomials p and q, there exist infinitely many n for which there exists no n-variable circuit of size at most q(n) that (1/2 - 1/p(n))-approximates f under Dn.  Proof.  Throughout this proof, we will assume without loss of generality that p(n) = q(n)  = n k for some integer k _ 1.  Suppose first that there exists some k such that for all n and every distribution D on {0, 1} n, there exists a circuit of size at most n k that (1/2 - 1/nk)-approximatesfunder D.  Then f can, in a sense, be weakly learned.  More precisely, there exists an (exponential- time) procedure that, by searching exhaustively the set of all circuits of size n k, will find one that (1/2 - 1/nk)-approximatesfunder some given distribution D.  Therefore, by Theo- rem 1, fis strongly learnable in a similar sense in exponential time.  Applying (whose validity depends only on the size of the output hypothesis, and not on the running time), this implies thatfcan be exactly computed by a family of polynomial-size circuits, contradicting the theorem's hypothesis.  Thus, for all k _ 1, there exists an integer n and a distribution D on {0, 1} n such that no circuit of size at most n k is able to (1/2 - 1/nk)-approximate f under D. To complete the proof, it suffices to show that this implies the theorem's conclusion.  Let D~n be the set of distributions D on {0, 1} n for which no circuit of size at most n k (1/2 - 1/nk)-approximatesfunder D. It is easy to verify that/~n -----  Dff +1 for all k, n. Also, since every function can be computed by exponential size circuits, there must exist a constant c > 0 for which/9~ ~ = 0 for all n. Let n[k] be the smallest n for which ~ ~ 0.  By the preceding argument, n[k] must exist.  Furthermore, n[k] >_ k/c, which implies that the set N = {n[k] I k _> 1} cannot have finite cardinality.  To eliminate repeated elements from N, let k~ _ 1} = N. Let Di be defined as follows: if i = n[kj] for some j, then let Di be any distribution in/~/J (which cannot be empty by our definition of n[k]); otherwise, if i t[ N, then define D i arbitrarily.  Then D1, D2, • • • is the desired family of \"hard\" distributions.  For if k is any integer, then for all ki >- k, Dn[ki I ~ 19~iki] c /~n[ki].  This proves the theorem.  •\nInformally, states that any language not in the complexity class P/poly cannot be even weakly approximated by any other language in P/poly under some \"hard\" family of distributions.  In fact, the theorem can easily be modified to apply to other circuit classes as well, including monotone P/poly, and monotone or non-monotone NC k for fixed k. (The class NC k consists of all languages accepted by polynomial-size circuits of depth at most O(log k n), and a monotone circuit is one in which no negated variables appear.)  In general, the theorem applies to all circuit classes closed under the transformation on hypotheses resulting from the construction of Section 3 and 4.  Finally, we consider implications of for on-line learning algorithms.  In the on- line learning model, the learner is presented one (randomly selected) instance at a time in a series of trials.  Before being told its correct classification, the learner must try to predict whether the instance is a positive or negative example.  An incorrect prediction is called a mistake.  In this model, the learner's goal is to minimize the number of mistakes.  Previously, Haussler, Littlestone and Warrnuth (1987) have shown that a concept class C is learnable if and only if there exists an on-line learning algorithm for C with the prop- erties that: • the probability of a mistake on the m-th trial is at worst linear in m -~ for some constant 0 2p.  Thus, m examples suffice to find an hypothesis whose chance of error is at most e(m).  To convert A into an on-line learning algorithm in a manner that preserves time and space efficiency, imagine breaking the sequence of trials into blocks of increasing size: the first block consists of the first 2p trials, and each new block has twice the size of the last.  Thus, in general, the i-th block has size si = 2ip, and consists of trials ai = 2(2 H - 1)p + 1 through bi = 2(2 / - 1)p.  h i misclassifies a new instance.  (Note that there are enough instances available in this block for A to compute an hypothesis of the desired accuracy.)  On the next block, as the (i + 1)st hypothesis is being computed, h i is used to make predictions; at the end of this block, h i is discarded as hi+ 1 takes its place.  Thus, if the m-th trial occurs in the i-th block (i.e., if a i <_ m <_ bi), then the probability of a mistake is bounded by e(Si_l) , the error rate of hi_ 1 .  From the definition of e(), this implies the desired bound on the probability of a mistake on the m-th trial, and, in turn, on the expected number of mistakes on the first m trials.  Finally, note that on the i-th block, space is needed only to store the hypothesis from the last block hi_l, and to simulate A's computation of block i's hypothesis.  By , both of these quantities grow polynomially in log(l/e).  By our choice of e, this implies the desired bound on the algorithm's space efficiency.  The time complexity of the procedure is bounded in a similar fashion.  •\nWe have shown that a model of learnability in which the learner is only required to perform slightly better than guessing is as strong as a model in which the learner's error can be made arbitrarily small.  The proof of this result was based on the filtering of the distribution in a manner causing the weak learning algorithm to eventually learn nearly the entire distribu- tion.  We have also shown that this proof implies a set of general bounds on the complexity of PAC-learning (both batch and on-line), and have discussed some of the applications of these bounds.  It is hoped that these results will open the way on a new method of algorithm design for PAC-learning.  As previously mentioned, the vast majority of currently known algorithms work by finding a hypothesis consistent with a large sample.  An alternative approach sug- gested by the main result is to seek instead a hypothesis covering slightly more than half the distribution.  Perhaps, such an hypothesis is easier to find, at least from the point of view of the algorithm designer.  This approach leads to algorithms with a flavor similar to the one described for k-term DNF in Section 5.3, and it is possible to find similar algo- rithms for a number of other concept classes that are already known to be learnable (for example, k-decision lists (Rivest, 1987) and rank r decision trees (Ehrenfeucht and Haussler, 1989)).  To what extent will this approach be fruitful for other classes not presently known to be learnable?  This is an open question.  Another open problem concerns the robustness of the construction described in this paper.  Intuitively, it seems that there should be a close relationship between reducing the error of the hypothesis, and overcoming noise in the data.  Is this a valid intuition?  Can our con- struction be modified to handle noise?  Finally, turning away from the theoretical side of machine learning, we can ask how well our construction would perform in practice.  Often, a learning problem (for instance, a neural network) is designed, implemented, and found empirically to achieve a \"good\" error rate, but no way is seen of improving the program further to enable it to achieve a \"great\" error rate.  Suppose our construction is implemented on top of this learning program.  Would it help?  This is not a theoretical question, but one that can only be answered experimentally, and one that obviously depends on the domain and the underlying learning program.  Never-  theless, it seems plausible that the construction might in some cases give good results in practice.  This paper was prepared with support from ARO Grant DAAL03-86-K-0171, DARPA Con- tract N00014-89-J-1988, and a grant from the Siemens Corporation.  Thanks to Sally Goldman, Michael Kearns, and Ron Rivest for their helpful comments and suggestions.  Thanks also to the anonymous referees of this paper for their careful reading and thoughtful comments.", "metadata": {"source_file": "the-strength-of-weak-learnability-3kx3bkg7w2.pdf", "title": "The Strength of Weak Learnability", "authors": ["Robert E. Schapire"], "year": null, "detected_language": "en", "page_count": 31, "origin_chunk_file": "the-strength-of-weak-learnability-3kx3bkg7w2.chunks.json"}}
