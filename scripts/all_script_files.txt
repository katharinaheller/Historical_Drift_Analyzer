==== C:\Users\katha\historical-drift-analyzer\scripts\all_script_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\scripts\all_script_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\scripts\export_eval_table.py ==== 
from __future__ import annotations
import argparse
from src.core.evaluation.evaluation_table_exporter import EvaluationTableExporter

def main() -> None:
    parser = argparse.ArgumentParser(description="Export evaluation summary as LaTeX/CSV/Markdown tables.")
    parser.add_argument("--charts_dir", type=str, default="data/eval_charts", help="Directory containing summary.json")
    args = parser.parse_args()
    exp = EvaluationTableExporter(args.charts_dir)
    res = exp.export()
    print("Export completed:")
    for k, v in res.items():
        if k != "rows":
            print(f"  {k}: {v}")

if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_benchmark_series.py ==== 
# scripts/run_benchmark_series.py
from __future__ import annotations

# --- ensure project root is on sys.path ---
import sys, os
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import subprocess
import time
import argparse
from pathlib import Path

def run_cmd(cmd: list[str]) -> None:
    """Execute subprocess and stream output."""
    print(">>>", " ".join(cmd))
    start = time.time()
    p = subprocess.Popen(cmd)
    p.wait()
    print(f"⏱Finished in {time.time() - start:.1f}s\n")

def main() -> None:
    p = argparse.ArgumentParser(description="Automated multi-stage benchmark runner.")
    p.add_argument("--prompt_file", default="data/prompts/benchmark_prompts.txt")
    p.add_argument("--logs_dir", default="data/logs")
    p.add_argument("--eval_dir", default="data/eval_logs")
    p.add_argument("--charts_dir", default="data/eval_charts")
    p.add_argument("--bootstrap_iters", type=int, default=2000)
    p.add_argument("--k", type=int, default=5)
    p.add_argument("--ns", nargs="+", type=int, default=[10, 30, 50, 100])
    p.add_argument("--seeds", nargs="+", type=int, default=[13, 37, 101])
    args = p.parse_args()

    Path(args.logs_dir).mkdir(parents=True, exist_ok=True)
    Path(args.eval_dir).mkdir(parents=True, exist_ok=True)
    Path(args.charts_dir).mkdir(parents=True, exist_ok=True)

    for n in args.ns:
        print(f"\n=== Benchmark stage n={n} ===")
        for seed in args.seeds:
            cmd = [
                "poetry", "run", "python", "scripts/run_full_benchmark.py",
                "--prompt_file", args.prompt_file,
                "--num_prompts", str(n),
                "--logs_dir", args.logs_dir,
                "--eval_dir", args.eval_dir,
                "--charts_dir", args.charts_dir,
                "--k", str(args.k),
                "--bootstrap_iters", str(args.bootstrap_iters),
                "--seed", str(seed)
            ]
            run_cmd(cmd)

        print(f"\n=== Post-analysis for n={n} ===")
        eval_cmd = [
            "poetry", "run", "python", "scripts/run_eval_analytics.py",
            "--logs_dir", args.eval_dir,
            "--out_dir", args.charts_dir,
            "--iters", str(args.bootstrap_iters)
        ]
        run_cmd(eval_cmd)

if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_convergence_plot.py ==== 
# scripts/run_convergence_plot.py
from __future__ import annotations
import argparse
from src.core.evaluation.convergence_plotter import ConvergencePlotter

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Plot convergence of evaluation metrics (mean ±95% CI vs sample size)."
    )
    parser.add_argument(
        "--charts_dir",
        type=str,
        default="data/eval_charts",
        help="Directory containing summary_n*.json files from benchmark series."
    )
    args = parser.parse_args()

    print(f"=== Convergence Plot Generation ===")
    plotter = ConvergencePlotter(charts_dir=args.charts_dir)
    plotter.plot()
    print(f"Convergence plot created in {args.charts_dir}")

if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_eval_analytics.py ==== 
# scripts/run_eval_analytics.py
from __future__ import annotations

# --- ensure project root is on sys.path ---
import sys, os
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import argparse
from src.core.evaluation.evaluation_visualizer import EvaluationVisualizer, VizConfig

def main() -> None:
    p = argparse.ArgumentParser(description="Run evaluation analytics and plots.")
    p.add_argument("--logs_dir", type=str, default="data/eval_logs")
    p.add_argument("--out_dir", type=str, default="data/eval_charts")
    p.add_argument("--iters", type=int, default=2000)
    args = p.parse_args()

    cfg = VizConfig(logs_dir=args.logs_dir, out_dir=args.out_dir, bootstrap_iters=args.iters)
    viz = EvaluationVisualizer(cfg)
    summary = viz.run_all()
    print(summary)

if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_full_benchmark.py ==== 
from __future__ import annotations

import sys
import os
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import argparse
import json
import time
import random
import numpy as np
from pathlib import Path
import shutil

from src.core.llm.llm_orchestrator import LLMOrchestrator
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.evaluation.evaluation_orchestrator import EvaluationOrchestrator
from src.core.evaluation.evaluation_visualizer import EvaluationVisualizer, VizConfig
from src.core.evaluation.evaluation_table_exporter import EvaluationTableExporter


def _count_files(dir_path: Path, pattern: str) -> int:
    # Count files using glob
    return sum(1 for _ in dir_path.glob(pattern))


def _load_json(path: Path) -> dict | None:
    # Safe JSON loader
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def _clean_directory(path: Path) -> None:
    # Delete directory contents
    if path.exists():
        for f in path.glob("*"):
            if f.is_file():
                f.unlink()
            else:
                shutil.rmtree(f)
    path.mkdir(parents=True, exist_ok=True)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Full automated benchmark: LLM → Evaluation → Visualization → Table export."
    )

    parser.add_argument("--prompt_file", type=str, default="data/prompts/benchmark_prompts.txt")
    parser.add_argument("--num_prompts", type=int, default=100)
    parser.add_argument("--logs_dir", type=str, default="data/logs")
    parser.add_argument("--eval_dir", type=str, default="data/eval_logs")
    parser.add_argument("--charts_dir", type=str, default="data/eval_charts")
    parser.add_argument("--k", type=int, default=10)
    parser.add_argument("--bootstrap_iters", type=int, default=2000)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument(
        "--intent",
        type=str,
        default=None,
        choices=["conceptual", "chronological", "analytical", "comparative"],
        help="Fixed intent; if omitted, automatic classification is used."
    )
    args = parser.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    start_time = time.time()

    logs_dir = Path(args.logs_dir)
    eval_dir = Path(args.eval_dir)
    charts_dir = Path(args.charts_dir)

    # Clean directories
    _clean_directory(logs_dir)
    _clean_directory(eval_dir)
    _clean_directory(charts_dir)

    print("=== Benchmark configuration ===")
    print(f"prompt_file  : {Path(args.prompt_file).resolve()}")
    print(f"num_prompts  : {args.num_prompts}")
    print(f"logs_dir     : {logs_dir.resolve()}")
    print(f"eval_dir     : {eval_dir.resolve()}")
    print(f"charts_dir   : {charts_dir.resolve()}")
    print(f"k            : {args.k}")
    print(f"boot iters   : {args.bootstrap_iters}")
    print(f"seed         : {args.seed}")
    print(f"intent       : {args.intent or 'auto'}")
    print()

    # -------------------------------------------------------
    print("=== Phase 1: LLM Generation ===")
    prompt_file = Path(args.prompt_file)
    if not prompt_file.exists():
        raise FileNotFoundError(f"Prompt file not found: {prompt_file}")

    prompts = [
        ln.strip()
        for ln in prompt_file.read_text(encoding="utf-8").splitlines()
        if ln.strip()
    ]
    prompts = prompts[: args.num_prompts]
    print(f"Loaded {len(prompts)} prompts.\n")

    before_logs = _count_files(logs_dir, "llm_*.json")

    llm_orch = LLMOrchestrator(logs_dir=str(logs_dir))
    prompt_orch = PromptOrchestrator()

    for idx, prompt in enumerate(prompts, start=1):
        print(f"[{idx}/{len(prompts)}] {prompt}")

        try:
            if args.intent:
                query_obj = {"processed_query": prompt.strip(), "intent": args.intent}
            else:
                pre = prompt_orch.preprocessor.process(prompt)
                intent = pre["intent"]
                refined = prompt_orch.prompt_builder.reformulate_query(
                    pre["processed_query"], intent
                )
                query_obj = {"processed_query": refined, "intent": intent}
                prompt_orch.logger.info(
                    f"Auto intent='{intent}' → refined query='{refined}'"
                )

            _ = llm_orch.process_query(query_obj)

        except Exception as e:
            (logs_dir / f"error_{idx}.json").write_text(
                json.dumps({"prompt": prompt, "error": str(e)}, indent=2),
                encoding="utf-8"
            )

    if hasattr(llm_orch, "close"):
        try:
            llm_orch.close()
        except Exception:
            pass

    after_logs = _count_files(logs_dir, "llm_*.json")
    produced = max(0, after_logs - before_logs)
    print(f"LLM phase completed. Produced {produced} valid llm_*.json logs.\n")

    if produced == 0:
        print("No LLM outputs. Stopping.")
        return

    # -------------------------------------------------------
    print("=== Phase 2: Evaluation ===")

    model_name = Path(args.logs_dir).name.replace("logs_", "") or "benchmark"

    evaluator = EvaluationOrchestrator(
        base_output_dir=str(eval_dir),
        model_name=model_name,
        k=args.k,
        bootstrap_iters=args.bootstrap_iters
    )

    summary_eval = evaluator.evaluate_batch_from_logs(logs_dir=str(logs_dir))
    print(json.dumps(summary_eval, indent=2))

    if not summary_eval or int(summary_eval.get("evaluated_files", 0)) == 0:
        print("No evaluatable data. Stopping before visualization.")
        return

    # -------------------------------------------------------
    print("=== Phase 3: Visualization ===")
    cfg = VizConfig(
        logs_dir=str(eval_dir),
        out_dir=str(charts_dir),
        bootstrap_iters=args.bootstrap_iters
    )
    viz = EvaluationVisualizer(cfg)
    summary_viz = viz.run_all()
    print(json.dumps(summary_viz, indent=2))

    summary_json = charts_dir / "summary.json"
    summary = _load_json(summary_json)
    has_summary = bool(summary and "ndcg@k_mean" in summary and "faith_mean" in summary)

    # -------------------------------------------------------
    print("=== Phase 4: Table Export ===")
    if has_summary:
        exp = EvaluationTableExporter(charts_dir=str(charts_dir))
        try:
            paths = exp.export()
            for k, v in paths.items():
                if k != "rows":
                    print(f"{k}: {v}")
        except Exception as e:
            print(f"Table export failed: {e}")
    else:
        print("Skipping table export (no summary.json with means).")

    # -------------------------------------------------------
    print("=== Phase 5: PDF Report ===")
    if has_summary:
        try:
            from src.core.evaluation.report_builder import ReportBuilder
            rb = ReportBuilder(charts_dir=str(charts_dir), eval_dir=str(eval_dir))
            pdf_path = rb.build(custom_name=f"benchmark_report_n{args.num_prompts}_seed{args.seed}.pdf")
            print(f"Generated PDF report: {pdf_path}")
        except Exception as e:
            print(f"PDF generation failed: {e}")
    else:
        print("Skipping PDF generation (no summary.json with means).")

    elapsed = (time.time() - start_time) / 60
    print(f"\n=== BENCHMARK COMPLETE ({elapsed:.1f} min) ===")
    print(f"Results in: {eval_dir} | {charts_dir}")


if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_full_multi_evaluation.py ==== 
# scripts/run_multi_model_benchmark.py
from __future__ import annotations
import sys
import os
import subprocess
import time
from pathlib import Path
import pandas as pd
import json
import numpy as np
import matplotlib.pyplot as plt

# ensure project root on path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# traffic-light colors consistent with your visualizer
FAITH_COLORS = {
    "high": "#1a9850",
    "medium": "#fee08b",
    "low": "#d73027",
}

FAITH_LABELS = {
    "high": "High (≥0.80)",
    "medium": "Medium (0.50–0.79)",
    "low": "Low (<0.50)",
}


def run(cmd: list[str]) -> None:
    # execute subprocess and wait
    print(">>>", " ".join(cmd))
    start = time.time()
    p = subprocess.Popen(cmd)
    p.wait()
    print(f"finished in {time.time() - start:.1f}s\n")


def load_eval_df(eval_dir: Path) -> pd.DataFrame:
    # load *_evaluation.json rows into one dataframe
    rows = []
    for fp in eval_dir.glob("*_evaluation.json"):
        try:
            d = json.loads(fp.read_text(encoding="utf-8"))
            rows.append({
                "query_id": d.get("query_id"),
                "ndcg": float(d.get("ndcg@k", np.nan)),
                "faith": float(d.get("faithfulness", np.nan)),
                "model": d.get("model_name", "unknown"),
            })
        except Exception:
            continue
    return pd.DataFrame(rows)


def faith_band(f: float) -> str:
    # map continuous score to qualitative band
    if np.isnan(f):
        return "missing"
    if f >= 0.80:
        return "high"
    if f >= 0.50:
        return "medium"
    return "low"


def plot_global_comparison(df: pd.DataFrame, out_path: Path) -> None:
    # group counts by model + faithfulness band
    df["band"] = df["faith"].apply(faith_band)

    models = sorted(df["model"].unique())
    bands = ["high", "medium", "low"]

    counts = (
        df.groupby(["model", "band"])["query_id"]
        .count()
        .unstack(fill_value=0)
        .reindex(columns=bands, fill_value=0)
    )

    x = np.arange(len(models))
    width = 0.22

    fig, ax = plt.subplots(figsize=(7, 4.5))

    for i, b in enumerate(bands):
        offsets = x + (i - 1) * width
        ax.bar(
            offsets,
            counts[b].values,
            width=width,
            color=FAITH_COLORS[b],
            edgecolor="black",
            alpha=0.9,
            label=FAITH_LABELS[b]
        )

    ax.set_xticks(x)
    ax.set_xticklabels(models, rotation=10)
    ax.set_ylabel("Number of queries")
    ax.set_title("Faithfulness band comparison across models")
    ax.legend(frameon=False)

    fig.tight_layout()
    fig.savefig(out_path.with_suffix(".png"), dpi=150, bbox_inches="tight")
    fig.savefig(out_path.with_suffix(".svg"), bbox_inches="tight")
    plt.close(fig)


def main() -> None:
    models = {
        "phi3_4b":    "phi3-4b",
        "mistral_7b": "mistral-7b-instruct",
        "llama3_8b":  "llama3-8b-instruct",
    }

    num_prompts = 100
    prompt_file = "data/prompts/benchmark_prompts.txt"

    all_dfs = []

    for profile, model in models.items():
        print(f"=== RUNNING MODEL: {profile} ===")

        logs   = f"data/logs_{profile}"
        evals  = f"data/eval_logs_{profile}"
        charts = f"data/eval_charts_{profile}"

        cmd = [
            "poetry", "run", "python", "scripts/run_full_benchmark.py",
            "--prompt_file", prompt_file,
            "--num_prompts", str(num_prompts),
            "--logs_dir", logs,
            "--eval_dir", evals,
            "--charts_dir", charts,
            "--k", "10",
            "--bootstrap_iters", "2000",
            "--seed", "42",
        ]
        run(cmd)

        df = load_eval_df(Path(evals))
        df["model"] = profile
        all_dfs.append(df)

    df_all = pd.concat(all_dfs, ignore_index=True)
    outdir = Path("data/model_comparison")
    outdir.mkdir(parents=True, exist_ok=True)

    df_all.to_csv(outdir / "all_models_evaluation.csv", index=False, encoding="utf-8")

    plot_global_comparison(df_all, outdir / "faithfulness_model_comparison")

    print("Done. Outputs:")
    print("→ Combined CSV:", outdir / "all_models_evaluation.csv")
    print("→ Comparison plots:", outdir / "faithfulness_model_comparison.*")


if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_intrinsic_eval.py ==== 
from __future__ import annotations

import sys
import os
import argparse

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from src.core.intrinsic import IntrinsicRetrievalOrchestrator
from src.core.intrinsic.intrinsic_visualizer import IntrinsicVisualizer, IntrinsicVizConfig


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Run intrinsic retrieval evaluation (NDCG@k + RelevanceSeparationAUC) "
                    "based on existing LLM logs."
    )
    parser.add_argument(
        "--logs_dir",
        type=str,
        default="data/logs",
        help="Directory containing llm_*.json logs",
    )
    parser.add_argument(
        "--intrinsic_dir",
        type=str,
        default="data/intrinsic",
        help="Base directory for intrinsic evaluation outputs",
    )
    parser.add_argument(
        "--k",
        type=int,
        default=10,
        help="Cut-off k for NDCG@k",
    )
    parser.add_argument(
        "--bootstrap_iters",
        type=int,
        default=2000,
        help="Number of bootstrap iterations for confidence intervals",
    )
    args = parser.parse_args()

    orch = IntrinsicRetrievalOrchestrator(
        logs_dir=args.logs_dir,
        base_intrinsic_dir=args.intrinsic_dir,
        k=args.k,
        bootstrap_iters=args.bootstrap_iters,
    )

    summary_eval = orch.evaluate_batch()
    print("=== Intrinsic evaluation summary ===")
    print(summary_eval)

    cfg = IntrinsicVizConfig(
        eval_dir=os.path.join(args.intrinsic_dir, "eval_logs"),
        out_dir=os.path.join(args.intrinsic_dir, "charts"),
        bootstrap_iters=args.bootstrap_iters,
    )
    viz = IntrinsicVisualizer(cfg)
    summary_viz = viz.run_all()
    print("=== Intrinsic visualization summary ===")
    print(summary_viz)


if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\scripts\run_multi_model_benchmark.py ==== 
from __future__ import annotations

# Add project root
import sys
import os
import subprocess
import time
import argparse
import json
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.core.evaluation.multi_model_report_builder import MultiModelReportBuilder

# Color scale for six-level classification
FAITH_COLORS = {
    "excellent":  "#006d2c",
    "good":       "#31a354",
    "fair":       "#a1d99b",
    "borderline": "#fed976",
    "poor":       "#fd8d3c",
    "critical":   "#e31a1c",
}

FAITH_LABELS = {
    "excellent":  "Excellent (≥0.95)",
    "good":       "Good (0.85–0.94)",
    "fair":       "Fair (0.75–0.84)",
    "borderline": "Borderline (0.60–0.74)",
    "poor":       "Poor (0.40–0.59)",
    "critical":   "Critical (<0.40)",
}


# ---------------------------------------------------------

def run(cmd: list[str]) -> None:
    # Wrapper for subprocess execution
    print(">>>", " ".join(cmd))
    start = time.time()
    p = subprocess.Popen(cmd)
    p.wait()
    print(f"finished in {time.time() - start:.1f}s\n")


# ---------------------------------------------------------

def load_eval_df(eval_dir: Path) -> pd.DataFrame:
    # Load all per-query evaluation json files in a directory
    rows = []
    for fp in eval_dir.glob("*_evaluation.json"):
        try:
            d = json.loads(fp.read_text(encoding="utf-8"))
        except Exception:
            continue

        nd = d.get("ndcg@k")
        fa = d.get("faithfulness")

        rows.append({
            "query_id": d.get("query_id"),
            "ndcg": float(nd) if nd is not None else np.nan,
            "faith": float(fa) if fa is not None else np.nan,
            "model": d.get("model_name", "unknown"),
        })

    df = pd.DataFrame(rows)
    if df.empty:
        # Ensure expected columns exist even when no rows are present
        df = pd.DataFrame(columns=["query_id", "ndcg", "faith", "model"])
    if "faith" not in df:
        df["faith"] = np.nan
    return df


# ---------------------------------------------------------

def faith_band(f: float) -> str:
    # Six-level categorization of faithfulness scores
    if np.isnan(f):
        return "missing"
    v = float(f)
    if v >= 0.95:
        return "excellent"
    if v >= 0.85:
        return "good"
    if v >= 0.75:
        return "fair"
    if v >= 0.60:
        return "borderline"
    if v >= 0.40:
        return "poor"
    return "critical"


# ---------------------------------------------------------

def clear_and_prepare(path: Path) -> None:
    # Clear directory and reconstruct it
    if path.exists():
        for f in path.glob("*"):
            if f.is_file():
                f.unlink()
            else:
                import shutil
                shutil.rmtree(f)
    path.mkdir(parents=True, exist_ok=True)


# ---------------------------------------------------------

def plot_global_comparison(df: pd.DataFrame, out_path: Path) -> None:
    # Draw multi-model band distribution
    df = df.copy()
    if df.empty:
        print("No evaluation data available for global comparison plot.")
        return

    df["band"] = df["faith"].apply(faith_band)

    models = sorted(df["model"].unique())
    bands = ["excellent", "good", "fair", "borderline", "poor", "critical"]

    counts = (
        df.groupby(["model", "band"])["query_id"]
        .count()
        .unstack(fill_value=0)
        .reindex(columns=bands, fill_value=0)
    )

    x = np.arange(len(models))
    width = 0.12

    fig, ax = plt.subplots(figsize=(10, 5))

    for i, b in enumerate(bands):
        offsets = x + (i - (len(bands) - 1) / 2) * width
        ax.bar(
            offsets,
            counts[b].values,
            width=width,
            color=FAITH_COLORS[b],
            edgecolor="black",
            alpha=0.9,
            label=FAITH_LABELS[b],
        )

    ax.set_xticks(x)
    ax.set_xticklabels(models, rotation=10)
    ax.set_ylabel("Number of queries")
    ax.set_title("Faithfulness band comparison across models")
    ax.legend(frameon=False, ncol=3)

    fig.tight_layout()
    fig.savefig(out_path.with_suffix(".png"), dpi=150, bbox_inches="tight")
    fig.savefig(out_path.with_suffix(".svg"), bbox_inches="tight")
    plt.close(fig)


# ---------------------------------------------------------

def main() -> None:
    parser = argparse.ArgumentParser(
        description="Run multi-model benchmark with strict isolation and build comparison report."
    )
    parser.add_argument("--num_prompts", type=int, default=10)
    parser.add_argument("--prompt_file", type=str, default="data/prompts/benchmark_prompts.txt")
    parser.add_argument("--k", type=int, default=10)
    parser.add_argument("--bootstrap_iters", type=int, default=2000)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    models = {
        "phi3_4b": "phi3-4b",
        "mistral_7b": "mistral-7b-instruct",
        "llama3_8b": "llama3-8b-instruct",
    }

    all_dfs = []

    for profile, model_id in models.items():
        print(f"=== RUNNING MODEL: {profile} ({model_id}) ===")

        logs_dir = Path(f"data/logs_{profile}")
        eval_dir = Path(f"data/eval_logs_{profile}")
        charts_dir = Path(f"data/eval_charts_{profile}")

        clear_and_prepare(logs_dir)
        clear_and_prepare(eval_dir)
        clear_and_prepare(charts_dir)

        # Run isolated benchmark for this profile
        cmd = [
            "poetry", "run", "python", "scripts/run_full_benchmark.py",
            "--prompt_file", args.prompt_file,
            "--num_prompts", str(args.num_prompts),
            "--logs_dir", str(logs_dir),
            "--eval_dir", str(eval_dir),
            "--charts_dir", str(charts_dir),
            "--k", str(args.k),
            "--bootstrap_iters", str(args.bootstrap_iters),
            "--seed", str(args.seed),
        ]
        run(cmd)

        df = load_eval_df(eval_dir)
        if df.empty:
            print(f"No evaluation files found for model '{profile}' in {eval_dir}")
        df["model"] = profile
        all_dfs.append(df)

    df_all = pd.concat(all_dfs, ignore_index=True)

    outdir = Path("data/model_comparison")
    outdir.mkdir(parents=True, exist_ok=True)

    df_all.to_csv(outdir / "all_models_evaluation.csv", index=False, encoding="utf-8")

    if df_all.empty:
        print("No evaluation data across models. Skipping plots and multi-model report.")
        return

    plot_global_comparison(df_all, outdir / "faithfulness_model_comparison")

    rb = MultiModelReportBuilder(base_dir="data")
    pdf_path = rb.build(name="multi_model_benchmark_report.pdf")

    print("\nAll done.")
    print("→ CSV :", outdir / "all_models_evaluation.csv")
    print("→ Plots:", outdir / "faithfulness_model_comparison.*")
    print("→ PDF :", pdf_path)


if __name__ == "__main__":
    main()

