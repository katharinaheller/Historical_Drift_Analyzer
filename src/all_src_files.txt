==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\api\server.py ==== 
# src/api/server.py
from __future__ import annotations
import uvicorn
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any

from src.core.llm.llm_orchestrator import LLMOrchestrator
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator

# ---------------------------------------------------------------------
# App setup
# ---------------------------------------------------------------------
app = FastAPI(title="HDA API", version="1.3")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger = logging.getLogger("uvicorn.error")

# ---------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------
class ChatRequest(BaseModel):
    query: str
    intent: str | None = "conceptual"
    return_context: bool | None = True

class RetrieveRequest(BaseModel):
    query: str
    intent: str | None = "conceptual"

class ChatResponse(BaseModel):
    answer: str
    retrieved: List[Dict[str, Any]] | None = None

# ---------------------------------------------------------------------
# Singletons (avoid reloading heavy components)
# ---------------------------------------------------------------------
_llm_orch: LLMOrchestrator | None = None
_ret_orch: RetrievalOrchestrator | None = None

def _llm() -> LLMOrchestrator:
    global _llm_orch
    if _llm_orch is None:
        _llm_orch = LLMOrchestrator()
        logger.info("LLMOrchestrator initialized.")
    return _llm_orch

def _retriever() -> RetrievalOrchestrator:
    global _ret_orch
    if _ret_orch is None:
        _ret_orch = RetrievalOrchestrator()
        logger.info("RetrievalOrchestrator initialized.")
    return _ret_orch

# ---------------------------------------------------------------------
# Endpoints
# ---------------------------------------------------------------------
@app.post("/api/retrieve", response_model=List[Dict[str, Any]])
def retrieve(req: RetrieveRequest):
    """Retrieve ranked context chunks."""
    try:
        return _retriever().retrieve(req.query, req.intent or "conceptual")
    except Exception as e:
        logger.exception("Retrieval failed.")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """
    Chat endpoint:
    Runs the full RAG pipeline including retrieval, prompt composition,
    LLM generation, and automatic logging of prompt + response.
    """
    try:
        llm = _llm()

        # Übergabe des refined query an den Orchestrator (inkl. automatischem Logging)
        query_obj = {"refined_query": req.query, "intent": req.intent or "conceptual"}
        answer = llm.process_query(query_obj)

        if not answer:
            return ChatResponse(answer="No relevant context found or generation failed.", retrieved=[])

        # Optional: Kontext zurückgeben (bereits im LLMOrchestrator enthalten)
        retrieved_chunks = llm.retriever.retrieve(req.query, req.intent or "conceptual")

        return ChatResponse(answer=answer.strip(), retrieved=retrieved_chunks if req.return_context else None)

    except Exception as e:
        logger.exception("Chat failed.")
        raise HTTPException(status_code=500, detail=str(e))

# ---------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------
def main():
    uvicorn.run("src.api.server:app", host="0.0.0.0", port=8001, reload=False)

if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\main_orchestrator.py ==== 
# src/core/main_orchestrator.py
from __future__ import annotations
import argparse
import logging
import sys
import os
import json
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.ingestion_orchestrator import main as run_ingestion
from src.core.embedding.embedding_orchestrator import main as run_embedding
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.llm.llm_orchestrator import LLMOrchestrator

# Import evaluation orchestrator via abstraction
try:
    from src.core.evaluation.evaluation_orchestrator import EvaluationOrchestrator
except Exception:
    EvaluationOrchestrator = None  # # Allow pipeline to run without evaluation module


class MainOrchestrator:
    """Central controller coordinating all pipeline phases."""

    def __init__(self, config_path: str = "configs/config.yaml"):
        # Ensure consistent UTF-8 runtime
        os.environ["PYTHONIOENCODING"] = "utf-8"
        os.environ["PYTHONUTF8"] = "1"
        os.environ["LC_ALL"] = "C.UTF-8"
        os.environ["LANG"] = "C.UTF-8"

        # Reconfigure streams for UTF-8
        if hasattr(sys, "stdout"):
            try:
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass
        if hasattr(sys, "stderr"):
            try:
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass

        # Load merged configuration and setup logging
        self.cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = self.cfg_loader.config
        self.logger = self._setup_logger()

    # ------------------------------------------------------------------
    def _setup_logger(self) -> logging.Logger:
        """Initialize process-wide logger based on config."""
        opts = self.cfg.get("global", {})
        level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
        logging.basicConfig(level=level, format="%(levelname)s | %(message)s")
        logger = logging.getLogger("MainOrchestrator")
        logger.info("Initialized main orchestrator")
        return logger

    # ------------------------------------------------------------------
    def run_phase(self, phase: str) -> None:
        """Dispatch to the selected pipeline phase."""
        self.logger.info(f"Starting phase: {phase.upper()}")

        # Ensure project src in sys.path
        base_dir = Path(self.cfg["global"]["base_dir"]).resolve()
        sys.path.append(str(base_dir / "src"))

        try:
            if phase == "ingestion":
                run_ingestion()

            elif phase == "embedding":
                run_embedding()

            elif phase == "retrieval":
                self._run_prompt_retrieval_chain()

            elif phase == "llm":
                self.logger.info("Launching LLM phase — interactive mode active.")
                orchestrator = LLMOrchestrator()
                orchestrator.run_interactive()
                orchestrator.close()

            elif phase == "evaluation":
                self._run_evaluation()

            elif phase == "all":
                self.logger.info("Running full pipeline (ingestion → embedding → prompt → retrieval → llm)")
                run_ingestion()
                run_embedding()
                self._run_prompt_retrieval_chain()
                orchestrator = LLMOrchestrator()
                orchestrator.run_interactive()
                orchestrator.close()

            else:
                self.logger.error(f"Unknown phase: {phase}")
                sys.exit(1)

            self.logger.info(f"Phase '{phase}' completed successfully.")

        except UnicodeDecodeError as ue:
            self.logger.error(f"Unicode decoding failed: {ue}. Retrying with UTF-8 replacement.")
            try:
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass
            raise

        except KeyboardInterrupt:
            self.logger.info("Execution manually interrupted by user.")
            sys.exit(0)

        except Exception as e:
            self.logger.exception(f"Phase '{phase}' failed: {e}")
            raise

    # ------------------------------------------------------------------
    def _run_prompt_retrieval_chain(self) -> None:
        """Execute prompt → retrieval phase."""
        self.logger.info("Executing prompt → retrieval phase")

        # Prompt phase
        prompt_orch = PromptOrchestrator()
        prompt_data = prompt_orch.get_prompt_object()
        if not prompt_data or "processed_query" not in prompt_data:
            self.logger.warning("Prompt phase returned no valid query. Aborting retrieval.")
            return

        query = prompt_data["processed_query"]
        intent = prompt_data["intent"]

        # Retrieval phase
        retrieval = RetrievalOrchestrator(config_path="configs/retrieval.yaml")
        self.logger.info(f"Query intent='{intent}' → executing retrieval flow")
        retrieved: List[Dict[str, Any]] = retrieval.retrieve(query, intent)
        retrieval.close()

        if not retrieved:
            self.logger.warning("No results retrieved.")
            return

        # Output results summary
        print("\n" + "=" * 80)
        print(f"Retrieved Top-{len(retrieved)} Chunks (intent={intent})")
        for i, r in enumerate(retrieved, start=1):
            meta = r.get("metadata", {}) or {}
            year = meta.get("year", "n/a")
            title = meta.get("source_file") or meta.get("title") or "Unknown"
            score = r.get("final_score", r.get("score", 0.0))
            print(f"[{i}] ({year}) {title} | score={float(score):.3f}")
        print("=" * 80 + "\n")

    # ------------------------------------------------------------------
    def _run_evaluation(self) -> None:
        """Batch evaluation over prior LLM logs using registered metrics."""
        if EvaluationOrchestrator is None:
            self.logger.error("Evaluation module not available. Please ensure src/core/evaluation exists.")
            return

        # Resolve evaluation config with sensible defaults
        eval_cfg = self.cfg.get("evaluation", {}) if isinstance(self.cfg, dict) else {}
        logs_dir = Path(eval_cfg.get("logs_dir", "data/logs")).resolve()
        out_dir = Path(eval_cfg.get("eval_logs_dir", "data/eval_logs")).resolve()
        k = int(eval_cfg.get("k", 5))
        glob_pat = eval_cfg.get("glob", "llm_*.json")
        gt_path = eval_cfg.get("ground_truth_path", "data/eval/ground_truth.json")

        # Create orchestrator and run batch evaluation
        self.logger.info(
            f"Evaluation settings → logs_dir={logs_dir} | out_dir={out_dir} | k={k} | glob={glob_pat}"
        )
        orch = EvaluationOrchestrator(
            output_dir=str(out_dir),
            k=k,
            ground_truth_path=gt_path
        )
        summary = orch.evaluate_batch_from_logs(logs_dir=str(logs_dir), pattern=glob_pat)

        # Human-readable console summary
        print("\n=== EVALUATION SUMMARY ===")
        print(f"files             : {summary.get('files', 0)}")
        print(f"mean NDCG@{k:>2}   : {summary.get('mean_ndcg@k', 0.0):.3f}")
        print(f"mean Faithfulness : {summary.get('mean_faithfulness', 0.0):.3f}")
        print("==========================\n")

    # ------------------------------------------------------------------
    def run(self, args: argparse.Namespace) -> None:
        """Entrypoint dispatcher using parsed CLI args."""
        if args.phase:
            self.run_phase(args.phase)
        else:
            self.logger.warning("No phase specified. Use --phase <name> or --phase all")


# ----------------------------------------------------------------------
def parse_args() -> argparse.Namespace:
    """CLI argument parser."""
    parser = argparse.ArgumentParser(description="Historical Drift Analyzer – Main Orchestrator")
    parser.add_argument(
        "--phase",
        type=str,
        required=True,
        choices=["ingestion", "embedding", "retrieval", "llm", "evaluation", "all"],
        help="Select which phase of the pipeline to execute",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="Path to the master configuration YAML file",
    )
    return parser.parse_args()


# ----------------------------------------------------------------------
if __name__ == "__main__":
    args = parse_args()
    orchestrator = MainOrchestrator(config_path=args.config)
    orchestrator.run(args)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\analysis_orchestrator.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\i_analyzer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\config\config_loader.py ==== 
from __future__ import annotations
import yaml
from pathlib import Path
from typing import Any, Dict


class ConfigLoader:
    """
    Universal YAML configuration loader for all pipeline phases.
    - Supports ${PROJECT_ROOT} / ${base_dir} placeholders
    - Can inherit from a master config (for global settings)
    - Provides safe defaults for missing sections
    """

    def __init__(self, path: str, master_path: str | None = "configs/config.yaml"):
        self.path = Path(path)
        if not self.path.exists():
            raise FileNotFoundError(f"Configuration file not found: {self.path}")

        # Load phase-specific YAML (e.g. ingestion.yaml)
        with open(self.path, "r", encoding="utf-8") as f:
            phase_cfg = yaml.safe_load(f) or {}

        # Load master config if available
        master_cfg: Dict[str, Any] = {}
        if master_path:
            master_file = Path(master_path)
            if master_file.exists():
                with open(master_file, "r", encoding="utf-8") as f:
                    master_cfg = yaml.safe_load(f) or {}

        # Merge configurations (phase overrides master)
        self._raw = self._merge_dicts(master_cfg, phase_cfg)

        # Detect project root
        self.project_root = self._detect_project_root()

        # Resolve all placeholders like ${base_dir}
        self.config = self._expand_vars(self._raw)

        # Ensure minimal safe defaults
        for section in ["paths", "options", "chunking"]:
            self.config.setdefault(section, {})

    # ------------------------------------------------------------------
    def _detect_project_root(self) -> Path:
        """Infer project root (the directory above 'configs')."""
        p = self.path.resolve()
        if "configs" in p.parts:
            idx = p.parts.index("configs")
            return Path(*p.parts[:idx])
        return p.parent

    # ------------------------------------------------------------------
    def _merge_dicts(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        """Recursively merge dicts, with override taking precedence."""
        merged = base.copy()
        for k, v in override.items():
            if isinstance(v, dict) and k in merged and isinstance(merged[k], dict):
                merged[k] = self._merge_dicts(merged[k], v)
            else:
                merged[k] = v
        return merged

    # ------------------------------------------------------------------
    def _expand_single_var(self, value: Any) -> Any:
        """Replace placeholders only when explicitly present."""
        if not isinstance(value, str) or "${" not in value:
            return value

        replacements = {
            "${PROJECT_ROOT}": str(self.project_root),
            "${project_root}": str(self.project_root),
            "${BASE_DIR}": str(self.project_root),
            "${base_dir}": str(self.project_root),
        }
        for ph, real in replacements.items():
            value = value.replace(ph, real)
        return str(Path(value).resolve())

    # ------------------------------------------------------------------
    def _expand_vars(self, data: Any) -> Any:
        """Recursively expand placeholders in nested structures."""
        if isinstance(data, dict):
            return {k: self._expand_vars(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._expand_vars(v) for v in data]
        elif isinstance(data, str):
            return self._expand_single_var(data)
        else:
            return data

    # ------------------------------------------------------------------
    def get(self, key: str, default: Any = None) -> Any:
        """Safely access top-level config sections."""
        return self.config.get(key, default)

    # ------------------------------------------------------------------
    @property
    def raw(self) -> Dict[str, Any]:
        """Return unexpanded raw YAML structure."""
        return self._raw


# ----------------------------------------------------------------------
def load_config(path: str, master_path: str | None = "configs/config.yaml") -> Dict[str, Any]:
    """Convenience function: directly load and expand a config dictionary."""
    return ConfigLoader(path, master_path).config
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedder_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict
import logging

from src.core.embedding.interfaces.i_embedder import IEmbedder


class SentenceTransformerEmbedder(IEmbedder):
    """Local embedder using sentence-transformers."""

    def __init__(self, model_name: str, dimension: int | None = None, normalize_embeddings: bool = True):
        # Lazy import to avoid hard dependency
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError("sentence-transformers is required for SentenceTransformerEmbedder. "
                              "Install via: poetry add sentence-transformers") from e

        self._model = SentenceTransformer(model_name)
        self._normalize = normalize_embeddings
        # If dimension not given, infer from model
        if dimension is None:
            test_vec = self._model.encode("test", normalize_embeddings=self._normalize)
            self._dimension = len(test_vec)
        else:
            self._dimension = dimension

    def embed_text(self, text: str) -> list[float]:
        # Encode single text
        return self._model.encode(text, normalize_embeddings=self._normalize).tolist()

    def embed_batch(self, texts, batch_size=None) -> list[list[float]]:
        # Encode multiple texts
        return self._model.encode(
            list(texts),
            batch_size=batch_size or 32,
            normalize_embeddings=self._normalize
        ).tolist()

    @property
    def dimension(self) -> int:
        # Return embedding dimension
        return self._dimension

    def close(self) -> None:
        # Nothing to close for sentence-transformers
        pass


class EmbedderFactory:
    """Factory for creating IEmbedder instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IEmbedder:
        opts: Dict[str, Any] = cfg.get("options", {})
        model_name = opts.get("embedding_model", "all-MiniLM-L6-v2")
        normalize = bool(opts.get("normalize_embeddings", True))
        dimension = opts.get("dimension", None)

        backend = opts.get("embedding_backend", "sentence-transformers").lower()

        if backend == "sentence-transformers":
            return SentenceTransformerEmbedder(
                model_name=model_name,
                dimension=dimension,
                normalize_embeddings=normalize,
            )
        else:
            raise ValueError(f"Unsupported embedding backend: {backend}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedding_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.embedding.embedder_factory import EmbedderFactory
from src.core.embedding.vector_store_factory import VectorStoreFactory

logger = logging.getLogger("EmbeddingOrchestrator")


def _load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _iter_chunk_files(chunks_dir: Path):
    for p in chunks_dir.glob("*.json"):
        if p.is_file():
            yield p


def _extract_chunks(chunk_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    if "chunks" in chunk_data and isinstance(chunk_data["chunks"], list):
        return chunk_data["chunks"]
    elif "text" in chunk_data:
        return [{"text": chunk_data["text"]}]
    return []


def _resolve_metadata_for_chunk(chunk_file: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Try to locate matching metadata JSON for a given chunk file."""
    base = chunk_file.stem.replace(".chunks", "")
    candidates = [
        metadata_dir / f"{chunk_file.name}",
        metadata_dir / f"{base}.json",
        metadata_dir / f"{base}.metadata.json",
    ]
    for candidate in candidates:
        if candidate.exists():
            return _load_json(candidate)
    logger.warning(f"No metadata found for {chunk_file.name}")
    return {}


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load merged configuration (embedding + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/embedding.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger.info("Starting embedding pipeline")

    paths: Dict[str, Any] = cfg.get("paths", {})
    chunks_dir = Path(paths.get("chunks_dir", "data/processed/chunks")).resolve()
    metadata_dir = Path(paths.get("metadata_dir", "data/processed/metadata")).resolve()

    if not chunks_dir.exists():
        raise FileNotFoundError(f"Chunks directory does not exist: {chunks_dir}")
    if not metadata_dir.exists():
        logger.warning(f"Metadata directory does not exist: {metadata_dir} (metadata will be empty)")

    # ------------------------------------------------------------------
    # 2. Initialize embedding backend + vector store
    # ------------------------------------------------------------------
    embedder = EmbedderFactory.from_config(cfg)
    logger.info(f"Initialized embedder with dimension={embedder.dimension}")

    vector_store = VectorStoreFactory.from_config(cfg, dimension=embedder.dimension)
    logger.info("Initialized vector store")

    batch_size = int(opts.get("batch_size", 16))
    texts_batch: List[str] = []
    metas_batch: List[Dict[str, Any]] = []

    try:
        for chunk_file in _iter_chunk_files(chunks_dir):
            chunk_json = _load_json(chunk_file)
            chunks = _extract_chunks(chunk_json)
            if not chunks:
                logger.warning(f"No chunks found in {chunk_file.name}")
                continue

            meta_data = _resolve_metadata_for_chunk(chunk_file, metadata_dir)

            for ch in chunks:
                text = ch.get("text", "").strip()
                if not text:
                    continue

                merged_meta = {
                    "source_file": meta_data.get("source_file", chunk_file.stem),
                    "title": meta_data.get("title"),
                    "authors": meta_data.get("authors"),
                    "year": meta_data.get("year"),
                    "detected_language": meta_data.get("detected_language"),
                    "page_count": meta_data.get("page_count"),
                    "origin_chunk_file": str(chunk_file.name),
                }

                texts_batch.append(text)
                metas_batch.append(merged_meta)

                if len(texts_batch) >= batch_size:
                    try:
                        vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
                        vector_store.add_vectors(vectors, texts_batch, metas_batch)
                        logger.info(f"Embedded and stored batch of size {len(texts_batch)}")
                    except Exception as e:
                        logger.error(f"Error during embedding or storing batch: {e}")
                    finally:
                        texts_batch.clear()
                        metas_batch.clear()

        if texts_batch:
            vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
            vector_store.add_vectors(vectors, texts_batch, metas_batch)
            logger.info(f"Embedded and stored final batch of size {len(texts_batch)}")

        vector_store.persist()
        logger.info("Embedding pipeline finished successfully.")

    finally:
        embedder.close()
        vector_store.close()


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\vector_store_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import logging
import os
import json
import sqlite3

from src.core.embedding.interfaces.i_vector_store import IVectorStore


class FAISSVectorStore(IVectorStore):
    """FAISS-based local vector store."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import faiss  # type: ignore
        except ImportError as e:
            raise ImportError("faiss-cpu is required for FAISSVectorStore. "
                              "Install via: poetry add faiss-cpu") from e

        self.faiss = faiss
        self.dimension = dimension
        self.persist_dir = persist_dir
        os.makedirs(persist_dir, exist_ok=True)
        self.index_path = os.path.join(persist_dir, "index.faiss")
        self.meta_path = os.path.join(persist_dir, "metadata.jsonl")

        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
        else:
            self.index = faiss.IndexFlatIP(self.dimension)

        # Metadata is stored separately as JSONL
        self._meta_fh = open(self.meta_path, "a", encoding="utf-8")

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import to avoid hard dependency at class load

        arr = np.array(vectors).astype("float32")
        self.index.add(arr)

        for i, doc in enumerate(documents):
            meta = metadatas[i] if metadatas and i < len(metadatas) else {}
            payload = {
                "text": doc,
                "metadata": meta,
            }
            self._meta_fh.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def persist(self) -> None:
        self.faiss.write_index(self.index, self.index_path)
        self._meta_fh.flush()

    def close(self) -> None:
        try:
            self.persist()
        finally:
            self._meta_fh.close()


class LanceDBVectorStore(IVectorStore):
    """LanceDB-based vector store (local file-based)."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import lancedb  # type: ignore
        except ImportError as e:
            raise ImportError("lancedb is required for LanceDBVectorStore. "
                              "Install via: poetry add lancedb") from e

        self.dimension = dimension
        os.makedirs(persist_dir, exist_ok=True)
        self.db = lancedb.connect(persist_dir)
        self.table = self.db.open_table("embeddings") if "embeddings" in self.db.table_names() else \
            self.db.create_table("embeddings", data=[
                {
                    "vector": [0.0] * dimension,
                    "text": "",
                    "metadata": {},
                }
            ])

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        rows = []
        for i, vec in enumerate(vectors):
            rows.append({
                "vector": vec,
                "text": documents[i],
                "metadata": metadatas[i] if metadatas and i < len(metadatas) else {},
            })
        self.table.add(rows)

    def persist(self) -> None:
        # LanceDB persists automatically
        pass

    def close(self) -> None:
        # Nothing to close
        pass


class SQLiteVectorStore(IVectorStore):
    """Very simple SQLite-based vector store (for debugging / small-scale)."""

    def __init__(self, persist_dir: str, dimension: int):
        os.makedirs(persist_dir, exist_ok=True)
        db_path = os.path.join(persist_dir, "vectors.sqlite3")
        self.conn = sqlite3.connect(db_path)
        self.dimension = dimension
        self._ensure_schema()

    def _ensure_schema(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                metadata TEXT,
                vector BLOB NOT NULL
            )
            """
        )
        self.conn.commit()

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import
        cur = self.conn.cursor()
        for i, vec in enumerate(vectors):
            meta_str = json.dumps(metadatas[i], ensure_ascii=False) if metadatas and i < len(metadatas) else "{}"
            arr = np.array(vec, dtype="float32").tobytes()
            cur.execute(
                "INSERT INTO embeddings (text, metadata, vector) VALUES (?, ?, ?)",
                (documents[i], meta_str, arr)
            )
        self.conn.commit()

    def persist(self) -> None:
        self.conn.commit()

    def close(self) -> None:
        self.conn.close()


class VectorStoreFactory:
    """Factory for creating IVectorStore instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any], dimension: int) -> IVectorStore:
        opts: Dict[str, Any] = cfg.get("options", {})
        store_name = opts.get("vector_store", "FAISS").upper()

        paths: Dict[str, Any] = cfg.get("paths", {})
        persist_dir = paths.get("vector_store_dir", "data/vector_store")

        if store_name == "FAISS":
            return FAISSVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "LANCEDB":
            return LanceDBVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "SQLITE":
            return SQLiteVectorStore(persist_dir=persist_dir, dimension=dimension)
        else:
            raise ValueError(f"Unsupported vector store: {store_name}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_embedder.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Sequence, Optional


class IEmbedder(ABC):
    """Interface for all embedding backends."""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        # Return embedding vector for a single text
        pass

    @abstractmethod
    def embed_batch(self, texts: Sequence[str], batch_size: Optional[int] = None) -> List[List[float]]:
        # Return embedding vectors for a batch of texts
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        # Return embedding dimension
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_vector_store.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional


class IVectorStore(ABC):
    """Interface for all vector store backends."""

    @abstractmethod
    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: Optional[List[Dict[str, Any]]] = None) -> None:
        # Add vectors and associated payloads to the store
        pass

    @abstractmethod
    def persist(self) -> None:
        # Persist the store to disk
        pass

    @abstractmethod
    def close(self) -> None:
        # Close resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\auto_gt.py ==== 
from __future__ import annotations
import re
import math
from typing import Dict, List, Tuple, Any
from collections import Counter, defaultdict

# -----------------------------
# Citation parsing
# -----------------------------
_CITATION_PATTERN = re.compile(r"\[(\d+)\]")

def extract_citation_indices(text: str) -> List[int]:
    # Parse numeric citations like [1], [2], [10]
    if not text:
        return []
    return [int(m.group(1)) for m in _CITATION_PATTERN.finditer(text)]

# -----------------------------
# Lightweight TF-IDF utilities
# -----------------------------
def _tokenize(s: str) -> List[str]:
    # Very simple alnum tokenizer, lowercased
    return re.findall(r"[a-z0-9]+", (s or "").lower())

def _tf(doc_tokens: List[str]) -> Dict[str, float]:
    # Term frequency as normalized counts
    c = Counter(doc_tokens)
    n = float(sum(c.values())) or 1.0
    return {t: v / n for t, v in c.items()}

def _idf(docs_tokens: List[List[str]]) -> Dict[str, float]:
    # Inverse document frequency with log smoothing
    N = float(len(docs_tokens)) or 1.0
    df = Counter()
    for dt in docs_tokens:
        df.update(set(dt))
    return {t: math.log((N + 1.0) / (df[t] + 1.0)) + 1.0 for t in df}

def _dot(a: Dict[str, float], b: Dict[str, float]) -> float:
    # Sparse dot product
    if len(a) > len(b):
        a, b = b, a
    return sum(v * b.get(k, 0.0) for k, v in a.items())

def _norm(a: Dict[str, float]) -> float:
    # L2 norm
    return math.sqrt(sum(v * v for v in a.values())) or 1e-12

def cosine_tfidf(a_text: str, b_text: str) -> float:
    # Compute cosine similarity over small, local TF-IDF
    a_tok = _tokenize(a_text)
    b_tok = _tokenize(b_text)
    if not a_tok or not b_tok:
        return 0.0
    idf = _idf([a_tok, b_tok])
    a_vec = {t: _tf(a_tok).get(t, 0.0) * idf.get(t, 0.0) for t in set(a_tok)}
    b_vec = {t: _tf(b_tok).get(t, 0.0) * idf.get(t, 0.0) for t in set(b_tok)}
    return _dot(a_vec, b_vec) / (_norm(a_vec) * _norm(b_vec))

# -----------------------------
# Auto-GT grading
# -----------------------------
def auto_grade_relevance(
    query_id: str,
    retrieved_chunks: List[Dict[str, Any]],
    model_output: str,
    high_sim: float = 0.35,
    mid_sim: float = 0.18,
    top_rank_bonus: int = 3
) -> Dict[str, int]:
    """
    Build graded relevance 0..3 without any manual labels.
    Rules:
      3: cited + sim >= high_sim
      2: cited + sim >= mid_sim (or strong)
      1: not cited + sim >= high_sim OR within top 'top_rank_bonus'
      0: else
    """
    # Map numeric citations [i] to chunk indices (1-based)
    cited_numbers = set(extract_citation_indices(model_output))
    # Prepare ranks and ids
    grades: Dict[str, int] = {}
    for rank, ch in enumerate(retrieved_chunks, start=1):
        cid = ch.get("id") or f"auto::{rank}"
        text = ch.get("text", "") or ""
        # Similarity against full answer
        sim = cosine_tfidf(model_output, text)
        # Citation check: try index field or fallback by heuristics
        idx = ch.get("index") or ch.get("rank") or rank
        cited = int(idx) in cited_numbers

        # Score-based grade
        if cited and sim >= high_sim:
            g = 3
        elif cited and sim >= mid_sim:
            g = 2
        elif (not cited) and (sim >= high_sim or rank <= top_rank_bonus):
            g = 1
        else:
            g = 0
        grades[cid] = int(g)

    return grades
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\convergence_plotter.py ==== 
from __future__ import annotations
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List
from src.core.evaluation.plot_style import apply_scientific_style, annotate_sample_info

class ConvergencePlotter:
    """Aggregates summary.json files across n-stages and visualizes mean ±95% CI convergence."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        self.charts_dir = Path(charts_dir)
        apply_scientific_style()

    def _load_stage_summaries(self) -> List[dict]:
        """Collect all summary_n*.json files (one per n-stage)."""
        summaries = []
        for fp in sorted(self.charts_dir.glob("summary_n*.json")):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                data["n"] = int("".join([c for c in fp.stem if c.isdigit()]))
                summaries.append(data)
            except Exception:
                continue
        return sorted(summaries, key=lambda x: x["n"])

    def plot(self) -> None:
        """Plot mean ±95% CI vs n for NDCG@k and Faithfulness."""
        data = self._load_stage_summaries()
        if not data:
            print("No stage summaries found in charts directory.")
            return

        ns = np.array([d["n"] for d in data])
        nd_mean = np.array([d["ndcg@k_mean"] for d in data])
        nd_lo = np.array([d["ndcg@k_ci95_lo"] for d in data])
        nd_hi = np.array([d["ndcg@k_ci95_hi"] for d in data])

        fa_mean = np.array([d["faith_mean"] for d in data])
        fa_lo = np.array([d["faith_ci95_lo"] for d in data])
        fa_hi = np.array([d["faith_ci95_hi"] for d in data])

        fig, ax = plt.subplots(figsize=(6.5, 4.5))

        # NDCG@k line
        ax.plot(ns, nd_mean, "-o", color="#1b9e77", label="NDCG@k mean")
        ax.fill_between(ns, nd_lo, nd_hi, color="#1b9e77", alpha=0.2)

        # Faithfulness line
        ax.plot(ns, fa_mean, "-o", color="#d95f02", label="Faithfulness mean")
        ax.fill_between(ns, fa_lo, fa_hi, color="#d95f02", alpha=0.2)

        ax.set_xlabel("Sample size n")
        ax.set_ylabel("Metric value")
        ax.set_title("Convergence of Evaluation Metrics (mean ±95% CI)")
        ax.set_ylim(0, 1.05)
        ax.legend(frameon=False)
        annotate_sample_info(ax, n=len(ns))

        fig.tight_layout()
        for ext in ("png", "svg"):
            fig.savefig(self.charts_dir / f"convergence_plot.{ext}", dpi=150, bbox_inches="tight")
        plt.close(fig)
        print(f"Convergence plot saved to {self.charts_dir}/convergence_plot.*")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from src.core.evaluation.interfaces.i_metric import IMetric
from src.core.evaluation.metrics.ndcg_metric import NDCGMetric
from src.core.evaluation.metrics.faithfulness_metric import FaithfulnessMetric
from src.core.evaluation.ground_truth_builder import GroundTruthBuilder
from src.core.evaluation.utils import make_chunk_id

logger = logging.getLogger("EvaluationOrchestrator")


class EvaluationOrchestrator:
    """Coordinates metric computation and automatic semantic ground-truth generation."""

    def __init__(self, output_dir: str = "data/eval_logs", k: int = 10):
        self.out = Path(output_dir)
        self.out.mkdir(parents=True, exist_ok=True)
        self.k = k

        # Core evaluation metrics
        self.metrics: Dict[str, IMetric] = {
            "ndcg@k": NDCGMetric(k=k),
            "faithfulness": FaithfulnessMetric(),
        }

        # Semantic ground truth builder
        self.gt_builder = GroundTruthBuilder()

        logger.info(f"Evaluation orchestrator ready | k={self.k}")

    # ------------------------------------------------------------------
    def _ensure_chunk_ids(self, items: List[Dict[str, Any]]) -> None:
        """Ensure every retrieved chunk has a stable, unique ID."""
        for ch in items:
            if not ch.get("id"):
                ch["id"] = make_chunk_id(ch)

    # ------------------------------------------------------------------
    def _safe_id(self, s: str | None) -> str:
        """Generate a filesystem-safe identifier for query strings."""
        if not s:
            return "query"
        return "".join(ch if ch.isalnum() or ch in "-_" else "_" for ch in s)[:80] or "query"

    # ------------------------------------------------------------------
    def evaluate_single(self, query: str, retrieved_chunks: List[Dict[str, Any]], model_output: str) -> Dict[str, float]:
        """Compute NDCG and Faithfulness for a single query."""
        self._ensure_chunk_ids(retrieved_chunks)

        # Build semantic ground truth dynamically per query
        gt_map = self.gt_builder.build(query, retrieved_chunks)
        relevance_scores = [int(gt_map.get(ch["id"], ch.get("relevance", 0))) for ch in retrieved_chunks]

        ndcg_val = self.metrics["ndcg@k"].compute(relevance_scores=relevance_scores)
        faith_val = self.metrics["faithfulness"].compute(
            context_chunks=[c.get("text", "") for c in retrieved_chunks],
            answer=model_output,
        )

        qid = self._safe_id(query)
        result = {"query_id": qid, "ndcg@k": ndcg_val, "faithfulness": faith_val}

        out_file = self.out / f"{qid}_evaluation.json"
        out_file.write_text(json.dumps(result, indent=2), encoding="utf-8")
        logger.info(f"Evaluation completed → {out_file}")
        return result

    # ------------------------------------------------------------------
    def evaluate_batch_from_logs(self, logs_dir: str = "data/logs", pattern: str = "llm_*.json") -> Dict[str, float]:
        """Run evaluation for all LLM log files and compute aggregate metrics."""
        files = sorted(Path(logs_dir).glob(pattern))
        nd_vals, fa_vals = [], []

        for fp in files:
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                query = data.get("query") or data.get("user_query") or data.get("prompt") or ""
                model_output = data.get("model_output") or data.get("answer") or ""
                retrieved = data.get("retrieved_chunks") or data.get("context_snippets") or []

                # Normalize schema
                for rank, ch in enumerate(retrieved, start=1):
                    ch.setdefault("rank", rank)
                    ch.setdefault("final_score", ch.get("score", 0.0))
                    if "text" not in ch and "snippet" in ch:
                        ch["text"] = ch["snippet"]

                if not query or not retrieved:
                    logger.warning(f"Skipped incomplete log: {fp.name}")
                    continue

                res = self.evaluate_single(query, retrieved, model_output)
                nd_vals.append(float(res["ndcg@k"]))
                fa_vals.append(float(res["faithfulness"]))

            except Exception as e:
                err_path = self.out / f"{fp.stem}_eval_error.json"
                err_path.write_text(json.dumps({"error": str(e)}, indent=2), encoding="utf-8")
                logger.error(f"Evaluation failed for {fp.name}: {e}")

        summary = {
            "files": len(files),
            "mean_ndcg@k": float(sum(nd_vals) / len(nd_vals)) if nd_vals else 0.0,
            "mean_faithfulness": float(sum(fa_vals) / len(fa_vals)) if fa_vals else 0.0,
        }

        (self.out / "evaluation_summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
        logger.info(f"Batch evaluation completed for {len(files)} logs.")
        return summary
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_table_exporter.py ==== 
from __future__ import annotations
import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List

class EvaluationTableExporter:
    """Exports evaluation summary statistics as LaTeX, CSV, and Markdown tables."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        self.charts_dir = Path(charts_dir)
        self.summary_path = self.charts_dir / "summary.json"
        self.out_tex = self.charts_dir / "evaluation_table.tex"
        self.out_csv = self.charts_dir / "evaluation_table.csv"
        self.out_md  = self.charts_dir / "evaluation_table.md"

    # ------------------------------------------------------------------
    def _load_summary(self) -> Dict[str, Any]:
        if not self.summary_path.exists():
            raise FileNotFoundError(f"Missing summary.json in {self.charts_dir}")
        return json.loads(self.summary_path.read_text(encoding="utf-8"))

    # ------------------------------------------------------------------
    def _format_value(self, mean: float, lo: float, hi: float, digits: int = 3) -> str:
        """Format mean ± CI string."""
        return f"{mean:.{digits}f} ± {((hi - lo) / 2):.{digits}f}"

    # ------------------------------------------------------------------
    def export(self) -> Dict[str, Any]:
        """Generate LaTeX/CSV/MD tables from summary.json."""
        data = self._load_summary()
        if not data or "files" not in data:
            raise ValueError("Invalid or incomplete summary.json")

        rows: List[Dict[str, Any]] = [
            {
                "Metric": "NDCG@k",
                "Mean ± CI": self._format_value(data["ndcg@k_mean"],
                                                data["ndcg@k_ci95_lo"],
                                                data["ndcg@k_ci95_hi"]),
                "Median": f"{data['ndcg@k_median']:.3f}",
                "Std": f"{data['ndcg@k_std']:.3f}"
            },
            {
                "Metric": "Faithfulness",
                "Mean ± CI": self._format_value(data["faith_mean"],
                                                data["faith_ci95_lo"],
                                                data["faith_ci95_hi"]),
                "Median": f"{data['faith_median']:.3f}",
                "Std": f"{data['faith_std']:.3f}"
            }
        ]

        df = pd.DataFrame(rows)
        # CSV
        df.to_csv(self.out_csv, index=False, encoding="utf-8")

        # Markdown
        md_lines = ["| Metric | Mean ± CI | Median | Std |",
                    "|:--|--:|--:|--:|"]
        for r in rows:
            md_lines.append(f"| {r['Metric']} | {r['Mean ± CI']} | {r['Median']} | {r['Std']} |")
        self.out_md.write_text("\n".join(md_lines), encoding="utf-8")

        # LaTeX
        tex = [
            "\\begin{table}[h]",
            "\\centering",
            "\\caption{Evaluation metrics with 95\\% confidence intervals (bootstrap).}",
            "\\label{tab:evaluation_results}",
            "\\begin{tabular}{lccc}",
            "\\toprule",
            "Metric & Mean $\\pm$ CI & Median & Std \\\\",
            "\\midrule",
        ]
        for r in rows:
            tex.append(f"{r['Metric']} & {r['Mean ± CI']} & {r['Median']} & {r['Std']} \\\\")
        tex.extend([
            "\\bottomrule",
            "\\end{tabular}",
            "\\end{table}"
        ])
        self.out_tex.write_text("\n".join(tex), encoding="utf-8")

        return {
            "csv": str(self.out_csv),
            "md": str(self.out_md),
            "tex": str(self.out_tex),
            "rows": len(rows)
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_visualizer.py ==== 
# src/core/evaluation/evaluation_visualizer.py
from __future__ import annotations
import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr  # for rank-based correlation

from src.core.evaluation.plot_style import (
    apply_scientific_style,
    annotate_sample_info,
    add_violin_overlay
)


@dataclass
class VizConfig:
    logs_dir: str = "data/eval_logs"
    out_dir: str = "data/eval_charts"
    pattern: str = "*_evaluation.json"
    bootstrap_iters: int = 2000
    random_seed: int = 42
    iqr_k: float = 1.5
    z_thresh: float = 3.0


class EvaluationVisualizer:
    """Creates publication-ready plots and analytical summaries from evaluation JSON logs."""

    def __init__(self, cfg: VizConfig | None = None):
        self.cfg = cfg or VizConfig()
        self.logs_dir = Path(self.cfg.logs_dir)
        self.out_dir = Path(self.cfg.out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)
        np.random.seed(self.cfg.random_seed)
        apply_scientific_style()
        self._fig_no = 1

    # ------------------------------------------------------------------
    def _load_eval_rows(self) -> pd.DataFrame:
        rows: List[Dict[str, Any]] = []
        for fp in sorted(self.logs_dir.glob(self.cfg.pattern)):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                qid = data.get("query_id") or fp.stem
                ndcg = float(data.get("ndcg@k", np.nan))
                faith = float(data.get("faithfulness", np.nan))
                rows.append({"query_id": qid, "ndcg@k": ndcg, "faithfulness": faith})
            except Exception:
                continue
        df = pd.DataFrame(rows)
        if df.empty:
            return pd.DataFrame(columns=["query_id", "ndcg@k", "faithfulness"])
        return df.dropna(how="all", subset=["ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def _bootstrap_ci(self, arr: np.ndarray, iters: int) -> Tuple[float, float, float]:
        arr = np.asarray(arr, dtype=float)
        arr = arr[~np.isnan(arr)]
        if arr.size == 0:
            return (float("nan"), float("nan"), float("nan"))
        boot = np.empty(iters, dtype=float)
        n = arr.size
        idx = np.random.randint(0, n, size=(iters, n))
        boot[:] = np.mean(arr[idx], axis=1)
        m = float(np.mean(boot))
        lo = float(np.percentile(boot, 2.5))
        hi = float(np.percentile(boot, 97.5))
        return (m, lo, hi)

    # ------------------------------------------------------------------
    def _outliers_iqr(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, dtype=float)
        q1, q3 = np.nanpercentile(arr, [25, 75])
        iqr = q3 - q1
        lo = q1 - self.cfg.iqr_k * iqr
        hi = q3 + self.cfg.iqr_k * iqr
        return (arr < lo) | (arr > hi)

    def _outliers_z(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, dtype=float)
        mu = np.nanmean(arr)
        sd = np.nanstd(arr)
        if sd == 0 or np.isnan(sd):
            return np.zeros_like(arr, dtype=bool)
        z = (arr - mu) / sd
        return np.abs(z) > self.cfg.z_thresh

    # ------------------------------------------------------------------
    def _save_df(self, name: str, df: pd.DataFrame) -> None:
        path = self.out_dir / f"{name}.csv"
        df.to_csv(path, index=False, encoding="utf-8")

    def _save_fig(self, fig: plt.Figure, stem: str) -> None:
        png = self.out_dir / f"{stem}.png"
        svg = self.out_dir / f"{stem}.svg"
        fig.savefig(png, dpi=150, bbox_inches="tight")
        fig.savefig(svg, bbox_inches="tight")

    def _titled(self, base: str) -> str:
        title = f"Figure {self._fig_no}: {base}"
        self._fig_no += 1
        return title

    # ------------------------------------------------------------------
    def plot_histograms(self, df: pd.DataFrame) -> None:
        for col in ["ndcg@k", "faithfulness"]:
            series = df[col].astype(float).dropna().values
            fig = plt.figure(figsize=(6, 4))
            plt.hist(series, bins=12, edgecolor="black")
            plt.xlabel(col)
            plt.ylabel("Count")
            plt.title(self._titled(f"Histogram of {col}"))
            plt.xlim(0, 1)
            annotate_sample_info(plt.gca(), n=len(series), bootstrap_iters=self.cfg.bootstrap_iters)
            plt.tight_layout()
            self._save_fig(fig, f"hist_{col}")
            plt.close(fig)

    # ------------------------------------------------------------------
    def plot_box_violin(self, df: pd.DataFrame) -> None:
        for col in ["faithfulness", "ndcg@k"]:
            vals = df[col].astype(float).dropna().values
            if len(vals) == 0:
                continue
            fig = plt.figure(figsize=(4.5, 4.5))
            ax = plt.gca()
            if col == "ndcg@k" and np.nanstd(vals) < 0.02:
                plt.hist(vals, bins=10, range=(0.9, 1.0), color="#1b9e77", alpha=0.6)
                plt.xlabel(col)
                plt.ylabel("Count")
                plt.title(self._titled(f"Histogram (collapsed) of {col}"))
            else:
                ax.boxplot(vals, vert=True, labels=[col], showmeans=True)
                add_violin_overlay(ax, vals, color="#1b9e77")
                plt.ylabel(col)
                plt.title(self._titled(f"Box + Violin of {col}"))
            annotate_sample_info(ax, n=len(vals))
            plt.tight_layout()
            self._save_fig(fig, f"box_violin_{col}")
            plt.close(fig)

    # ------------------------------------------------------------------
    def plot_scatter_correlation(self, df: pd.DataFrame) -> Tuple[float, float]:
        """Scatter ndcg vs faithfulness with Pearson and Spearman correlation."""
        x = df["ndcg@k"].astype(float).values
        y = df["faithfulness"].astype(float).values
        mask = ~np.isnan(x) & ~np.isnan(y)
        if mask.sum() < 3:
            r_p, r_s = float("nan"), float("nan")
        else:
            r_p = float(np.corrcoef(x[mask], y[mask])[0, 1])
            r_s, _ = spearmanr(x[mask], y[mask])
        fig = plt.figure(figsize=(6, 4))
        plt.scatter(x[mask], y[mask], s=25, alpha=0.7)
        plt.xlabel("NDCG@k")
        plt.ylabel("Faithfulness")
        plt.title(self._titled(f"Scatter NDCG@k vs Faithfulness (r={r_p:.3f}, ρ={r_s:.3f})"))
        annotate_sample_info(plt.gca(), n=int(mask.sum()))
        plt.tight_layout()
        self._save_fig(fig, "scatter_ndcg_vs_faithfulness")
        plt.close(fig)
        return r_p, r_s

    # ------------------------------------------------------------------
    def plot_run_order_control(self, df: pd.DataFrame) -> None:
        df = df.reset_index(drop=True).copy()
        df["idx"] = np.arange(len(df))
        for col in ["ndcg@k", "faithfulness"]:
            vals = df[col].astype(float).values
            mu = float(np.nanmean(vals))
            sd = float(np.nanstd(vals))
            ucl = mu + 3 * sd
            lcl = mu - 3 * sd
            fig = plt.figure(figsize=(8, 4))
            plt.fill_between(df["idx"], mu - sd, mu + sd, color="gray", alpha=0.2, label="±1σ")
            plt.plot(df["idx"], vals, marker="o", linestyle="-", linewidth=1)
            plt.axhline(mu, linestyle="--", label="mean")
            plt.axhline(ucl, linestyle=":", label="+3σ")
            plt.axhline(lcl, linestyle=":", label="-3σ")
            plt.xlabel("Run index")
            plt.ylabel(col)
            plt.legend(frameon=False)
            plt.title(self._titled(f"Run-order chart for {col}"))
            annotate_sample_info(plt.gca(), n=len(df))
            plt.tight_layout()
            self._save_fig(fig, f"run_order_{col}")
            plt.close(fig)

    # ------------------------------------------------------------------
    def detect_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        out = []
        for _, row in df.iterrows():
            qid = row["query_id"]
            nd = float(row["ndcg@k"])
            fa = float(row["faithfulness"])
            out.append((qid, nd, fa))
        if not out:
            return pd.DataFrame(columns=["query_id", "ndcg@k", "faithfulness"])
        arr_nd = np.array([x[1] for x in out], dtype=float)
        arr_fa = np.array([x[2] for x in out], dtype=float)
        mask = self._outliers_iqr(arr_nd) | self._outliers_z(arr_nd) | self._outliers_iqr(arr_fa) | self._outliers_z(arr_fa)
        return pd.DataFrame([out[i] for i in range(len(out)) if mask[i]], columns=["query_id", "ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def summarize(self, df: pd.DataFrame) -> Dict[str, Any]:
        nd = df["ndcg@k"].astype(float).values
        fa = df["faithfulness"].astype(float).values
        nd_m, nd_lo, nd_hi = self._bootstrap_ci(nd, self.cfg.bootstrap_iters)
        fa_m, fa_lo, fa_hi = self._bootstrap_ci(fa, self.cfg.bootstrap_iters)
        summary = {
            "files": int(df.shape[0]),
            "ndcg@k_mean": nd_m,
            "ndcg@k_ci95_lo": nd_lo,
            "ndcg@k_ci95_hi": nd_hi,
            "faith_mean": fa_m,
            "faith_ci95_lo": fa_lo,
            "faith_ci95_hi": fa_hi,
            "ndcg@k_median": float(np.nanmedian(nd)),
            "faith_median": float(np.nanmedian(fa)),
            "ndcg@k_std": float(np.nanstd(nd)),
            "faith_std": float(np.nanstd(fa)),
            "bootstrap_iters": int(self.cfg.bootstrap_iters),
            "iqr_k": float(self.cfg.iqr_k),
            "z_thresh": float(self.cfg.z_thresh),
        }
        (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
        return summary

    # ------------------------------------------------------------------
    def save_markdown_report(self, df: pd.DataFrame, summary: Dict[str, Any],
                             r_p: float, r_s: float, outliers: pd.DataFrame) -> None:
        md = [
            "# Evaluation Analytics Report",
            "",
            f"- Files (n): **{summary['files']}**",
            f"- Mean NDCG@k: **{summary['ndcg@k_mean']:.4f}** (95% CI: {summary['ndcg@k_ci95_lo']:.4f} … {summary['ndcg@k_ci95_hi']:.4f})",
            f"- Mean Faithfulness: **{summary['faith_mean']:.4f}** (95% CI: {summary['faith_ci95_lo']:.4f} … {summary['faith_ci95_hi']:.4f})",
            "",
            "## Korrelationsanalyse",
        ]
        if not math.isnan(r_p):
            md.append(f"- Pearson r: **{r_p:.3f}**")
        if not math.isnan(r_s):
            md.append(f"- Spearman ρ: **{r_s:.3f}**")
        md.append("→ Schwache bis moderate positive Abhängigkeit zwischen Ranking-Güte und Antworttreue; "
                  "Faithfulness kann zusätzlich durch semantische Kohärenzmetriken (BERTScore, FactScore) kontrastiert werden.")
        md += [
            "",
            "## Method Parameters",
            f"- bootstrap iterations: `{summary.get('bootstrap_iters', self.cfg.bootstrap_iters)}`",
            f"- IQR fence k: `{summary.get('iqr_k', self.cfg.iqr_k)}`",
            f"- z-score threshold: `{summary.get('z_thresh', self.cfg.z_thresh)}`",
            "",
            "## Plots",
            "- Histograms: `hist_*.png|svg`",
            "- Box/Density: `box_violin_*`",
            "- Scatter: `scatter_ndcg_vs_faithfulness.*`",
            "- Run-order: `run_order_*.*`",
            "",
            "## Fazit & nächste Schritte",
            "✅ Retrieval-Architektur gilt als stabil; Optimierungspotenzial liegt im Faithfulness-Level.",
            "",
            "🔬 **Empfohlene Experimente:**",
            "- Vergleich mit/ohne temporale Gewichtung (`temporal_mode=True/False`) – erwarteter Δ Faithfulness ≈ 0.02–0.04.",
            "- Dekaden-basierte Analyse (nach Implementierung der erweiterten Jahrerkennung).",
            "- Konvergenzstudie mit n = 30, 50, 100 → Beobachtung der CI-Stabilisierung.",
            "",
        ]
        if not outliers.empty:
            md.append("## Outliers (IQR or z-score flagged)")
            md.append("")
            md.append("| query_id | ndcg@k | faithfulness |")
            md.append("|---|---:|---:|")
            for _, rrow in outliers.iterrows():
                md.append(f"| {rrow['query_id']} | {float(rrow['ndcg@k']):.4f} | {float(rrow['faithfulness']):.4f} |")
        else:
            md.append("## Outliers")
            md.append("No outliers detected by IQR/z-score criteria.")
        (self.out_dir / "report.md").write_text("\n".join(md), encoding="utf-8")

    # ------------------------------------------------------------------
    def run_all(self) -> Dict[str, Any]:
        df = self._load_eval_rows()
        self._save_df("raw_eval", df)
        if df.empty:
            summary = {"files": 0}
            (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
            (self.out_dir / "report.md").write_text("# Evaluation Analytics Report\n\nNo data.", encoding="utf-8")
            return summary
        self.plot_histograms(df)
        self.plot_box_violin(df)
        r_p, r_s = self.plot_scatter_correlation(df)
        self.plot_run_order_control(df)
        outliers = self.detect_outliers(df)
        self._save_df("outliers", outliers)
        summary = self.summarize(df)
        self.save_markdown_report(df, summary, r_p, r_s, outliers)
        return summary
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\ground_truth_builder.py ==== 
from __future__ import annotations
from sentence_transformers import SentenceTransformer, util
from typing import Dict, Any, List
import numpy as np
import logging

logger = logging.getLogger("GroundTruthBuilder")

class GroundTruthBuilder:
    """Generates automatic, semantic ground-truth relevance labels."""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2", high_thr: float = 0.35, mid_thr: float = 0.20):
        self.model = SentenceTransformer(model_name)
        self.high_thr = high_thr
        self.mid_thr = mid_thr

    # ------------------------------------------------------------------
    def build(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, int]:
        """Return dict of {doc_id: relevance_score (0..3)}."""
        if not query or not retrieved_docs:
            return {}

        q_emb = self.model.encode(query, normalize_embeddings=True)
        truth: Dict[str, int] = {}
        for d in retrieved_docs:
            text = d.get("text", "")
            doc_id = d.get("id") or f"{d.get('metadata', {}).get('source_file')}"
            d_emb = self.model.encode(text, normalize_embeddings=True)
            sim = float(util.cos_sim(q_emb, d_emb))
            if sim >= self.high_thr:
                rel = 3
            elif sim >= self.mid_thr:
                rel = 2
            elif sim >= self.mid_thr / 2:
                rel = 1
            else:
                rel = 0
            truth[doc_id] = rel

        logger.info(f"Generated semantic ground truth for query (avg rel={np.mean(list(truth.values())):.2f})")
        return truth
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\plot_style.py ==== 
# src/core/evaluation/plot_style.py
"""
Unified visualization style for scientific evaluation plots.
Applies consistent layout, fonts, and color palette across all figures.
Follows principles from Tufte (1983), IEEE Vis Guidelines, and RSS Data Viz Guide (2023).
"""

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np


# ----------------------------------------------------------------------
def apply_scientific_style() -> None:
    """Configure Matplotlib for publication-grade scientific figures."""
    mpl.rcParams.update({
        # Rendering quality
        "figure.dpi": 300,
        "savefig.dpi": 300,
        "savefig.format": "svg",
        "savefig.bbox": "tight",

        # Fonts and layout
        "font.family": "sans-serif",
        "font.sans-serif": ["Arial", "DejaVu Sans", "Liberation Sans"],
        "font.size": 10,
        "axes.labelsize": 10,
        "axes.titlesize": 11,
        "legend.fontsize": 9,
        "xtick.labelsize": 9,
        "ytick.labelsize": 9,

        # Axes and grid
        "axes.linewidth": 0.8,
        "axes.grid": True,
        "grid.alpha": 0.25,
        "grid.linestyle": "--",
        "axes.spines.top": False,
        "axes.spines.right": False,

        # Perceptually uniform, colorblind-safe palette
        "axes.prop_cycle": mpl.cycler(color=[
            "#1b9e77",  # teal
            "#d95f02",  # orange
            "#7570b3",  # purple
            "#e7298a",  # magenta
            "#66a61e",  # green
            "#e6ab02"   # yellow-brown
        ]),

        # Figure layout
        "figure.figsize": (6, 4),
        "figure.autolayout": True,
        "savefig.transparent": False,
    })


# ----------------------------------------------------------------------
def annotate_sample_info(
    ax: plt.Axes,
    n: int | None = None,
    k: int | None = None,
    bootstrap_iters: int | None = None,
    show_conf_int: tuple[float, float] | None = None
) -> None:
    """
    Add standardized annotation text (sample info, parameters) inside a figure.
    Example: n=100, k=5, boot=2000, CI=95%
    """
    txt_parts = []
    if n is not None:
        txt_parts.append(f"n={n}")
    if k is not None:
        txt_parts.append(f"k={k}")
    if bootstrap_iters is not None:
        txt_parts.append(f"boot={bootstrap_iters}")
    if show_conf_int is not None:
        lo, hi = show_conf_int
        txt_parts.append(f"95% CI=[{lo:.3f}, {hi:.3f}]")

    if not txt_parts:
        return

    # Minimalist annotation, bottom-right, partially transparent
    ax.text(
        0.98, 0.02,
        ", ".join(txt_parts),
        ha="right", va="bottom",
        fontsize=8,
        color="gray",
        alpha=0.85,
        transform=ax.transAxes,
        bbox=dict(facecolor="white", edgecolor="none", alpha=0.55, boxstyle="round,pad=0.2")
    )


# ----------------------------------------------------------------------
def add_violin_overlay(ax: plt.Axes, data: np.ndarray, color: str = "#1b9e77") -> None:
    """
    Add a violin-style density overlay (no seaborn dependency).
    Uses kernel density estimation for smooth distribution visualization.
    """
    from scipy.stats import gaussian_kde

    if len(data) < 5:
        return  # too few samples for a meaningful KDE
    data = np.asarray(data, dtype=float)
    data = data[~np.isnan(data)]
    if data.size == 0:
        return

    # Numerical stability: broaden domain slightly if constant values
    dmin, dmax = float(np.min(data)), float(np.max(data))
    if dmax - dmin < 1e-6:
        dmin -= 1e-3
        dmax += 1e-3

    kde = gaussian_kde(data)
    xs = np.linspace(dmin, dmax, 200)
    ys = kde(xs)
    ys = ys / ys.max() * 0.25  # normalized width (relative to axis scale)

    # Symmetric fill for violin overlay
    ax.fill_betweenx(xs, -ys, ys, facecolor=color, alpha=0.18, linewidth=0)
    ax.plot(ys, xs, color=color, alpha=0.5, linewidth=0.6)
    ax.plot(-ys, xs, color=color, alpha=0.5, linewidth=0.6)

    # Maintain centered x-limits (prevents horizontal shift)
    cur_xlim = ax.get_xlim()
    pad = (cur_xlim[1] - cur_xlim[0]) * 0.05
    ax.set_xlim(cur_xlim[0] - pad, cur_xlim[1] + pad)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\report_builder.py ==== 
# src/core/evaluation/report_builder.py
from __future__ import annotations
import logging
from pathlib import Path
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak
)
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors
import json
import pandas as pd
import numpy as np
from scipy.stats import pearsonr, spearmanr

logger = logging.getLogger("ReportBuilder")


class ReportBuilder:
    """Generates a publication-ready PDF report summarizing benchmark results with correlations and outlier analysis."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        self.charts_dir = Path(charts_dir)
        self.styles = getSampleStyleSheet()
        self.styleN = self.styles["Normal"]
        self.styleH = self.styles["Heading1"]
        self.styleH2 = self.styles["Heading2"]

    # ------------------------------------------------------------------
    def _load_summary(self) -> dict:
        summary_path = self.charts_dir / "summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"Missing summary.json in {self.charts_dir}")
        return json.loads(summary_path.read_text(encoding="utf-8"))

    def _load_detailed_results(self) -> pd.DataFrame:
        """Load all evaluation JSON files to compute correlations."""
        eval_logs = sorted(self.charts_dir.parent.glob("eval_logs/*_evaluation.json"))
        records = []
        for fp in eval_logs:
            try:
                d = json.loads(fp.read_text(encoding="utf-8"))
                records.append({
                    "query_id": d.get("query_id", fp.stem),
                    "ndcg@k": float(d.get("ndcg@k", np.nan)),
                    "faithfulness": float(d.get("faithfulness", np.nan))
                })
            except Exception:
                continue
        return pd.DataFrame(records).dropna(subset=["ndcg@k", "faithfulness"])

    def _load_table(self) -> list[list[str]]:
        table_path = self.charts_dir / "evaluation_table.csv"
        if not table_path.exists():
            return []
        rows = [ln.strip().split(",") for ln in table_path.read_text(encoding="utf-8").splitlines()]
        return rows

    def _find_images(self) -> list[Path]:
        """Collect all visualization images (.png) for inclusion."""
        return sorted(self.charts_dir.glob("*.png"))

    # ------------------------------------------------------------------
    def _compute_correlations(self, df: pd.DataFrame) -> dict:
        """Compute Pearson and Spearman correlations between NDCG@k and Faithfulness."""
        if df.empty:
            return {}
        pearson_corr, pearson_p = pearsonr(df["ndcg@k"], df["faithfulness"])
        spearman_corr, spearman_p = spearmanr(df["ndcg@k"], df["faithfulness"])
        return {
            "pearson_r": pearson_corr,
            "pearson_p": pearson_p,
            "spearman_rho": spearman_corr,
            "spearman_p": spearman_p
        }

    def _detect_outliers(self, df: pd.DataFrame, z_thresh: float = 2.0) -> pd.DataFrame:
        """Identify statistical outliers based on z-scores."""
        if df.empty:
            return df
        df = df.copy()
        df["z_faith"] = (df["faithfulness"] - df["faithfulness"].mean()) / (df["faithfulness"].std() + 1e-6)
        df["z_ndcg"] = (df["ndcg@k"] - df["ndcg@k"].mean()) / (df["ndcg@k"].std() + 1e-6)
        df["is_outlier"] = (abs(df["z_faith"]) > z_thresh) | (abs(df["z_ndcg"]) > z_thresh)
        return df[df["is_outlier"]]

    # ------------------------------------------------------------------
    def build(self, custom_name: str | None = None) -> Path:
        """Compose and export the extended PDF benchmark report.

        Parameters
        ----------
        custom_name : str | None
            Optional custom PDF filename, e.g. 'benchmark_report_n50.pdf'.
            If None, report name is derived automatically from the number of evaluated prompts.
        """
        summary = self._load_summary()
        num_prompts = summary.get("files", 0)
        auto_name = f"benchmark_report_n{num_prompts}.pdf" if num_prompts else "benchmark_report.pdf"
        pdf_name = custom_name or auto_name
        pdf_path = self.charts_dir / pdf_name

        doc = SimpleDocTemplate(
            str(pdf_path),
            pagesize=A4,
            rightMargin=2 * cm,
            leftMargin=2 * cm,
            topMargin=2 * cm,
            bottomMargin=2 * cm,
        )

        story = []
        now = datetime.now().strftime("%Y-%m-%d %H:%M")

        # --- Title page ---
        story.append(Paragraph("<b>Benchmark Evaluation Report</b>", self.styles["Title"]))
        story.append(Spacer(1, 1 * cm))
        story.append(Paragraph(f"Generated automatically on {now}", self.styleN))
        story.append(Spacer(1, 0.5 * cm))
        story.append(Paragraph(
            "This report summarizes automated evaluations of retrieval-augmented generation "
            "systems within the <i>Historical Drift Analyzer</i> architecture. "
            "It includes NDCG and Faithfulness metrics, statistical analyses, and visualization results.",
            self.styleN,
        ))
        story.append(PageBreak())

        # --- Summary metrics ---
        story.append(Paragraph("1. Summary Statistics", self.styleH))
        story.append(Spacer(1, 0.3 * cm))
        kv_pairs = [[k.replace('_', ' '), f"{v:.4f}" if isinstance(v, (int, float)) else str(v)]
                    for k, v in summary.items()]
        table = Table(kv_pairs, colWidths=[8 * cm, 6 * cm])
        table.setStyle(TableStyle([
            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
            ("ALIGN", (0, 0), (-1, -1), "CENTER"),
        ]))
        story.append(table)
        story.append(PageBreak())

        # --- Correlation & Outlier Analysis ---
        df = self._load_detailed_results()
        if not df.empty:
            story.append(Paragraph("2. Correlation and Outlier Analysis", self.styleH))
            story.append(Spacer(1, 0.3 * cm))

            corr = self._compute_correlations(df)
            if corr:
                corr_rows = [
                    ["Metric Pair", "r / ρ", "p-value"],
                    ["Pearson (linear)", f"{corr['pearson_r']:.3f}", f"{corr['pearson_p']:.3e}"],
                    ["Spearman (rank)", f"{corr['spearman_rho']:.3f}", f"{corr['spearman_p']:.3e}"],
                ]
                corr_table = Table(corr_rows, colWidths=[6 * cm, 4 * cm, 4 * cm])
                corr_table.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                ]))
                story.append(corr_table)
                story.append(Spacer(1, 0.5 * cm))
                story.append(Paragraph(
                    "Pearson r reflects linear association; Spearman ρ captures rank correlation. "
                    "Typical weak-to-moderate positive dependency indicates retrieval homogeneity, "
                    "suggesting that Faithfulness could be further contrasted with semantic coherence metrics "
                    "(e.g., BERTScore or FactScore).",
                    self.styleN,
                ))
                story.append(Spacer(1, 0.3 * cm))

            outliers = self._detect_outliers(df)
            if not outliers.empty:
                story.append(Paragraph("Detected Outliers", self.styleH2))
                story.append(Spacer(1, 0.3 * cm))
                data_rows = [["Query ID", "NDCG@k", "Faithfulness", "z_ndcg", "z_faith"]]
                for _, r in outliers.iterrows():
                    data_rows.append([
                        r["query_id"][:40],
                        f"{r['ndcg@k']:.3f}",
                        f"{r['faithfulness']:.3f}",
                        f"{r['z_ndcg']:.2f}",
                        f"{r['z_faith']:.2f}",
                    ])
                tab = Table(data_rows, colWidths=[6 * cm, 2.5 * cm, 2.5 * cm, 2.5 * cm, 2.5 * cm])
                tab.setStyle(TableStyle([
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                ]))
                story.append(tab)
            else:
                story.append(Paragraph("No statistical outliers detected.", self.styleN))
            story.append(PageBreak())

        # --- Figures ---
        images = self._find_images()
        if images:
            story.append(Paragraph("3. Visual Analytics", self.styleH))
            story.append(Spacer(1, 0.3 * cm))
            for img_path in images:
                story.append(Paragraph(img_path.name.replace("_", " "), self.styleH2))
                story.append(Image(str(img_path), width=15 * cm, height=9 * cm))
                story.append(Spacer(1, 0.5 * cm))

        # --- Interpretation ---
        story.append(PageBreak())
        story.append(Paragraph("4. Interpretation and Next Steps", self.styleH))
        story.append(Paragraph(
            "The retrieval architecture is in its target state. "
            "Remaining limitations are primarily in the Faithfulness level rather than in retrieval quality. "
            "Future experiments should contrast temporal weighting (temporal_mode=True vs. False) "
            "and perform decade-based query analyses after year-detection enhancement.",
            self.styleN,
        ))
        story.append(Spacer(1, 0.3 * cm))
        story.append(Paragraph(
            "A convergence plot (mean ±95% CI vs. n) should be used to demonstrate statistical stabilization "
            "as sample size increases. All results are reproducible by rerunning the benchmark scripts "
            "with identical configurations.",
            self.styleN,
        ))

        # --- Build PDF ---
        doc.build(story)
        logger.info(f"PDF report with correlations & outlier analysis generated → {pdf_path}")
        return pdf_path
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\utils.py ==== 
from __future__ import annotations
from typing import Dict, Any

def make_chunk_id(chunk: Dict[str, Any]) -> str:
    # Build a stable chunk id from metadata and content hash
    meta = chunk.get("metadata", {}) or {}
    src = meta.get("source_file") or meta.get("title") or "unknown"
    year = meta.get("year", "na")
    text = (chunk.get("text") or "")[:120]
    h = abs(hash(text)) % (10**8)
    return f"{src}::{year}::{h}"
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\i_metric.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict

class IMetric(ABC):
    """Interface defining a unified metric contract for all evaluators."""

    @abstractmethod
    def compute(self, **kwargs: Any) -> float:
        """Compute the metric based on explicit input parameters."""
        pass

    @abstractmethod
    def describe(self) -> Dict[str, str]:
        """Return metadata about the metric (name, type, short description)."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\faithfulness_metric.py ==== 
from __future__ import annotations
from typing import List, Dict
from sentence_transformers import SentenceTransformer, util
from src.core.evaluation.interfaces.i_metric import IMetric

class FaithfulnessMetric(IMetric):
    """Semantic faithfulness between generated answer and retrieved context (extrinsic)."""

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        # Lightweight sentence transformer
        self.model = SentenceTransformer(model_name)

    def compute(self, context_chunks: List[str], answer: str) -> float:
        """Compute mean cosine similarity between answer and context embeddings."""
        if not context_chunks or not answer:
            return 0.0
        ctx_embeds = self.model.encode(context_chunks, convert_to_tensor=True)
        ans_embed = self.model.encode([answer], convert_to_tensor=True)
        sims = util.cos_sim(ans_embed, ctx_embeds)[0]
        return float(sims.mean().item())

    def describe(self) -> Dict[str, str]:
        return {
            "name": "Faithfulness",
            "type": "extrinsic",
            "description": "Mean cosine similarity between retrieved context embeddings and LLM answer embedding."
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\ndcg_metric.py ==== 
from __future__ import annotations
import math
from typing import List, Dict
from src.core.evaluation.interfaces.i_metric import IMetric

class NDCGMetric(IMetric):
    """Normalized Discounted Cumulative Gain (intrinsic retrieval metric)."""

    def __init__(self, k: int = 5):
        self.k = k

    def compute(self, relevance_scores: List[int]) -> float:
        """Compute NDCG@k from graded relevance list."""
        def dcg(scores: List[int]) -> float:
            return sum(s / math.log2(i + 2) for i, s in enumerate(scores[:self.k]))
        if not relevance_scores:
            return 0.0
        ideal = sorted(relevance_scores, reverse=True)
        idcg = dcg(ideal)
        return (dcg(relevance_scores) / idcg) if idcg > 0 else 0.0

    def describe(self) -> Dict[str, str]:
        return {
            "name": "NDCG@k",
            "type": "intrinsic",
            "description": "Measures retrieval ranking quality using graded relevance with logarithmic discount."
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestion_orchestrator.py ==== 
from __future__ import annotations
import logging
import json
from pathlib import Path
from typing import Any, Dict

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.parser.parser_factory import ParserFactory
from src.core.ingestion.metadata.metadata_extractor_factory import MetadataExtractorFactory
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner
from src.core.ingestion.chunking.chunking_orchestrator import ChunkingOrchestrator


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration (merge phase + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/ingestion.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("IngestionOrchestrator")
    logger.info("Starting ingestion pipeline")

    # ------------------------------------------------------------------
    # 2. Initialize core components
    # ------------------------------------------------------------------
    parser_factory = ParserFactory(cfg, logger=logger)
    metadata_factory = MetadataExtractorFactory.from_config(cfg)
    cleaner = RagTextCleaner.default()
    chunking_orchestrator = ChunkingOrchestrator(config=cfg)

    active_metadata_fields = opts.get("metadata_fields", [])
    parallelism = opts.get("parallelism", "auto")

    # ------------------------------------------------------------------
    # 3. Resolve all paths
    # ------------------------------------------------------------------
    paths = cfg.get("paths", {})
    raw_dir = Path(paths.get("raw_pdfs", "data/raw_pdfs")).resolve()
    parsed_dir = Path(paths.get("parsed", "data/processed/parsed")).resolve()
    cleaned_dir = Path(paths.get("cleaned", "data/processed/cleaned")).resolve()
    metadata_dir = Path(paths.get("metadata", "data/processed/metadata")).resolve()
    chunks_dir = Path(paths.get("chunks", "data/processed/chunks")).resolve()

    for d in [parsed_dir, cleaned_dir, metadata_dir, chunks_dir]:
        ensure_dir(d)

    pdf_files = sorted(raw_dir.glob("*.pdf"))
    if not pdf_files:
        logger.warning(f"No PDF files found in {raw_dir}")
        return
    logger.info(f"Found {len(pdf_files)} PDF(s) in {raw_dir}")

    # ------------------------------------------------------------------
    # 4. Parallel or sequential mode
    # ------------------------------------------------------------------
    use_parallel = (
        isinstance(parallelism, int) and parallelism > 1
    ) or (isinstance(parallelism, str) and parallelism.lower() == "auto")

    if use_parallel:
        try:
            parallel_parser = parser_factory.create_parallel_parser()
            logger.info("Running ingestion in parallel mode ...")
            results = parallel_parser.parse_all(raw_dir, parsed_dir)

            for res in results:
                if "text" not in res:
                    continue
                res["text"] = cleaner.clean(res["text"])
                res["chunks"] = chunking_orchestrator.process(res["text"], metadata=res.get("metadata", {}))

                pdf_name = Path(res["metadata"]["source_file"]).stem
                pdf_path = raw_dir / f"{pdf_name}.pdf"
                all_meta = metadata_factory.extract_all(str(pdf_path))
                filtered_meta = {k: v for k, v in all_meta.items()
                                 if not active_metadata_fields or k in active_metadata_fields}
                res["metadata"].update(filtered_meta)

                # Save outputs
                (parsed_dir / f"{pdf_name}.parsed.json").write_text(
                    json.dumps({"text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (cleaned_dir / f"{pdf_name}.cleaned.json").write_text(
                    json.dumps({"cleaned_text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (metadata_dir / f"{pdf_name}.metadata.json").write_text(
                    json.dumps(res["metadata"], ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (chunks_dir / f"{pdf_name}.chunks.json").write_text(
                    json.dumps({"chunks": res["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )

            logger.info(f"Parallel ingestion completed ({len(results)} file(s)).")
            return
        except Exception as e:
            logger.error(f"Parallel ingestion failed → fallback to sequential: {e}")

    # ------------------------------------------------------------------
    # 5. Sequential fallback mode
    # ------------------------------------------------------------------
    parser = parser_factory.create_parser()
    for pdf in pdf_files:
        logger.info(f"Parsing {pdf.name} ...")
        try:
            parsed_result = parser.parse(str(pdf))
            if "text" not in parsed_result:
                continue

            parsed_result["text"] = cleaner.clean(parsed_result["text"])
            parsed_result["chunks"] = chunking_orchestrator.process(
                parsed_result["text"], metadata=parsed_result.get("metadata", {})
            )

            base_metadata = parsed_result.get("metadata", {})
            all_metadata = metadata_factory.extract_all(str(pdf))
            if active_metadata_fields:
                all_metadata = {k: v for k, v in all_metadata.items() if k in active_metadata_fields}
            base_metadata.update(all_metadata)
            parsed_result["metadata"] = base_metadata

            # Write outputs
            (parsed_dir / f"{pdf.stem}.parsed.json").write_text(
                json.dumps({"text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (cleaned_dir / f"{pdf.stem}.cleaned.json").write_text(
                json.dumps({"cleaned_text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (metadata_dir / f"{pdf.stem}.metadata.json").write_text(
                json.dumps(base_metadata, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (chunks_dir / f"{pdf.stem}.chunks.json").write_text(
                json.dumps({"chunks": parsed_result["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )

            logger.info(f"Completed {pdf.name}")
        except Exception as e:
            logger.error(f"Failed to parse {pdf.name}: {e}")

    # ------------------------------------------------------------------
    # 6. Finish
    # ------------------------------------------------------------------
    logger.info("Ingestion complete.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestor.py ==== 
# src/core/ingestion/ingestor.py
from __future__ import annotations

from pathlib import Path
from typing import Dict, Any, List, Optional

from docling.document_converter import DocumentConverter
from src.core.ingestion.interfaces.i_ingestor import IIngestor


class Ingestor(IIngestor):
    # Handles PDF ingestion using Docling (no chunking)
    def __init__(self, enable_ocr: bool = True):
        self.enable_ocr = enable_ocr
        self.converter = DocumentConverter()

    def ingest(self, source_path: str) -> Dict[str, Any]:
        pdf_path = Path(source_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        # Perform robust conversion using Docling
        result = self.converter.convert(str(pdf_path))
        document = getattr(result, "document", None)

        # Extract plain text (fallback if needed)
        text = ""
        if document:
            if hasattr(document, "export_to_markdown"):
                text = document.export_to_markdown()
            elif hasattr(document, "text"):
                text = document.text

        # Build metadata
        metadata = {
            "source_file": pdf_path.name,
            "source_path": str(pdf_path),
            "page_count": self._get_page_count(document),
            "has_ocr": self._has_ocr(document),
            "toc": self._extract_toc(document),
        }

        # Single-chunk representation (no splitting yet)
        chunks = [{"text": text, "page": None, "bbox": None}] if text else []

        return {"metadata": metadata, "chunks": chunks}

    # -------------------------------------------------------
    def _get_page_count(self, document: Any) -> Optional[int]:
        if document is None:
            return None
        return len(getattr(document, "pages", []))

    def _has_ocr(self, document: Any) -> bool:
        if document is None:
            return False
        return bool(getattr(document, "ocr_content", None))

    def _extract_toc(self, document: Any) -> List[Dict[str, Any]]:
        toc: List[Dict[str, Any]] = []
        if document and hasattr(document, "outline_items") and document.outline_items:
            for item in document.outline_items:
                toc.append({
                    "title": getattr(item, "title", ""),
                    "page": getattr(item, "page_number", None),
                })
        return toc
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata_merger.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\adaptive_chunker.py ==== 
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor
import spacy
import logging

class AdaptiveChunker(IChunker):
    """Chunker that adapts to the content structure by splitting at semantic breaks."""

    def __init__(self,
                 chunk_size: int = 500,
                 overlap: int = 200,
                 min_chunk_length: int = 400,
                 pdf_path: Optional[str] = None,
                 page_number_extractor: Optional[IPageNumberExtractor] = None,
                 text_length: Optional[int] = None):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length
        self.pdf_path = pdf_path
        self.page_number_extractor = page_number_extractor
        self.text_length = text_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

        # Extract page count if available
        if pdf_path and page_number_extractor:
            try:
                self.page_count = self.page_number_extractor.extract_page_number(pdf_path)
                self.adjust_chunking_based_on_page_count()
            except Exception as e:
                logging.warning(f"Error extracting page count from PDF: {e}")
                self.page_count = None
                self.chunk_size = 1000  # Fallback

        if text_length:
            self.adjust_chunking_based_on_text_length(text_length)

    def adjust_chunking_based_on_page_count(self):
        """Adjust chunk size and overlap based on the page count."""
        if self.page_count:
            if self.page_count > 50:
                self.chunk_size = 1000
                self.overlap = 700
            elif self.page_count > 30:
                self.chunk_size = 1500
                self.overlap = 500
            elif self.page_count > 10:
                self.chunk_size = 2000
                self.overlap = 300
            else:
                self.chunk_size = 2500
                self.overlap = 200
        else:
            self.chunk_size = 1000
            self.overlap = 200

    def adjust_chunking_based_on_text_length(self, text_length: int):
        """Adjust chunk size and overlap based on the length of the text."""
        if text_length:
            if text_length > 5000:
                self.chunk_size = 1000
                self.overlap = 600
            elif text_length > 2000:
                self.chunk_size = 1500
                self.overlap = 400
            else:
                self.chunk_size = 2000
                self.overlap = 200

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Chunk the text into adaptive chunks based on content size and overlap."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)  # Use spaCy to process the text
        sentences = [sent.text.strip() for sent in doc.sents]  # Extract sentences from the spaCy doc

        for sentence in sentences:
            # If the current chunk plus sentence exceeds chunk size, store the chunk and start a new one
            if len(current_chunk) + len(sentence) > self.chunk_size:
                if current_chunk:  # Prevent empty chunks
                    chunks.append({
                        "text": current_chunk.strip(),
                        # "chunk_size": len(current_chunk.strip()),  # Removed from output
                        # "overlap": self.overlap  # Removed from output
                    })
                current_chunk = sentence
            else:
                current_chunk += " " + sentence

            # Merge chunks if they are too small
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                # "chunk_size": len(current_chunk.strip()),  # Removed from output
                # "overlap": self.overlap  # Removed from output
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\chunking_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Any, Dict, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.chunking.adaptive_chunker import AdaptiveChunker
from src.core.ingestion.chunking.static_chunker import StaticChunker
from src.core.ingestion.metadata.implementations.page_number_extractor import PageNumberExtractor


class ChunkingOrchestrator:
    """Handles the selection and execution of chunking strategies based on YAML config."""

    def __init__(self, config: Dict[str, Any], pdf_path: Optional[str] = None):
        """Initialize with YAML configuration and an optional PDF path."""
        self.config = config
        self.pdf_path = pdf_path
        
        # Überprüfen, ob der 'chunking' Abschnitt in der Konfiguration vorhanden ist
        if "chunking" not in self.config:
            raise KeyError("The 'chunking' section is missing in the configuration file")

        # Debugging: Gibt den 'chunking' Abschnitt aus
        logging.debug(f"Chunking config: {self.config['chunking']}")

        # Wählt die passende Chunking-Strategie aus der Konfiguration
        self.chunker = self.select_chunker()  

    def select_chunker(self) -> IChunker:
        """Select chunking strategy based on the configuration."""
        chunking_config = self.config["chunking"]  # Zugriff auf den 'chunking'-Abschnitt der Konfiguration
        chunking_mode = chunking_config["mode"]
        chunk_size = chunking_config["chunk_size"]  # Get the chunk size from config
        overlap = chunking_config["overlap"]
        enable_overlap = chunking_config["enable_overlap"]
        min_chunk_length = chunking_config["min_chunk_length"]
        sentence_boundary_detection = chunking_config["sentence_boundary_detection"]
        merge_short_chunks = chunking_config["merge_short_chunks"]

        # Erstelle die Chunker-Instanz basierend auf dem gewählten Modus
        if chunking_mode == "adaptive":
            # Instanziiere AdaptiveChunker mit den relevanten Konfigurationswerten
            page_number_extractor = PageNumberExtractor() if self.pdf_path else None
            return AdaptiveChunker(
                chunk_size=chunk_size,
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
                pdf_path=self.pdf_path,
                page_number_extractor=page_number_extractor,
            )
        elif chunking_mode == "static":
            # Instanziiere StaticChunker mit den relevanten Konfigurationswerten
            return StaticChunker(
                chunk_size=chunk_size,  # Pass the chunk_size from config
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
            )
        else:
            raise ValueError(f"Unknown chunking strategy: {chunking_mode}")

    def process(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Process the text and return chunks with metadata using the selected chunking strategy."""
        return self.chunker.chunk(text, metadata)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\i_chunker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IChunker(ABC):
    """Interface for all chunking strategies."""

    @abstractmethod
    def chunk(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Split the cleaned text into smaller chunks.
        
        Each returned item should be a dictionary with:
        - "text": the chunked text as a string.
        - "metadata": additional information (e.g., document info, chunking context).
        
        Args:
            text: The cleaned text to be chunked.
            metadata: Optional metadata related to the chunking process. Default is an empty dictionary.
        
        Returns:
            A list of dictionaries, each containing:
            - "text": A chunk of the input text.
            - "metadata": The metadata associated with the chunk.
        """
        if metadata is None:
            metadata = {}
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\static_chunker.py ==== 
# src/core/ingestion/chunking/static_chunker.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
import spacy

class StaticChunker(IChunker):
    """Chunker that uses fixed chunk size and overlap for chunking."""

    def __init__(self, 
                 chunk_size: int,  # Configuration-based chunk size
                 overlap: int, 
                 min_chunk_length: int):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Split text into overlapping fixed-size chunks with sentence boundaries."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]

        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= self.chunk_size:
                current_chunk += ". " + sentence
            else:
                chunks.append({
                    "text": current_chunk.strip(),
                    "chunk_size": len(current_chunk.strip()),  # Display actual chunk length
                    "configured_chunk_size": self.chunk_size,   # Config value for comparison
                    "overlap": self.overlap                    # Configured overlap
                })
                current_chunk = sentence

            # Merge small chunks if necessary
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                last_chunk["chunk_size"] = len(last_chunk["text"])
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                "chunk_size": len(current_chunk.strip()),
                "configured_chunk_size": self.chunk_size,
                "overlap": self.overlap
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\utils_chunking.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\base_cleaner.py ==== 
from __future__ import annotations
from typing import final
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner


class BaseTextCleaner(ITextCleaner):
    """Base class providing a stable clean(...) interface."""

    @final
    def clean(self, text: str) -> str:
        # Guarantee non-null string
        if not text:
            return ""
        return self._clean_impl(text)

    def _clean_impl(self, text: str) -> str:
        # Must be implemented by subclasses
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\cleaner_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict

from src.core.ingestion.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration
    # ------------------------------------------------------------------
    cfg = ConfigLoader("configs/ingestion.yaml").config
    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)

    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("CleanerOrchestrator")
    logger.info("Starting cleaning phase")

    # ------------------------------------------------------------------
    # 2. Resolve paths
    # ------------------------------------------------------------------
    parsed_dir = Path(cfg["paths"]["parsed"]).resolve()
    cleaned_dir = Path(cfg["paths"].get("cleaned", "data/processed/cleaned")).resolve()
    ensure_dir(cleaned_dir)

    parsed_files = sorted(parsed_dir.glob("*.parsed.json"))
    if not parsed_files:
        logger.warning(f"No parsed files found in {parsed_dir}")
        return

    logger.info(f"Found {len(parsed_files)} parsed file(s) for cleaning")

    # ------------------------------------------------------------------
    # 3. Initialize deterministic multi-stage cleaner
    # ------------------------------------------------------------------
    cleaner = RagTextCleaner.default()

    # ------------------------------------------------------------------
    # 4. Iterate over parsed JSONs
    # ------------------------------------------------------------------
    for idx, parsed_path in enumerate(parsed_files, start=1):
        try:
            with open(parsed_path, "r", encoding="utf-8") as f:
                parsed_data = json.load(f)

            raw_text = parsed_data.get("text", "")
            if not raw_text:
                logger.warning(f"Skipping {parsed_path.name}: no text field")
                continue

            # --- Step 1: Clean text ---
            cleaned_text = cleaner.clean(raw_text)

            # --- Step 2: Write cleaned output as JSON ---
            cleaned_filename = parsed_path.stem.replace(".parsed", "") + ".cleaned.json"
            cleaned_path = cleaned_dir / cleaned_filename

            cleaned_data = {
                "cleaned_text": cleaned_text,
            }

            with open(cleaned_path, "w", encoding="utf-8") as cf:
                json.dump(cleaned_data, cf, ensure_ascii=False, indent=2)

            logger.info(f"✓ Cleaned {parsed_path.name} ({idx}/{len(parsed_files)})")

        except Exception as e:
            logger.error(f"✗ Failed to clean {parsed_path.name}: {e}")

    # ------------------------------------------------------------------
    # 5. Finish
    # ------------------------------------------------------------------
    logger.info("Cleaning phase completed successfully.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\deep_text_cleaner.py ==== 
from __future__ import annotations
import re
from cleantext import clean
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner

class DeepTextCleaner(BaseTextCleaner):
    """
    Advanced text cleaner using heuristic and statistical patterns
    to remove non-content sections like references, tables, and equations.
    """

    def _clean_impl(self, text: str) -> str:
        # Step 1: Global clean using clean-text
        text = clean(
            text,
            fix_unicode=True,
            to_ascii=False,
            lower=False,
            no_urls=True,
            no_emails=True,
            no_phone_numbers=True,
            no_numbers=False,
            no_currency_symbols=True,
            no_punct=False,
        )

        # Step 2: Remove reference-like or appendix sections
        text = re.sub(
            r"(?is)(references|bibliography|literaturverzeichnis|acknowledg(e)?ments|appendix).*",
            "",
            text,
        )

        # Step 3: Drop DOI/arXiv/URL lines
        text = re.sub(r"(?im)^\s*(doi|arxiv|http|https)[:\s].*$", "", text)

        # Step 4: Drop lines that are mostly numbers, formulas, or citation lists
        filtered_lines = []
        for line in text.splitlines():
            clean_line = line.strip()
            if not clean_line:
                continue
            # Discard lines with >30% digits or symbols
            digit_ratio = sum(ch.isdigit() for ch in clean_line) / max(len(clean_line), 1)
            symbol_ratio = sum(not ch.isalnum() and not ch.isspace() for ch in clean_line) / max(len(clean_line), 1)
            if digit_ratio > 0.3 or symbol_ratio > 0.4:
                continue
            # Drop if line looks like citation or table
            if re.match(r"^\[\d+\]\s*[A-Z][a-z]+", clean_line):
                continue
            if re.match(r"(?i)^table\s+\d+", clean_line):
                continue
            filtered_lines.append(clean_line)

        text = "\n".join(filtered_lines)

        # Step 5: Collapse whitespace and blank lines
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\i_text_cleaner.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod


class ITextCleaner(ABC):
    """Interface for all text cleaners in the ingestion pipeline."""

    @abstractmethod
    def clean(self, text: str) -> str:
        """Return a cleaned version of the given text."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rag_text_cleaner.py ==== 
from __future__ import annotations
from typing import List
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner

from src.core.ingestion.cleaner.simple_cleaners import (
    UnicodeNormalizer,
    SoftHyphenCleaner,
    HeaderFooterCleaner,
    LayoutLineJoinCleaner,
    TrailingWhitespaceCleaner,
    HTMLCleaner,                  # removes HTML tags and entities
    ScientificNotationCleaner,    # removes scientific notation terms like "Eq. 1"
    ReferencePatternCleaner,      # NEW – removes APA / IEEE / arXiv / ACM / MLA style refs
    ReferencesCleaner,            # removes everything after 'References' or 'Bibliography'
)

from src.core.ingestion.cleaner.deep_text_cleaner import DeepTextCleaner  # advanced cleaning


class RagTextCleaner(ITextCleaner):
    """
    Composite text cleaner for RAG ingestion.
    Executes a deterministic sequence of cleaners to normalize and denoise scientific text.
    """

    def __init__(self, cleaners: List[ITextCleaner]):
        self.cleaners = cleaners

    # ------------------------------------------------------------------
    @classmethod
    def default(cls) -> "RagTextCleaner":
        """
        Build a deterministic chain of text cleaners combining rule-based and deep cleaning.
        """
        cleaners: List[ITextCleaner] = [
            UnicodeNormalizer(),          # normalize spaces and zero-width chars
            SoftHyphenCleaner(),          # remove soft hyphens and join split words
            HeaderFooterCleaner(),        # drop headers, footers, funding info
            LayoutLineJoinCleaner(),      # repair layout-induced line breaks
            TrailingWhitespaceCleaner(),  # trim redundant spaces and newlines
            HTMLCleaner(),                # remove HTML tags and entities
            ScientificNotationCleaner(),  # remove equations, theorem/lemma markers
            ReferencePatternCleaner(),    # remove inline citation patterns, DOIs, arXiv etc.
            ReferencesCleaner(),          # cut after "References"/"Bibliography"
            DeepTextCleaner(),            # deep filter for noise, refs, tables, math etc.
        ]
        return cls(cleaners)

    # ------------------------------------------------------------------
    def clean(self, text: str) -> str:
        """
        Run all sub-cleaners consecutively in deterministic order.
        """
        for cleaner in self.cleaners:
            text = cleaner.clean(text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rules.py ==== 
# src/core/ingestion/cleaner/rules.py
from __future__ import annotations
import re

# Common residual headings we want to drop only if they appear alone in a line
SINGLE_LINE_HEADER_PATTERNS = [
    r"(?i)^\s*table\s+of\s+contents\s*$",
    r"(?i)^\s*inhaltsverzeichnis\s*$",
    r"(?i)^\s*contents\s*$",
]

# Footer / page indicator
FOOTER_PATTERNS = [
    r"(?i)^\s*page\s+\d+\s*$",
    r"(?i)^\s*p\.?\s*\d+\s*$",
    r"^\s*\d+\s*$",
]

# Funding and preprint noise – but only if line is short
FUNDING_PATTERNS = [
    r"(?i)^\s*funded\s+by.*$",
    r"(?i)^\s*supported\s+by.*$",
    r"(?i)^\s*this\s+preprint\s+.*$",
    r"(?i)^\s*arxiv:\s*\d{4}\.\d{4,5}.*$",
    r"(?i)^\s*doi:\s*10\.\d{4,9}/[-._;()/:A-Z0-9]+$",
]

# Lines we consider layout trash if they are too short
MAX_LEN_FOR_STRICT_DROP = 80

SOFT_HYPHEN = "\xad"
SOFT_HYPHEN_RE = re.compile(SOFT_HYPHEN)

# Hyphenated line break like "prob-\nabilistic"
HYPHEN_LINEBREAK_RE = re.compile(r"(\w+)-\n(\w+)")

# Linebreak in the middle of sentence like "proba-\nbilistic"
INLINE_LINEBREAK_RE = re.compile(r"(\S)\n(\S)")

# Multiple blank lines
MULTI_NEWLINE_RE = re.compile(r"\n{3,}")

# Unicode spaces
UNICODE_SPACES_RE = re.compile(r"[\u00a0\u2000-\u200b]+")


def is_short_line(line: str) -> bool:
    # Heuristic: many layout lines are short
    return len(line.strip()) <= MAX_LEN_FOR_STRICT_DROP


def match_any(patterns: list[str], line: str) -> bool:
    for pat in patterns:
        if re.match(pat, line):
            return True
    return False
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\simple_cleaners.py ==== 
from __future__ import annotations
import re
from typing import List
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner
from src.core.ingestion.cleaner import rules


class HTMLCleaner(BaseTextCleaner):
    """Removes HTML tags and entities like &nbsp;."""

    def _clean_impl(self, text: str) -> str:
        # Remove all HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove HTML entities like &nbsp;
        text = re.sub(r'&[a-zA-Z]+;', '', text)
        return text


class ScientificNotationCleaner(BaseTextCleaner):
    """Removes scientific notations and references like 'Eq. 1', 'Theorem 3'."""

    def _clean_impl(self, text: str) -> str:
        # Remove scientific expressions such as Eq. 1 or Lemma 3
        text = re.sub(r'\b(Eq|Theorem|Lemma)\s+\d+\b', '', text)
        return text


class UnicodeNormalizer(BaseTextCleaner):
    """Normalize unicode spaces and zero-width chars."""

    def _clean_impl(self, text: str) -> str:
        # Replace unicode spaces with a normal space
        text = rules.UNICODE_SPACES_RE.sub(" ", text)
        return text


class SoftHyphenCleaner(BaseTextCleaner):
    """Remove soft hyphens and join line-broken words."""

    def _clean_impl(self, text: str) -> str:
        # Remove soft hyphen character
        text = text.replace(rules.SOFT_HYPHEN, "")
        # Join words split by hyphen and linebreak
        text = rules.HYPHEN_LINEBREAK_RE.sub(r"\1\2", text)
        return text


class LayoutLineJoinCleaner(BaseTextCleaner):
    """
    Join lines that were broken by the PDF layout but belong together.
    """

    def _clean_impl(self, text: str) -> str:
        # Join inline linebreaks like "probabilistic\nreasoning" -> "probabilistic reasoning"
        text = rules.INLINE_LINEBREAK_RE.sub(r"\1 \2", text)
        # Collapse too many blank lines
        text = rules.MULTI_NEWLINE_RE.sub("\n\n", text)
        return text.strip()


class HeaderFooterCleaner(BaseTextCleaner):
    """Remove obvious header/footer/single-line noise."""

    def _clean_impl(self, text: str) -> str:
        cleaned_lines: List[str] = []
        for line in text.splitlines():
            raw = line.rstrip()

            # Skip headers
            if rules.match_any(rules.SINGLE_LINE_HEADER_PATTERNS, raw):
                continue

            # Skip short footers
            if rules.match_any(rules.FOOTER_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            # Skip short funding or preprint notices
            if rules.match_any(rules.FUNDING_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            cleaned_lines.append(raw)

        return "\n".join(cleaned_lines).strip()


class TrailingWhitespaceCleaner(BaseTextCleaner):
    """Normalize whitespace globally."""

    def _clean_impl(self, text: str) -> str:
        # Strip trailing spaces per line
        lines = [ln.rstrip() for ln in text.splitlines()]
        text = "\n".join(lines)
        # Collapse 3+ newlines into 2
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()


# ----------------------------------------------------------------------
# NEW CLEANER: removes APA / IEEE / arXiv / ACM / MLA style in-text citations
# ----------------------------------------------------------------------
class ReferencePatternCleaner(BaseTextCleaner):
    """
    Removes inline reference patterns such as (Smith, 2020), [12], [1–5], (Smith et al., 2021),
    DOIs, arXiv IDs, and conference citation snippets (Proc. IEEE, JMLR, NeurIPS, etc.).
    """

    def _clean_impl(self, text: str) -> str:
        # APA/Harvard-style: (Smith, 2020), (Doe & Roe, 2021)
        text = re.sub(
            r"\([A-Z][A-Za-z\-]+(?:,?\s(?:[A-Z]\.)+)*(?:\s&\s[A-Z][A-Za-z\-]+)*,\s?\d{4}[a-z]?\)",
            "",
            text,
        )

        # et al. style: (Smith et al., 2020)
        text = re.sub(
            r"\([A-Z][A-Za-z\-]+\s+et\s+al\.,\s*\d{4}[a-z]?\)",
            "",
            text,
        )

        # IEEE numeric: [1], [12], [1,2], [1–5], [1-3]
        text = re.sub(
            r"\[\s?\d+(?:[\-,–]\s?\d+)*(?:\s*,\s*\d+)*\s?\]",
            "",
            text,
        )

        # Year-only: (2020), (1999a)
        text = re.sub(
            r"\(\s?\d{4}[a-z]?\s?\)",
            "",
            text,
        )

        # Inline DOIs and arXiv references
        text = re.sub(r"\bdoi:\s*\S+", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\barXiv:\s*\S+", "", text, flags=re.IGNORECASE)

        # Conference/journal abbreviations with year numbers
        text = re.sub(
            r"\b(Proc\.|Proceedings|Conf\.|Conference|JMLR|ICML|NeurIPS|NIPS|AAAI|IJCAI|ACL|EMNLP|COLING|IEEE|ACM)\b.*?\d{4}",
            "",
            text,
            flags=re.IGNORECASE,
        )

        # Author-name style in brackets: [Smith 2020]
        text = re.sub(
            r"\[[A-Z][A-Za-z\-]+(?:\s+et\s+al\.)?,?\s*\d{4}[a-z]?\]",
            "",
            text,
        )

        # “In Proceedings of …” phrases
        text = re.sub(
            r"(?i)\bin\s+proceedings\s+of\b.*?(?=[\.\n])",
            "",
            text,
        )

        # Normalize multiple spaces and punctuation spacing
        text = re.sub(r"\s{2,}", " ", text)
        text = re.sub(r"\s+([\.,;:])", r"\1", text)

        return text.strip()


class ReferencesCleaner(BaseTextCleaner):
    """Removes everything starting from 'References', 'Bibliography', or 'Literaturverzeichnis'."""

    def _clean_impl(self, text: str) -> str:
        pattern = re.compile(
            r"(?im)^\s*(references|bibliography|literaturverzeichnis)\s*$"
        )
        match = pattern.search(text)
        if match:
            cutoff_index = match.start()
            # Only cut if reference section is beyond 20% of text
            if cutoff_index > len(text) * 0.2:
                text = text[:cutoff_index]
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\i_ingestor.py ==== 
from abc import ABC, abstractmethod
from typing import Any

class IIngestor(ABC):
    """Interface for all ingestion components."""

    @abstractmethod
    def ingest(self, source_path: str) -> Any:
        """Convert one document into structured JSON chunks and metadata."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\metadata_extractor_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import importlib
import logging
from pathlib import Path


class MetadataExtractorFactory:
    """
    Dynamically loads and executes PDF-based metadata extractors.
    Each extractor implementation must expose a class named <Name>Extractor in
    src.core.ingestion.metadata.implementations.<name>_extractor.
    """

    def __init__(self, active_fields: List[str], pdf_root: str | Path):
        self.logger = logging.getLogger(__name__)
        self.active_fields = active_fields
        self.pdf_root = Path(pdf_root).resolve()
        self.extractors: Dict[str, Any] = {}
        self._load_extractors()

    # ------------------------------------------------------------------
    @classmethod
    def from_config(cls, cfg: Dict[str, Any]) -> MetadataExtractorFactory:
        """Initialize factory from ingestion.yaml configuration."""
        opts = cfg.get("options", {})
        active = opts.get("metadata_fields", [])
        pdf_root = cfg.get("paths", {}).get("raw_pdfs", "./data/raw_pdfs")
        return cls(active_fields=active, pdf_root=pdf_root)

    # ------------------------------------------------------------------
    def _load_extractors(self) -> None:
        """Dynamically import extractor implementations for requested fields."""
        base_pkg = "src.core.ingestion.metadata.implementations"

        mapping = {
            "title": "title_extractor",
            "authors": "author_extractor",
            "year": "year_extractor",
            # "abstract": "abstract_extractor",  # Deactivated abstract extraction
            "detected_language": "language_detector",
            "file_size": "file_size_extractor",
            "toc": "toc_extractor",
            "page_number": "page_number_extractor",  # Page number extractor
        }

        for field in self.active_fields:
            module_name = mapping.get(field)
            if not module_name:
                self.logger.warning(f"No extractor mapping found for field '{field}' → skipped.")
                continue

            try:
                # Dynamically import the module
                module = importlib.import_module(f"{base_pkg}.{module_name}")
                # Construct the extractor class name from the field (e.g., "PageNumberExtractor" from "page_number")
                class_name = "".join([p.capitalize() for p in field.split("_")]) + "Extractor"
                # Get the extractor class from the module
                extractor_class = getattr(module, class_name)
                # Instantiate the extractor class and store it in the dictionary
                self.extractors[field] = extractor_class(base_dir=self.pdf_root)
                self.logger.debug(f"Loaded extractor: {extractor_class.__name__} for '{field}'")
            except ModuleNotFoundError:
                self.logger.warning(f"Implementation missing for '{field}' ({module_name}.py not found).")
            except Exception as e:
                self.logger.error(f"Failed to load extractor for '{field}': {e}")

    # ------------------------------------------------------------------
    def extract_all(self, pdf_path: str) -> Dict[str, Any]:
        """Run all configured extractors directly on a given PDF file path."""
        pdf_path = str(Path(pdf_path).resolve())
        results: Dict[str, Any] = {}

        for field, extractor in self.extractors.items():
            try:
                value = extractor.extract(pdf_path)
                results[field] = value
            except Exception as e:
                self.logger.warning(f"Extractor '{field}' failed for {pdf_path}: {e}")
                results[field] = None

        return results
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\abstract_extractor.py ==== 
from __future__ import annotations
from typing import Optional
from pathlib import Path
import fitz


# class AbstractExtractor:
#     """Extracts abstract text directly from GROBID TEI XML or PDF XMP metadata."""

#     def __init__(self, base_dir: Path | str | None = None):
#         self.base_dir = Path(base_dir).resolve() if base_dir else None

#     # ------------------------------------------------------------------
#     def extract(self, pdf_path: str) -> Optional[str]:
#         pdf_file = Path(pdf_path)

#         # 1. Try GROBID XML
#         xml_path = self._find_grobid_xml(pdf_file)
#         if xml_path and xml_path.exists():
#             abstract = self._extract_from_grobid(xml_path)
#             if abstract:
#                 return abstract

#         # 2. Try PDF metadata (some PDFs include "subject"/"keywords"/"description")
#         abstract = self._extract_from_pdf_metadata(pdf_file)
#         if abstract:
#             return abstract

#         # 3. Fallback: None (no text heuristics)
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
#         """Read abstract-like information from PDF metadata fields."""
#         try:
#             with fitz.open(pdf_file) as doc:
#                 meta = doc.metadata or {}
#                 for key in ("subject", "Subject", "description", "Description", "keywords", "Keywords"):
#                     val = meta.get(key)
#                     if isinstance(val, str) and len(val.strip()) > 20:
#                         return re.sub(r"\s+", " ", val.strip())
#         except Exception:
#             return None
#         return None

#     # ------------------------------------------------------------------
#     def _find_grobid_xml(self, pdf_file: Path) -> Path | None:
#         xml_candidate = pdf_file.with_suffix(".tei.xml")
#         if xml_candidate.exists():
#             return xml_candidate
#         if self.base_dir:
#             alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
#             if alt.exists():
#                 return alt
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
#         """Parse TEI XML to extract the abstract section."""
#         try:
#             with open(xml_path, "r", encoding="utf-8") as f:
#                 xml = f.read()
#             root = etree.fromstring(xml.encode("utf-8"))
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             abs_text = root.xpath("string(//tei:abstract)", namespaces=ns)
#             if abs_text and len(abs_text.strip()) > 10:
#                 return re.sub(r"\s+", " ", abs_text.strip())
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\author_extractor.py ==== 
# src/core/ingestion/metadata/implementations/author_extractor.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from pathlib import Path
import fitz
import logging
from lxml import etree
import re

# optional imports
try:
    import spacy
    from spacy.language import Language
    from spacy.pipeline import EntityRuler
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

try:
    from nameparser import HumanName
    NAMEPARSER_AVAILABLE = True
except ImportError:
    NAMEPARSER_AVAILABLE = False


class LocalNameVerifier:
    """Deterministic local fallback for name validation without ML models."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def __init__(self):
        # extended bad keyword set for academic false positives
        self.bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "school", "machine", "learning", "arxiv", "neural", "transformer",
            "appendix", "algorithm", "keywords", "vol", "doi", "intelligence",
            "markov", "chain", "computing", "zentrum", "sciences",
            "centre", "center", "data", "model", "probability", "network"
        }

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 80:
            return False
        if any(ch.isdigit() for ch in text):
            return False
        if any(tok.lower() in self.bad_keywords for tok in text.split()):
            return False

        if NAMEPARSER_AVAILABLE:
            try:
                hn = HumanName(text)
                if hn.first or hn.last:
                    return True
            except Exception:
                pass

        if self.TITLE_PATTERN.search(text):
            return True
        return bool(self.NAME_PATTERN.search(text))


class AuthorsExtractor:
    """Deterministic, language-agnostic author extractor with silent mode (no info logs)."""

    def __init__(self, base_dir: Path | str | None = None):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.name_verifier = LocalNameVerifier()
        self.nlp = None

        if SPACY_AVAILABLE:
            self.nlp = self._init_spacy_pipeline()
        else:
            self.logger.warning("spaCy not installed; PERSON detection disabled.")

    def _init_spacy_pipeline(self):
        """Try to load spaCy model, else build local EntityRuler fallback."""
        try:
            return spacy.load("en_core_web_sm", disable=["tagger", "lemmatizer", "parser"])
        except Exception:
            try:
                return spacy.load("de_core_news_sm", disable=["tagger", "lemmatizer", "parser"])
            except Exception:
                self.logger.warning("No spaCy model available; using local EntityRuler fallback.")
                return self._build_local_person_ruler()

    def _build_local_person_ruler(self) -> "Language":
        """Constructs a blank English spaCy pipeline with a rule-based PERSON detector."""
        nlp = spacy.blank("en")
        ruler = nlp.add_pipe("entity_ruler")
        patterns = [
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_TITLE": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_ALPHA": True}, {"IS_ALPHA": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"TEXT": {"REGEX": "^[A-Z][a-z]+\\.$"}}]},
        ]
        ruler.add_patterns(patterns)
        return nlp

    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[str]:
        candidates: List[str] = []

        if parsed_document and parsed_document.get("grobid_xml"):
            candidates.extend(self._extract_from_grobid_xml(parsed_document["grobid_xml"]))
        else:
            xml_path = self._find_grobid_sidecar(Path(pdf_path))
            if xml_path:
                candidates.extend(self._extract_from_grobid_file(xml_path))

        if not candidates:
            candidates.extend(self._extract_from_pdf_metadata(Path(pdf_path)))

        if not candidates:
            candidates.extend(self._extract_from_first_page(Path(pdf_path)))

        candidates = [self._normalize(c) for c in candidates if c.strip()]
        verified = [n for n in candidates if self.name_verifier.is_person_name(n)]

        cleaned = [
            re.sub(
                r"\b(Google|Inc\.?|University|Institute|Lab|Labs|Dept\.?|Center|Centre|Zentrum|Sciences?)\b",
                "",
                n,
                flags=re.IGNORECASE
            ).strip()
            for n in verified
        ]

        seen: set[str] = set()
        return [a for a in cleaned if not (a in seen or seen.add(a))]

    def _find_grobid_sidecar(self, pdf_file: Path) -> Optional[Path]:
        local = pdf_file.with_suffix(".tei.xml")
        if local.exists():
            return local
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid_file(self, xml_path: Path) -> List[str]:
        try:
            return self._extract_from_grobid_xml(xml_path.read_text(encoding="utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID XML read error: {e}")
            return []

    def _extract_from_grobid_xml(self, xml_text: str) -> List[str]:
        ns = {"tei": "http://www.tei-c.org/ns/1.0"}
        try:
            root = etree.fromstring(xml_text.encode("utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID parse error: {e}")
            return []

        authors: List[str] = []
        for xp in [
            "//tei:analytic//tei:author",
            "//tei:monogr//tei:author",
            "//tei:respStmt//tei:author",
            "//tei:editor",
        ]:
            for node in root.xpath(xp, namespaces=ns):
                fn = node.xpath("string(.//tei:forename)", namespaces=ns).strip()
                ln = node.xpath("string(.//tei:surname)", namespaces=ns).strip()
                if fn or ln:
                    authors.append(f"{fn} {ln}".strip())
                else:
                    txt = " ".join(node.xpath(".//text()", namespaces=ns)).strip()
                    if txt:
                        authors.extend(self._extract_persons_ner(txt))
        return authors

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
        except Exception as e:
            self.logger.warning(f"PDF metadata read error: {e}")
            return []
        author_field = meta.get("author") or meta.get("Author") or meta.get("authors")
        return self._extract_persons_ner(str(author_field)) if author_field else []

    def _extract_from_first_page(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return []
                text = doc.load_page(0).get_text("text")
        except Exception as e:
            self.logger.warning(f"First-page read error: {e}")
            return []
        text = text.split("Abstract")[0] if "Abstract" in text else text
        return self._extract_persons_ner(text)

    def _extract_persons_ner(self, text: str) -> List[str]:
        if not self.nlp or not text.strip():
            return []
        doc = self.nlp(text)
        persons = []
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if any(e.start <= ent.start < e.end for e in doc.ents if e.label_ in {"ORG", "GPE", "LOC"}):
                    continue
                persons.append(ent.text.strip())
        return [p for p in persons if self._is_name_like(p)]

    def _is_name_like(self, text: str) -> bool:
        bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "arxiv", "model", "learning", "probability", "appendix", "keywords",
            "ai", "neural", "transformer", "algorithm", "vol",
            "intelligence", "computing", "zentrum", "sciences", "centre", "center"
        }
        if any(tok.lower() in bad_keywords for tok in text.split()):
            return False
        if any(ch.isdigit() for ch in text):
            return False
        return 1 <= len(text.split()) <= 5 and text[0].isupper()

    def _normalize(self, s: str) -> str:
        """Normalize spacing and special characters."""
        return " ".join(s.replace("•", "").replace("†", "").split()).strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\file_size_extractor.py ==== 
from __future__ import annotations
import os
from pathlib import Path


class FileSizeExtractor:
    """Extracts the exact file size (in bytes) directly from the PDF file."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> int | None:
        """Return the file size of the given PDF file in bytes."""
        pdf_file = Path(pdf_path)
        try:
            if not pdf_file.is_file():
                return None
            return pdf_file.stat().st_size
        except Exception:
            return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\language_detector.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import re
import fitz

try:
    from langdetect import detect
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False


class DetectedLanguageExtractor:
    """Detects the dominant document language from PDF text or XMP metadata."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Identify document language via PDF content or metadata."""
        pdf_file = Path(pdf_path)

        # 1. Try PDF XMP metadata
        lang = self._extract_from_metadata(pdf_file)
        if lang:
            return lang

        # 2. Try text-based detection (without parser)
        lang = self._detect_from_text(pdf_file)
        if lang:
            return lang

        return None

    # ------------------------------------------------------------------
    def _extract_from_metadata(self, pdf_file: Path) -> Optional[str]:
        """Check embedded language fields in PDF metadata."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                for key in ("language", "Language", "lang", "Lang"):
                    val = meta.get(key)
                    if val and isinstance(val, str):
                        val_norm = val.lower().strip()
                        if val_norm.startswith("en"):
                            return "en"
                        if val_norm.startswith("de"):
                            return "de"
        except Exception:
            return None
        return None

    # ------------------------------------------------------------------
    def _detect_from_text(self, pdf_file: Path) -> Optional[str]:
        """Extract small text sample from first pages and apply language detection."""
        sample_text = ""
        try:
            with fitz.open(pdf_file) as doc:
                max_pages = min(3, len(doc))
                for i in range(max_pages):
                    sample_text += doc.load_page(i).get_text("text") + "\n"
        except Exception:
            return None

        if not sample_text or len(sample_text.strip()) < 30:
            return None

        # Use langdetect if installed
        if LANGDETECT_AVAILABLE:
            try:
                lang = detect(sample_text)
                if lang in ("en", "de"):
                    return lang
            except Exception:
                pass

        # Simple regex-based fallback
        text_lower = sample_text.lower()
        if re.search(r"\b(und|nicht|sein|dass|werden|ein|eine|mit|auf|für|dem|den|das|ist|sind|der|die|das)\b", text_lower):
            return "de"
        if re.search(r"\b(the|and|of|for|with|from|that|this|by|is|are|we|deep|learning|language|model)\b", text_lower):
            return "en"
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\page_number_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz  # PyMuPDF
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor


class PageNumberExtractor(IPageNumberExtractor):
    """Extracts the page number from the first page of the PDF document."""
    
    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the first page of the PDF.
        
        :param pdf_path: Path to the PDF file.
        :return: The page number (if available), otherwise None.
        """
        pdf_file = Path(pdf_path)

        try:
            with fitz.open(pdf_file) as doc:
                # Getting the total page count
                total_pages = len(doc)
                if total_pages > 0:
                    # For this example, we're assuming the page number is extracted from the first page
                    return 1  # For example, we return the first page number; adapt as needed
        except Exception as e:
            print(f"Error extracting page number from {pdf_file}: {e}")
        
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\title_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz
import re
from lxml import etree

class TitleExtractor:
    """
    Robust, deterministic extractor for scientific paper titles.
    Filename fallback *is not used* (explicitly excluded).
    """

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract(self, pdf_path: str) -> Optional[str]:
        """
        Extract the most probable title of a given PDF.
        Priority:
          1. PDF metadata
          2. GROBID TEI XML
          3. Layout + heuristic on first page
        Returns None if no reliable title is found.
        """
        pdf_file = Path(pdf_path)

        # 1. Extract from PDF metadata
        title = self._extract_from_pdf_metadata(pdf_file)
        if self._is_valid(title):
            return title

        # 2. Extract from GROBID TEI XML
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            grobid_title = self._extract_from_grobid(xml_path)
            if self._is_valid(grobid_title):
                return grobid_title

        # 3. Layout-based extraction (heuristic on first page)
        title_from_layout = self._extract_from_first_page_layout(pdf_file)
        if self._is_valid(title_from_layout):
            return title_from_layout

        # No valid title found after all methods
        return None

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
        """Extract title from embedded PDF metadata fields."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                title_meta = meta.get("title") or meta.get("Title")
                if title_meta:
                    cleaned = self._normalize(title_meta)
                    if self._is_valid(cleaned):
                        return cleaned
        except Exception as e:
            print(f"Error extracting from PDF metadata: {e}")
        return None

    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Locate associated GROBID TEI XML file (same stem .tei.xml)."""
        candidate1 = pdf_file.with_suffix(".tei.xml")
        if candidate1.exists():
            return candidate1
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Extract title from structured GROBID TEI XML."""
        try:
            xml_content = xml_path.read_text(encoding="utf-8")
            root = etree.fromstring(xml_content.encode("utf-8"))
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}
            xpaths = [
                "//tei:analytic/tei:title[@type='main']",
                "//tei:monogr/tei:title[@type='main']",
                "//tei:titleStmt/tei:title[@type='main']",
                "//tei:titleStmt/tei:title"
            ]
            for xp in xpaths:
                title = root.xpath(f"string({xp})", namespaces=ns)
                cleaned = self._normalize(title)
                if self._is_valid(cleaned):
                    return cleaned
        except Exception as e:
            print(f"Error extracting from GROBID XML: {e}")
        return None

    def _extract_from_first_page_layout(self, pdf_file: Path) -> Optional[str]:
        """
        Layout + heuristic extraction: choose candidate block(s) from first page based on
        font size, vertical position, then filter out author/affiliation lines.
        """
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return None
                page = doc.load_page(0)
                blocks = page.get_text("dict")["blocks"]
        except Exception as e:
            print(f"Error extracting layout from first page: {e}")
            return None

        # Collect text segments with font size & position
        candidates: list[tuple[float, float, str]] = []  # (fontsize, y0, text)
        for block in blocks:
            if "lines" not in block:
                continue
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue
                    fontsize = span.get("size", 0)
                    y0 = span.get("origin", (0, 0))[1]
                    candidates.append((fontsize, y0, text))

        if not candidates:
            return None

        # Sort by font size descending, then vertical position ascending (top of page)
        candidates.sort(key=lambda x: (-x[0], x[1]))

        # Choose top-N spans (e.g., top 3) as potential title block
        top_spans = candidates[:3]
        combined = " ".join(span[2] for span in top_spans)
        cleaned = self._normalize(combined)

        # Filter out if it looks like author/affiliation block
        if self._looks_like_author_block(cleaned):
            # Try next spans if possible
            if len(candidates) > 3:
                alt_spans = candidates[3:6]
                combined2 = " ".join(span[2] for span in alt_spans)
                cleaned2 = self._normalize(combined2)
                if self._is_valid(cleaned2) and not self._looks_like_author_block(cleaned2):
                    return cleaned2
            return None

        if self._is_valid(cleaned):
            return cleaned

        return None

    def _looks_like_author_block(self, text: str) -> bool:
        """Heuristic to detect if a block is likely authors/affiliations rather than title."""
        if not text:
            return False
        # Frequent keywords in affiliations/authors
        if re.search(r"\b(university|dept|department|institute|school|college|laboratory|lab|affiliat|author|authors?)\b", text, re.I):
            return True
        # Many names separated by commas/and before a newline
        if re.match(r"[A-Z][a-z]+(\s[A-Z][a-z]+)+,?\s(and|&)\s[A-Z][a-z]+", text):
            return True
        return False

    def _is_valid(self, text: Optional[str]) -> bool:
        """Check if a title candidate is semantically plausible."""
        if not text:
            return False
        t = text.strip()

        # Title should not be too short
        if len(t) < 10:
            return False
        # Title should have a reasonable word count
        if len(t.split()) < 3 or len(t.split()) > 30:
            return False

        # Exclude if it's just numbers or version identifiers
        if re.match(r"^[0-9._-]+$", t):
            return False

        # Exclude arXiv IDs and DOIs
        if re.search(r"arXiv:\d+\.\d+", t) or re.search(r"doi:\S+", t, re.IGNORECASE):
            return False

        # Exclude non-title fragments like "ENTREE, P2 Pl"
        if re.search(r"(P2\sPl|ENTREE|v\d+)", t, re.IGNORECASE):
            return False

        return True

    def _normalize(self, text: Optional[str]) -> str:
        """Normalize whitespace, remove soft-hyphens and ligatures."""
        if not text:
            return ""
        # Merge hyphenated line breaks
        text = re.sub(r"(\w)-\s+(\w)", r"\1\2", text)
        text = text.replace("ﬁ", "fi").replace("ﬂ", "fl")
        text = text.replace("_", " ")
        text = re.sub(r"\s+", " ", text)
        # Remove arXiv IDs and version tags
        text = re.sub(r"arXiv:\d+\.\d+", "", text)  # Remove arXiv ID
        text = re.sub(r"v\d+", "", text)  # Remove version tags
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\toc_extractor.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
import fitz
import re
from pathlib import Path


class TocExtractor:
    """Extracts table of contents (TOC) entries directly from a PDF using PyMuPDF."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Return structured TOC entries, purely PDF-based."""
        pdf_file = Path(pdf_path)
        toc_entries: List[Dict[str, Any]] = []
        try:
            with fitz.open(pdf_file) as doc:
                toc = doc.get_toc(simple=True)
                if toc:
                    for level, title, page in toc:
                        title_clean = re.sub(r"\s+", " ", title.strip())
                        if title_clean:
                            toc_entries.append(
                                {"level": int(level), "title": title_clean, "page": int(page)}
                            )
                    return toc_entries

                # fallback to heuristic textual TOC
                toc_entries = self._extract_textual_toc(doc)
                return toc_entries

        except Exception as e:
            print(f"[WARN] Failed to extract TOC from {pdf_file}: {e}")
            return []

    # ------------------------------------------------------------------
    def _extract_textual_toc(self, doc: fitz.Document) -> List[Dict[str, Any]]:
        """Detect textual TOCs on the first pages when outline metadata is missing."""
        toc_entries: List[Dict[str, Any]] = []
        try:
            max_pages = min(5, len(doc))
            pattern = re.compile(r"^\s*(\d{0,2}\.?)+\s*[A-ZÄÖÜa-zäöü].{3,}\s+(\d{1,3})\s*$")

            for i in range(max_pages):
                text = doc.load_page(i).get_text("text")
                if not re.search(r"(Inhalt|Contents|Table of Contents)", text, re.I):
                    continue
                for line in text.splitlines():
                    if pattern.match(line.strip()):
                        parts = re.split(r"\s{2,}", line.strip())
                        if len(parts) >= 2:
                            title = re.sub(r"^\d+(\.\d+)*\s*", "", parts[0])
                            page = re.findall(r"\d+", parts[-1])
                            page_num = int(page[0]) if page else None
                            toc_entries.append(
                                {"level": 1, "title": title.strip(), "page": page_num}
                            )
            return toc_entries

        except Exception:
            return []
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\year_extractor.py ==== 
from __future__ import annotations
import re
import datetime
import time
from typing import Iterable, Tuple, Optional, Dict, List
from pathlib import Path
import fitz  # PyMuPDF
from lxml import etree

# optional normalizer
try:
    from unidecode import unidecode
except Exception:
    unidecode = None

# optional Crossref client
try:
    from habanero import Crossref
except Exception:
    Crossref = None  # type: ignore

CURRENT_YEAR = datetime.datetime.now().year
FUTURE_GRACE = 1  # allow slight future offset for clock drift


class YearExtractor:
    """Highly robust publication year extractor using multi-source heuristics, TEI, and Crossref."""

    def __init__(self, base_dir: Path | str | None = None, max_text_pages: int = 3, enable_crossref: bool = True):
        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.max_text_pages = max(1, int(max_text_pages))
        self.crossref = None
        if enable_crossref and Crossref is not None:
            try:
                self.crossref = Crossref(mailto="contact@example.com")
            except Exception:
                self.crossref = None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Main orchestrator for multi-source year extraction."""
        pdf_file = Path(pdf_path)
        candidates: List[Tuple[int, int, str]] = []  # (priority, score, year_str)

        # 1) TEI XML (most reliable)
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            year = self._extract_from_grobid(xml_path)
            if year:
                candidates.append((1, 100, year))

        # 2) PDF metadata (check & refine)
        year_meta, meta_score = self._extract_from_pdf_metadata(pdf_file)
        if year_meta:
            refined = self._refine_with_text_consistency(pdf_file, int(year_meta))
            if refined:
                candidates.append((2, meta_score, refined))

        # 3) Page text (explicit © or Published year)
        year_text, text_score = self._extract_from_page_text(pdf_file, self.max_text_pages)
        if year_text:
            candidates.append((3, text_score, year_text))

        # 4) Filename patterns / arXiv ID
        year_fn, fn_score = self._extract_from_filename(pdf_file.name)
        if year_fn:
            candidates.append((4, fn_score, year_fn))

        # 5) Optional DOI / Crossref fallback
        if not candidates:
            title, doi, arxiv = self._extract_title_doi_arxiv(pdf_file)
            if arxiv:
                y = self._year_from_arxiv_id(arxiv)
                if y:
                    candidates.append((5, 60, str(y)))
            if (doi or title) and self.crossref:
                y = self._lookup_via_crossref(title, doi)
                if y:
                    candidates.append((6, 70, y))

        if not candidates:
            return None

        # final prioritization
        candidates.sort(key=lambda t: (t[0], -t[1], -int(t[2])))
        best_year = candidates[0][2]
        return best_year

    # ------------------------------------------------------------------
    def _lookup_via_crossref(self, title: Optional[str], doi: Optional[str]) -> Optional[str]:
        """Crossref lookup (last resort)."""
        if not self.crossref:
            return None
        for attempt in range(2):
            try:
                if doi:
                    result = self.crossref.works(ids=doi, timeout=3)
                    msg = result.get("message", {})
                elif title:
                    result = self.crossref.works(query=title, limit=1, timeout=3)
                    msg = result.get("message", {}).get("items", [{}])[0]
                else:
                    return None

                for key in ("published-print", "published-online", "issued"):
                    info = msg.get(key)
                    if info and "date-parts" in info:
                        y = info["date-parts"][0][0]
                        if self._valid_year(y):
                            return str(y)
            except Exception:
                time.sleep(1)
        return None

    # ------------------------------------------------------------------
    def _extract_title_doi_arxiv(self, pdf_file: Path) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """Extract DOI, arXiv ID, and title from early pages."""
        doi_re = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b", re.I)
        arxiv_re = re.compile(r"\b(?:arxiv[:/ ]?)?(\d{4}\.\d{4,5}|[a-z\-]+/\d{7}|[0-9]{7,8})(v\d+)?\b", re.I)
        title = doi = arxiv = None

        try:
            with fitz.open(pdf_file) as doc:
                meta_title = (doc.metadata or {}).get("title")
                if meta_title:
                    title = meta_title.strip()
                text = ""
                for i in range(min(3, len(doc))):
                    text += (doc.load_page(i).get_text("text") or "") + "\n"
        except Exception:
            text = ""

        if unidecode and text:
            text = unidecode(text)

        m = doi_re.search(text)
        if m:
            doi = m.group(0).strip().rstrip(".,)")

        m = arxiv_re.search(text)
        if m:
            arxiv = m.group(1).strip()

        if not title and text:
            for line in text.splitlines():
                s = line.strip()
                if len(s) > 10 and not re.search(r"(abstract|introduction|contents)", s, re.I):
                    title = s
                    break

        return title, doi, arxiv

    # ------------------------------------------------------------------
    def _year_from_arxiv_id(self, arxiv_id: str) -> Optional[int]:
        """Infer year from arXiv ID pattern."""
        try:
            m = re.match(r"^(\d{2})(\d{2})\.\d{4,5}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
            m = re.match(r"^[a-z\-]+/(\d{2})(\d{2})\d{3,4}$", arxiv_id, re.I)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
            m = re.match(r"^(\d{2})(\d{2})\d{3,4}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _valid_year(self, y: int) -> bool:
        """Check plausible range (1900–current+grace)."""
        return isinstance(y, int) and 1900 <= y <= (CURRENT_YEAR + FUTURE_GRACE)

    # ------------------------------------------------------------------
    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Find possible TEI XML companion file."""
        xml_candidate = pdf_file.with_suffix(".tei.xml")
        if xml_candidate.exists():
            return xml_candidate
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    # ------------------------------------------------------------------
    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Parse TEI XML for earliest valid date."""
        try:
            xml_bytes = xml_path.read_bytes()
            root = etree.fromstring(xml_bytes)
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}
            xpaths = [
                "//tei:sourceDesc//tei:imprint/tei:date",
                "//tei:profileDesc//tei:creation/tei:date",
                "//tei:biblStruct//tei:imprint/tei:date",
            ]
            for xp in xpaths:
                for node in root.xpath(xp, namespaces=ns):
                    for key in ("when", "when-iso", "notBefore", "notAfter"):
                        val = node.get(key)
                        if val:
                            y = self._year_from_date_string(val)
                            if y and self._valid_year(y):
                                return str(y)
                    text_val = (node.text or "").strip()
                    if text_val:
                        for y in self._years_from_string(text_val):
                            if self._valid_year(y):
                                return str(y)
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Tuple[Optional[str], int]:
        """Parse PDF metadata and filter out nonsense years (1970, 1980, 1600)."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                kv: Dict[str, str] = {k.lower(): v for k, v in meta.items() if isinstance(v, str) and v.strip()}
                best: Tuple[Optional[int], int] = (None, -1)
                for key, score in [("creationdate", 80), ("moddate", 60), ("date", 50)]:
                    val = kv.get(key)
                    if not val:
                        continue
                    y = self._year_from_date_string(val)
                    if not y or y in {0, 1, 1600, 1970, 1980}:
                        continue
                    if self._valid_year(y) and score > best[1]:
                        best = (y, score)
                if best[0]:
                    return str(best[0]), best[1]
        except Exception:
            pass
        return None, -1

    # ------------------------------------------------------------------
    def _refine_with_text_consistency(self, pdf_file: Path, year_meta: Optional[int]) -> Optional[str]:
        """Cross-check metadata with visible text for explicit © years."""
        try:
            with fitz.open(pdf_file) as doc:
                text_parts = []
                for i in range(min(3, len(doc))):
                    text_parts.append(doc.load_page(i).get_text("text") or "")
                for i in range(max(0, len(doc) - 2), len(doc)):
                    text_parts.append(doc.load_page(i).get_text("text") or "")
                text = "\n".join(text_parts)
        except Exception:
            return str(year_meta) if year_meta else None

        if unidecode and text:
            text = unidecode(text)

        # check for explicit © or "Published in"
        explicit_match = re.findall(r"(?:©|Copyright|Published\s+in|Reprinted\s+in)\s*(19|20)\d{2}", text, re.I)
        if explicit_match:
            ys = [int(y[-4:]) for y in re.findall(r"(19|20)\d{2}", text)]
            ys = [y for y in ys if self._valid_year(y)]
            if ys:
                y_max = max(ys)
                return str(y_max)

        # fallback: most recent plausible number
        years = [int(m.group()) for m in re.finditer(r"(19|20)\d{2}", text)]
        years = [y for y in years if self._valid_year(y)]
        if not years:
            return str(year_meta) if year_meta else None
        y_max = max(years)

        # adjust if metadata is suspiciously old
        if year_meta and (year_meta < 1950 or abs(year_meta - y_max) >= 10):
            return str(y_max)
        return str(year_meta if year_meta else y_max)

    # ------------------------------------------------------------------
    def _extract_from_page_text(self, pdf_file: Path, pages: int = 2) -> Tuple[Optional[str], int]:
        """Scan early pages for explicit publication indicators."""
        try:
            with fitz.open(pdf_file) as doc:
                best: Tuple[Optional[int], int] = (None, -1)
                for i in range(min(len(doc), pages)):
                    text = doc.load_page(i).get_text("text") or ""
                    if unidecode and text:
                        text = unidecode(text)
                    for pattern in [
                        r"(?:©|Copyright|Published\s+in|Reprinted\s+in)\s*(19|20)\d{2}",
                        r"(19|20)\d{2}",
                    ]:
                        years = [int(y[-4:]) for y in re.findall(pattern, text)]
                        years = [y for y in years if self._valid_year(y)]
                        if years:
                            y_pick = max(years)
                            score = 45 + (pages - i)
                            if score > best[1]:
                                best = (y_pick, score)
                if best[0]:
                    return str(best[0]), best[1]
        except Exception:
            pass
        return None, -1

    # ------------------------------------------------------------------
    def _extract_from_filename(self, filename: str) -> Tuple[Optional[str], int]:
        """Infer from filename (e.g., arXiv IDs, embedded years)."""
        base = unidecode(filename) if unidecode else filename
        base = re.sub(r"v\d+\b", "", base, flags=re.I)

        m = re.search(r"\b(\d{4}\.\d{4,5})\b", base)
        if m:
            y = self._year_from_arxiv_id(m.group(1))
            if y:
                return str(y), 50

        m = re.search(r"\b([a-z\-]+/\d{7,8}|\d{7,8})\b", base, re.I)
        if m:
            y = self._year_from_arxiv_id(m.group(1))
            if y:
                return str(y), 45

        years = [y for y in self._years_from_string(base) if self._valid_year(y) and y != 1970]
        if years:
            return str(max(years)), 30
        return None, -1

    # ------------------------------------------------------------------
    def _year_from_date_string(self, s: str) -> Optional[int]:
        """Parse ISO/XMP date strings like D:YYYYMMDD or YYYY-MM-DD."""
        if not s:
            return None
        s = s.strip()
        m = re.match(r"^D:(\d{4})", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None
        m = re.match(r"^(\d{4})(?:[-/]\d{2}(?:[-/]\d{2})?)?$", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None
        for y in self._years_from_string(s):
            if self._valid_year(y):
                return y
        return None

    # ------------------------------------------------------------------
    def _years_from_string(self, s: str) -> Iterable[int]:
        """Yield distinct plausible years in order."""
        seen = set()
        for m in re.finditer(r"(?<!\d)(19|20)\d{2}(?!\d)", s):
            y = int(m.group(0))
            if y not in seen:
                seen.add(y)
                yield y
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_abstract_extractor.py ==== 
from __future__ import annotations
from typing import Any, Dict
from lxml import etree
import re
# from src.core.ingestion.metadata.interfaces.i_abstract_extractor import IAbstractExtractor


# class AbstractExtractor(IAbstractExtractor):
#     """Extracts abstract section using GROBID TEI XML."""

#     def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
#         xml_data = parsed_document.get("grobid_xml")
#         if not xml_data or not xml_data.strip().startswith("<"):
#             return None

#         try:
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             root = etree.fromstring(xml_data.encode("utf8"))
#             # collect all text under <abstract> or <div type='abstract'>
#             xpath_candidates = [
#                 "//tei:abstract",
#                 "//tei:div[@type='abstract']",
#                 "//tei:profileDesc/tei:abstract",
#             ]
#             for path in xpath_candidates:
#                 text = root.xpath(f"string({path})", namespaces=ns)
#                 if text and len(text.strip()) > 20:
#                     cleaned = re.sub(r"\s+", " ", text.strip())
#                     return cleaned
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_author_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict, List

class IAuthorExtractor(ABC):
    """Interface for extracting document authors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> List[str]:
        """Extract author names as a list of strings."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_file_size_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IFileSizeExtractor(ABC):
    """Interface for extracting the file size of a PDF."""

    @abstractmethod
    def extract(self, pdf_path: str) -> int | None:
        """Return file size in bytes."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_language_detector.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ILanguageDetector(ABC):
    """Interface for detecting the main document language."""

    @abstractmethod
    def detect(self, text: str, metadata: Dict[str, Any] | None = None) -> str | None:
        """Detect the dominant language code (e.g. 'en', 'de')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_page_number_extractor.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Optional


class IPageNumberExtractor(ABC):
    """Interface for extracting page numbers from a document."""

    @abstractmethod
    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the given PDF path.
        
        :param pdf_path: Path to the PDF file
        :return: The page number (if available), otherwise None.
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_title_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ITitleExtractor(ABC):
    """Interface for extracting the main document title."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the document title."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_year_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IYearExtractor(ABC):
    """Interface for extracting publication year."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the publication year as string (e.g. '2023')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\toc_extractor_interface.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
from abc import ABC, abstractmethod


class TocExtractorInterface(ABC):
    """Interface for table-of-contents extractors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[Dict[str, Any]]:
        """
        Extracts the table of contents (TOC) from a given PDF file.

        Returns:
            A list of dictionaries with keys:
                - level: int
                - title: str
                - page: int
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\__init__.py ==== 
# Expose all interfaces for clean import
from .i_title_extractor import ITitleExtractor
from .i_author_extractor import IAuthorExtractor
from .i_year_extractor import IYearExtractor
#from .i_abstract_extractor import IAbstractExtractor
from .i_language_detector import ILanguageDetector
from .i_file_size_extractor import IFileSizeExtractor
#from .i_has_ocr_extractor import IHasOcrExtractor

__all__ = [
    "ITitleExtractor",
    "IAuthorExtractor",
    "IYearExtractor",
    #"IAbstractExtractor",
    "ILanguageDetector",
    "IFileSizeExtractor",
    #"IHasOcrExtractor",
]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\utils\name_verifier.py ==== 
import re
from nameparser import HumanName

class LocalNameVerifier:
    """Local deterministic fallback for name validation."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 60:
            return False
        if any(ch.isdigit() for ch in text):
            return False

        hn = HumanName(text)
        # requires at least one capitalized component
        valid_structure = bool(hn.first or hn.last)
        valid_chars = bool(self.NAME_PATTERN.search(text)) or bool(self.TITLE_PATTERN.search(text))
        invalid_tokens = any(tok.lower() in {
            "university", "institute", "department", "school",
            "machine", "learning", "arxiv", "neural", "transformer",
            "abstract", "introduction", "appendix"
        } for tok in text.split())

        return valid_structure and valid_chars and not invalid_tokens
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}


==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}

.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\file_utils.py ==== 
import os
from PyPDF2 import PdfFileReader
from pathlib import Path

def ensure_dir(path: Path):
    """Create directory if it doesn't exist."""
    path.mkdir(parents=True, exist_ok=True)

def get_file_size(file_path: Path) -> int:
    """Returns the file size in bytes."""
    return os.path.getsize(file_path)

def get_pdf_page_count(file_path: Path) -> int:
    """Returns the number of pages in a PDF."""
    with open(file_path, "rb") as file:
        reader = PdfFileReader(file)
        return reader.getNumPages()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\language_detector.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\performance_timer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\llm_orchestrator.py ==== 
# src/core/llm/llm_orchestrator.py
from __future__ import annotations
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
import json
import time

from src.core.config.config_loader import ConfigLoader
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator
from src.core.llm.ollama_llm import OllamaLLM
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.prompt.query.prompt_builder import PromptBuilder


class LLMOrchestrator:
    """Refined query → Retrieval → Prompt assembly → LLM"""

    def __init__(self, config_path: str = "configs/llm.yaml"):
        self.cfg = ConfigLoader(config_path).config
        self.logger = logging.getLogger("LLMOrchestrator")
        self._setup_logging()
        self.prompt_phase = PromptOrchestrator()
        self.retriever = self._init_retriever()
        self.prompt_builder = PromptBuilder()
        self.llm = self._init_llm()
        self.logger.info("LLMOrchestrator initialized successfully.")

    def _setup_logging(self) -> None:
        log_level = self.cfg.get("global", {}).get("log_level", "INFO").upper()
        logging.basicConfig(level=getattr(logging, log_level), format="%(levelname)s | %(message)s")
        self.logger.info("Logging configured.")

    def _init_retriever(self) -> RetrievalOrchestrator:
        retriever = RetrievalOrchestrator()
        self.logger.info("Retriever initialized.")
        return retriever

    def _init_llm(self) -> OllamaLLM:
        profile_name = self.cfg.get("generation", {}).get("llm", {}).get("profile", "default")
        llm = OllamaLLM(config_path="configs/llm.yaml", profile=profile_name)
        self.logger.info(f"LLM backend ready (profile='{profile_name}').")
        return llm

    def process_query(self, query_obj: Optional[Dict[str, Any]]) -> str:
        """Full pipeline execution."""
        if not query_obj or not query_obj.get("refined_query"):
            self.logger.warning("Invalid or empty refined query object.")
            return ""

        query = query_obj["refined_query"]
        intent = query_obj.get("intent", "conceptual")

        self.logger.info(f"Retrieving context for refined query='{query}' (intent='{intent}')")
        try:
            retrieved_docs = self.retriever.retrieve(query, intent)
        except Exception as e:
            self.logger.exception(f"Retrieval failed: {e}")
            return ""
        if not retrieved_docs:
            self.logger.warning("No relevant documents retrieved.")
            return ""

        try:
            final_prompt = self._compose_full_prompt(query, intent, retrieved_docs)
        except Exception as e:
            self.logger.exception(f"Prompt construction failed: {e}")
            return ""

        llm_input = {
            "system_prompt": final_prompt,
            "query_refined": query.strip(),
            "intent": intent,
            "context_chunks": retrieved_docs,
        }
        self._log_llm_input(llm_input)

        try:
            output = self.llm.generate(final_prompt.strip())
            qid = self._log_llm_run(query, intent, retrieved_docs, output, final_prompt)
            self.logger.info(f"LLM generation successful. Run logged (query_id={qid}).")
            return output.strip()
        except Exception as e:
            self.logger.exception(f"LLM generation failed: {e}")
            return ""

    # ---------------------------------------------------------------
    def _compose_full_prompt(
        self,
        refined_query: str,
        intent: str,
        retrieved_chunks: List[Dict[str, Any]]
    ) -> str:
        """Builds final prompt with refined query (no raw user query)."""
        system_prompt = self.prompt_builder.build_prompt(refined_query, intent)

        if intent == "chronological":
            def safe_year(meta: Dict[str, Any]) -> int:
                y = meta.get("year")
                try:
                    return int(y)
                except Exception:
                    return 9999
            retrieved_chunks = sorted(
                retrieved_chunks,
                key=lambda c: safe_year(c.get("metadata", {}))
            )
            self.logger.info("Chunks sorted chronologically (ascending).")

        lines: List[str] = []
        lines.append(system_prompt.strip())
        lines.append("")
        lines.append(f"Refined query:\n{refined_query.strip()}\n")
        lines.append(
            "You are given the following context snippets from historical AI-related documents. "
            "Each snippet is associated with a numeric source id in square brackets. "
            "When you answer, cite these sources using their numeric ids, e.g. [1], [2]. "
            "Do not invent new references or sources that are not listed below."
        )
        lines.append("")
        lines.append("Context snippets:")

        for idx, chunk in enumerate(retrieved_chunks, start=1):
            meta = chunk.get("metadata", {}) or {}
            src = meta.get("source_file", "Unknown.pdf")
            year = meta.get("year", "n/a")
            header = f"[{idx}] {src} ({year})"
            lines.append(header)
            lines.append(chunk.get("text", ""))
            lines.append("")

        lines.append(
            "Now answer the refined query above using ONLY the information from the context snippets. "
            "Use numeric citations like [1], [3] that refer to the snippet ids. "
            "Do not add a bibliography or 'References' section; use inline numeric citations only."
        )
        return "\n".join(lines)

    # ---------------------------------------------------------------
    def _log_llm_input(self, llm_input: Dict[str, Any]) -> Path:
        ts = time.strftime("%Y-%m-%dT%H-%M-%S")
        log_dir = Path("data/logs_llm")
        log_dir.mkdir(parents=True, exist_ok=True)
        payload = {
            "timestamp": ts,
            "query_refined": llm_input["query_refined"],
            "intent": llm_input["intent"],
            "prompt_final_to_llm": llm_input["system_prompt"],
            "chunks_final_to_llm": llm_input["context_chunks"],
        }
        out_path = log_dir / f"llm_input_{ts}.json"
        out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        self.logger.info(f"Logged refined LLM input → {out_path}")
        return out_path

    def _log_llm_run(
        self,
        query: str,
        intent: str,
        retrieved: List[Dict[str, Any]],
        output: str,
        final_prompt: str,
    ) -> str:
        ts = time.strftime("%Y-%m-%dT%H-%M-%S")
        qid = "".join(ch if ch.isalnum() or ch in "-_" else "_" for ch in query)[:80] or "query"
        log_dir = Path("data/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        payload = {
            "timestamp": ts,
            "query_id": qid,
            "query_refined": query,
            "intent": intent,
            "prompt_final_to_llm": final_prompt,
            "retrieved_chunks": retrieved,
            "model_output": output,
        }
        (log_dir / f"llm_{ts}.json").write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        return qid
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\ollama_llm.py ==== 
from __future__ import annotations
import subprocess
import logging
from typing import Any
from src.core.llm.interfaces.i_llm import ILLM
from src.core.config.config_loader import ConfigLoader

logger = logging.getLogger(__name__)


class OllamaLLM(ILLM):
    """
    Lightweight local Ollama backend.

    WICHTIG:
    - Bekommt nur noch einen einzigen Prompt-String.
    - KEIN separates context-Argument mehr – alles steht im Prompt.
    """

    def __init__(self, config_path: str = "configs/llm.yaml", profile: str | None = None):
        cfg = ConfigLoader(config_path).config
        global_cfg = cfg.get("global", {})
        profiles = cfg.get("profiles", {})
        self.profile = profile or "default"
        profile_cfg = profiles.get(self.profile, {})

        # Load model configuration
        self.model = profile_cfg.get("model", "mistral:7b-instruct")
        self.temperature = float(profile_cfg.get("temperature", 0.2))
        self.max_tokens = int(profile_cfg.get("max_tokens", 512))
        self.auto_pull = bool(profile_cfg.get("auto_pull", True))

        log_level = global_cfg.get("log_level", "INFO").upper()
        logging.basicConfig(level=getattr(logging, log_level), format="%(levelname)s | %(message)s")

        logger.info(f"OllamaLLM ready (profile={self.profile}, model={self.model})")
        self._ensure_model_available()

    # ------------------------------------------------------------------
    def _ensure_model_available(self) -> None:
        """Ensure the configured Ollama model is available locally."""
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
            )
            if result.returncode != 0:
                raise RuntimeError(result.stderr.strip())

            if self.model.lower() not in result.stdout.lower():
                if not self.auto_pull:
                    logger.warning(f"Model '{self.model}' missing (auto_pull disabled).")
                    return
                logger.info(f"Pulling model '{self.model}' ...")
                pull_result = subprocess.run(
                    ["ollama", "pull", self.model],
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    errors="replace",
                )
                if pull_result.returncode != 0:
                    raise RuntimeError(pull_result.stderr.strip())
                logger.info(f"Successfully pulled '{self.model}'.")
            else:
                logger.info(f"Model '{self.model}' available locally.")
        except Exception as e:
            logger.error(f"Model check failed: {e}")

    # ------------------------------------------------------------------
    def generate(self, prompt: str) -> str:
        """
        Run the Ollama model and return its output.

        Erwartung:
        - `prompt` ist bereits der komplette zusammengesetzte Prompt
          (Instruktion + Query + alle Context-Chunks).
        """
        try:
            cmd = ["ollama", "run", self.model, prompt]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
            )

            if result.returncode != 0:
                raise RuntimeError(result.stderr.strip())

            output = result.stdout.strip()
            logger.info(f"Ollama generation successful | model={self.model} | len={len(output)}")
            return output

        except Exception as e:
            logger.exception(f"Ollama model run failed: {e}")
            raise

    # ------------------------------------------------------------------
    def close(self) -> None:
        """No persistent connections or open streams."""
        logger.info("OllamaLLM closed cleanly.")
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\interfaces\i_llm.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod


class ILLM(ABC):
    """Interface for any local or remote LLM backend."""

    @abstractmethod
    def generate(self, prompt: str) -> str:
        """
        Generate an answer given a COMPLETE prompt.

        Der Prompt enthält:
        - Systeminstruktion,
        - User-Query,
        - eingebettete Kontextsnippets (falls RAG).
        """
        raise NotImplementedError

    @abstractmethod
    def close(self) -> None:
        """Gracefully close model connection or session."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\formatters.py ==== 
from __future__ import annotations
import logging
import json
from datetime import datetime

try:
    # Use colorama for colored console output if available
    from colorama import Fore, Style
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class PlainFormatter(logging.Formatter):
    # Simple, deterministic log format without colors
    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S")
        return f"{ts} | {record.levelname:<8} | {record.getMessage()}"


class ColorFormatter(logging.Formatter):
    # Colored console formatter for better readability
    COLORS = {
        "DEBUG": Fore.BLUE,
        "INFO": Fore.GREEN,
        "WARNING": Fore.YELLOW,
        "ERROR": Fore.RED,
        "CRITICAL": Fore.MAGENTA,
    }

    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        color = self.COLORS.get(record.levelname, "")
        reset = Style.RESET_ALL if COLORAMA_AVAILABLE else ""
        return f"{color}{ts} | {record.levelname:<8} | {record.getMessage()}{reset}"


class JSONFormatter(logging.Formatter):
    # JSON-based formatter for structured, machine-readable logs
    def format(self, record: logging.LogRecord) -> str:
        entry = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        return json.dumps(entry, ensure_ascii=False)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\logger.py ==== 
from __future__ import annotations
import logging
import os
import json
from pathlib import Path
from logging.handlers import RotatingFileHandler
from datetime import datetime
from typing import Any, Dict
from src.core.logging.formatters import PlainFormatter, ColorFormatter, JSONFormatter

try:
    # Initialize colorama if available for colored console output
    from colorama import init as colorama_init
    colorama_init()
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class StructuredLogger:
    # Singleton-based, configurable logger for all pipeline phases
    _instance: logging.Logger | None = None

    @classmethod
    def get_logger(cls, name: str = "HDA", config: Dict[str, Any] | None = None) -> logging.Logger:
        # Reuse existing logger if already created
        if cls._instance:
            return cls._instance

        cfg = config or {}
        level = getattr(logging, cfg.get("level", "INFO").upper(), logging.INFO)
        log_to_file = cfg.get("log_to_file", True)
        log_dir = Path(cfg.get("log_dir", "logs"))
        file_name = cfg.get("file_name", "hda.log")
        rotate = cfg.get("rotate_logs", True)
        json_log = cfg.get("json_log", False)
        console_color = cfg.get("console_color", True)

        # Create and configure logger
        logger = logging.getLogger(name)
        logger.setLevel(level)
        logger.propagate = False

        # Console handler setup
        console_handler = logging.StreamHandler()
        if console_color and COLORAMA_AVAILABLE:
            console_handler.setFormatter(ColorFormatter())
        else:
            console_handler.setFormatter(PlainFormatter())
        logger.addHandler(console_handler)

        # File handler setup
        if log_to_file:
            log_dir.mkdir(parents=True, exist_ok=True)
            file_path = log_dir / file_name
            if rotate:
                handler = RotatingFileHandler(file_path, maxBytes=5_000_000, backupCount=5, encoding="utf-8")
            else:
                handler = logging.FileHandler(file_path, encoding="utf-8")
            handler.setFormatter(JSONFormatter() if json_log else PlainFormatter())
            logger.addHandler(handler)

        # Store singleton instance
        cls._instance = logger
        return logger
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\prompt_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Dict, Any, Optional

from src.core.prompt.query.query_input import QueryInput
from src.core.prompt.query.query_preprocessor import QueryPreprocessor
from src.core.prompt.query.prompt_builder import PromptBuilder


class PromptOrchestrator:
    """
    Coordinates the full prompt understanding pipeline:
    (1) user input → (2) preprocessing → (3) intent classification → (4) intent-guided query reformulation.
    Returns a refined query that aligns with the detected analytical intent.
    """

    def __init__(self, log_level: str = "INFO"):
        self.logger = logging.getLogger("PromptOrchestrator")
        if not self.logger.handlers:
            logging.basicConfig(
                level=getattr(logging, log_level.upper(), logging.INFO),
                format="%(levelname)s | %(message)s"
            )

        self.query_input = QueryInput()
        self.preprocessor = QueryPreprocessor()
        self.prompt_builder = PromptBuilder()
        self.logger.info("PromptOrchestrator initialized successfully.")

    # ------------------------------------------------------------------
    def get_prompt_object(self) -> Dict[str, Any]:
        """
        Execute the complete prompt phase: read, clean, classify, reformulate.

        Returns
        -------
        dict
            {
                "refined_query": str,
                "intent": str
            }
        """
        try:
            raw_query: Optional[str] = self.query_input.read_interactive()
            if not raw_query or not raw_query.strip():
                self.logger.warning("No input provided. Skipping prompt phase.")
                return {}

            # Preprocess & classify
            result: Dict[str, Any] = self.preprocessor.process(raw_query)
            if not result or "processed_query" not in result:
                self.logger.warning("Preprocessing failed or returned empty result.")
                return {}

            processed_query = result["processed_query"]
            intent = result["intent"]

            # Intent-guided reformulation
            refined_query = self.prompt_builder.reformulate_query(processed_query, intent)

            self.logger.info(f"Prompt phase complete → intent='{intent}', refined query='{refined_query}'")
            return {"refined_query": refined_query, "intent": intent}

        except KeyboardInterrupt:
            self.logger.info("Prompt input cancelled by user (Ctrl+C). Exiting interactive mode.")
            raise

        except EOFError:
            self.logger.info("Input stream closed (EOF). Terminating prompt phase.")
            raise KeyboardInterrupt

        except Exception as e:
            self.logger.exception(f"Unexpected error in prompt phase: {e}")
            return {}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\prompt_builder.py ==== 
from __future__ import annotations
import logging
from typing import Literal

logger = logging.getLogger(__name__)
Intent = Literal["chronological", "conceptual", "analytical", "comparative"]


class PromptBuilder:
    """
    Builds intent-specific system prompts for RAG/LLM orchestration
    and provides intent-guided query reformulation for improved retrieval alignment.
    """

    def __init__(self):
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

    # ------------------------------------------------------------------
    def _system_prompt_for(self, intent: Intent) -> str:
        """Return intent-specific base instruction with citation control."""
        cite_rule = (
            "Use numeric citations [1], [2], etc. "
            "Do not output a bibliography or any 'References' section. "
            "Do not list full literature items, author names, or publication years in parentheses. "
            "Only use numeric markers to refer to the provided document indices."
        )

        if intent == "chronological":
            return (
                "You are an analytical historian of Artificial Intelligence. "
                "Explain how the concept evolved over time, emphasizing paradigm shifts, milestones, and key research phases. "
                "Present the findings strictly in chronological order of publication years. "
                f"{cite_rule}"
            )

        if intent == "conceptual":
            return (
                "You are an AI expert. Provide a precise definition, describe its theoretical foundations, "
                "and explain its essential principles. "
                f"{cite_rule}"
            )

        if intent == "analytical":
            return (
                "You are a rigorous AI researcher. Analyze mechanisms, trade-offs, and implications "
                "with logically structured reasoning and explicit reference to retrieved sources. "
                f"{cite_rule}"
            )

        # comparative intent
        return (
            "You are an analytical researcher. Compare and contrast positions, frameworks, or definitions, "
            "and evaluate their empirical or theoretical grounding. "
            f"{cite_rule}"
        )

    # ------------------------------------------------------------------
    def reformulate_query(self, query: str, intent: Intent) -> str:
        """
        Intent-guided reformulation of the user's query into a canonical analytical form.

        The goal is to standardize phrasing for better retrieval and prompt conditioning.
        """
        if not query or not query.strip():
            raise ValueError("Empty query cannot be reformulated")

        q = query.strip()

        if intent == "chronological":
            return f"Trace the historical development and evolution of {q} over time."

        if intent == "conceptual":
            return f"Define {q}, describe its theoretical foundations, and explain its core principles."

        if intent == "analytical":
            return f"Analyze the mechanisms, advantages, and limitations of {q}."

        if intent == "comparative":
            return f"Compare and contrast key perspectives, frameworks, or interpretations of {q}."

        return q

    # ------------------------------------------------------------------
    def build_prompt(self, query: str, intent: Intent) -> str:
        """Return the clean base system prompt without context."""
        if not query or not query.strip():
            raise ValueError("Empty query passed to PromptBuilder")

        system_prompt = self._system_prompt_for(intent)
        logger.debug(f"Built system prompt for intent='{intent}' (no context embedded).")
        return system_prompt
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_classifier.py ==== 
# src/core/prompt/query/query_classifier.py 
from __future__ import annotations
import logging
import numpy as np
from typing import Literal, Dict, List, Optional
from sentence_transformers import SentenceTransformer, util

Intent = Literal["chronological", "conceptual", "analytical", "comparative"]

class QueryClassifier:
    """
    Embedding-based intent classifier.
    No heuristics, no language dependency, fully model-driven.
    """
    def __init__(
        self,
        model_name: str = "all-MiniLM-L6-v2",
        label_texts: Optional[Dict[str, str]] = None
    ):
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

        self.model = SentenceTransformer(model_name)
        # Canonical label representations (configurable)
        self.label_texts = label_texts or {
            "chronological": "questions about historical development or changes over time",
            "conceptual": "questions asking for definition, explanation or theoretical meaning",
            "analytical": "questions asking for comparison, evaluation or analysis",
            "comparative": "questions asking for contrast or difference between ideas",
        }

        self.labels = list(self.label_texts.keys())
        # Precompute label embeddings for efficient similarity calculation
        self.label_embeddings = self.model.encode(
            list(self.label_texts.values()), normalize_embeddings=True
        )
        self.logger.info(f"Initialized semantic intent classifier with {len(self.labels)} labels")

    # ------------------------------------------------------------------
    def classify(self, query: str) -> Intent:
        """Compute embedding similarity to predefined intent prototypes."""
        if not query or not query.strip():
            return "conceptual"
        # Encode user query
        q_emb = self.model.encode(query, normalize_embeddings=True)
        # Compute cosine similarity between query and label embeddings
        sims = util.cos_sim(q_emb, self.label_embeddings)[0].cpu().numpy()
        # Select intent with highest similarity
        idx = int(np.argmax(sims))
        intent = self.labels[idx]
        self.logger.info(f"Predicted semantic intent='{intent}' (sim={sims[idx]:.3f})")
        return intent  # type: ignore
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_input.py ==== 
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional

logger = logging.getLogger(__name__)

@dataclass
class QueryInputConfig:
    prompt_text: str = "> "
    strip_input: bool = True
    allow_empty: bool = False

class QueryInput:
    """Handles raw query intake (interactive or programmatic)."""
    def __init__(self, cfg: Optional[QueryInputConfig] = None):
        self.cfg = cfg or QueryInputConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

    def read_interactive(self) -> Optional[str]:
        """Read query from stdin (interactive mode)."""
        try:
            raw = input(self.cfg.prompt_text)
        except (KeyboardInterrupt, EOFError):
            # Raise further to allow graceful termination at orchestrator level
            print()  # newline for clean CLI exit
            raise
        q = raw.strip() if self.cfg.strip_input else raw
        if not q and not self.cfg.allow_empty:
            logger.warning("Empty query ignored.")
            return None
        return q

    def from_program(self, query: Optional[str]) -> Optional[str]:
        """Receive query from another process or script."""
        if query is None:
            return None
        return query.strip() if self.cfg.strip_input else query
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_preprocessor.py ==== 
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional
from src.core.prompt.query.query_classifier import QueryClassifier

logger = logging.getLogger(__name__)

@dataclass
class QueryPreprocessorConfig:
    lowercase: bool = True
    remove_double_spaces: bool = True
    embedding_model: str = "all-MiniLM-L6-v2"

class QueryPreprocessor:
    """Clean, normalize, and classify queries without heuristics."""
    def __init__(self, cfg: Optional[QueryPreprocessorConfig] = None):
        self.cfg = cfg or QueryPreprocessorConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
        # Initialize classifier for semantic intent detection
        self.classifier = QueryClassifier(model_name=self.cfg.embedding_model)

    def validate(self, query: Optional[str]) -> str:
        """Ensure query is not empty or invalid."""
        if not query or not query.strip():
            raise ValueError("Empty query is not allowed")
        return query.strip()

    def clean(self, query: str) -> str:
        """Normalize casing and spacing according to configuration."""
        q = query.lower() if self.cfg.lowercase else query
        if self.cfg.remove_double_spaces:
            q = " ".join(q.split())
        return q.strip()

    def process(self, raw_query: Optional[str]) -> dict:
        """Return cleaned text + semantic intent."""
        q = self.validate(raw_query)
        clean_q = self.clean(q)
        intent = self.classifier.classify(clean_q)
        logger.info(f"Processed query='{clean_q}' intent='{intent}'")
        return {"raw_query": q, "processed_query": clean_q, "intent": intent}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\faiss_retriever.py ==== 
# src/core/retrieval/faiss_retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import re
import numpy as np
import logging
from math import exp
from src.core.retrieval.interfaces.i_retriever import IRetriever


class FAISSRetriever(IRetriever):
    """FAISS-based semantic retriever with optional temporal and source diversification."""

    def __init__(
        self,
        vector_store_dir: str,
        model_name: str,
        top_k_retrieve: int = 50,
        normalize_embeddings: bool = True,
        use_gpu: bool = False,
        similarity_metric: str = "cosine",
        temporal_awareness: bool = True,
        temporal_tau: float = 8.0,
        temporal_weight: float = 0.30,
        valid_year_range: Tuple[int, int] = (1900, 2100),
        diversify_sources: bool = True,  # enable balanced source selection
    ):
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError(
                "faiss-cpu and sentence-transformers are required. "
                "Install via: poetry add faiss-cpu sentence-transformers"
            ) from e

        self.faiss = faiss
        self.vector_store_dir = Path(vector_store_dir).resolve()
        self.index_path = self.vector_store_dir / "index.faiss"
        self.meta_path = self.vector_store_dir / "metadata.jsonl"

        if not self.index_path.exists() or not self.meta_path.exists():
            raise FileNotFoundError(f"Incomplete vector store: {self.vector_store_dir}")

        self.model = SentenceTransformer(model_name)
        self.top_k_retrieve = int(top_k_retrieve)
        self.normalize_embeddings = bool(normalize_embeddings)
        self.use_gpu = bool(use_gpu)
        self.similarity_metric = similarity_metric.lower().strip()
        self.temporal_awareness = bool(temporal_awareness)
        self.temporal_tau = float(temporal_tau)
        self.temporal_weight = float(temporal_weight)
        self.valid_year_range = valid_year_range
        self.diversify_sources = bool(diversify_sources)

        if self.similarity_metric not in {"cosine", "dot"}:
            raise ValueError(f"Unsupported similarity metric: {self.similarity_metric}")

        # Load FAISS index
        self.logger.info(f"Loading FAISS index: {self.index_path}")
        self.index = faiss.read_index(str(self.index_path))
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                self.logger.info("FAISS GPU acceleration enabled")
            except Exception as e:
                self.logger.warning(f"GPU mode failed, falling back to CPU: {e}")

        # Load metadata
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.metadata = [json.loads(line) for line in f]

        self.logger.info(
            f"FAISSRetriever ready | entries={len(self.metadata)} | metric={self.similarity_metric.upper()} "
            f"| temporal_awareness={self.temporal_awareness} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def _encode_query(self, query: str) -> np.ndarray:
        # Encode query with normalization
        vec = self.model.encode([query], normalize_embeddings=self.normalize_embeddings)
        return np.asarray(vec, dtype="float32")

    def _normalize_scores(self, distances: np.ndarray) -> np.ndarray:
        # Normalize distances depending on metric
        if self.similarity_metric in {"cosine", "dot"}:
            return distances
        return 1 - distances

    # ------------------------------------------------------------------
    def _extract_years_from_query(self, query: str) -> List[int]:
        """
        Extract explicit years (e.g. 2021), decade mentions (e.g. 'in the 2020s'),
        or century references (e.g. '21st century') from a text query.
        Returns a sorted list of representative years.
        """
        if not query:
            return []

        text = query.lower()
        years: set[int] = set()
        lo, hi = self.valid_year_range

        # 1) Explicit years (e.g. 1999, 2023)
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", text):
            try:
                y = int(m)
                if lo <= y <= hi:
                    years.add(y)
            except ValueError:
                continue

        # 2) Decades (e.g. 1980s, 2020s, early 1990s)
        for m in re.findall(r"\b(19|20)\d0s\b", text):
            try:
                decade = int(m + "0")
                if lo <= decade <= hi:
                    years.update(range(decade, decade + 10))
            except ValueError:
                continue

        # 3) Centuries (e.g. "20th century", "21st century")
        if "20th century" in text:
            years.update(range(1900, 2000))
        if "21st century" in text:
            years.update(range(2000, 2100))

        # Optional compact logging: only start years of detected decades
        unique_decades = sorted({(y // 10) * 10 for y in years})
        return unique_decades

    # ------------------------------------------------------------------
    def _temporal_modulate(self, base_score: float, doc_year: Optional[int], query_years: List[int]) -> float:
        # Exponential weighting for temporal alignment
        if doc_year is None or not query_years:
            return base_score
        nearest = min(abs(doc_year - y) for y in query_years)
        w = exp(-nearest / max(self.temporal_tau, 1e-6))
        return base_score * (1.0 + self.temporal_weight * w)

    def _safe_doc_year(self, entry: Dict[str, Any]) -> Optional[int]:
        # Extract publication year safely from metadata
        meta = entry.get("metadata", {}) or {}
        y = meta.get("year")
        try:
            yi = int(str(y))
            lo, hi = self.valid_year_range
            if lo <= yi <= hi:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _apply_source_diversity(self, results: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """Ensure chunks from different PDFs dominate early ranks."""
        if not self.diversify_sources or not results:
            return results[:top_k]

        diversified, seen = [], set()
        for r in results:
            src = r["metadata"].get("source_file", "unknown")
            if src not in seen:
                diversified.append(r)
                seen.add(src)
            if len(diversified) >= top_k:
                break

        # Fill up with remaining if fewer than top_k unique
        if len(diversified) < top_k:
            for r in results:
                if r not in diversified:
                    diversified.append(r)
                if len(diversified) >= top_k:
                    break
        return diversified

    # ------------------------------------------------------------------
    def search(self, query: str, top_k: Optional[int] = None, temporal_mode: bool = True) -> List[Dict[str, Any]]:
        """Similarity search with optional temporal and source diversification."""
        self.temporal_awareness = bool(temporal_mode)
        k = int(top_k or self.top_k_retrieve)
        k = max(1, min(k, self.index.ntotal))

        q_vec = self._encode_query(query)
        D, I = self.index.search(q_vec, k * 5 if self.diversify_sources else k)
        scores = self._normalize_scores(D[0])
        query_years = self._extract_years_from_query(query) if self.temporal_awareness else []

        results: List[Dict[str, Any]] = []
        for score, idx in zip(scores, I[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue
            entry = self.metadata[idx]
            doc_year = self._safe_doc_year(entry)
            mod_score = (
                self._temporal_modulate(float(score), doc_year, query_years)
                if self.temporal_awareness else float(score)
            )
            results.append({
                "score": float(mod_score),
                "text": (entry.get("text", "") or "")[:500],
                "metadata": entry.get("metadata", {}) or {},
            })

        results.sort(key=lambda r: r["score"], reverse=True)
        diversified = self._apply_source_diversity(results, top_k=k)

        self.logger.info(
            f"Retrieved {len(diversified)} candidates | temporal_mode={self.temporal_awareness} "
            f"| diversify_sources={self.diversify_sources} | years_in_query={query_years or 'none'}"
        )
        return diversified

    # ------------------------------------------------------------------
    def close(self) -> None:
        self.logger.info("FAISS retriever closed")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\reranker_factory.py ==== 
# src/core/retrieval/reranker_factory.py
from __future__ import annotations
import logging
from typing import Any, Dict, Type
from src.core.retrieval.temporal_reranker import TemporalReranker
from src.core.retrieval.semantic_reranker import SemanticReranker
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class RerankerFactory:
    """Factory for deterministic reranker construction."""

    _registry: Dict[str, Type[IReranker]] = {
        "temporal": TemporalReranker,
        "semantic": SemanticReranker,
    }

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IReranker:
        """Instantiate reranker from configuration dictionary."""
        opts = cfg.get("options", {})
        rtype = str(opts.get("reranker", "semantic")).lower()

        if rtype not in RerankerFactory._registry:
            raise ValueError(
                f"Unsupported reranker type: {rtype}. "
                f"Available: {list(RerankerFactory._registry.keys())}"
            )

        cls = RerankerFactory._registry[rtype]
        logger.info(f"Initializing reranker of type='{rtype}'")

        if cls is TemporalReranker:
            return TemporalReranker(
                lambda_weight=float(opts.get("lambda_weight", 0.55)),
                min_year=int(opts.get("min_year", 1900)),
                enforce_decade_balance=bool(opts.get("enforce_decade_balance", True)),
                age_score_boost=float(opts.get("age_score_boost", 0.25)),
                min_decade_threshold=int(opts.get("min_decade_threshold", 3)),
                nonlinear_boost=str(opts.get("nonlinear_boost", "sigmoid")),
                ignore_years=list(opts.get("ignore_years", [])),
                recency_cutoff_year=opts.get("recency_cutoff_year"),
                allow_legacy_backfill=bool(opts.get("allow_legacy_backfill", True)),
                legacy_backfill_max_ratio=float(opts.get("legacy_backfill_max_ratio", 0.3)),
                must_include=list(opts.get("must_include", [])),
                blacklist_sources=list(opts.get("blacklist_sources", [])),
                zscore_normalization=bool(opts.get("zscore_normalization", False)),
            )

        if cls is SemanticReranker:
            return SemanticReranker(
                model_name=str(
                    opts.get("semantic_model", "cross-encoder/ms-marco-MiniLM-L-6-v2")
                ),
                top_k=int(opts.get("top_k_rerank", 25)),
                semantic_weight=float(opts.get("semantic_weight", 0.75)),
                use_gpu=bool(opts.get("use_gpu", False)),
            )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retrieval_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import List, Dict, Any, Optional
from collections import defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer, util

from src.core.retrieval.faiss_retriever import FAISSRetriever
from src.core.retrieval.reranker_factory import RerankerFactory
from src.core.config.config_loader import ConfigLoader
from src.core.evaluation.utils import make_chunk_id  # # stable id builder for evaluation


class RetrievalOrchestrator:
    """
    Unified retrieval orchestrator — controlled externally by prompt intent.
    Pipeline:
      1. Receive (query, intent)
      2. Retrieve chunks from FAISS
      3. Apply reranker (semantic / temporal)
      4. Enforce optional diversity
      5. Calibrate graded relevance (0..3) via score distribution (proxy)
      6. Assign stable id + rank and return exactly top_k results
    """

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = cfg_loader.config

        opts = self.cfg.get("options", {})
        paths = self.cfg.get("paths", {})

        self.top_k = int(opts.get("top_k", 10))
        self.vector_store_dir = str(paths.get("vector_store_dir", "data/vector_store"))
        self.embedding_model = opts.get("embedding_model", "all-MiniLM-L6-v2")

        # # Feature flags
        self.diversify_sources = bool(opts.get("diversify_sources", True))
        self.max_initial = max(self.top_k * 8, int(opts.get("oversample_factor", 8)) * self.top_k)

        # # Initialize FAISS retriever
        self.retriever = FAISSRetriever(
            vector_store_dir=self.vector_store_dir,
            model_name=self.embedding_model,
            top_k_retrieve=self.max_initial,
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_awareness=False,
            diversify_sources=self.diversify_sources,
        )

        # # Embedding model for diversity computations
        self.embed_model = SentenceTransformer(self.embedding_model)

        # # Cache for reranker to avoid re-instantiation overhead
        self._cached_reranker_type: Optional[str] = None
        self._cached_reranker = None

        self.logger.info(
            f"RetrievalOrchestrator initialized | top_k={self.top_k} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        # # retrieve relevant chunks according to user intent
        if not query or not query.strip():
            self.logger.warning("Empty query ignored.")
            return []

        is_historical = intent == "chronological"
        self.logger.info(f"Retrieval started | intent={intent} | top_k={self.top_k}")

        try:
            raw_results = self.retriever.search(query, top_k=self.max_initial, temporal_mode=is_historical)
        except Exception as e:
            self.logger.exception(f"FAISS retrieval failed: {e}")
            return []

        if not raw_results:
            self.logger.warning("No retrieval results found.")
            return []

        # # Step 2: reranking with cached instance
        reranker_type = "temporal" if is_historical else "semantic"
        if reranker_type != self._cached_reranker_type or self._cached_reranker is None:
            self._cached_reranker = RerankerFactory.from_config({"options": {"reranker": reranker_type}})
            self._cached_reranker_type = reranker_type

        try:
            reranked = self._cached_reranker.rerank(raw_results, top_k=len(raw_results))
        except Exception as e:
            self.logger.exception(f"Reranking failed ({reranker_type}): {e}")
            reranked = raw_results

        # # normalize score field and sort deterministically
        for x in reranked:
            x["final_score"] = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
        reranked.sort(key=lambda x: (x["final_score"], x.get("id", "")), reverse=True)

        # # Step 3: optional historical diversity enforcement
        diversified = self._enforce_diversity(reranked, self.top_k, is_historical)

        # # Step 4: graded relevance (0..3) via score quantiles as white-box proxy
        diversified = self._attach_graded_relevance(diversified, ref_population=reranked)

        # # Step 5: ensure exact k and assign stable ids + ranks
        final = self._ensure_exact_k(diversified, self.top_k)
        for i, x in enumerate(final, start=1):
            if not x.get("id"):
                x["id"] = make_chunk_id(x)  # # stable id for evaluation mapping
            x["rank"] = i                  # # deterministic rank for citation mapping

        self._log_decade_distribution(final)
        self.logger.info(f"Retrieval finished | returned={len(final)} | mode={intent}")
        return final

    # ------------------------------------------------------------------
    def _enforce_diversity(self, results: List[Dict[str, Any]], k: int, historical: bool) -> List[Dict[str, Any]]:
        # # diversify by decade and source for chronological intent
        if not results:
            return []

        if not historical or not self.diversify_sources:
            # # simple deduplication on text hash to avoid near-duplicates
            seen, out = set(), []
            for r in results:
                t = (r.get("text") or "").strip()
                if not t:
                    continue
                h = hash(t)
                if h in seen:
                    continue
                seen.add(h)
                out.append(r)
                if len(out) >= k:
                    break
            return out

        # # historical: add semantic diversity + decade/source spread
        selected, used_sources, used_decades = [], set(), set()
        pool_texts = [self._clean_text(r.get("text", "")) for r in results]
        pool_idxs = [i for i, t in enumerate(pool_texts) if t]
        if not pool_idxs:
            return results[:k]

        # # batch-encode to reduce latency for diversity check
        embs = self.embed_model.encode([pool_texts[i] for i in pool_idxs], normalize_embeddings=True)
        kept_embs = []

        for j, idx in enumerate(pool_idxs):
            r = results[idx]
            if len(selected) >= k:
                break
            meta = r.get("metadata", {}) or {}
            src = (meta.get("source_file") or "unknown").lower()
            year = self._safe_year(r)
            decade = (year // 10) * 10 if year else None

            if src in used_sources and decade in used_decades and len(selected) < int(k * 0.8):
                continue

            cand_emb = embs[j]
            if kept_embs:
                sims = util.cos_sim(cand_emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue

            selected.append(r)
            kept_embs.append(cand_emb)
            used_sources.add(src)
            if decade:
                used_decades.add(decade)

        # # if not enough, fill up with remaining best scores
        if len(selected) < k:
            used_ids = {id(x) for x in selected}
            for r in results:
                if id(r) in used_ids:
                    continue
                selected.append(r)
                if len(selected) >= k:
                    break

        return selected

    # ------------------------------------------------------------------
    def _attach_graded_relevance(self, items: List[Dict[str, Any]], ref_population: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # # attach graded relevance labels (0..3) derived from score quantiles
        if not items:
            return items

        scores = np.array([float(x.get("final_score", x.get("score", 0.0)) or 0.0) for x in ref_population], dtype=float)
        if scores.size == 0 or np.allclose(scores.std(), 0.0):
            for x in items:
                x["relevance"] = 1
            return items

        try:
            q1, q2, q3 = np.quantile(scores, [0.25, 0.5, 0.75])
        except Exception:
            smin, smax = float(scores.min()), float(scores.max())
            step = (smax - smin) / 4.0 if smax > smin else 1.0
            q1, q2, q3 = smin + step, smin + 2 * step, smin + 3 * step

        for x in items:
            s = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
            if s <= q1:
                rel = 0
            elif s <= q2:
                rel = 1
            elif s <= q3:
                rel = 2
            else:
                rel = 3
            x["relevance"] = int(rel)

        return items

    # ------------------------------------------------------------------
    def _ensure_exact_k(self, results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        # # ensure deterministic top-k output length
        if not results:
            return []
        if len(results) > k:
            return results[:k]
        if 0 < len(results) < k:
            pad = results[-1].copy()
            padding = [pad.copy() for _ in range(k - len(results))]
            return results + padding
        return results

    # ------------------------------------------------------------------
    def _safe_year(self, r: Dict[str, Any]) -> Optional[int]:
        # # safely extract valid publication year
        meta = r.get("metadata", {}) or {}
        y = meta.get("year", r.get("year"))
        try:
            y = int(y)
            if 1900 <= y <= 2100:
                return y
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _clean_text(self, t: str) -> str:
        # # conservative cleanup to stabilize hashing/embeddings
        if not t:
            return ""
        s = t.replace("\n", " ").replace("\r", " ")
        return " ".join(s.split())

    # ------------------------------------------------------------------
    def _log_decade_distribution(self, items: List[Dict[str, Any]]) -> None:
        # # log decade distribution for diagnostics
        hist: Dict[str, int] = defaultdict(int)
        for r in items:
            y = self._safe_year(r)
            decade = f"{(y // 10) * 10}s" if y else "unknown"
            hist[decade] += 1
        msg = ", ".join(f"{k}:{v}" for k, v in sorted(hist.items()))
        self.logger.info(f"Decade distribution: {msg}")

    # ------------------------------------------------------------------
    def close(self) -> None:
        # # close retriever resources gracefully
        try:
            self.retriever.close()
        except Exception as e:
            self.logger.warning(f"Failed to close retriever: {e}")
        self.logger.info("RetrievalOrchestrator closed cleanly.")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retriever_factory.py ==== 
# src/core/retrieval/retriever_factory.py
from __future__ import annotations
import logging
from typing import Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

logger = logging.getLogger(__name__)

class RetrieverFactory:
    """Factory für Intent-spezifische Retriever-Instanzen (vollständig entkoppelte Datenflüsse)."""

    @staticmethod
    def build(intent: str, cfg: Dict[str, Any]) -> FAISSRetriever:
        """Erzeuge deterministischen Retriever für den angegebenen Intent."""
        paths = cfg.get("paths", {})
        opts = cfg.get("options", {})

        # Basisparameter, deterministisch
        common_args = dict(
            model_name=opts.get("embedding_model", "all-MiniLM-L6-v2"),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_tau=float(opts.get("temporal_tau", 8.0)),
            temporal_weight=float(opts.get("temporal_weight", 0.30)),
            top_k_retrieve=int(opts.get("top_k_retrieve", 80)),
        )

        # Intent → Index-Ordner Mapping
        index_map = {
            "conceptual": paths.get("conceptual_vector_dir", "data/vector_store/conceptual"),
            "chronological": paths.get("chronological_vector_dir", "data/vector_store/chronological"),
            "analytical": paths.get("analytical_vector_dir", "data/vector_store/analytical"),
            "comparative": paths.get("comparative_vector_dir", "data/vector_store/comparative"),
        }

        # Fallback auf globales Standardverzeichnis
        vdir = index_map.get(intent, paths.get("vector_store_dir", "data/vector_store/default"))
        temporal_flag = (intent == "chronological")

        logger.info(f"Building FAISSRetriever for intent='{intent}' | dir={vdir} | temporal={temporal_flag}")

        return FAISSRetriever(
            vector_store_dir=vdir,
            temporal_awareness=temporal_flag,
            **common_args,
        )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\semantic_reranker.py ==== 
# src/core/retrieval/semantic_reranker.py
from __future__ import annotations
import logging
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class SemanticReranker(IReranker):
    """Cross-Encoder-basiertes Re-Ranking nach semantischer Relevanz."""

    def __init__(
        self,
        model_name: str,
        top_k: int = 25,
        semantic_weight: float = 0.75,
        use_gpu: bool = False,
    ):
        self.model_name = model_name
        self.top_k = top_k
        self.semantic_weight = semantic_weight
        self.device = "cuda" if use_gpu else "cpu"
        self.model = CrossEncoder(model_name, device=self.device)
        logger.info(f"Semantic Cross-Encoder loaded: {model_name} ({self.device})")

    def rerank(self, docs: List[Dict[str, Any]], top_k: int | None = None) -> List[Dict[str, Any]]:
        """Score documents via Cross-Encoder similarity."""
        if not docs:
            return []

        k = top_k or self.top_k
        pairs = [(d.get("query", ""), d.get("text", "")) for d in docs]
        scores = self.model.predict(pairs)

        for d, score in zip(docs, scores):
            base = float(d.get("score", 0.0))
            d["final_score"] = self.semantic_weight * float(score) + (1 - self.semantic_weight) * base

        docs.sort(key=lambda x: x["final_score"], reverse=True)
        return docs[:k]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\temporal_reranker.py ==== 
# src/core/retrieval/temporal_reranker.py
from __future__ import annotations
import logging, math
from statistics import mean, pstdev, median
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Set
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class TemporalReranker(IReranker):
    """Combines semantic scores with temporal diversity balancing."""

    def __init__(
        self,
        lambda_weight: float = 0.55,
        min_year: int = 1900,
        enforce_decade_balance: bool = True,
        age_score_boost: float = 0.25,
        min_decade_threshold: int = 3,
        nonlinear_boost: str = "sigmoid",
        ignore_years: List[int] | None = None,
        recency_cutoff_year: int | None = None,
        allow_legacy_backfill: bool = True,
        legacy_backfill_max_ratio: float = 0.3,
        must_include: List[str] | None = None,
        blacklist_sources: List[str] | None = None,
        zscore_normalization: bool = False,
    ):
        self.lambda_weight = lambda_weight
        self.age_score_boost = age_score_boost
        self.enforce_decade_balance = enforce_decade_balance
        self.min_decade_threshold = min_decade_threshold
        self.nonlinear_boost = nonlinear_boost.lower()
        self.min_year = min_year
        self.ignore_years: Set[int] = set(ignore_years or [])
        self.recency_cutoff_year = recency_cutoff_year
        self.allow_legacy_backfill = allow_legacy_backfill
        self.legacy_backfill_max_ratio = legacy_backfill_max_ratio
        self.must_include = must_include or []
        self.blacklist_sources = blacklist_sources or []
        self.zscore_normalization = zscore_normalization

    # ------------------------------------------------------------------
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        if not results:
            return []

        # Jahr extrahieren + Filter anwenden
        for r in results:
            r["year"] = self._extract_year(r)
        results = [r for r in results if r["year"] not in self.ignore_years]
        results = [r for r in results if not self._is_blacklisted(r)]
        if not results:
            return []

        # Z-Score-Normalisierung optional
        if self.zscore_normalization:
            self._normalize_scores(results)

        # Zeitliche Gewichtung
        now = datetime.utcnow().year
        for r in results:
            y = r["year"]
            r["adjusted_score"] = self._apply_age_boost(r.get("score", 0.0), y, now)

        results.sort(key=lambda x: x["adjusted_score"], reverse=True)
        decade_groups = self._group_by_decade(results)
        decades = sorted(decade_groups.keys())

        if not decades:
            return results[:top_k]

        λ = self._adaptive_lambda(len(decades))
        selected = (
            self._balanced_decade_selection(decade_groups, decades, top_k)
            if self.enforce_decade_balance
            else results[:top_k]
        )

        median_dec = int(median(decades))
        for r in selected:
            base = r.get("adjusted_score", 0.0)
            dec_diff = abs((r["year"] // 10) * 10 - median_dec)
            temporal_div = 1 / (1 + dec_diff / 10)
            r["final_score"] = λ * base + (1 - λ) * temporal_div

        ranked = sorted(selected, key=lambda x: x["final_score"], reverse=True)
        ranked = self._apply_recency_cutoff(ranked, results, top_k)
        ranked = self._inject_must_include(ranked, results, top_k)

        logger.info(
            f"Temporal reranking complete | decades={len(decades)} | λ={λ:.2f} | age_boost={self.age_score_boost:.2f}"
        )
        return ranked[:top_k]

    # ------------------------------------------------------------------
    def _extract_year(self, r: Dict[str, Any]) -> int:
        meta = r.get("metadata", {})
        y = meta.get("year") or r.get("year")
        try:
            val = int(str(y))
            if val < self.min_year or val > 2100:
                raise ValueError
            return val
        except Exception:
            return self.min_year

    def _group_by_decade(self, results: List[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:
        out: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
        for r in results:
            d = (r["year"] // 10) * 10
            out[d].append(r)
        for d in out:
            out[d].sort(key=lambda x: x.get("adjusted_score", 0.0), reverse=True)
        return out

    def _apply_age_boost(self, base: float, year: int, now: int) -> float:
        age = max(0, now - year)
        if self.nonlinear_boost == "sigmoid":
            s = 1 / (1 + math.exp((age - 6) / 4.0))
        elif self.nonlinear_boost == "sqrt":
            s = math.sqrt(max(0, 1 - min(age / 40, 1)))
        else:
            s = max(0, 1 - min(age / 30, 1))
        return base + self.age_score_boost * s

    def _adaptive_lambda(self, n_decades: int) -> float:
        if n_decades < self.min_decade_threshold:
            return max(0.3, self.lambda_weight * (n_decades / self.min_decade_threshold))
        return min(1.0, self.lambda_weight + 0.05 * math.log1p(n_decades))

    def _balanced_decade_selection(self, groups: Dict[int, List[Dict[str, Any]]], decades: List[int], top_k: int):
        selected: List[Dict[str, Any]] = []
        for d in decades:
            if groups[d]:
                selected.append(groups[d].pop(0))
                if len(selected) >= top_k:
                    return selected
        i = 0
        while len(selected) < top_k and any(groups.values()):
            d = decades[i % len(decades)]
            if groups[d]:
                selected.append(groups[d].pop(0))
            i += 1
        return selected

    def _normalize_scores(self, results: List[Dict[str, Any]]):
        vals = [r.get("score", 0.0) for r in results]
        if len(vals) < 2:
            return
        μ, σ = mean(vals), pstdev(vals)
        if σ < 1e-8:
            return
        for r in results:
            r["score"] = (r["score"] - μ) / σ

    def _apply_recency_cutoff(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        if not self.recency_cutoff_year:
            return ranked[:top_k]
        recent = [r for r in ranked if r["year"] >= self.recency_cutoff_year]
        legacy = [r for r in ranked if r["year"] < self.recency_cutoff_year]
        if len(recent) >= top_k:
            return recent[:top_k]
        if not self.allow_legacy_backfill:
            return recent
        max_legacy = int(top_k * self.legacy_backfill_max_ratio)
        return (recent + legacy[:max_legacy])[:top_k]

    def _inject_must_include(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        must = [r for r in all_results if self._matches_must_include(r)]
        if not must:
            return ranked
        merged, seen = [], set()
        for r in must + ranked:
            key = self._src_key(r)
            if key not in seen:
                seen.add(key)
                merged.append(r)
            if len(merged) >= top_k:
                break
        return merged

    def _src_key(self, r: Dict[str, Any]) -> str:
        meta = r.get("metadata", {})
        return (meta.get("source_file") or meta.get("title") or "unknown").lower()

    def _matches_must_include(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(m.lower() in key for m in self.must_include)

    def _is_blacklisted(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(b.lower() in key for b in self.blacklist_sources)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_reranker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IReranker(ABC):
    """Interface for reranker strategies (temporal, semantic, hybrid, etc.)."""
    @abstractmethod
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        # Rerank a list of retrieval results based on a custom strategy
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_retriever.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IRetriever(ABC):
    """Interface for all retriever implementations."""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # Return top-k relevant documents/chunks for a query
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\utils\__init__.py ==== 
.
