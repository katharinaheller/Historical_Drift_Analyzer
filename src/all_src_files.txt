==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\main_orchestrator.py ==== 
# src/core/main_orchestrator.py
from __future__ import annotations
import argparse
import logging
import sys
import os
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.ingestion_orchestrator import main as run_ingestion
from src.core.embedding.embedding_orchestrator import main as run_embedding
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.llm.llm_orchestrator import LLMOrchestrator


class MainOrchestrator:
    """Central controller coordinating all pipeline phases of the Historical Drift Analyzer."""

    def __init__(self, config_path: str = "configs/config.yaml"):
        # Ensure UTF-8 runtime consistency
        os.environ["PYTHONIOENCODING"] = "utf-8"
        os.environ["PYTHONUTF8"] = "1"
        os.environ["LC_ALL"] = "C.UTF-8"
        os.environ["LANG"] = "C.UTF-8"

        # Reconfigure stdout/stderr for UTF-8
        for stream_name in ("stdout", "stderr"):
            stream = getattr(sys, stream_name, None)
            if hasattr(stream, "reconfigure"):
                try:
                    stream.reconfigure(encoding="utf-8", errors="replace")
                except Exception:
                    pass

        # Load global configuration and setup logging
        self.cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = self.cfg_loader.config
        self.logger = self._setup_logger()

    # ------------------------------------------------------------------
    def _setup_logger(self) -> logging.Logger:
        """Initialize global logger."""
        opts = self.cfg.get("global", {})
        level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
        logging.basicConfig(level=level, format="%(levelname)s | %(message)s")
        logger = logging.getLogger("MainOrchestrator")
        logger.info("Initialized main orchestrator")
        return logger

    # ------------------------------------------------------------------
    def run_phase(self, phase: str) -> None:
        """Dispatch the orchestrator to the specified phase."""
        self.logger.info(f"Starting phase: {phase.upper()}")

        base_dir = Path(self.cfg["global"]["base_dir"]).resolve()
        sys.path.append(str(base_dir / "src"))

        try:
            if phase == "ingestion":
                run_ingestion()

            elif phase == "embedding":
                run_embedding()

            elif phase == "retrieval":
                self._run_prompt_retrieval_chain()

            elif phase == "llm":
                self._run_llm_interactive()

            elif phase == "all":
                self.logger.info("Running full pipeline (ingestion → embedding → interactive LLM phase)")
                run_ingestion()
                run_embedding()
                self._run_llm_interactive()

            else:
                self.logger.error(f"Unknown phase: {phase}")
                sys.exit(1)

            self.logger.info(f"Phase '{phase}' completed successfully.")

        except UnicodeDecodeError as ue:
            self.logger.error(f"Unicode decoding failed: {ue}. Retrying with UTF-8 replacement.")
            try:
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass
            raise

        except KeyboardInterrupt:
            self.logger.info("Execution manually interrupted by user.")
            sys.exit(0)

        except Exception as e:
            self.logger.exception(f"Phase '{phase}' failed: {e}")
            raise

    # ------------------------------------------------------------------
    def _run_prompt_retrieval_chain(self) -> None:
        """Execute prompt → retrieval → automatic LLM generation (single-shot mode)."""
        self.logger.info("Executing prompt → retrieval → LLM phase")

        prompt_orch = PromptOrchestrator()
        prompt_data = prompt_orch.get_prompt_object()
        if not prompt_data or "processed_query" not in prompt_data:
            self.logger.warning("Prompt phase returned no valid query. Aborting retrieval.")
            return

        query = prompt_data["processed_query"]
        intent = prompt_data["intent"]

        retrieval = RetrievalOrchestrator(config_path="configs/retrieval.yaml")
        self.logger.info(f"Query intent='{intent}' → executing retrieval flow")
        retrieved: List[Dict[str, Any]] = retrieval.retrieve(query, intent)
        retrieval.close()

        if not retrieved:
            self.logger.warning("No documents retrieved.")
            return

        print("\n" + "=" * 80)
        print(f"Retrieved Top-{len(retrieved)} Chunks (intent={intent})")
        for i, r in enumerate(retrieved, start=1):
            meta = r.get("metadata", {}) or {}
            year = meta.get("year", "n/a")
            src = meta.get("source_file") or meta.get("title") or "Unknown"
            print(f"[{i}] ({year}) {src} | score={r.get('score', 0.0):.3f}")
        print("=" * 80 + "\n")

        try:
            self.logger.info("Launching Ollama model for contextual generation...")
            llm_orch = LLMOrchestrator()
            output = llm_orch.run_with_context(query, intent, retrieved)
            print("\n=== MODEL OUTPUT ===\n")
            print(output)
            print("\n====================\n")
            llm_orch.close()
        except Exception as e:
            self.logger.error(f"Automatic LLM generation failed: {e}")

    # ------------------------------------------------------------------
    def _run_llm_interactive(self) -> None:
        """Start an interactive prompt → retrieval → LLM loop until Ctrl+C."""
        self.logger.info("Starting interactive LLM session. Press Ctrl+C to exit.")
        llm_orch = LLMOrchestrator()
        try:
            llm_orch.run_interactive()
        except KeyboardInterrupt:
            self.logger.info("Interactive session terminated by user.")
        finally:
            llm_orch.close()

    # ------------------------------------------------------------------
    def run(self, args: argparse.Namespace) -> None:
        """Entrypoint for orchestrator execution."""
        if args.phase:
            self.run_phase(args.phase)
        else:
            self.logger.warning("No phase specified. Use --phase <name> or --phase all.")


# ----------------------------------------------------------------------
def parse_args() -> argparse.Namespace:
    """CLI argument parser."""
    parser = argparse.ArgumentParser(description="Historical Drift Analyzer – Main Orchestrator")
    parser.add_argument(
        "--phase",
        type=str,
        required=True,
        choices=["ingestion", "embedding", "retrieval", "llm", "evaluation", "all"],
        help="Select which phase of the pipeline to execute",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="Path to the master configuration YAML file",
    )
    return parser.parse_args()


# ----------------------------------------------------------------------
if __name__ == "__main__":
    args = parse_args()
    orchestrator = MainOrchestrator(config_path=args.config)
    orchestrator.run(args)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\analysis_orchestrator.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\i_analyzer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\config\config_loader.py ==== 
from __future__ import annotations
import yaml
from pathlib import Path
from typing import Any, Dict


class ConfigLoader:
    """
    Universal YAML configuration loader for all pipeline phases.
    - Supports ${PROJECT_ROOT} / ${base_dir} placeholders
    - Can inherit from a master config (for global settings)
    - Provides safe defaults for missing sections
    """

    def __init__(self, path: str, master_path: str | None = "configs/config.yaml"):
        self.path = Path(path)
        if not self.path.exists():
            raise FileNotFoundError(f"Configuration file not found: {self.path}")

        # Load phase-specific YAML (e.g. ingestion.yaml)
        with open(self.path, "r", encoding="utf-8") as f:
            phase_cfg = yaml.safe_load(f) or {}

        # Load master config if available
        master_cfg: Dict[str, Any] = {}
        if master_path:
            master_file = Path(master_path)
            if master_file.exists():
                with open(master_file, "r", encoding="utf-8") as f:
                    master_cfg = yaml.safe_load(f) or {}

        # Merge configurations (phase overrides master)
        self._raw = self._merge_dicts(master_cfg, phase_cfg)

        # Detect project root
        self.project_root = self._detect_project_root()

        # Resolve all placeholders like ${base_dir}
        self.config = self._expand_vars(self._raw)

        # Ensure minimal safe defaults
        for section in ["paths", "options", "chunking"]:
            self.config.setdefault(section, {})

    # ------------------------------------------------------------------
    def _detect_project_root(self) -> Path:
        """Infer project root (the directory above 'configs')."""
        p = self.path.resolve()
        if "configs" in p.parts:
            idx = p.parts.index("configs")
            return Path(*p.parts[:idx])
        return p.parent

    # ------------------------------------------------------------------
    def _merge_dicts(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        """Recursively merge dicts, with override taking precedence."""
        merged = base.copy()
        for k, v in override.items():
            if isinstance(v, dict) and k in merged and isinstance(merged[k], dict):
                merged[k] = self._merge_dicts(merged[k], v)
            else:
                merged[k] = v
        return merged

    # ------------------------------------------------------------------
    def _expand_single_var(self, value: Any) -> Any:
        """Replace placeholders only when explicitly present."""
        if not isinstance(value, str) or "${" not in value:
            return value

        replacements = {
            "${PROJECT_ROOT}": str(self.project_root),
            "${project_root}": str(self.project_root),
            "${BASE_DIR}": str(self.project_root),
            "${base_dir}": str(self.project_root),
        }
        for ph, real in replacements.items():
            value = value.replace(ph, real)
        return str(Path(value).resolve())

    # ------------------------------------------------------------------
    def _expand_vars(self, data: Any) -> Any:
        """Recursively expand placeholders in nested structures."""
        if isinstance(data, dict):
            return {k: self._expand_vars(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._expand_vars(v) for v in data]
        elif isinstance(data, str):
            return self._expand_single_var(data)
        else:
            return data

    # ------------------------------------------------------------------
    def get(self, key: str, default: Any = None) -> Any:
        """Safely access top-level config sections."""
        return self.config.get(key, default)

    # ------------------------------------------------------------------
    @property
    def raw(self) -> Dict[str, Any]:
        """Return unexpanded raw YAML structure."""
        return self._raw


# ----------------------------------------------------------------------
def load_config(path: str, master_path: str | None = "configs/config.yaml") -> Dict[str, Any]:
    """Convenience function: directly load and expand a config dictionary."""
    return ConfigLoader(path, master_path).config
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedder_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict
import logging

from src.core.embedding.interfaces.i_embedder import IEmbedder


class SentenceTransformerEmbedder(IEmbedder):
    """Local embedder using sentence-transformers."""

    def __init__(self, model_name: str, dimension: int | None = None, normalize_embeddings: bool = True):
        # Lazy import to avoid hard dependency
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError("sentence-transformers is required for SentenceTransformerEmbedder. "
                              "Install via: poetry add sentence-transformers") from e

        self._model = SentenceTransformer(model_name)
        self._normalize = normalize_embeddings
        # If dimension not given, infer from model
        if dimension is None:
            test_vec = self._model.encode("test", normalize_embeddings=self._normalize)
            self._dimension = len(test_vec)
        else:
            self._dimension = dimension

    def embed_text(self, text: str) -> list[float]:
        # Encode single text
        return self._model.encode(text, normalize_embeddings=self._normalize).tolist()

    def embed_batch(self, texts, batch_size=None) -> list[list[float]]:
        # Encode multiple texts
        return self._model.encode(
            list(texts),
            batch_size=batch_size or 32,
            normalize_embeddings=self._normalize
        ).tolist()

    @property
    def dimension(self) -> int:
        # Return embedding dimension
        return self._dimension

    def close(self) -> None:
        # Nothing to close for sentence-transformers
        pass


class EmbedderFactory:
    """Factory for creating IEmbedder instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IEmbedder:
        opts: Dict[str, Any] = cfg.get("options", {})
        model_name = opts.get("embedding_model", "all-MiniLM-L6-v2")
        normalize = bool(opts.get("normalize_embeddings", True))
        dimension = opts.get("dimension", None)

        backend = opts.get("embedding_backend", "sentence-transformers").lower()

        if backend == "sentence-transformers":
            return SentenceTransformerEmbedder(
                model_name=model_name,
                dimension=dimension,
                normalize_embeddings=normalize,
            )
        else:
            raise ValueError(f"Unsupported embedding backend: {backend}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedding_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.embedding.embedder_factory import EmbedderFactory
from src.core.embedding.vector_store_factory import VectorStoreFactory

logger = logging.getLogger("EmbeddingOrchestrator")


def _load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _iter_chunk_files(chunks_dir: Path):
    for p in chunks_dir.glob("*.json"):
        if p.is_file():
            yield p


def _extract_chunks(chunk_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    if "chunks" in chunk_data and isinstance(chunk_data["chunks"], list):
        return chunk_data["chunks"]
    elif "text" in chunk_data:
        return [{"text": chunk_data["text"]}]
    return []


def _resolve_metadata_for_chunk(chunk_file: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Try to locate matching metadata JSON for a given chunk file."""
    base = chunk_file.stem.replace(".chunks", "")
    candidates = [
        metadata_dir / f"{chunk_file.name}",
        metadata_dir / f"{base}.json",
        metadata_dir / f"{base}.metadata.json",
    ]
    for candidate in candidates:
        if candidate.exists():
            return _load_json(candidate)
    logger.warning(f"No metadata found for {chunk_file.name}")
    return {}


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load merged configuration (embedding + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/embedding.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger.info("Starting embedding pipeline")

    paths: Dict[str, Any] = cfg.get("paths", {})
    chunks_dir = Path(paths.get("chunks_dir", "data/processed/chunks")).resolve()
    metadata_dir = Path(paths.get("metadata_dir", "data/processed/metadata")).resolve()

    if not chunks_dir.exists():
        raise FileNotFoundError(f"Chunks directory does not exist: {chunks_dir}")
    if not metadata_dir.exists():
        logger.warning(f"Metadata directory does not exist: {metadata_dir} (metadata will be empty)")

    # ------------------------------------------------------------------
    # 2. Initialize embedding backend + vector store
    # ------------------------------------------------------------------
    embedder = EmbedderFactory.from_config(cfg)
    logger.info(f"Initialized embedder with dimension={embedder.dimension}")

    vector_store = VectorStoreFactory.from_config(cfg, dimension=embedder.dimension)
    logger.info("Initialized vector store")

    batch_size = int(opts.get("batch_size", 16))
    texts_batch: List[str] = []
    metas_batch: List[Dict[str, Any]] = []

    try:
        for chunk_file in _iter_chunk_files(chunks_dir):
            chunk_json = _load_json(chunk_file)
            chunks = _extract_chunks(chunk_json)
            if not chunks:
                logger.warning(f"No chunks found in {chunk_file.name}")
                continue

            meta_data = _resolve_metadata_for_chunk(chunk_file, metadata_dir)

            for ch in chunks:
                text = ch.get("text", "").strip()
                if not text:
                    continue

                merged_meta = {
                    "source_file": meta_data.get("source_file", chunk_file.stem),
                    "title": meta_data.get("title"),
                    "authors": meta_data.get("authors"),
                    "year": meta_data.get("year"),
                    "detected_language": meta_data.get("detected_language"),
                    "page_count": meta_data.get("page_count"),
                    "origin_chunk_file": str(chunk_file.name),
                }

                texts_batch.append(text)
                metas_batch.append(merged_meta)

                if len(texts_batch) >= batch_size:
                    try:
                        vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
                        vector_store.add_vectors(vectors, texts_batch, metas_batch)
                        logger.info(f"Embedded and stored batch of size {len(texts_batch)}")
                    except Exception as e:
                        logger.error(f"Error during embedding or storing batch: {e}")
                    finally:
                        texts_batch.clear()
                        metas_batch.clear()

        if texts_batch:
            vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
            vector_store.add_vectors(vectors, texts_batch, metas_batch)
            logger.info(f"Embedded and stored final batch of size {len(texts_batch)}")

        vector_store.persist()
        logger.info("Embedding pipeline finished successfully.")

    finally:
        embedder.close()
        vector_store.close()


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\vector_store_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import logging
import os
import json
import sqlite3

from src.core.embedding.interfaces.i_vector_store import IVectorStore


class FAISSVectorStore(IVectorStore):
    """FAISS-based local vector store."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import faiss  # type: ignore
        except ImportError as e:
            raise ImportError("faiss-cpu is required for FAISSVectorStore. "
                              "Install via: poetry add faiss-cpu") from e

        self.faiss = faiss
        self.dimension = dimension
        self.persist_dir = persist_dir
        os.makedirs(persist_dir, exist_ok=True)
        self.index_path = os.path.join(persist_dir, "index.faiss")
        self.meta_path = os.path.join(persist_dir, "metadata.jsonl")

        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
        else:
            self.index = faiss.IndexFlatIP(self.dimension)

        # Metadata is stored separately as JSONL
        self._meta_fh = open(self.meta_path, "a", encoding="utf-8")

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import to avoid hard dependency at class load

        arr = np.array(vectors).astype("float32")
        self.index.add(arr)

        for i, doc in enumerate(documents):
            meta = metadatas[i] if metadatas and i < len(metadatas) else {}
            payload = {
                "text": doc,
                "metadata": meta,
            }
            self._meta_fh.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def persist(self) -> None:
        self.faiss.write_index(self.index, self.index_path)
        self._meta_fh.flush()

    def close(self) -> None:
        try:
            self.persist()
        finally:
            self._meta_fh.close()


class LanceDBVectorStore(IVectorStore):
    """LanceDB-based vector store (local file-based)."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import lancedb  # type: ignore
        except ImportError as e:
            raise ImportError("lancedb is required for LanceDBVectorStore. "
                              "Install via: poetry add lancedb") from e

        self.dimension = dimension
        os.makedirs(persist_dir, exist_ok=True)
        self.db = lancedb.connect(persist_dir)
        self.table = self.db.open_table("embeddings") if "embeddings" in self.db.table_names() else \
            self.db.create_table("embeddings", data=[
                {
                    "vector": [0.0] * dimension,
                    "text": "",
                    "metadata": {},
                }
            ])

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        rows = []
        for i, vec in enumerate(vectors):
            rows.append({
                "vector": vec,
                "text": documents[i],
                "metadata": metadatas[i] if metadatas and i < len(metadatas) else {},
            })
        self.table.add(rows)

    def persist(self) -> None:
        # LanceDB persists automatically
        pass

    def close(self) -> None:
        # Nothing to close
        pass


class SQLiteVectorStore(IVectorStore):
    """Very simple SQLite-based vector store (for debugging / small-scale)."""

    def __init__(self, persist_dir: str, dimension: int):
        os.makedirs(persist_dir, exist_ok=True)
        db_path = os.path.join(persist_dir, "vectors.sqlite3")
        self.conn = sqlite3.connect(db_path)
        self.dimension = dimension
        self._ensure_schema()

    def _ensure_schema(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                metadata TEXT,
                vector BLOB NOT NULL
            )
            """
        )
        self.conn.commit()

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import
        cur = self.conn.cursor()
        for i, vec in enumerate(vectors):
            meta_str = json.dumps(metadatas[i], ensure_ascii=False) if metadatas and i < len(metadatas) else "{}"
            arr = np.array(vec, dtype="float32").tobytes()
            cur.execute(
                "INSERT INTO embeddings (text, metadata, vector) VALUES (?, ?, ?)",
                (documents[i], meta_str, arr)
            )
        self.conn.commit()

    def persist(self) -> None:
        self.conn.commit()

    def close(self) -> None:
        self.conn.close()


class VectorStoreFactory:
    """Factory for creating IVectorStore instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any], dimension: int) -> IVectorStore:
        opts: Dict[str, Any] = cfg.get("options", {})
        store_name = opts.get("vector_store", "FAISS").upper()

        paths: Dict[str, Any] = cfg.get("paths", {})
        persist_dir = paths.get("vector_store_dir", "data/vector_store")

        if store_name == "FAISS":
            return FAISSVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "LANCEDB":
            return LanceDBVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "SQLITE":
            return SQLiteVectorStore(persist_dir=persist_dir, dimension=dimension)
        else:
            raise ValueError(f"Unsupported vector store: {store_name}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_embedder.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Sequence, Optional


class IEmbedder(ABC):
    """Interface for all embedding backends."""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        # Return embedding vector for a single text
        pass

    @abstractmethod
    def embed_batch(self, texts: Sequence[str], batch_size: Optional[int] = None) -> List[List[float]]:
        # Return embedding vectors for a batch of texts
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        # Return embedding dimension
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_vector_store.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional


class IVectorStore(ABC):
    """Interface for all vector store backends."""

    @abstractmethod
    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: Optional[List[Dict[str, Any]]] = None) -> None:
        # Add vectors and associated payloads to the store
        pass

    @abstractmethod
    def persist(self) -> None:
        # Persist the store to disk
        pass

    @abstractmethod
    def close(self) -> None:
        # Close resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestion_orchestrator.py ==== 
from __future__ import annotations
import logging
import json
from pathlib import Path
from typing import Any, Dict

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.parser.parser_factory import ParserFactory
from src.core.ingestion.metadata.metadata_extractor_factory import MetadataExtractorFactory
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner
from src.core.ingestion.chunking.chunking_orchestrator import ChunkingOrchestrator


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration (merge phase + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/ingestion.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("IngestionOrchestrator")
    logger.info("Starting ingestion pipeline")

    # ------------------------------------------------------------------
    # 2. Initialize core components
    # ------------------------------------------------------------------
    parser_factory = ParserFactory(cfg, logger=logger)
    metadata_factory = MetadataExtractorFactory.from_config(cfg)
    cleaner = RagTextCleaner.default()
    chunking_orchestrator = ChunkingOrchestrator(config=cfg)

    active_metadata_fields = opts.get("metadata_fields", [])
    parallelism = opts.get("parallelism", "auto")

    # ------------------------------------------------------------------
    # 3. Resolve all paths
    # ------------------------------------------------------------------
    paths = cfg.get("paths", {})
    raw_dir = Path(paths.get("raw_pdfs", "data/raw_pdfs")).resolve()
    parsed_dir = Path(paths.get("parsed", "data/processed/parsed")).resolve()
    cleaned_dir = Path(paths.get("cleaned", "data/processed/cleaned")).resolve()
    metadata_dir = Path(paths.get("metadata", "data/processed/metadata")).resolve()
    chunks_dir = Path(paths.get("chunks", "data/processed/chunks")).resolve()

    for d in [parsed_dir, cleaned_dir, metadata_dir, chunks_dir]:
        ensure_dir(d)

    pdf_files = sorted(raw_dir.glob("*.pdf"))
    if not pdf_files:
        logger.warning(f"No PDF files found in {raw_dir}")
        return
    logger.info(f"Found {len(pdf_files)} PDF(s) in {raw_dir}")

    # ------------------------------------------------------------------
    # 4. Parallel or sequential mode
    # ------------------------------------------------------------------
    use_parallel = (
        isinstance(parallelism, int) and parallelism > 1
    ) or (isinstance(parallelism, str) and parallelism.lower() == "auto")

    if use_parallel:
        try:
            parallel_parser = parser_factory.create_parallel_parser()
            logger.info("Running ingestion in parallel mode ...")
            results = parallel_parser.parse_all(raw_dir, parsed_dir)

            for res in results:
                if "text" not in res:
                    continue
                res["text"] = cleaner.clean(res["text"])
                res["chunks"] = chunking_orchestrator.process(res["text"], metadata=res.get("metadata", {}))

                pdf_name = Path(res["metadata"]["source_file"]).stem
                pdf_path = raw_dir / f"{pdf_name}.pdf"
                all_meta = metadata_factory.extract_all(str(pdf_path))
                filtered_meta = {k: v for k, v in all_meta.items()
                                 if not active_metadata_fields or k in active_metadata_fields}
                res["metadata"].update(filtered_meta)

                # Save outputs
                (parsed_dir / f"{pdf_name}.parsed.json").write_text(
                    json.dumps({"text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (cleaned_dir / f"{pdf_name}.cleaned.json").write_text(
                    json.dumps({"cleaned_text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (metadata_dir / f"{pdf_name}.metadata.json").write_text(
                    json.dumps(res["metadata"], ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (chunks_dir / f"{pdf_name}.chunks.json").write_text(
                    json.dumps({"chunks": res["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )

            logger.info(f"Parallel ingestion completed ({len(results)} file(s)).")
            return
        except Exception as e:
            logger.error(f"Parallel ingestion failed → fallback to sequential: {e}")

    # ------------------------------------------------------------------
    # 5. Sequential fallback mode
    # ------------------------------------------------------------------
    parser = parser_factory.create_parser()
    for pdf in pdf_files:
        logger.info(f"Parsing {pdf.name} ...")
        try:
            parsed_result = parser.parse(str(pdf))
            if "text" not in parsed_result:
                continue

            parsed_result["text"] = cleaner.clean(parsed_result["text"])
            parsed_result["chunks"] = chunking_orchestrator.process(
                parsed_result["text"], metadata=parsed_result.get("metadata", {})
            )

            base_metadata = parsed_result.get("metadata", {})
            all_metadata = metadata_factory.extract_all(str(pdf))
            if active_metadata_fields:
                all_metadata = {k: v for k, v in all_metadata.items() if k in active_metadata_fields}
            base_metadata.update(all_metadata)
            parsed_result["metadata"] = base_metadata

            # Write outputs
            (parsed_dir / f"{pdf.stem}.parsed.json").write_text(
                json.dumps({"text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (cleaned_dir / f"{pdf.stem}.cleaned.json").write_text(
                json.dumps({"cleaned_text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (metadata_dir / f"{pdf.stem}.metadata.json").write_text(
                json.dumps(base_metadata, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (chunks_dir / f"{pdf.stem}.chunks.json").write_text(
                json.dumps({"chunks": parsed_result["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )

            logger.info(f"Completed {pdf.name}")
        except Exception as e:
            logger.error(f"Failed to parse {pdf.name}: {e}")

    # ------------------------------------------------------------------
    # 6. Finish
    # ------------------------------------------------------------------
    logger.info("Ingestion complete.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestor.py ==== 
# src/core/ingestion/ingestor.py
from __future__ import annotations

from pathlib import Path
from typing import Dict, Any, List, Optional

from docling.document_converter import DocumentConverter
from src.core.ingestion.interfaces.i_ingestor import IIngestor


class Ingestor(IIngestor):
    # Handles PDF ingestion using Docling (no chunking)
    def __init__(self, enable_ocr: bool = True):
        self.enable_ocr = enable_ocr
        self.converter = DocumentConverter()

    def ingest(self, source_path: str) -> Dict[str, Any]:
        pdf_path = Path(source_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        # Perform robust conversion using Docling
        result = self.converter.convert(str(pdf_path))
        document = getattr(result, "document", None)

        # Extract plain text (fallback if needed)
        text = ""
        if document:
            if hasattr(document, "export_to_markdown"):
                text = document.export_to_markdown()
            elif hasattr(document, "text"):
                text = document.text

        # Build metadata
        metadata = {
            "source_file": pdf_path.name,
            "source_path": str(pdf_path),
            "page_count": self._get_page_count(document),
            "has_ocr": self._has_ocr(document),
            "toc": self._extract_toc(document),
        }

        # Single-chunk representation (no splitting yet)
        chunks = [{"text": text, "page": None, "bbox": None}] if text else []

        return {"metadata": metadata, "chunks": chunks}

    # -------------------------------------------------------
    def _get_page_count(self, document: Any) -> Optional[int]:
        if document is None:
            return None
        return len(getattr(document, "pages", []))

    def _has_ocr(self, document: Any) -> bool:
        if document is None:
            return False
        return bool(getattr(document, "ocr_content", None))

    def _extract_toc(self, document: Any) -> List[Dict[str, Any]]:
        toc: List[Dict[str, Any]] = []
        if document and hasattr(document, "outline_items") and document.outline_items:
            for item in document.outline_items:
                toc.append({
                    "title": getattr(item, "title", ""),
                    "page": getattr(item, "page_number", None),
                })
        return toc
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata_merger.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\adaptive_chunker.py ==== 
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor
import spacy
import logging

class AdaptiveChunker(IChunker):
    """Chunker that adapts to the content structure by splitting at semantic breaks."""

    def __init__(self,
                 chunk_size: int = 500,
                 overlap: int = 200,
                 min_chunk_length: int = 400,
                 pdf_path: Optional[str] = None,
                 page_number_extractor: Optional[IPageNumberExtractor] = None,
                 text_length: Optional[int] = None):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length
        self.pdf_path = pdf_path
        self.page_number_extractor = page_number_extractor
        self.text_length = text_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

        # Extract page count if available
        if pdf_path and page_number_extractor:
            try:
                self.page_count = self.page_number_extractor.extract_page_number(pdf_path)
                self.adjust_chunking_based_on_page_count()
            except Exception as e:
                logging.warning(f"Error extracting page count from PDF: {e}")
                self.page_count = None
                self.chunk_size = 1000  # Fallback

        if text_length:
            self.adjust_chunking_based_on_text_length(text_length)

    def adjust_chunking_based_on_page_count(self):
        """Adjust chunk size and overlap based on the page count."""
        if self.page_count:
            if self.page_count > 50:
                self.chunk_size = 1000
                self.overlap = 700
            elif self.page_count > 30:
                self.chunk_size = 1500
                self.overlap = 500
            elif self.page_count > 10:
                self.chunk_size = 2000
                self.overlap = 300
            else:
                self.chunk_size = 2500
                self.overlap = 200
        else:
            self.chunk_size = 1000
            self.overlap = 200

    def adjust_chunking_based_on_text_length(self, text_length: int):
        """Adjust chunk size and overlap based on the length of the text."""
        if text_length:
            if text_length > 5000:
                self.chunk_size = 1000
                self.overlap = 600
            elif text_length > 2000:
                self.chunk_size = 1500
                self.overlap = 400
            else:
                self.chunk_size = 2000
                self.overlap = 200

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Chunk the text into adaptive chunks based on content size and overlap."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)  # Use spaCy to process the text
        sentences = [sent.text.strip() for sent in doc.sents]  # Extract sentences from the spaCy doc

        for sentence in sentences:
            # If the current chunk plus sentence exceeds chunk size, store the chunk and start a new one
            if len(current_chunk) + len(sentence) > self.chunk_size:
                if current_chunk:  # Prevent empty chunks
                    chunks.append({
                        "text": current_chunk.strip(),
                        # "chunk_size": len(current_chunk.strip()),  # Removed from output
                        # "overlap": self.overlap  # Removed from output
                    })
                current_chunk = sentence
            else:
                current_chunk += " " + sentence

            # Merge chunks if they are too small
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                # "chunk_size": len(current_chunk.strip()),  # Removed from output
                # "overlap": self.overlap  # Removed from output
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\chunking_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Any, Dict, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.chunking.adaptive_chunker import AdaptiveChunker
from src.core.ingestion.chunking.static_chunker import StaticChunker
from src.core.ingestion.metadata.implementations.page_number_extractor import PageNumberExtractor


class ChunkingOrchestrator:
    """Handles the selection and execution of chunking strategies based on YAML config."""

    def __init__(self, config: Dict[str, Any], pdf_path: Optional[str] = None):
        """Initialize with YAML configuration and an optional PDF path."""
        self.config = config
        self.pdf_path = pdf_path
        
        # Überprüfen, ob der 'chunking' Abschnitt in der Konfiguration vorhanden ist
        if "chunking" not in self.config:
            raise KeyError("The 'chunking' section is missing in the configuration file")

        # Debugging: Gibt den 'chunking' Abschnitt aus
        logging.debug(f"Chunking config: {self.config['chunking']}")

        # Wählt die passende Chunking-Strategie aus der Konfiguration
        self.chunker = self.select_chunker()  

    def select_chunker(self) -> IChunker:
        """Select chunking strategy based on the configuration."""
        chunking_config = self.config["chunking"]  # Zugriff auf den 'chunking'-Abschnitt der Konfiguration
        chunking_mode = chunking_config["mode"]
        chunk_size = chunking_config["chunk_size"]  # Get the chunk size from config
        overlap = chunking_config["overlap"]
        enable_overlap = chunking_config["enable_overlap"]
        min_chunk_length = chunking_config["min_chunk_length"]
        sentence_boundary_detection = chunking_config["sentence_boundary_detection"]
        merge_short_chunks = chunking_config["merge_short_chunks"]

        # Erstelle die Chunker-Instanz basierend auf dem gewählten Modus
        if chunking_mode == "adaptive":
            # Instanziiere AdaptiveChunker mit den relevanten Konfigurationswerten
            page_number_extractor = PageNumberExtractor() if self.pdf_path else None
            return AdaptiveChunker(
                chunk_size=chunk_size,
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
                pdf_path=self.pdf_path,
                page_number_extractor=page_number_extractor,
            )
        elif chunking_mode == "static":
            # Instanziiere StaticChunker mit den relevanten Konfigurationswerten
            return StaticChunker(
                chunk_size=chunk_size,  # Pass the chunk_size from config
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
            )
        else:
            raise ValueError(f"Unknown chunking strategy: {chunking_mode}")

    def process(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Process the text and return chunks with metadata using the selected chunking strategy."""
        return self.chunker.chunk(text, metadata)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\i_chunker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IChunker(ABC):
    """Interface for all chunking strategies."""

    @abstractmethod
    def chunk(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Split the cleaned text into smaller chunks.
        
        Each returned item should be a dictionary with:
        - "text": the chunked text as a string.
        - "metadata": additional information (e.g., document info, chunking context).
        
        Args:
            text: The cleaned text to be chunked.
            metadata: Optional metadata related to the chunking process. Default is an empty dictionary.
        
        Returns:
            A list of dictionaries, each containing:
            - "text": A chunk of the input text.
            - "metadata": The metadata associated with the chunk.
        """
        if metadata is None:
            metadata = {}
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\static_chunker.py ==== 
# src/core/ingestion/chunking/static_chunker.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
import spacy

class StaticChunker(IChunker):
    """Chunker that uses fixed chunk size and overlap for chunking."""

    def __init__(self, 
                 chunk_size: int,  # Configuration-based chunk size
                 overlap: int, 
                 min_chunk_length: int):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Split text into overlapping fixed-size chunks with sentence boundaries."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]

        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= self.chunk_size:
                current_chunk += ". " + sentence
            else:
                chunks.append({
                    "text": current_chunk.strip(),
                    "chunk_size": len(current_chunk.strip()),  # Display actual chunk length
                    "configured_chunk_size": self.chunk_size,   # Config value for comparison
                    "overlap": self.overlap                    # Configured overlap
                })
                current_chunk = sentence

            # Merge small chunks if necessary
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                last_chunk["chunk_size"] = len(last_chunk["text"])
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                "chunk_size": len(current_chunk.strip()),
                "configured_chunk_size": self.chunk_size,
                "overlap": self.overlap
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\utils_chunking.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\base_cleaner.py ==== 
from __future__ import annotations
from typing import final
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner


class BaseTextCleaner(ITextCleaner):
    """Base class providing a stable clean(...) interface."""

    @final
    def clean(self, text: str) -> str:
        # Guarantee non-null string
        if not text:
            return ""
        return self._clean_impl(text)

    def _clean_impl(self, text: str) -> str:
        # Must be implemented by subclasses
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\cleaner_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict

from src.core.ingestion.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration
    # ------------------------------------------------------------------
    cfg = ConfigLoader("configs/ingestion.yaml").config
    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)

    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("CleanerOrchestrator")
    logger.info("Starting cleaning phase")

    # ------------------------------------------------------------------
    # 2. Resolve paths
    # ------------------------------------------------------------------
    parsed_dir = Path(cfg["paths"]["parsed"]).resolve()
    cleaned_dir = Path(cfg["paths"].get("cleaned", "data/processed/cleaned")).resolve()
    ensure_dir(cleaned_dir)

    parsed_files = sorted(parsed_dir.glob("*.parsed.json"))
    if not parsed_files:
        logger.warning(f"No parsed files found in {parsed_dir}")
        return

    logger.info(f"Found {len(parsed_files)} parsed file(s) for cleaning")

    # ------------------------------------------------------------------
    # 3. Initialize deterministic multi-stage cleaner
    # ------------------------------------------------------------------
    cleaner = RagTextCleaner.default()

    # ------------------------------------------------------------------
    # 4. Iterate over parsed JSONs
    # ------------------------------------------------------------------
    for idx, parsed_path in enumerate(parsed_files, start=1):
        try:
            with open(parsed_path, "r", encoding="utf-8") as f:
                parsed_data = json.load(f)

            raw_text = parsed_data.get("text", "")
            if not raw_text:
                logger.warning(f"Skipping {parsed_path.name}: no text field")
                continue

            # --- Step 1: Clean text ---
            cleaned_text = cleaner.clean(raw_text)

            # --- Step 2: Write cleaned output as JSON ---
            cleaned_filename = parsed_path.stem.replace(".parsed", "") + ".cleaned.json"
            cleaned_path = cleaned_dir / cleaned_filename

            cleaned_data = {
                "cleaned_text": cleaned_text,
            }

            with open(cleaned_path, "w", encoding="utf-8") as cf:
                json.dump(cleaned_data, cf, ensure_ascii=False, indent=2)

            logger.info(f"✓ Cleaned {parsed_path.name} ({idx}/{len(parsed_files)})")

        except Exception as e:
            logger.error(f"✗ Failed to clean {parsed_path.name}: {e}")

    # ------------------------------------------------------------------
    # 5. Finish
    # ------------------------------------------------------------------
    logger.info("Cleaning phase completed successfully.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\deep_text_cleaner.py ==== 
from __future__ import annotations
import re
from cleantext import clean
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner

class DeepTextCleaner(BaseTextCleaner):
    """
    Advanced text cleaner using heuristic and statistical patterns
    to remove non-content sections like references, tables, and equations.
    """

    def _clean_impl(self, text: str) -> str:
        # Step 1: Global clean using clean-text
        text = clean(
            text,
            fix_unicode=True,
            to_ascii=False,
            lower=False,
            no_urls=True,
            no_emails=True,
            no_phone_numbers=True,
            no_numbers=False,
            no_currency_symbols=True,
            no_punct=False,
        )

        # Step 2: Remove reference-like or appendix sections
        text = re.sub(
            r"(?is)(references|bibliography|literaturverzeichnis|acknowledg(e)?ments|appendix).*",
            "",
            text,
        )

        # Step 3: Drop DOI/arXiv/URL lines
        text = re.sub(r"(?im)^\s*(doi|arxiv|http|https)[:\s].*$", "", text)

        # Step 4: Drop lines that are mostly numbers, formulas, or citation lists
        filtered_lines = []
        for line in text.splitlines():
            clean_line = line.strip()
            if not clean_line:
                continue
            # Discard lines with >30% digits or symbols
            digit_ratio = sum(ch.isdigit() for ch in clean_line) / max(len(clean_line), 1)
            symbol_ratio = sum(not ch.isalnum() and not ch.isspace() for ch in clean_line) / max(len(clean_line), 1)
            if digit_ratio > 0.3 or symbol_ratio > 0.4:
                continue
            # Drop if line looks like citation or table
            if re.match(r"^\[\d+\]\s*[A-Z][a-z]+", clean_line):
                continue
            if re.match(r"(?i)^table\s+\d+", clean_line):
                continue
            filtered_lines.append(clean_line)

        text = "\n".join(filtered_lines)

        # Step 5: Collapse whitespace and blank lines
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\i_text_cleaner.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod


class ITextCleaner(ABC):
    """Interface for all text cleaners in the ingestion pipeline."""

    @abstractmethod
    def clean(self, text: str) -> str:
        """Return a cleaned version of the given text."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rag_text_cleaner.py ==== 
from __future__ import annotations
from typing import List
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner

from src.core.ingestion.cleaner.simple_cleaners import (
    UnicodeNormalizer,
    SoftHyphenCleaner,
    HeaderFooterCleaner,
    LayoutLineJoinCleaner,
    TrailingWhitespaceCleaner,
    HTMLCleaner,                 # removes HTML tags and entities
    ScientificNotationCleaner,   # removes scientific notation terms like "Eq. 1"
    ReferencesCleaner,           # removes everything after 'References' or 'Bibliography'
)

from src.core.ingestion.cleaner.deep_text_cleaner import DeepTextCleaner  # advanced cleaning


class RagTextCleaner(ITextCleaner):
    """
    Composite text cleaner for RAG ingestion.
    Executes a deterministic sequence of cleaners to normalize and denoise scientific text.
    """

    def __init__(self, cleaners: List[ITextCleaner]):
        self.cleaners = cleaners

    # ------------------------------------------------------------------
    @classmethod
    def default(cls) -> "RagTextCleaner":
        """
        Build a deterministic chain of text cleaners combining rule-based and deep cleaning.
        """
        cleaners: List[ITextCleaner] = [
            UnicodeNormalizer(),         # normalize spaces and zero-width chars
            SoftHyphenCleaner(),         # remove soft hyphens and join split words
            HeaderFooterCleaner(),       # drop headers, footers, funding info
            LayoutLineJoinCleaner(),     # repair layout-induced line breaks
            TrailingWhitespaceCleaner(), # trim redundant spaces and newlines
            HTMLCleaner(),               # remove HTML tags and entities
            ScientificNotationCleaner(), # remove equations, theorem/lemma markers
            ReferencesCleaner(),         # cut after "References"/"Bibliography"
            DeepTextCleaner(),           # deep filter for noise, refs, tables, math etc.
        ]
        return cls(cleaners)

    # ------------------------------------------------------------------
    def clean(self, text: str) -> str:
        """
        Run all sub-cleaners consecutively in deterministic order.
        """
        for cleaner in self.cleaners:
            text = cleaner.clean(text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rules.py ==== 
# src/core/ingestion/cleaner/rules.py
from __future__ import annotations
import re

# Common residual headings we want to drop only if they appear alone in a line
SINGLE_LINE_HEADER_PATTERNS = [
    r"(?i)^\s*table\s+of\s+contents\s*$",
    r"(?i)^\s*inhaltsverzeichnis\s*$",
    r"(?i)^\s*contents\s*$",
]

# Footer / page indicator
FOOTER_PATTERNS = [
    r"(?i)^\s*page\s+\d+\s*$",
    r"(?i)^\s*p\.?\s*\d+\s*$",
    r"^\s*\d+\s*$",
]

# Funding and preprint noise – but only if line is short
FUNDING_PATTERNS = [
    r"(?i)^\s*funded\s+by.*$",
    r"(?i)^\s*supported\s+by.*$",
    r"(?i)^\s*this\s+preprint\s+.*$",
    r"(?i)^\s*arxiv:\s*\d{4}\.\d{4,5}.*$",
    r"(?i)^\s*doi:\s*10\.\d{4,9}/[-._;()/:A-Z0-9]+$",
]

# Lines we consider layout trash if they are too short
MAX_LEN_FOR_STRICT_DROP = 80

SOFT_HYPHEN = "\xad"
SOFT_HYPHEN_RE = re.compile(SOFT_HYPHEN)

# Hyphenated line break like "prob-\nabilistic"
HYPHEN_LINEBREAK_RE = re.compile(r"(\w+)-\n(\w+)")

# Linebreak in the middle of sentence like "proba-\nbilistic"
INLINE_LINEBREAK_RE = re.compile(r"(\S)\n(\S)")

# Multiple blank lines
MULTI_NEWLINE_RE = re.compile(r"\n{3,}")

# Unicode spaces
UNICODE_SPACES_RE = re.compile(r"[\u00a0\u2000-\u200b]+")


def is_short_line(line: str) -> bool:
    # Heuristic: many layout lines are short
    return len(line.strip()) <= MAX_LEN_FOR_STRICT_DROP


def match_any(patterns: list[str], line: str) -> bool:
    for pat in patterns:
        if re.match(pat, line):
            return True
    return False
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\simple_cleaners.py ==== 
from __future__ import annotations
import re
from typing import List
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner
from src.core.ingestion.cleaner import rules


class HTMLCleaner(BaseTextCleaner):
    """Removes HTML tags and entities like &nbsp;."""

    def _clean_impl(self, text: str) -> str:
        # Remove all HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove HTML entities like &nbsp;
        text = re.sub(r'&[a-zA-Z]+;', '', text)
        return text


class ScientificNotationCleaner(BaseTextCleaner):
    """Removes scientific notations and references like 'Eq. 1', 'Theorem 3'."""

    def _clean_impl(self, text: str) -> str:
        # Remove scientific format like 'Eq. 1', 'Theorem 2'
        text = re.sub(r'\b(Eq|Theorem|Lemma)\s+\d+\b', '', text)
        return text


class UnicodeNormalizer(BaseTextCleaner):
    """Normalize unicode spaces and zero-width chars."""

    def _clean_impl(self, text: str) -> str:
        # Replace unicode spaces by normal space
        text = rules.UNICODE_SPACES_RE.sub(" ", text)
        return text


class SoftHyphenCleaner(BaseTextCleaner):
    """Remove soft hyphens and join line-broken words."""

    def _clean_impl(self, text: str) -> str:
        # Remove soft hyphen character
        text = text.replace(rules.SOFT_HYPHEN, "")
        # Join hyphenated line breaks
        text = rules.HYPHEN_LINEBREAK_RE.sub(r"\1\2", text)
        return text


class LayoutLineJoinCleaner(BaseTextCleaner):
    """
    Join lines that were broken by the PDF layout but belong together.
    We do this conservatively: only if both sides are non-space.
    """

    def _clean_impl(self, text: str) -> str:
        # Join inline linebreaks like "probabilistic\nreasoning" -> "probabilistic reasoning"
        text = rules.INLINE_LINEBREAK_RE.sub(r"\1 \2", text)
        # Collapse too many blank lines
        text = rules.MULTI_NEWLINE_RE.sub("\n\n", text)
        return text.strip()


class HeaderFooterCleaner(BaseTextCleaner):
    """Remove obvious header/footer/single-line noise."""

    def _clean_impl(self, text: str) -> str:
        cleaned_lines: List[str] = []
        for line in text.splitlines():
            raw = line.rstrip()

            # Drop header-like lines
            if rules.match_any(rules.SINGLE_LINE_HEADER_PATTERNS, raw):
                continue

            # Drop footer-like things only if short
            if rules.match_any(rules.FOOTER_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            # Drop funding/preprint if short
            if rules.match_any(rules.FUNDING_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            cleaned_lines.append(raw)

        return "\n".join(cleaned_lines).strip()


class TrailingWhitespaceCleaner(BaseTextCleaner):
    """Normalize whitespace globally."""

    def _clean_impl(self, text: str) -> str:
        # Strip each line
        lines = [ln.rstrip() for ln in text.splitlines()]
        text = "\n".join(lines)
        # Final collapse of multiple newlines
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()


class ReferencesCleaner(BaseTextCleaner):
    """Removes everything starting from 'References', 'Bibliography', or 'Literaturverzeichnis' sections."""

    def _clean_impl(self, text: str) -> str:
        # Pattern for reference section headers (case-insensitive)
        pattern = re.compile(
            r"(?im)^\s*(references|bibliography|literaturverzeichnis)\s*$"
        )
        match = pattern.search(text)
        if match:
            # Cut everything from the start of the reference section
            cutoff_index = match.start()
            # Only cut if it occurs after the first 20% of the document to avoid false positives
            if cutoff_index > len(text) * 0.2:
                text = text[:cutoff_index]
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\i_ingestor.py ==== 
from abc import ABC, abstractmethod
from typing import Any

class IIngestor(ABC):
    """Interface for all ingestion components."""

    @abstractmethod
    def ingest(self, source_path: str) -> Any:
        """Convert one document into structured JSON chunks and metadata."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\metadata_extractor_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import importlib
import logging
from pathlib import Path


class MetadataExtractorFactory:
    """
    Dynamically loads and executes PDF-based metadata extractors.
    Each extractor implementation must expose a class named <Name>Extractor in
    src.core.ingestion.metadata.implementations.<name>_extractor.
    """

    def __init__(self, active_fields: List[str], pdf_root: str | Path):
        self.logger = logging.getLogger(__name__)
        self.active_fields = active_fields
        self.pdf_root = Path(pdf_root).resolve()
        self.extractors: Dict[str, Any] = {}
        self._load_extractors()

    # ------------------------------------------------------------------
    @classmethod
    def from_config(cls, cfg: Dict[str, Any]) -> MetadataExtractorFactory:
        """Initialize factory from ingestion.yaml configuration."""
        opts = cfg.get("options", {})
        active = opts.get("metadata_fields", [])
        pdf_root = cfg.get("paths", {}).get("raw_pdfs", "./data/raw_pdfs")
        return cls(active_fields=active, pdf_root=pdf_root)

    # ------------------------------------------------------------------
    def _load_extractors(self) -> None:
        """Dynamically import extractor implementations for requested fields."""
        base_pkg = "src.core.ingestion.metadata.implementations"

        mapping = {
            "title": "title_extractor",
            "authors": "author_extractor",
            "year": "year_extractor",
            # "abstract": "abstract_extractor",  # Deactivated abstract extraction
            "detected_language": "language_detector",
            "file_size": "file_size_extractor",
            "toc": "toc_extractor",
            "page_number": "page_number_extractor",  # Page number extractor
        }

        for field in self.active_fields:
            module_name = mapping.get(field)
            if not module_name:
                self.logger.warning(f"No extractor mapping found for field '{field}' → skipped.")
                continue

            try:
                # Dynamically import the module
                module = importlib.import_module(f"{base_pkg}.{module_name}")
                # Construct the extractor class name from the field (e.g., "PageNumberExtractor" from "page_number")
                class_name = "".join([p.capitalize() for p in field.split("_")]) + "Extractor"
                # Get the extractor class from the module
                extractor_class = getattr(module, class_name)
                # Instantiate the extractor class and store it in the dictionary
                self.extractors[field] = extractor_class(base_dir=self.pdf_root)
                self.logger.debug(f"Loaded extractor: {extractor_class.__name__} for '{field}'")
            except ModuleNotFoundError:
                self.logger.warning(f"Implementation missing for '{field}' ({module_name}.py not found).")
            except Exception as e:
                self.logger.error(f"Failed to load extractor for '{field}': {e}")

    # ------------------------------------------------------------------
    def extract_all(self, pdf_path: str) -> Dict[str, Any]:
        """Run all configured extractors directly on a given PDF file path."""
        pdf_path = str(Path(pdf_path).resolve())
        results: Dict[str, Any] = {}

        for field, extractor in self.extractors.items():
            try:
                value = extractor.extract(pdf_path)
                results[field] = value
            except Exception as e:
                self.logger.warning(f"Extractor '{field}' failed for {pdf_path}: {e}")
                results[field] = None

        return results
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\abstract_extractor.py ==== 
from __future__ import annotations
from typing import Optional
from pathlib import Path
import fitz


# class AbstractExtractor:
#     """Extracts abstract text directly from GROBID TEI XML or PDF XMP metadata."""

#     def __init__(self, base_dir: Path | str | None = None):
#         self.base_dir = Path(base_dir).resolve() if base_dir else None

#     # ------------------------------------------------------------------
#     def extract(self, pdf_path: str) -> Optional[str]:
#         pdf_file = Path(pdf_path)

#         # 1. Try GROBID XML
#         xml_path = self._find_grobid_xml(pdf_file)
#         if xml_path and xml_path.exists():
#             abstract = self._extract_from_grobid(xml_path)
#             if abstract:
#                 return abstract

#         # 2. Try PDF metadata (some PDFs include "subject"/"keywords"/"description")
#         abstract = self._extract_from_pdf_metadata(pdf_file)
#         if abstract:
#             return abstract

#         # 3. Fallback: None (no text heuristics)
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
#         """Read abstract-like information from PDF metadata fields."""
#         try:
#             with fitz.open(pdf_file) as doc:
#                 meta = doc.metadata or {}
#                 for key in ("subject", "Subject", "description", "Description", "keywords", "Keywords"):
#                     val = meta.get(key)
#                     if isinstance(val, str) and len(val.strip()) > 20:
#                         return re.sub(r"\s+", " ", val.strip())
#         except Exception:
#             return None
#         return None

#     # ------------------------------------------------------------------
#     def _find_grobid_xml(self, pdf_file: Path) -> Path | None:
#         xml_candidate = pdf_file.with_suffix(".tei.xml")
#         if xml_candidate.exists():
#             return xml_candidate
#         if self.base_dir:
#             alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
#             if alt.exists():
#                 return alt
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
#         """Parse TEI XML to extract the abstract section."""
#         try:
#             with open(xml_path, "r", encoding="utf-8") as f:
#                 xml = f.read()
#             root = etree.fromstring(xml.encode("utf-8"))
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             abs_text = root.xpath("string(//tei:abstract)", namespaces=ns)
#             if abs_text and len(abs_text.strip()) > 10:
#                 return re.sub(r"\s+", " ", abs_text.strip())
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\author_extractor.py ==== 
# src/core/ingestion/metadata/implementations/author_extractor.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from pathlib import Path
import fitz
import logging
from lxml import etree
import re

# optional imports
try:
    import spacy
    from spacy.language import Language
    from spacy.pipeline import EntityRuler
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

try:
    from nameparser import HumanName
    NAMEPARSER_AVAILABLE = True
except ImportError:
    NAMEPARSER_AVAILABLE = False


class LocalNameVerifier:
    """Deterministic local fallback for name validation without ML models."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def __init__(self):
        # extended bad keyword set for academic false positives
        self.bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "school", "machine", "learning", "arxiv", "neural", "transformer",
            "appendix", "algorithm", "keywords", "vol", "doi", "intelligence",
            "markov", "chain", "computing", "zentrum", "sciences",
            "centre", "center", "data", "model", "probability", "network"
        }

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 80:
            return False
        if any(ch.isdigit() for ch in text):
            return False
        if any(tok.lower() in self.bad_keywords for tok in text.split()):
            return False

        if NAMEPARSER_AVAILABLE:
            try:
                hn = HumanName(text)
                if hn.first or hn.last:
                    return True
            except Exception:
                pass

        if self.TITLE_PATTERN.search(text):
            return True
        return bool(self.NAME_PATTERN.search(text))


class AuthorsExtractor:
    """Deterministic, language-agnostic author extractor with silent mode (no info logs)."""

    def __init__(self, base_dir: Path | str | None = None):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.name_verifier = LocalNameVerifier()
        self.nlp = None

        if SPACY_AVAILABLE:
            self.nlp = self._init_spacy_pipeline()
        else:
            self.logger.warning("spaCy not installed; PERSON detection disabled.")

    def _init_spacy_pipeline(self):
        """Try to load spaCy model, else build local EntityRuler fallback."""
        try:
            return spacy.load("en_core_web_sm", disable=["tagger", "lemmatizer", "parser"])
        except Exception:
            try:
                return spacy.load("de_core_news_sm", disable=["tagger", "lemmatizer", "parser"])
            except Exception:
                self.logger.warning("No spaCy model available; using local EntityRuler fallback.")
                return self._build_local_person_ruler()

    def _build_local_person_ruler(self) -> "Language":
        """Constructs a blank English spaCy pipeline with a rule-based PERSON detector."""
        nlp = spacy.blank("en")
        ruler = nlp.add_pipe("entity_ruler")
        patterns = [
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_TITLE": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_ALPHA": True}, {"IS_ALPHA": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"TEXT": {"REGEX": "^[A-Z][a-z]+\\.$"}}]},
        ]
        ruler.add_patterns(patterns)
        return nlp

    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[str]:
        candidates: List[str] = []

        if parsed_document and parsed_document.get("grobid_xml"):
            candidates.extend(self._extract_from_grobid_xml(parsed_document["grobid_xml"]))
        else:
            xml_path = self._find_grobid_sidecar(Path(pdf_path))
            if xml_path:
                candidates.extend(self._extract_from_grobid_file(xml_path))

        if not candidates:
            candidates.extend(self._extract_from_pdf_metadata(Path(pdf_path)))

        if not candidates:
            candidates.extend(self._extract_from_first_page(Path(pdf_path)))

        candidates = [self._normalize(c) for c in candidates if c.strip()]
        verified = [n for n in candidates if self.name_verifier.is_person_name(n)]

        cleaned = [
            re.sub(
                r"\b(Google|Inc\.?|University|Institute|Lab|Labs|Dept\.?|Center|Centre|Zentrum|Sciences?)\b",
                "",
                n,
                flags=re.IGNORECASE
            ).strip()
            for n in verified
        ]

        seen: set[str] = set()
        return [a for a in cleaned if not (a in seen or seen.add(a))]

    def _find_grobid_sidecar(self, pdf_file: Path) -> Optional[Path]:
        local = pdf_file.with_suffix(".tei.xml")
        if local.exists():
            return local
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid_file(self, xml_path: Path) -> List[str]:
        try:
            return self._extract_from_grobid_xml(xml_path.read_text(encoding="utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID XML read error: {e}")
            return []

    def _extract_from_grobid_xml(self, xml_text: str) -> List[str]:
        ns = {"tei": "http://www.tei-c.org/ns/1.0"}
        try:
            root = etree.fromstring(xml_text.encode("utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID parse error: {e}")
            return []

        authors: List[str] = []
        for xp in [
            "//tei:analytic//tei:author",
            "//tei:monogr//tei:author",
            "//tei:respStmt//tei:author",
            "//tei:editor",
        ]:
            for node in root.xpath(xp, namespaces=ns):
                fn = node.xpath("string(.//tei:forename)", namespaces=ns).strip()
                ln = node.xpath("string(.//tei:surname)", namespaces=ns).strip()
                if fn or ln:
                    authors.append(f"{fn} {ln}".strip())
                else:
                    txt = " ".join(node.xpath(".//text()", namespaces=ns)).strip()
                    if txt:
                        authors.extend(self._extract_persons_ner(txt))
        return authors

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
        except Exception as e:
            self.logger.warning(f"PDF metadata read error: {e}")
            return []
        author_field = meta.get("author") or meta.get("Author") or meta.get("authors")
        return self._extract_persons_ner(str(author_field)) if author_field else []

    def _extract_from_first_page(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return []
                text = doc.load_page(0).get_text("text")
        except Exception as e:
            self.logger.warning(f"First-page read error: {e}")
            return []
        text = text.split("Abstract")[0] if "Abstract" in text else text
        return self._extract_persons_ner(text)

    def _extract_persons_ner(self, text: str) -> List[str]:
        if not self.nlp or not text.strip():
            return []
        doc = self.nlp(text)
        persons = []
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if any(e.start <= ent.start < e.end for e in doc.ents if e.label_ in {"ORG", "GPE", "LOC"}):
                    continue
                persons.append(ent.text.strip())
        return [p for p in persons if self._is_name_like(p)]

    def _is_name_like(self, text: str) -> bool:
        bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "arxiv", "model", "learning", "probability", "appendix", "keywords",
            "ai", "neural", "transformer", "algorithm", "vol",
            "intelligence", "computing", "zentrum", "sciences", "centre", "center"
        }
        if any(tok.lower() in bad_keywords for tok in text.split()):
            return False
        if any(ch.isdigit() for ch in text):
            return False
        return 1 <= len(text.split()) <= 5 and text[0].isupper()

    def _normalize(self, s: str) -> str:
        """Normalize spacing and special characters."""
        return " ".join(s.replace("•", "").replace("†", "").split()).strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\file_size_extractor.py ==== 
from __future__ import annotations
import os
from pathlib import Path


class FileSizeExtractor:
    """Extracts the exact file size (in bytes) directly from the PDF file."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> int | None:
        """Return the file size of the given PDF file in bytes."""
        pdf_file = Path(pdf_path)
        try:
            if not pdf_file.is_file():
                return None
            return pdf_file.stat().st_size
        except Exception:
            return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\language_detector.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import re
import fitz

try:
    from langdetect import detect
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False


class DetectedLanguageExtractor:
    """Detects the dominant document language from PDF text or XMP metadata."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Identify document language via PDF content or metadata."""
        pdf_file = Path(pdf_path)

        # 1. Try PDF XMP metadata
        lang = self._extract_from_metadata(pdf_file)
        if lang:
            return lang

        # 2. Try text-based detection (without parser)
        lang = self._detect_from_text(pdf_file)
        if lang:
            return lang

        return None

    # ------------------------------------------------------------------
    def _extract_from_metadata(self, pdf_file: Path) -> Optional[str]:
        """Check embedded language fields in PDF metadata."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                for key in ("language", "Language", "lang", "Lang"):
                    val = meta.get(key)
                    if val and isinstance(val, str):
                        val_norm = val.lower().strip()
                        if val_norm.startswith("en"):
                            return "en"
                        if val_norm.startswith("de"):
                            return "de"
        except Exception:
            return None
        return None

    # ------------------------------------------------------------------
    def _detect_from_text(self, pdf_file: Path) -> Optional[str]:
        """Extract small text sample from first pages and apply language detection."""
        sample_text = ""
        try:
            with fitz.open(pdf_file) as doc:
                max_pages = min(3, len(doc))
                for i in range(max_pages):
                    sample_text += doc.load_page(i).get_text("text") + "\n"
        except Exception:
            return None

        if not sample_text or len(sample_text.strip()) < 30:
            return None

        # Use langdetect if installed
        if LANGDETECT_AVAILABLE:
            try:
                lang = detect(sample_text)
                if lang in ("en", "de"):
                    return lang
            except Exception:
                pass

        # Simple regex-based fallback
        text_lower = sample_text.lower()
        if re.search(r"\b(und|nicht|sein|dass|werden|ein|eine|mit|auf|für|dem|den|das|ist|sind|der|die|das)\b", text_lower):
            return "de"
        if re.search(r"\b(the|and|of|for|with|from|that|this|by|is|are|we|deep|learning|language|model)\b", text_lower):
            return "en"
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\page_number_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz  # PyMuPDF
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor


class PageNumberExtractor(IPageNumberExtractor):
    """Extracts the page number from the first page of the PDF document."""
    
    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the first page of the PDF.
        
        :param pdf_path: Path to the PDF file.
        :return: The page number (if available), otherwise None.
        """
        pdf_file = Path(pdf_path)

        try:
            with fitz.open(pdf_file) as doc:
                # Getting the total page count
                total_pages = len(doc)
                if total_pages > 0:
                    # For this example, we're assuming the page number is extracted from the first page
                    return 1  # For example, we return the first page number; adapt as needed
        except Exception as e:
            print(f"Error extracting page number from {pdf_file}: {e}")
        
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\title_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz
import re
from lxml import etree

class TitleExtractor:
    """
    Robust, deterministic extractor for scientific paper titles.
    Filename fallback *is not used* (explicitly excluded).
    """

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract(self, pdf_path: str) -> Optional[str]:
        """
        Extract the most probable title of a given PDF.
        Priority:
          1. PDF metadata
          2. GROBID TEI XML
          3. Layout + heuristic on first page
        Returns None if no reliable title is found.
        """
        pdf_file = Path(pdf_path)

        # 1. Extract from PDF metadata
        title = self._extract_from_pdf_metadata(pdf_file)
        if self._is_valid(title):
            return title

        # 2. Extract from GROBID TEI XML
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            grobid_title = self._extract_from_grobid(xml_path)
            if self._is_valid(grobid_title):
                return grobid_title

        # 3. Layout-based extraction (heuristic on first page)
        title_from_layout = self._extract_from_first_page_layout(pdf_file)
        if self._is_valid(title_from_layout):
            return title_from_layout

        # No valid title found after all methods
        return None

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
        """Extract title from embedded PDF metadata fields."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                title_meta = meta.get("title") or meta.get("Title")
                if title_meta:
                    cleaned = self._normalize(title_meta)
                    if self._is_valid(cleaned):
                        return cleaned
        except Exception as e:
            print(f"Error extracting from PDF metadata: {e}")
        return None

    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Locate associated GROBID TEI XML file (same stem .tei.xml)."""
        candidate1 = pdf_file.with_suffix(".tei.xml")
        if candidate1.exists():
            return candidate1
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Extract title from structured GROBID TEI XML."""
        try:
            xml_content = xml_path.read_text(encoding="utf-8")
            root = etree.fromstring(xml_content.encode("utf-8"))
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}
            xpaths = [
                "//tei:analytic/tei:title[@type='main']",
                "//tei:monogr/tei:title[@type='main']",
                "//tei:titleStmt/tei:title[@type='main']",
                "//tei:titleStmt/tei:title"
            ]
            for xp in xpaths:
                title = root.xpath(f"string({xp})", namespaces=ns)
                cleaned = self._normalize(title)
                if self._is_valid(cleaned):
                    return cleaned
        except Exception as e:
            print(f"Error extracting from GROBID XML: {e}")
        return None

    def _extract_from_first_page_layout(self, pdf_file: Path) -> Optional[str]:
        """
        Layout + heuristic extraction: choose candidate block(s) from first page based on
        font size, vertical position, then filter out author/affiliation lines.
        """
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return None
                page = doc.load_page(0)
                blocks = page.get_text("dict")["blocks"]
        except Exception as e:
            print(f"Error extracting layout from first page: {e}")
            return None

        # Collect text segments with font size & position
        candidates: list[tuple[float, float, str]] = []  # (fontsize, y0, text)
        for block in blocks:
            if "lines" not in block:
                continue
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue
                    fontsize = span.get("size", 0)
                    y0 = span.get("origin", (0, 0))[1]
                    candidates.append((fontsize, y0, text))

        if not candidates:
            return None

        # Sort by font size descending, then vertical position ascending (top of page)
        candidates.sort(key=lambda x: (-x[0], x[1]))

        # Choose top-N spans (e.g., top 3) as potential title block
        top_spans = candidates[:3]
        combined = " ".join(span[2] for span in top_spans)
        cleaned = self._normalize(combined)

        # Filter out if it looks like author/affiliation block
        if self._looks_like_author_block(cleaned):
            # Try next spans if possible
            if len(candidates) > 3:
                alt_spans = candidates[3:6]
                combined2 = " ".join(span[2] for span in alt_spans)
                cleaned2 = self._normalize(combined2)
                if self._is_valid(cleaned2) and not self._looks_like_author_block(cleaned2):
                    return cleaned2
            return None

        if self._is_valid(cleaned):
            return cleaned

        return None

    def _looks_like_author_block(self, text: str) -> bool:
        """Heuristic to detect if a block is likely authors/affiliations rather than title."""
        if not text:
            return False
        # Frequent keywords in affiliations/authors
        if re.search(r"\b(university|dept|department|institute|school|college|laboratory|lab|affiliat|author|authors?)\b", text, re.I):
            return True
        # Many names separated by commas/and before a newline
        if re.match(r"[A-Z][a-z]+(\s[A-Z][a-z]+)+,?\s(and|&)\s[A-Z][a-z]+", text):
            return True
        return False

    def _is_valid(self, text: Optional[str]) -> bool:
        """Check if a title candidate is semantically plausible."""
        if not text:
            return False
        t = text.strip()

        # Title should not be too short
        if len(t) < 10:
            return False
        # Title should have a reasonable word count
        if len(t.split()) < 3 or len(t.split()) > 30:
            return False

        # Exclude if it's just numbers or version identifiers
        if re.match(r"^[0-9._-]+$", t):
            return False

        # Exclude arXiv IDs and DOIs
        if re.search(r"arXiv:\d+\.\d+", t) or re.search(r"doi:\S+", t, re.IGNORECASE):
            return False

        # Exclude non-title fragments like "ENTREE, P2 Pl"
        if re.search(r"(P2\sPl|ENTREE|v\d+)", t, re.IGNORECASE):
            return False

        return True

    def _normalize(self, text: Optional[str]) -> str:
        """Normalize whitespace, remove soft-hyphens and ligatures."""
        if not text:
            return ""
        # Merge hyphenated line breaks
        text = re.sub(r"(\w)-\s+(\w)", r"\1\2", text)
        text = text.replace("ﬁ", "fi").replace("ﬂ", "fl")
        text = text.replace("_", " ")
        text = re.sub(r"\s+", " ", text)
        # Remove arXiv IDs and version tags
        text = re.sub(r"arXiv:\d+\.\d+", "", text)  # Remove arXiv ID
        text = re.sub(r"v\d+", "", text)  # Remove version tags
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\toc_extractor.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
import fitz
import re
from pathlib import Path


class TocExtractor:
    """Extracts table of contents (TOC) entries directly from a PDF using PyMuPDF."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Return structured TOC entries, purely PDF-based."""
        pdf_file = Path(pdf_path)
        toc_entries: List[Dict[str, Any]] = []
        try:
            with fitz.open(pdf_file) as doc:
                toc = doc.get_toc(simple=True)
                if toc:
                    for level, title, page in toc:
                        title_clean = re.sub(r"\s+", " ", title.strip())
                        if title_clean:
                            toc_entries.append(
                                {"level": int(level), "title": title_clean, "page": int(page)}
                            )
                    return toc_entries

                # fallback to heuristic textual TOC
                toc_entries = self._extract_textual_toc(doc)
                return toc_entries

        except Exception as e:
            print(f"[WARN] Failed to extract TOC from {pdf_file}: {e}")
            return []

    # ------------------------------------------------------------------
    def _extract_textual_toc(self, doc: fitz.Document) -> List[Dict[str, Any]]:
        """Detect textual TOCs on the first pages when outline metadata is missing."""
        toc_entries: List[Dict[str, Any]] = []
        try:
            max_pages = min(5, len(doc))
            pattern = re.compile(r"^\s*(\d{0,2}\.?)+\s*[A-ZÄÖÜa-zäöü].{3,}\s+(\d{1,3})\s*$")

            for i in range(max_pages):
                text = doc.load_page(i).get_text("text")
                if not re.search(r"(Inhalt|Contents|Table of Contents)", text, re.I):
                    continue
                for line in text.splitlines():
                    if pattern.match(line.strip()):
                        parts = re.split(r"\s{2,}", line.strip())
                        if len(parts) >= 2:
                            title = re.sub(r"^\d+(\.\d+)*\s*", "", parts[0])
                            page = re.findall(r"\d+", parts[-1])
                            page_num = int(page[0]) if page else None
                            toc_entries.append(
                                {"level": 1, "title": title.strip(), "page": page_num}
                            )
            return toc_entries

        except Exception:
            return []
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\year_extractor.py ==== 
from __future__ import annotations
import re
import datetime
import time
from typing import Iterable, Tuple, Optional, Dict, List
from pathlib import Path
import fitz  # PyMuPDF
from lxml import etree

# optional normalizer
try:
    from unidecode import unidecode
except Exception:
    unidecode = None

# optional Crossref client
try:
    from habanero import Crossref
except Exception:
    Crossref = None  # type: ignore

CURRENT_YEAR = datetime.datetime.now().year
FUTURE_GRACE = 1  # allow slight future offset for clock drift


class YearExtractor:
    """Highly robust publication year extractor using multi-source heuristics, TEI, and Crossref."""

    def __init__(self, base_dir: Path | str | None = None, max_text_pages: int = 3, enable_crossref: bool = True):
        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.max_text_pages = max(1, int(max_text_pages))
        self.crossref = None
        if enable_crossref and Crossref is not None:
            try:
                self.crossref = Crossref(mailto="contact@example.com")
            except Exception:
                self.crossref = None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Main orchestrator for multi-source year extraction."""
        pdf_file = Path(pdf_path)
        candidates: List[Tuple[int, int, str]] = []  # (priority, score, year_str)

        # 1) TEI XML (most reliable)
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            year = self._extract_from_grobid(xml_path)
            if year:
                candidates.append((1, 100, year))

        # 2) PDF metadata (check & refine)
        year_meta, meta_score = self._extract_from_pdf_metadata(pdf_file)
        if year_meta:
            refined = self._refine_with_text_consistency(pdf_file, int(year_meta))
            if refined:
                candidates.append((2, meta_score, refined))

        # 3) Page text (explicit © or Published year)
        year_text, text_score = self._extract_from_page_text(pdf_file, self.max_text_pages)
        if year_text:
            candidates.append((3, text_score, year_text))

        # 4) Filename patterns / arXiv ID
        year_fn, fn_score = self._extract_from_filename(pdf_file.name)
        if year_fn:
            candidates.append((4, fn_score, year_fn))

        # 5) Optional DOI / Crossref fallback
        if not candidates:
            title, doi, arxiv = self._extract_title_doi_arxiv(pdf_file)
            if arxiv:
                y = self._year_from_arxiv_id(arxiv)
                if y:
                    candidates.append((5, 60, str(y)))
            if (doi or title) and self.crossref:
                y = self._lookup_via_crossref(title, doi)
                if y:
                    candidates.append((6, 70, y))

        if not candidates:
            return None

        # final prioritization
        candidates.sort(key=lambda t: (t[0], -t[1], -int(t[2])))
        best_year = candidates[0][2]
        return best_year

    # ------------------------------------------------------------------
    def _lookup_via_crossref(self, title: Optional[str], doi: Optional[str]) -> Optional[str]:
        """Crossref lookup (last resort)."""
        if not self.crossref:
            return None
        for attempt in range(2):
            try:
                if doi:
                    result = self.crossref.works(ids=doi, timeout=3)
                    msg = result.get("message", {})
                elif title:
                    result = self.crossref.works(query=title, limit=1, timeout=3)
                    msg = result.get("message", {}).get("items", [{}])[0]
                else:
                    return None

                for key in ("published-print", "published-online", "issued"):
                    info = msg.get(key)
                    if info and "date-parts" in info:
                        y = info["date-parts"][0][0]
                        if self._valid_year(y):
                            return str(y)
            except Exception:
                time.sleep(1)
        return None

    # ------------------------------------------------------------------
    def _extract_title_doi_arxiv(self, pdf_file: Path) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """Extract DOI, arXiv ID, and title from early pages."""
        doi_re = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b", re.I)
        arxiv_re = re.compile(r"\b(?:arxiv[:/ ]?)?(\d{4}\.\d{4,5}|[a-z\-]+/\d{7}|[0-9]{7,8})(v\d+)?\b", re.I)
        title = doi = arxiv = None

        try:
            with fitz.open(pdf_file) as doc:
                meta_title = (doc.metadata or {}).get("title")
                if meta_title:
                    title = meta_title.strip()
                text = ""
                for i in range(min(3, len(doc))):
                    text += (doc.load_page(i).get_text("text") or "") + "\n"
        except Exception:
            text = ""

        if unidecode and text:
            text = unidecode(text)

        m = doi_re.search(text)
        if m:
            doi = m.group(0).strip().rstrip(".,)")

        m = arxiv_re.search(text)
        if m:
            arxiv = m.group(1).strip()

        if not title and text:
            for line in text.splitlines():
                s = line.strip()
                if len(s) > 10 and not re.search(r"(abstract|introduction|contents)", s, re.I):
                    title = s
                    break

        return title, doi, arxiv

    # ------------------------------------------------------------------
    def _year_from_arxiv_id(self, arxiv_id: str) -> Optional[int]:
        """Infer year from arXiv ID pattern."""
        try:
            m = re.match(r"^(\d{2})(\d{2})\.\d{4,5}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
            m = re.match(r"^[a-z\-]+/(\d{2})(\d{2})\d{3,4}$", arxiv_id, re.I)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
            m = re.match(r"^(\d{2})(\d{2})\d{3,4}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                year = 2000 + yy if yy < 25 else 1900 + yy
                return year if self._valid_year(year) else None
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _valid_year(self, y: int) -> bool:
        """Check plausible range (1900–current+grace)."""
        return isinstance(y, int) and 1900 <= y <= (CURRENT_YEAR + FUTURE_GRACE)

    # ------------------------------------------------------------------
    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Find possible TEI XML companion file."""
        xml_candidate = pdf_file.with_suffix(".tei.xml")
        if xml_candidate.exists():
            return xml_candidate
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    # ------------------------------------------------------------------
    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Parse TEI XML for earliest valid date."""
        try:
            xml_bytes = xml_path.read_bytes()
            root = etree.fromstring(xml_bytes)
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}
            xpaths = [
                "//tei:sourceDesc//tei:imprint/tei:date",
                "//tei:profileDesc//tei:creation/tei:date",
                "//tei:biblStruct//tei:imprint/tei:date",
            ]
            for xp in xpaths:
                for node in root.xpath(xp, namespaces=ns):
                    for key in ("when", "when-iso", "notBefore", "notAfter"):
                        val = node.get(key)
                        if val:
                            y = self._year_from_date_string(val)
                            if y and self._valid_year(y):
                                return str(y)
                    text_val = (node.text or "").strip()
                    if text_val:
                        for y in self._years_from_string(text_val):
                            if self._valid_year(y):
                                return str(y)
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Tuple[Optional[str], int]:
        """Parse PDF metadata and filter out nonsense years (1970, 1980, 1600)."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                kv: Dict[str, str] = {k.lower(): v for k, v in meta.items() if isinstance(v, str) and v.strip()}
                best: Tuple[Optional[int], int] = (None, -1)
                for key, score in [("creationdate", 80), ("moddate", 60), ("date", 50)]:
                    val = kv.get(key)
                    if not val:
                        continue
                    y = self._year_from_date_string(val)
                    if not y or y in {0, 1, 1600, 1970, 1980}:
                        continue
                    if self._valid_year(y) and score > best[1]:
                        best = (y, score)
                if best[0]:
                    return str(best[0]), best[1]
        except Exception:
            pass
        return None, -1

    # ------------------------------------------------------------------
    def _refine_with_text_consistency(self, pdf_file: Path, year_meta: Optional[int]) -> Optional[str]:
        """Cross-check metadata with visible text for explicit © years."""
        try:
            with fitz.open(pdf_file) as doc:
                text_parts = []
                for i in range(min(3, len(doc))):
                    text_parts.append(doc.load_page(i).get_text("text") or "")
                for i in range(max(0, len(doc) - 2), len(doc)):
                    text_parts.append(doc.load_page(i).get_text("text") or "")
                text = "\n".join(text_parts)
        except Exception:
            return str(year_meta) if year_meta else None

        if unidecode and text:
            text = unidecode(text)

        # check for explicit © or "Published in"
        explicit_match = re.findall(r"(?:©|Copyright|Published\s+in|Reprinted\s+in)\s*(19|20)\d{2}", text, re.I)
        if explicit_match:
            ys = [int(y[-4:]) for y in re.findall(r"(19|20)\d{2}", text)]
            ys = [y for y in ys if self._valid_year(y)]
            if ys:
                y_max = max(ys)
                return str(y_max)

        # fallback: most recent plausible number
        years = [int(m.group()) for m in re.finditer(r"(19|20)\d{2}", text)]
        years = [y for y in years if self._valid_year(y)]
        if not years:
            return str(year_meta) if year_meta else None
        y_max = max(years)

        # adjust if metadata is suspiciously old
        if year_meta and (year_meta < 1950 or abs(year_meta - y_max) >= 10):
            return str(y_max)
        return str(year_meta if year_meta else y_max)

    # ------------------------------------------------------------------
    def _extract_from_page_text(self, pdf_file: Path, pages: int = 2) -> Tuple[Optional[str], int]:
        """Scan early pages for explicit publication indicators."""
        try:
            with fitz.open(pdf_file) as doc:
                best: Tuple[Optional[int], int] = (None, -1)
                for i in range(min(len(doc), pages)):
                    text = doc.load_page(i).get_text("text") or ""
                    if unidecode and text:
                        text = unidecode(text)
                    for pattern in [
                        r"(?:©|Copyright|Published\s+in|Reprinted\s+in)\s*(19|20)\d{2}",
                        r"(19|20)\d{2}",
                    ]:
                        years = [int(y[-4:]) for y in re.findall(pattern, text)]
                        years = [y for y in years if self._valid_year(y)]
                        if years:
                            y_pick = max(years)
                            score = 45 + (pages - i)
                            if score > best[1]:
                                best = (y_pick, score)
                if best[0]:
                    return str(best[0]), best[1]
        except Exception:
            pass
        return None, -1

    # ------------------------------------------------------------------
    def _extract_from_filename(self, filename: str) -> Tuple[Optional[str], int]:
        """Infer from filename (e.g., arXiv IDs, embedded years)."""
        base = unidecode(filename) if unidecode else filename
        base = re.sub(r"v\d+\b", "", base, flags=re.I)

        m = re.search(r"\b(\d{4}\.\d{4,5})\b", base)
        if m:
            y = self._year_from_arxiv_id(m.group(1))
            if y:
                return str(y), 50

        m = re.search(r"\b([a-z\-]+/\d{7,8}|\d{7,8})\b", base, re.I)
        if m:
            y = self._year_from_arxiv_id(m.group(1))
            if y:
                return str(y), 45

        years = [y for y in self._years_from_string(base) if self._valid_year(y) and y != 1970]
        if years:
            return str(max(years)), 30
        return None, -1

    # ------------------------------------------------------------------
    def _year_from_date_string(self, s: str) -> Optional[int]:
        """Parse ISO/XMP date strings like D:YYYYMMDD or YYYY-MM-DD."""
        if not s:
            return None
        s = s.strip()
        m = re.match(r"^D:(\d{4})", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None
        m = re.match(r"^(\d{4})(?:[-/]\d{2}(?:[-/]\d{2})?)?$", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None
        for y in self._years_from_string(s):
            if self._valid_year(y):
                return y
        return None

    # ------------------------------------------------------------------
    def _years_from_string(self, s: str) -> Iterable[int]:
        """Yield distinct plausible years in order."""
        seen = set()
        for m in re.finditer(r"(?<!\d)(19|20)\d{2}(?!\d)", s):
            y = int(m.group(0))
            if y not in seen:
                seen.add(y)
                yield y
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_abstract_extractor.py ==== 
from __future__ import annotations
from typing import Any, Dict
from lxml import etree
import re
# from src.core.ingestion.metadata.interfaces.i_abstract_extractor import IAbstractExtractor


# class AbstractExtractor(IAbstractExtractor):
#     """Extracts abstract section using GROBID TEI XML."""

#     def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
#         xml_data = parsed_document.get("grobid_xml")
#         if not xml_data or not xml_data.strip().startswith("<"):
#             return None

#         try:
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             root = etree.fromstring(xml_data.encode("utf8"))
#             # collect all text under <abstract> or <div type='abstract'>
#             xpath_candidates = [
#                 "//tei:abstract",
#                 "//tei:div[@type='abstract']",
#                 "//tei:profileDesc/tei:abstract",
#             ]
#             for path in xpath_candidates:
#                 text = root.xpath(f"string({path})", namespaces=ns)
#                 if text and len(text.strip()) > 20:
#                     cleaned = re.sub(r"\s+", " ", text.strip())
#                     return cleaned
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_author_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict, List

class IAuthorExtractor(ABC):
    """Interface for extracting document authors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> List[str]:
        """Extract author names as a list of strings."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_file_size_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IFileSizeExtractor(ABC):
    """Interface for extracting the file size of a PDF."""

    @abstractmethod
    def extract(self, pdf_path: str) -> int | None:
        """Return file size in bytes."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_language_detector.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ILanguageDetector(ABC):
    """Interface for detecting the main document language."""

    @abstractmethod
    def detect(self, text: str, metadata: Dict[str, Any] | None = None) -> str | None:
        """Detect the dominant language code (e.g. 'en', 'de')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_page_number_extractor.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Optional


class IPageNumberExtractor(ABC):
    """Interface for extracting page numbers from a document."""

    @abstractmethod
    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the given PDF path.
        
        :param pdf_path: Path to the PDF file
        :return: The page number (if available), otherwise None.
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_title_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ITitleExtractor(ABC):
    """Interface for extracting the main document title."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the document title."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_year_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IYearExtractor(ABC):
    """Interface for extracting publication year."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the publication year as string (e.g. '2023')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\toc_extractor_interface.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
from abc import ABC, abstractmethod


class TocExtractorInterface(ABC):
    """Interface for table-of-contents extractors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[Dict[str, Any]]:
        """
        Extracts the table of contents (TOC) from a given PDF file.

        Returns:
            A list of dictionaries with keys:
                - level: int
                - title: str
                - page: int
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\__init__.py ==== 
# Expose all interfaces for clean import
from .i_title_extractor import ITitleExtractor
from .i_author_extractor import IAuthorExtractor
from .i_year_extractor import IYearExtractor
#from .i_abstract_extractor import IAbstractExtractor
from .i_language_detector import ILanguageDetector
from .i_file_size_extractor import IFileSizeExtractor
#from .i_has_ocr_extractor import IHasOcrExtractor

__all__ = [
    "ITitleExtractor",
    "IAuthorExtractor",
    "IYearExtractor",
    #"IAbstractExtractor",
    "ILanguageDetector",
    "IFileSizeExtractor",
    #"IHasOcrExtractor",
]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\utils\name_verifier.py ==== 
import re
from nameparser import HumanName

class LocalNameVerifier:
    """Local deterministic fallback for name validation."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 60:
            return False
        if any(ch.isdigit() for ch in text):
            return False

        hn = HumanName(text)
        # requires at least one capitalized component
        valid_structure = bool(hn.first or hn.last)
        valid_chars = bool(self.NAME_PATTERN.search(text)) or bool(self.TITLE_PATTERN.search(text))
        invalid_tokens = any(tok.lower() in {
            "university", "institute", "department", "school",
            "machine", "learning", "arxiv", "neural", "transformer",
            "abstract", "introduction", "appendix"
        } for tok in text.split())

        return valid_structure and valid_chars and not invalid_tokens
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}


==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}

.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\file_utils.py ==== 
import os
from PyPDF2 import PdfFileReader
from pathlib import Path

def ensure_dir(path: Path):
    """Create directory if it doesn't exist."""
    path.mkdir(parents=True, exist_ok=True)

def get_file_size(file_path: Path) -> int:
    """Returns the file size in bytes."""
    return os.path.getsize(file_path)

def get_pdf_page_count(file_path: Path) -> int:
    """Returns the number of pages in a PDF."""
    with open(file_path, "rb") as file:
        reader = PdfFileReader(file)
        return reader.getNumPages()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\language_detector.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\performance_timer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\llm_orchestrator.py ==== 
# src/core/llm/llm_orchestrator.py
from __future__ import annotations
import logging
from typing import Dict, Any, List, Optional

from src.core.config.config_loader import ConfigLoader
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator
from src.core.llm.ollama_llm import OllamaLLM
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.prompt.query.prompt_builder import PromptBuilder


class LLMOrchestrator:
    """Full orchestration: Prompt → Retrieval → Reranking → LLM → IEEE-style Output."""

    def __init__(self, config_path: str = "configs/llm.yaml"):
        # Load configuration
        self.cfg = ConfigLoader(config_path).config
        self.logger = logging.getLogger("LLMOrchestrator")
        self._setup_logging()

        # Initialize pipeline components
        self.prompt_phase = PromptOrchestrator()
        self.retriever = self._init_retriever()
        self.prompt_builder = PromptBuilder()
        self.llm = self._init_llm()

        self.logger.info("Initialized full LLM pipeline orchestrator (Prompt → Retrieval → LLM).")

    # ------------------------------------------------------------------
    def _setup_logging(self) -> None:
        """Configure global logging based on configuration file."""
        log_level = self.cfg.get("global", {}).get("log_level", "INFO").upper()
        logging.basicConfig(level=getattr(logging, log_level), format="%(levelname)s | %(message)s")
        self.logger.info("Logging configured.")

    # ------------------------------------------------------------------
    def _init_retriever(self) -> RetrievalOrchestrator:
        """Initialize the retrieval orchestrator."""
        try:
            retriever = RetrievalOrchestrator()
            self.logger.info("Retriever ready.")
            return retriever
        except Exception as e:
            self.logger.exception(f"Failed to initialize retriever: {e}")
            raise

    # ------------------------------------------------------------------
    def _init_llm(self) -> OllamaLLM:
        """Initialize the LLM backend (Ollama) from configuration."""
        try:
            profile_name = self.cfg.get("generation", {}).get("llm", {}).get("profile", "default")
            llm = OllamaLLM(config_path="configs/llm.yaml", profile=profile_name)
            self.logger.info(f"LLM initialized (profile={profile_name}).")
            return llm
        except Exception as e:
            self.logger.exception(f"Failed to initialize LLM: {e}")
            raise

    # ------------------------------------------------------------------
    def run_interactive(self) -> None:
        """Interactive mode for continuous Prompt → Retrieval → LLM interaction."""
        self.logger.info("Ready for interactive mode. Press Ctrl+C to exit.")
        while True:
            try:
                # Step 1: get user query
                query_obj = self.prompt_phase.get_prompt_object()
                if not query_obj:
                    continue
                # Step 2: process the query
                answer = self.process_query(query_obj)
                if answer:
                    print("\n=== MODEL OUTPUT ===\n")
                    print(answer)
                    print("\n====================\n")
            except KeyboardInterrupt:
                self.logger.info("Session terminated by user.")
                break
            except Exception as e:
                self.logger.error(f"Unexpected runtime error: {e}")

    # ------------------------------------------------------------------
    def process_query(self, query_obj: Optional[Dict[str, Any]]) -> str:
        """End-to-end query execution: retrieval, reranking, prompt building, generation."""
        if not query_obj or not query_obj.get("processed_query"):
            self.logger.warning("Invalid or empty query object.")
            return ""

        query = query_obj["processed_query"]
        intent = query_obj.get("intent", "conceptual")
        temporal_mode = intent in ["chronological", "temporal"]

        # --- Retrieval ---
        try:
            self.logger.info(f"Retrieving context (temporal_mode={temporal_mode}) for: '{query}'")
            retrieved_docs = self.retriever.retrieve(query, intent)
        except Exception as e:
            self.logger.exception(f"Retrieval failed: {e}")
            return ""

        if not retrieved_docs:
            self.logger.warning("No relevant documents retrieved.")
            return ""

        self.logger.info(f"Retrieved {len(retrieved_docs)} top-ranked chunks.")

        # --- Prompt Building ---
        try:
            full_prompt = self.prompt_builder.build_prompt(query, intent, retrieved_docs)
        except Exception as e:
            self.logger.exception(f"Prompt building failed: {e}")
            return ""

        # --- Generation ---
        try:
            output = self.llm.generate(full_prompt, context=retrieved_docs)
            refs = self._format_references_grouped(retrieved_docs)
            self.logger.info("LLM generation successful.")
            return output.strip() + "\n\n" + refs
        except Exception as e:
            self.logger.exception(f"LLM generation failed: {e}")
            return ""

    # ------------------------------------------------------------------
    def run_with_context(self, query: str, intent: str, retrieved_chunks: List[Dict[str, Any]]) -> str:
        """Run generation phase with externally provided retrieval context."""
        self.logger.info(f"Running automatic contextual generation for intent='{intent}'")
        if not retrieved_chunks:
            return "No context provided."

        try:
            full_prompt = self.prompt_builder.build_prompt(query, intent, retrieved_chunks)
        except Exception as e:
            self.logger.error(f"Prompt construction failed: {e}")
            return f"Prompt build failed: {e}"

        try:
            output = self.llm.generate(full_prompt, context=retrieved_chunks)
            refs = self._format_references_grouped(retrieved_chunks)
            self.logger.info("Contextual generation completed successfully.")
            return output.strip() + "\n\n" + refs
        except Exception as e:
            self.logger.error(f"Contextual LLM generation failed: {e}")
            return f"[Error] Contextual generation failed: {e}"

    # ------------------------------------------------------------------
    def _format_references_grouped(self, results: List[Dict[str, Any]]) -> str:
        """Create IEEE-style reference list (one per unique PDF)."""
        if not results:
            return "References: none"

        grouped = {}
        for r in results:
            meta = r.get("metadata", {}) or {}
            src = meta.get("source_file") or "Unknown.pdf"
            year = meta.get("year") or r.get("year", "n/a")
            grouped[src] = year

        ordered = sorted(grouped.items(), key=lambda x: str(x[1]))
        refs = [f"[{i}] {src}, {year}" for i, (src, year) in enumerate(ordered, start=1)]
        return "References:\n" + "\n".join(refs)

    # ------------------------------------------------------------------
    def close(self) -> None:
        """Gracefully close all components."""
        try:
            self.retriever.close()
        except Exception as e:
            self.logger.warning(f"Retriever close failed: {e}")
        try:
            self.llm.close()
        except Exception as e:
            self.logger.warning(f"LLM close failed: {e}")
        self.logger.info("Pipeline shutdown complete.")


def main() -> None:
    """Standalone entry point for full interactive LLM orchestration."""
    orchestrator = LLMOrchestrator()
    orchestrator.run_interactive()
    orchestrator.close()


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\ollama_llm.py ==== 
# src/core/llm/ollama_llm.py
from __future__ import annotations
import subprocess
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from src.core.llm.interfaces.i_llm import ILLM
from src.core.config.config_loader import ConfigLoader

logger = logging.getLogger(__name__)

class OllamaLLM(ILLM):
    """Local Ollama backend (configurable via YAML profile, logs structured JSON)."""

    def __init__(self, config_path: str = "configs/llm.yaml", profile: str | None = None):
        cfg = ConfigLoader(config_path).config
        global_cfg = cfg.get("global", {})
        profiles = cfg.get("profiles", {})
        self.profile = profile or "default"
        profile_cfg = profiles.get(self.profile, {})

        self.model = profile_cfg.get("model", "mistral:7b-instruct")
        self.temperature = float(profile_cfg.get("temperature", 0.2))
        self.max_tokens = int(profile_cfg.get("max_tokens", 512))
        self.auto_pull = bool(profile_cfg.get("auto_pull", True))

        log_level = global_cfg.get("log_level", "INFO").upper()
        logging.basicConfig(level=getattr(logging, log_level), format="%(levelname)s | %(message)s")

        self.log_dir = Path("data/logs")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"OllamaLLM ready (profile={self.profile}, model={self.model})")

        self._ensure_model_available()

    # ------------------------------------------------------------------
    def _ensure_model_available(self) -> None:
        """Ensure selected Ollama model exists (pull if missing)."""
        try:
            result = subprocess.run(
                ["ollama", "list"], capture_output=True, text=True,
                encoding="utf-8", errors="replace"
            )
            if result.returncode != 0:
                raise RuntimeError(result.stderr.strip())

            if self.model.lower() not in result.stdout.lower():
                if not self.auto_pull:
                    logger.warning(f"Model '{self.model}' missing (auto_pull disabled).")
                    return
                logger.info(f"Pulling model '{self.model}' ...")
                pull_result = subprocess.run(
                    ["ollama", "pull", self.model],
                    capture_output=True, text=True,
                    encoding="utf-8", errors="replace"
                )
                if pull_result.returncode != 0:
                    raise RuntimeError(pull_result.stderr.strip())
                logger.info(f"Successfully pulled '{self.model}'.")
            else:
                logger.info(f"Model '{self.model}' available locally.")
        except Exception as e:
            logger.error(f"Model check failed: {e}")

    # ------------------------------------------------------------------
    def generate(self, prompt: str, context: List[Dict[str, Any]]) -> str:
        """Run Ollama model, logging prompt and context as structured JSON."""
        n_ctx = len(context)
        snippets = [
            {
                "index": i + 1,
                "source_file": (c.get("metadata", {}) or {}).get("source_file"),
                "year": (c.get("metadata", {}) or {}).get("year"),
                "text": c.get("text", "")
            }
            for i, c in enumerate(context[:n_ctx])
        ]

        timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
        log_path = self.log_dir / f"llm_{timestamp}.json"
        log_data = {
            "timestamp": timestamp,
            "profile": self.profile,
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "prompt": prompt,
            "context_snippets": snippets
        }

        try:
            with log_path.open("w", encoding="utf-8") as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Logged LLM input to {log_path}")
        except Exception as e:
            logger.warning(f"Failed to write input log: {e}")

        cmd = ["ollama", "run", self.model, prompt]
        result = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="replace")
        if result.returncode != 0:
            raise RuntimeError(result.stderr.strip())

        output = result.stdout.strip()
        log_data["model_output"] = output

        try:
            with log_path.open("w", encoding="utf-8") as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            logger.info(f"Updated log with model output → {log_path}")
        except Exception as e:
            logger.warning(f"Failed to append model output: {e}")

        return output

    # ------------------------------------------------------------------
    def close(self) -> None:
        """No persistent connections to close."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\interfaces\i_llm.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Dict


class ILLM(ABC):
    """Interface for any local or remote LLM backend."""

    @abstractmethod
    def generate(self, prompt: str, context: List[Dict[str, str]]) -> str:
        """Generate an answer given a prompt and retrieved context chunks."""
        raise NotImplementedError

    @abstractmethod
    def close(self) -> None:
        """Gracefully close model connection or session."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\formatters.py ==== 
from __future__ import annotations
import logging
import json
from datetime import datetime

try:
    # Use colorama for colored console output if available
    from colorama import Fore, Style
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class PlainFormatter(logging.Formatter):
    # Simple, deterministic log format without colors
    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S")
        return f"{ts} | {record.levelname:<8} | {record.getMessage()}"


class ColorFormatter(logging.Formatter):
    # Colored console formatter for better readability
    COLORS = {
        "DEBUG": Fore.BLUE,
        "INFO": Fore.GREEN,
        "WARNING": Fore.YELLOW,
        "ERROR": Fore.RED,
        "CRITICAL": Fore.MAGENTA,
    }

    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        color = self.COLORS.get(record.levelname, "")
        reset = Style.RESET_ALL if COLORAMA_AVAILABLE else ""
        return f"{color}{ts} | {record.levelname:<8} | {record.getMessage()}{reset}"


class JSONFormatter(logging.Formatter):
    # JSON-based formatter for structured, machine-readable logs
    def format(self, record: logging.LogRecord) -> str:
        entry = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        return json.dumps(entry, ensure_ascii=False)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\logger.py ==== 
from __future__ import annotations
import logging
import os
import json
from pathlib import Path
from logging.handlers import RotatingFileHandler
from datetime import datetime
from typing import Any, Dict
from src.core.logging.formatters import PlainFormatter, ColorFormatter, JSONFormatter

try:
    # Initialize colorama if available for colored console output
    from colorama import init as colorama_init
    colorama_init()
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class StructuredLogger:
    # Singleton-based, configurable logger for all pipeline phases
    _instance: logging.Logger | None = None

    @classmethod
    def get_logger(cls, name: str = "HDA", config: Dict[str, Any] | None = None) -> logging.Logger:
        # Reuse existing logger if already created
        if cls._instance:
            return cls._instance

        cfg = config or {}
        level = getattr(logging, cfg.get("level", "INFO").upper(), logging.INFO)
        log_to_file = cfg.get("log_to_file", True)
        log_dir = Path(cfg.get("log_dir", "logs"))
        file_name = cfg.get("file_name", "hda.log")
        rotate = cfg.get("rotate_logs", True)
        json_log = cfg.get("json_log", False)
        console_color = cfg.get("console_color", True)

        # Create and configure logger
        logger = logging.getLogger(name)
        logger.setLevel(level)
        logger.propagate = False

        # Console handler setup
        console_handler = logging.StreamHandler()
        if console_color and COLORAMA_AVAILABLE:
            console_handler.setFormatter(ColorFormatter())
        else:
            console_handler.setFormatter(PlainFormatter())
        logger.addHandler(console_handler)

        # File handler setup
        if log_to_file:
            log_dir.mkdir(parents=True, exist_ok=True)
            file_path = log_dir / file_name
            if rotate:
                handler = RotatingFileHandler(file_path, maxBytes=5_000_000, backupCount=5, encoding="utf-8")
            else:
                handler = logging.FileHandler(file_path, encoding="utf-8")
            handler.setFormatter(JSONFormatter() if json_log else PlainFormatter())
            logger.addHandler(handler)

        # Store singleton instance
        cls._instance = logger
        return logger
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\prompt_orchestrator.py ==== 
# src/core/prompt/prompt_orchestrator.py
from __future__ import annotations
import logging
from typing import Dict, Any, Optional

from src.core.prompt.query.query_input import QueryInput
from src.core.prompt.query.query_preprocessor import QueryPreprocessor


class PromptOrchestrator:
    """Coordinates the full prompt understanding pipeline:
       (1) user input → (2) preprocessing → (3) intent classification.
    """

    def __init__(self, log_level: str = "INFO"):
        # Initialize logger
        self.logger = logging.getLogger("PromptOrchestrator")
        if not self.logger.handlers:
            logging.basicConfig(level=getattr(logging, log_level.upper(), logging.INFO),
                                format="%(levelname)s | %(message)s")

        # Initialize submodules
        self.query_input = QueryInput()
        self.preprocessor = QueryPreprocessor()
        self.logger.info("PromptOrchestrator initialized successfully.")

    # ------------------------------------------------------------------
    def get_prompt_object(self) -> Dict[str, Any]:
        """Execute the complete prompt phase: read, clean, classify.

        Returns
        -------
        dict
            Structured prompt object with:
            {
                "raw_query": str,
                "processed_query": str,
                "intent": str,
                "timestamp": str,
                ...
            }
            Returns an empty dict if no valid input was given.
        """
        try:
            # Step 1: read user input interactively
            raw_query: Optional[str] = self.query_input.read_interactive()
            if not raw_query or not raw_query.strip():
                self.logger.warning("No input provided. Skipping prompt phase.")
                return {}

            # Step 2: preprocess and classify
            result: Dict[str, Any] = self.preprocessor.process(raw_query)
            if not result or "processed_query" not in result:
                self.logger.warning("Preprocessing failed or returned empty result.")
                return {}

            self.logger.info(f"Prompt phase complete: intent='{result.get('intent', 'unknown')}'")
            return result

        except KeyboardInterrupt:
            self.logger.info("Prompt input cancelled by user.")
            return {}

        except Exception as e:
            self.logger.exception(f"Unexpected error in prompt phase: {e}")
            return {}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\prompt_builder.py ==== 
# src/core/prompt/query/prompt_builder.py
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Literal, Optional

logger = logging.getLogger(__name__)
Intent = Literal["chronological", "conceptual", "analytical", "comparative"]


@dataclass
class PromptBuilderConfig:
    snippet_char_limit: int = 700       # allows slightly larger excerpts
    sort_chronologically: bool = True
    include_overview: bool = True
    numeric_citations_only: bool = True


class PromptBuilder:
    """Compose the final LLM prompt using grouped, IEEE-style context references."""

    def __init__(self, cfg: Optional[PromptBuilderConfig] = None):
        self.cfg = cfg or PromptBuilderConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

    # ------------------------------------------------------------------
    def _safe_year(self, item: Dict[str, Any]) -> int:
        meta = item.get("metadata", {}) or {}
        year = meta.get("year") or item.get("year") or 0
        try:
            return int(year)
        except Exception:
            return 0

    # ------------------------------------------------------------------
    def _group_by_source(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Group chunks belonging to the same PDF and preserve chronological order."""
        grouped: Dict[str, Dict[str, Any]] = {}
        for chunk in items:
            meta = chunk.get("metadata", {}) or {}
            src = meta.get("source_file") or "Unknown.pdf"
            year = meta.get("year") or chunk.get("year", "n/a")
            if src not in grouped:
                grouped[src] = {"year": year, "chunks": []}
            grouped[src]["chunks"].append(chunk.get("text", ""))

        # Sort by year (oldest → newest)
        ordered = sorted(grouped.items(), key=lambda x: self._safe_year({"metadata": {"year": x[1]["year"]}}))
        grouped_list = [
            {"index": i + 1, "source_file": src, "year": meta["year"], "text": " ".join(meta["chunks"])}
            for i, (src, meta) in enumerate(ordered)
        ]
        return grouped_list

    # ------------------------------------------------------------------
    def _context_overview(self, grouped: List[Dict[str, Any]]) -> str:
        lines = [f"[{g['index']}] ({g['year']}) {g['source_file']}" for g in grouped]
        return "Retrieved Context (oldest → newest):\n" + "\n".join(lines)

    # ------------------------------------------------------------------
    def _system_prompt_for(self, intent: Intent) -> str:
        if intent == "chronological":
            return (
                "You are an analytical historian of Artificial Intelligence. "
                "Explain how the concept evolved over time, focusing on paradigm shifts, research trends, and milestones. "
                "Use numeric citations [1], [2], etc., and avoid author names or years in parentheses."
            )
        if intent == "conceptual":
            return (
                "You are an AI expert. Provide a precise definition, core principles, and clear explanation of the concept. "
                "Use numeric citations [1], [2], etc., and avoid author names or years in parentheses."
            )
        return (
            "You are an analytical researcher. Compare, contrast, and evaluate the ideas in a rigorous manner. "
            "Use numeric citations [1], [2], etc., and avoid author names or years in parentheses."
        )

    # ------------------------------------------------------------------
    def _snippets_block(self, grouped: List[Dict[str, Any]]) -> str:
        """Build consolidated context snippets per unique PDF."""
        limit = max(1, self.cfg.snippet_char_limit)
        parts: List[str] = []
        for g in grouped:
            text = g["text"][:limit].replace("\n", " ").strip()
            parts.append(f"[{g['index']}] ({g['year']}) {text}")
        return "\n\n".join(parts)

    # ------------------------------------------------------------------
    def build_prompt(self, query: str, intent: Intent, retrieved_topk: List[Dict[str, Any]]) -> str:
        """Construct a clean, grouped, IEEE-conform LLM prompt."""
        if not query or not query.strip():
            raise ValueError("Empty query passed to PromptBuilder")
        if not isinstance(retrieved_topk, list) or len(retrieved_topk) == 0:
            raise ValueError("Empty retrieved list passed to PromptBuilder")

        grouped = self._group_by_source(retrieved_topk)
        sys_prompt = self._system_prompt_for(intent)
        overview = self._context_overview(grouped) if self.cfg.include_overview else ""
        snippets = self._snippets_block(grouped)

        prompt = f"{sys_prompt}\n\n"
        if overview:
            prompt += f"{overview}\n\n"
        prompt += (
            f"User Question:\n{query.strip()}\n\n"
            f"Context Snippets:\n{snippets}\n\n"
            "Answer requirements:\n"
            "- Be concise, logically structured, and evidence-based.\n"
            "- Base claims only on the Context Snippets.\n"
            "- Cite using only numeric indices like [1], [2], etc.\n"
            "- Do not include author names or explicit years in parentheses.\n"
        )
        return prompt
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_classifier.py ==== 
# src/core/prompt/query/query_classifier.py 
from __future__ import annotations
import logging
import numpy as np
from typing import Literal, Dict, List, Optional
from sentence_transformers import SentenceTransformer, util

Intent = Literal["chronological", "conceptual", "analytical", "comparative"]

class QueryClassifier:
    """
    Embedding-based intent classifier.
    No heuristics, no language dependency, fully model-driven.
    """
    def __init__(
        self,
        model_name: str = "all-MiniLM-L6-v2",
        label_texts: Optional[Dict[str, str]] = None
    ):
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

        self.model = SentenceTransformer(model_name)
        # Canonical label representations (configurable)
        self.label_texts = label_texts or {
            "chronological": "questions about historical development or changes over time",
            "conceptual": "questions asking for definition, explanation or theoretical meaning",
            "analytical": "questions asking for comparison, evaluation or analysis",
            "comparative": "questions asking for contrast or difference between ideas",
        }

        self.labels = list(self.label_texts.keys())
        # Precompute label embeddings for efficient similarity calculation
        self.label_embeddings = self.model.encode(
            list(self.label_texts.values()), normalize_embeddings=True
        )
        self.logger.info(f"Initialized semantic intent classifier with {len(self.labels)} labels")

    # ------------------------------------------------------------------
    def classify(self, query: str) -> Intent:
        """Compute embedding similarity to predefined intent prototypes."""
        if not query or not query.strip():
            return "conceptual"
        # Encode user query
        q_emb = self.model.encode(query, normalize_embeddings=True)
        # Compute cosine similarity between query and label embeddings
        sims = util.cos_sim(q_emb, self.label_embeddings)[0].cpu().numpy()
        # Select intent with highest similarity
        idx = int(np.argmax(sims))
        intent = self.labels[idx]
        self.logger.info(f"Predicted semantic intent='{intent}' (sim={sims[idx]:.3f})")
        return intent  # type: ignore
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_input.py ==== 
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional

logger = logging.getLogger(__name__)

@dataclass
class QueryInputConfig:
    prompt_text: str = "> "
    strip_input: bool = True
    allow_empty: bool = False

class QueryInput:
    """Handles raw query intake (interactive or programmatic)."""
    def __init__(self, cfg: Optional[QueryInputConfig] = None):
        self.cfg = cfg or QueryInputConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

    def read_interactive(self) -> Optional[str]:
        """Read query from stdin (interactive mode)."""
        try:
            raw = input(self.cfg.prompt_text)
        except (KeyboardInterrupt, EOFError):
            # Raise further to allow graceful termination at orchestrator level
            print()  # newline for clean CLI exit
            raise
        q = raw.strip() if self.cfg.strip_input else raw
        if not q and not self.cfg.allow_empty:
            logger.warning("Empty query ignored.")
            return None
        return q

    def from_program(self, query: Optional[str]) -> Optional[str]:
        """Receive query from another process or script."""
        if query is None:
            return None
        return query.strip() if self.cfg.strip_input else query
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_preprocessor.py ==== 
# src/core/prompt/query/query_preprocessor.py
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional
from src.core.prompt.query.query_classifier import QueryClassifier

logger = logging.getLogger(__name__)

@dataclass
class QueryPreprocessorConfig:
    lowercase: bool = True
    remove_double_spaces: bool = True
    embedding_model: str = "all-MiniLM-L6-v2"

class QueryPreprocessor:
    """Clean, normalize, and classify queries without heuristics."""
    def __init__(self, cfg: Optional[QueryPreprocessorConfig] = None):
        self.cfg = cfg or QueryPreprocessorConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
        # Initialize classifier for semantic intent detection
        self.classifier = QueryClassifier(model_name=self.cfg.embedding_model)

    def validate(self, query: Optional[str]) -> str:
        """Ensure query is not empty or invalid."""
        if not query or not query.strip():
            raise ValueError("Empty query is not allowed")
        return query.strip()

    def clean(self, query: str) -> str:
        """Normalize casing and spacing according to configuration."""
        q = query.lower() if self.cfg.lowercase else query
        if self.cfg.remove_double_spaces:
            q = " ".join(q.split())
        return q.strip()

    def process(self, raw_query: Optional[str]) -> dict:
        """Return cleaned text + semantic intent."""
        q = self.validate(raw_query)
        clean_q = self.clean(q)
        intent = self.classifier.classify(clean_q)
        logger.info(f"Processed query='{clean_q}' intent='{intent}'")
        return {"raw_query": q, "processed_query": clean_q, "intent": intent}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\faiss_retriever.py ==== 
# src/core/retrieval/faiss_retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import re
import numpy as np
import logging
from math import exp
from src.core.retrieval.interfaces.i_retriever import IRetriever


class FAISSRetriever(IRetriever):
    """FAISS-based semantic retriever with optional temporal modulation."""

    def __init__(
        self,
        vector_store_dir: str,
        model_name: str,
        top_k_retrieve: int = 50,            # # wide initial recall
        normalize_embeddings: bool = True,
        use_gpu: bool = False,
        similarity_metric: str = "cosine",   # # 'cosine' or 'dot'
        temporal_awareness: bool = True,     # # enable temporal prior
        temporal_tau: float = 8.0,           # # smoothing for |year - query_year|
        temporal_weight: float = 0.30,       # # strength of temporal modulation
        valid_year_range: Tuple[int, int] = (1900, 2100),  # # plausible year range
    ):
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError(
                "faiss-cpu and sentence-transformers are required. "
                "Install via: poetry add faiss-cpu sentence-transformers"
            ) from e

        self.faiss = faiss
        self.vector_store_dir = Path(vector_store_dir).resolve()
        self.index_path = self.vector_store_dir / "index.faiss"
        self.meta_path = self.vector_store_dir / "metadata.jsonl"

        if not self.index_path.exists() or not self.meta_path.exists():
            raise FileNotFoundError(f"Incomplete vector store: {self.vector_store_dir}")

        self.model = SentenceTransformer(model_name)
        self.top_k_retrieve = int(top_k_retrieve)
        self.normalize_embeddings = bool(normalize_embeddings)
        self.use_gpu = bool(use_gpu)
        self.similarity_metric = similarity_metric.lower().strip()
        self.temporal_awareness = bool(temporal_awareness)
        self.temporal_tau = float(temporal_tau)
        self.temporal_weight = float(temporal_weight)
        self.valid_year_range = valid_year_range

        if self.similarity_metric not in {"cosine", "dot"}:
            raise ValueError(f"Unsupported similarity metric: {self.similarity_metric}")

        # # Load FAISS index (optionally move to GPU)
        self.logger.info(f"Loading FAISS index: {self.index_path}")
        self.index = faiss.read_index(str(self.index_path))
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                self.logger.info("FAISS GPU acceleration enabled")
            except Exception as e:
                self.logger.warning(f"GPU mode failed, falling back to CPU: {e}")

        # # Load metadata into memory once
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.metadata = [json.loads(line) for line in f]

        self.logger.info(
            f"FAISSRetriever ready | entries={len(self.metadata)} | metric={self.similarity_metric.upper()} "
            f"| temporal_awareness={self.temporal_awareness}"
        )

    # ------------------------------------------------------------------
    def _encode_query(self, query: str) -> np.ndarray:
        # # Encode with optional normalization
        vec = self.model.encode([query], normalize_embeddings=self.normalize_embeddings)
        return np.asarray(vec, dtype="float32")

    # ------------------------------------------------------------------
    def _normalize_scores(self, distances: np.ndarray) -> np.ndarray:
        # # For cosine/dot (IP), higher is better -> use as-is
        if self.similarity_metric in {"cosine", "dot"}:
            return distances
        return 1 - distances

    # ------------------------------------------------------------------
    def _extract_years_from_query(self, query: str) -> List[int]:
        # # Only keep plausible 4-digit years
        yrs = [int(y) for y in re.findall(r"\b(19\d{2}|20\d{2})\b", query)]
        lo, hi = self.valid_year_range
        return [y for y in yrs if lo <= y <= hi]

    # ------------------------------------------------------------------
    def _temporal_modulate(self, base_score: float, doc_year: Optional[int], query_years: List[int]) -> float:
        # # Exponential distance weighting: nearer years -> higher boost
        if doc_year is None or not query_years:
            return base_score
        nearest = min(abs(doc_year - y) for y in query_years)
        w = exp(-nearest / max(self.temporal_tau, 1e-6))
        return base_score * (1.0 + self.temporal_weight * w)

    # ------------------------------------------------------------------
    def _safe_doc_year(self, entry: Dict[str, Any]) -> Optional[int]:
        # # Parse year from metadata robustly
        meta = entry.get("metadata", {}) or {}
        y = meta.get("year", None)
        try:
            yi = int(str(y))
            lo, hi = self.valid_year_range
            if lo <= yi <= hi:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def search(self, query: str, top_k: Optional[int] = None, temporal_mode: bool = True) -> List[Dict[str, Any]]:
        """Similarity search with optional temporal modulation."""
        # # Toggle temporal behavior per-call for deterministic flows
        self.temporal_awareness = bool(temporal_mode)

        k = int(top_k or self.top_k_retrieve)
        k = max(1, min(k, self.index.ntotal))

        q_vec = self._encode_query(query)
        self.logger.debug(f"FAISS search | k={k} | query='{query[:60]}' | temporal={self.temporal_awareness}")

        D, I = self.index.search(q_vec, k)
        scores = self._normalize_scores(D[0])

        results: List[Dict[str, Any]] = []
        query_years = self._extract_years_from_query(query) if self.temporal_awareness else []

        for score, idx in zip(scores, I[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue
            entry = self.metadata[idx]
            doc_year = self._safe_doc_year(entry)
            mod_score = (
                self._temporal_modulate(float(score), doc_year, query_years)
                if self.temporal_awareness else float(score)
            )
            results.append({
                "score": float(mod_score),
                "text": (entry.get("text", "") or "")[:500],
                "metadata": entry.get("metadata", {}) or {},
            })

        results.sort(key=lambda r: r.get("score", 0.0), reverse=True)
        self.logger.info(
            f"Retrieved {len(results)} candidates | temporal_mode={self.temporal_awareness} "
            f"| years_in_query={query_years if query_years else 'none'}"
        )
        return results

    # ------------------------------------------------------------------
    def close(self) -> None:
        """No-op for symmetry; kept for interface compliance."""
        self.logger.info("FAISS retriever closed")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\reranker_factory.py ==== 
# src/core/retrieval/reranker_factory.py
from __future__ import annotations
import logging
from typing import Any, Dict, Type
from src.core.retrieval.temporal_reranker import TemporalReranker
from src.core.retrieval.semantic_reranker import SemanticReranker
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class RerankerFactory:
    """Factory for deterministic reranker construction."""

    _registry: Dict[str, Type[IReranker]] = {
        "temporal": TemporalReranker,
        "semantic": SemanticReranker,
    }

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IReranker:
        """Instantiate reranker from configuration dictionary."""
        opts = cfg.get("options", {})
        rtype = str(opts.get("reranker", "semantic")).lower()

        if rtype not in RerankerFactory._registry:
            raise ValueError(
                f"Unsupported reranker type: {rtype}. "
                f"Available: {list(RerankerFactory._registry.keys())}"
            )

        cls = RerankerFactory._registry[rtype]
        logger.info(f"Initializing reranker of type='{rtype}'")

        if cls is TemporalReranker:
            return TemporalReranker(
                lambda_weight=float(opts.get("lambda_weight", 0.55)),
                min_year=int(opts.get("min_year", 1900)),
                enforce_decade_balance=bool(opts.get("enforce_decade_balance", True)),
                age_score_boost=float(opts.get("age_score_boost", 0.25)),
                min_decade_threshold=int(opts.get("min_decade_threshold", 3)),
                nonlinear_boost=str(opts.get("nonlinear_boost", "sigmoid")),
                ignore_years=list(opts.get("ignore_years", [])),
                recency_cutoff_year=opts.get("recency_cutoff_year"),
                allow_legacy_backfill=bool(opts.get("allow_legacy_backfill", True)),
                legacy_backfill_max_ratio=float(opts.get("legacy_backfill_max_ratio", 0.3)),
                must_include=list(opts.get("must_include", [])),
                blacklist_sources=list(opts.get("blacklist_sources", [])),
                zscore_normalization=bool(opts.get("zscore_normalization", False)),
            )

        if cls is SemanticReranker:
            return SemanticReranker(
                model_name=str(
                    opts.get("semantic_model", "cross-encoder/ms-marco-MiniLM-L-6-v2")
                ),
                top_k=int(opts.get("top_k_rerank", 25)),
                semantic_weight=float(opts.get("semantic_weight", 0.75)),
                use_gpu=bool(opts.get("use_gpu", False)),
            )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retrieval_orchestrator.py ==== 
# src/core/retrieval/retrieval_orchestrator.py
from __future__ import annotations
import logging
from typing import List, Dict, Any, Optional
from collections import defaultdict
from sentence_transformers import SentenceTransformer, util
import numpy as np

from src.core.retrieval.faiss_retriever import FAISSRetriever
from src.core.retrieval.reranker_factory import RerankerFactory
from src.core.config.config_loader import ConfigLoader


class RetrievalOrchestrator:
    """
    Unified retrieval orchestrator — controlled externally by prompt intent.
    Pipeline:
      1. Receive (query, intent)
      2. Retrieve chunks from FAISS
      3. Apply reranker (semantic / temporal)
      4. Return exactly top_k results
    """

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = cfg_loader.config

        opts = self.cfg.get("options", {})
        paths = self.cfg.get("paths", {})

        self.top_k = int(opts.get("top_k", 10))
        self.vector_store_dir = str(paths.get("vector_store_dir", "data/vector_store"))
        self.embedding_model = opts.get("embedding_model", "all-MiniLM-L6-v2")

        # Initialize FAISS retriever
        self.retriever = FAISSRetriever(
            vector_store_dir=self.vector_store_dir,
            model_name=self.embedding_model,
            top_k_retrieve=max(self.top_k * 8, 80),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_awareness=False,
        )

        # Embedding model for similarity and diversity computations
        self.embed_model = SentenceTransformer(self.embedding_model)

        self.logger.info(f"RetrievalOrchestrator initialized | top_k={self.top_k}")

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        """
        Retrieve relevant chunks according to user intent.
        intent ∈ {'conceptual', 'chronological', 'analytical', 'comparative'}
        """
        if not query or not query.strip():
            self.logger.warning("Empty query ignored.")
            return []

        is_historical = intent == "chronological"
        self.logger.info(f"Retrieval started | intent={intent} | top_k={self.top_k}")

        # Step 1: retrieve raw candidates from FAISS
        try:
            raw_results = self.retriever.search(query, top_k=self.top_k * 8, temporal_mode=is_historical)
        except Exception as e:
            self.logger.exception(f"FAISS retrieval failed: {e}")
            return []

        if not raw_results:
            self.logger.warning("No retrieval results found.")
            return []

        # Step 2: choose reranker (semantic or temporal)
        reranker_type = "temporal" if is_historical else "semantic"
        self.reranker = RerankerFactory.from_config({"options": {"reranker": reranker_type}})
        try:
            reranked = self.reranker.rerank(raw_results, top_k=len(raw_results))
        except Exception as e:
            self.logger.exception(f"Reranking failed ({reranker_type}): {e}")
            reranked = raw_results

        # Sort by reranked scores
        reranked.sort(key=lambda x: x.get("final_score", x.get("score", 0.0)), reverse=True)

        # Step 3: enforce diversity (historical only)
        final = self._ensure_exact_k(
            self._enforce_diversity(reranked, self.top_k, is_historical),
            self.top_k,
        )

        self._log_decade_distribution(final)
        self.logger.info(f"Retrieval finished | returned={len(final)} | mode={intent}")
        return final

    # ------------------------------------------------------------------
    def _enforce_diversity(self, results: List[Dict[str, Any]], k: int, historical: bool) -> List[Dict[str, Any]]:
        """Diversify by decade and source for chronological intent, otherwise maximize precision."""
        if not results:
            return []

        selected, seen = [], set()
        used_sources, used_decades = set(), set()
        kept_embs = []

        for r in results:
            if len(selected) >= k:
                break
            text = (r.get("text") or "").strip()
            if not text:
                continue

            meta = r.get("metadata", {}) or {}
            src = meta.get("source_file", "unknown").lower()
            year = self._safe_year(r)
            decade = (year // 10) * 10 if year else None

            if not historical:
                # For conceptual/analytical queries, avoid duplicates only
                if hash(text) in seen:
                    continue
                seen.add(hash(text))
                selected.append(r)
                continue

            # For historical mode, enforce source/decade diversity
            if src in used_sources and decade in used_decades and len(selected) < k * 0.8:
                continue

            emb = self.embed_model.encode(text, normalize_embeddings=True)
            if kept_embs:
                sims = util.cos_sim(emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue

            selected.append(r)
            kept_embs.append(emb)
            used_sources.add(src)
            if decade:
                used_decades.add(decade)

        return selected

    # ------------------------------------------------------------------
    def _ensure_exact_k(self, results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        """Ensure deterministic top-k output length."""
        if not results:
            return []
        if len(results) > k:
            return results[:k]
        if len(results) < k:
            return results + results[-1:] * (k - len(results))
        return results

    # ------------------------------------------------------------------
    def _safe_year(self, r: Dict[str, Any]) -> Optional[int]:
        """Safely extract valid publication year from metadata."""
        meta = r.get("metadata", {}) or {}
        y = meta.get("year", r.get("year"))
        try:
            y = int(y)
            if 1900 <= y <= 2100:
                return y
        except Exception:
            pass
        return None

    def _log_decade_distribution(self, items: List[Dict[str, Any]]) -> None:
        """Log decade distribution for temporal diagnostics."""
        hist: Dict[str, int] = defaultdict(int)
        for r in items:
            y = self._safe_year(r)
            decade = f"{(y // 10) * 10}s" if y else "unknown"
            hist[decade] += 1
        msg = ", ".join(f"{k}:{v}" for k, v in sorted(hist.items()))
        self.logger.info(f"Decade distribution: {msg}")

    # ------------------------------------------------------------------
    def close(self) -> None:
        """Close retriever resources gracefully."""
        try:
            self.retriever.close()
        except Exception as e:
            self.logger.warning(f"Failed to close retriever: {e}")
        self.logger.info("RetrievalOrchestrator closed cleanly.")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retriever_factory.py ==== 
# src/core/retrieval/retriever_factory.py
from __future__ import annotations
import logging
from typing import Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

logger = logging.getLogger(__name__)

class RetrieverFactory:
    """Factory für Intent-spezifische Retriever-Instanzen (vollständig entkoppelte Datenflüsse)."""

    @staticmethod
    def build(intent: str, cfg: Dict[str, Any]) -> FAISSRetriever:
        """Erzeuge deterministischen Retriever für den angegebenen Intent."""
        paths = cfg.get("paths", {})
        opts = cfg.get("options", {})

        # Basisparameter, deterministisch
        common_args = dict(
            model_name=opts.get("embedding_model", "all-MiniLM-L6-v2"),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_tau=float(opts.get("temporal_tau", 8.0)),
            temporal_weight=float(opts.get("temporal_weight", 0.30)),
            top_k_retrieve=int(opts.get("top_k_retrieve", 80)),
        )

        # Intent → Index-Ordner Mapping
        index_map = {
            "conceptual": paths.get("conceptual_vector_dir", "data/vector_store/conceptual"),
            "chronological": paths.get("chronological_vector_dir", "data/vector_store/chronological"),
            "analytical": paths.get("analytical_vector_dir", "data/vector_store/analytical"),
            "comparative": paths.get("comparative_vector_dir", "data/vector_store/comparative"),
        }

        # Fallback auf globales Standardverzeichnis
        vdir = index_map.get(intent, paths.get("vector_store_dir", "data/vector_store/default"))
        temporal_flag = (intent == "chronological")

        logger.info(f"Building FAISSRetriever for intent='{intent}' | dir={vdir} | temporal={temporal_flag}")

        return FAISSRetriever(
            vector_store_dir=vdir,
            temporal_awareness=temporal_flag,
            **common_args,
        )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\semantic_reranker.py ==== 
# src/core/retrieval/semantic_reranker.py
from __future__ import annotations
import logging
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class SemanticReranker(IReranker):
    """Cross-Encoder-basiertes Re-Ranking nach semantischer Relevanz."""

    def __init__(
        self,
        model_name: str,
        top_k: int = 25,
        semantic_weight: float = 0.75,
        use_gpu: bool = False,
    ):
        self.model_name = model_name
        self.top_k = top_k
        self.semantic_weight = semantic_weight
        self.device = "cuda" if use_gpu else "cpu"
        self.model = CrossEncoder(model_name, device=self.device)
        logger.info(f"Semantic Cross-Encoder loaded: {model_name} ({self.device})")

    def rerank(self, docs: List[Dict[str, Any]], top_k: int | None = None) -> List[Dict[str, Any]]:
        """Score documents via Cross-Encoder similarity."""
        if not docs:
            return []

        k = top_k or self.top_k
        pairs = [(d.get("query", ""), d.get("text", "")) for d in docs]
        scores = self.model.predict(pairs)

        for d, score in zip(docs, scores):
            base = float(d.get("score", 0.0))
            d["final_score"] = self.semantic_weight * float(score) + (1 - self.semantic_weight) * base

        docs.sort(key=lambda x: x["final_score"], reverse=True)
        return docs[:k]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\temporal_reranker.py ==== 
# src/core/retrieval/temporal_reranker.py
from __future__ import annotations
import logging, math
from statistics import mean, pstdev, median
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Set
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class TemporalReranker(IReranker):
    """Combines semantic scores with temporal diversity balancing."""

    def __init__(
        self,
        lambda_weight: float = 0.55,
        min_year: int = 1900,
        enforce_decade_balance: bool = True,
        age_score_boost: float = 0.25,
        min_decade_threshold: int = 3,
        nonlinear_boost: str = "sigmoid",
        ignore_years: List[int] | None = None,
        recency_cutoff_year: int | None = None,
        allow_legacy_backfill: bool = True,
        legacy_backfill_max_ratio: float = 0.3,
        must_include: List[str] | None = None,
        blacklist_sources: List[str] | None = None,
        zscore_normalization: bool = False,
    ):
        self.lambda_weight = lambda_weight
        self.age_score_boost = age_score_boost
        self.enforce_decade_balance = enforce_decade_balance
        self.min_decade_threshold = min_decade_threshold
        self.nonlinear_boost = nonlinear_boost.lower()
        self.min_year = min_year
        self.ignore_years: Set[int] = set(ignore_years or [])
        self.recency_cutoff_year = recency_cutoff_year
        self.allow_legacy_backfill = allow_legacy_backfill
        self.legacy_backfill_max_ratio = legacy_backfill_max_ratio
        self.must_include = must_include or []
        self.blacklist_sources = blacklist_sources or []
        self.zscore_normalization = zscore_normalization

    # ------------------------------------------------------------------
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        if not results:
            return []

        # Jahr extrahieren + Filter anwenden
        for r in results:
            r["year"] = self._extract_year(r)
        results = [r for r in results if r["year"] not in self.ignore_years]
        results = [r for r in results if not self._is_blacklisted(r)]
        if not results:
            return []

        # Z-Score-Normalisierung optional
        if self.zscore_normalization:
            self._normalize_scores(results)

        # Zeitliche Gewichtung
        now = datetime.utcnow().year
        for r in results:
            y = r["year"]
            r["adjusted_score"] = self._apply_age_boost(r.get("score", 0.0), y, now)

        results.sort(key=lambda x: x["adjusted_score"], reverse=True)
        decade_groups = self._group_by_decade(results)
        decades = sorted(decade_groups.keys())

        if not decades:
            return results[:top_k]

        λ = self._adaptive_lambda(len(decades))
        selected = (
            self._balanced_decade_selection(decade_groups, decades, top_k)
            if self.enforce_decade_balance
            else results[:top_k]
        )

        median_dec = int(median(decades))
        for r in selected:
            base = r.get("adjusted_score", 0.0)
            dec_diff = abs((r["year"] // 10) * 10 - median_dec)
            temporal_div = 1 / (1 + dec_diff / 10)
            r["final_score"] = λ * base + (1 - λ) * temporal_div

        ranked = sorted(selected, key=lambda x: x["final_score"], reverse=True)
        ranked = self._apply_recency_cutoff(ranked, results, top_k)
        ranked = self._inject_must_include(ranked, results, top_k)

        logger.info(
            f"Temporal reranking complete | decades={len(decades)} | λ={λ:.2f} | age_boost={self.age_score_boost:.2f}"
        )
        return ranked[:top_k]

    # ------------------------------------------------------------------
    def _extract_year(self, r: Dict[str, Any]) -> int:
        meta = r.get("metadata", {})
        y = meta.get("year") or r.get("year")
        try:
            val = int(str(y))
            if val < self.min_year or val > 2100:
                raise ValueError
            return val
        except Exception:
            return self.min_year

    def _group_by_decade(self, results: List[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:
        out: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
        for r in results:
            d = (r["year"] // 10) * 10
            out[d].append(r)
        for d in out:
            out[d].sort(key=lambda x: x.get("adjusted_score", 0.0), reverse=True)
        return out

    def _apply_age_boost(self, base: float, year: int, now: int) -> float:
        age = max(0, now - year)
        if self.nonlinear_boost == "sigmoid":
            s = 1 / (1 + math.exp((age - 6) / 4.0))
        elif self.nonlinear_boost == "sqrt":
            s = math.sqrt(max(0, 1 - min(age / 40, 1)))
        else:
            s = max(0, 1 - min(age / 30, 1))
        return base + self.age_score_boost * s

    def _adaptive_lambda(self, n_decades: int) -> float:
        if n_decades < self.min_decade_threshold:
            return max(0.3, self.lambda_weight * (n_decades / self.min_decade_threshold))
        return min(1.0, self.lambda_weight + 0.05 * math.log1p(n_decades))

    def _balanced_decade_selection(self, groups: Dict[int, List[Dict[str, Any]]], decades: List[int], top_k: int):
        selected: List[Dict[str, Any]] = []
        for d in decades:
            if groups[d]:
                selected.append(groups[d].pop(0))
                if len(selected) >= top_k:
                    return selected
        i = 0
        while len(selected) < top_k and any(groups.values()):
            d = decades[i % len(decades)]
            if groups[d]:
                selected.append(groups[d].pop(0))
            i += 1
        return selected

    def _normalize_scores(self, results: List[Dict[str, Any]]):
        vals = [r.get("score", 0.0) for r in results]
        if len(vals) < 2:
            return
        μ, σ = mean(vals), pstdev(vals)
        if σ < 1e-8:
            return
        for r in results:
            r["score"] = (r["score"] - μ) / σ

    def _apply_recency_cutoff(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        if not self.recency_cutoff_year:
            return ranked[:top_k]
        recent = [r for r in ranked if r["year"] >= self.recency_cutoff_year]
        legacy = [r for r in ranked if r["year"] < self.recency_cutoff_year]
        if len(recent) >= top_k:
            return recent[:top_k]
        if not self.allow_legacy_backfill:
            return recent
        max_legacy = int(top_k * self.legacy_backfill_max_ratio)
        return (recent + legacy[:max_legacy])[:top_k]

    def _inject_must_include(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        must = [r for r in all_results if self._matches_must_include(r)]
        if not must:
            return ranked
        merged, seen = [], set()
        for r in must + ranked:
            key = self._src_key(r)
            if key not in seen:
                seen.add(key)
                merged.append(r)
            if len(merged) >= top_k:
                break
        return merged

    def _src_key(self, r: Dict[str, Any]) -> str:
        meta = r.get("metadata", {})
        return (meta.get("source_file") or meta.get("title") or "unknown").lower()

    def _matches_must_include(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(m.lower() in key for m in self.must_include)

    def _is_blacklisted(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(b.lower() in key for b in self.blacklist_sources)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_reranker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IReranker(ABC):
    """Interface for reranker strategies (temporal, semantic, hybrid, etc.)."""
    @abstractmethod
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        # Rerank a list of retrieval results based on a custom strategy
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_retriever.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IRetriever(ABC):
    """Interface for all retriever implementations."""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # Return top-k relevant documents/chunks for a query
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\utils\__init__.py ==== 
.
