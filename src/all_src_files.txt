==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\all_src_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\api\server.py ==== 
# src/api/server.py
from __future__ import annotations
import uvicorn
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any

from src.core.llm.llm_orchestrator import LLMOrchestrator
from src.core.retrieval.retrieval_orchestrator import RetrievalOrchestrator

# ---------------------------------------------------------------------
# App setup
# ---------------------------------------------------------------------
app = FastAPI(title="HDA API", version="1.3")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logger = logging.getLogger("uvicorn.error")

# ---------------------------------------------------------------------
# Models
# ---------------------------------------------------------------------
class ChatRequest(BaseModel):
    query: str
    intent: str | None = "conceptual"
    return_context: bool | None = True

class RetrieveRequest(BaseModel):
    query: str
    intent: str | None = "conceptual"

class ChatResponse(BaseModel):
    answer: str
    retrieved: List[Dict[str, Any]] | None = None

# ---------------------------------------------------------------------
# Singletons (avoid reloading heavy components)
# ---------------------------------------------------------------------
_llm_orch: LLMOrchestrator | None = None
_ret_orch: RetrievalOrchestrator | None = None

def _llm() -> LLMOrchestrator:
    global _llm_orch
    if _llm_orch is None:
        _llm_orch = LLMOrchestrator()
        logger.info("LLMOrchestrator initialized.")
    return _llm_orch

def _retriever() -> RetrievalOrchestrator:
    global _ret_orch
    if _ret_orch is None:
        _ret_orch = RetrievalOrchestrator()
        logger.info("RetrievalOrchestrator initialized.")
    return _ret_orch

# ---------------------------------------------------------------------
# Endpoints
# ---------------------------------------------------------------------
@app.post("/api/retrieve", response_model=List[Dict[str, Any]])
def retrieve(req: RetrieveRequest):
    """Retrieve ranked context chunks."""
    try:
        return _retriever().retrieve(req.query, req.intent or "conceptual")
    except Exception as e:
        logger.exception("Retrieval failed.")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    """
    Chat endpoint:
    Runs the full RAG pipeline including retrieval, prompt composition,
    LLM generation, and automatic logging of prompt + response.
    """
    try:
        llm = _llm()

        # Übergabe des refined query an den Orchestrator (inkl. automatischem Logging)
        query_obj = {"refined_query": req.query, "intent": req.intent or "conceptual"}
        answer = llm.process_query(query_obj)

        if not answer:
            return ChatResponse(answer="No relevant context found or generation failed.", retrieved=[])

        # Optional: Kontext zurückgeben (bereits im LLMOrchestrator enthalten)
        retrieved_chunks = llm.retriever.retrieve(req.query, req.intent or "conceptual")

        return ChatResponse(answer=answer.strip(), retrieved=retrieved_chunks if req.return_context else None)

    except Exception as e:
        logger.exception("Chat failed.")
        raise HTTPException(status_code=500, detail=str(e))

# ---------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------
def main():
    uvicorn.run("src.api.server:app", host="0.0.0.0", port=8001, reload=False)

if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\main_orchestrator.py ==== 
# src/core/main_orchestrator.py
from __future__ import annotations
import argparse
import logging
import sys
import os
import json
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.ingestion_orchestrator import main as run_ingestion
from src.core.embedding.embedding_orchestrator import main as run_embedding
from src.core.retrieval.orchestrator.retrieval_orchestrator import RetrievalOrchestrator
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.llm.llm_orchestrator import LLMOrchestrator

# Import evaluation orchestrator via abstraction
try:
    from src.core.evaluation.evaluation_orchestrator import EvaluationOrchestrator
except Exception:
    EvaluationOrchestrator = None  # # Allow pipeline to run without evaluation module


class MainOrchestrator:
    """Central controller coordinating all pipeline phases."""

    def __init__(self, config_path: str = "configs/config.yaml"):
        # Ensure consistent UTF-8 runtime
        os.environ["PYTHONIOENCODING"] = "utf-8"
        os.environ["PYTHONUTF8"] = "1"
        os.environ["LC_ALL"] = "C.UTF-8"
        os.environ["LANG"] = "C.UTF-8"

        # Reconfigure streams for UTF-8
        if hasattr(sys, "stdout"):
            try:
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass
        if hasattr(sys, "stderr"):
            try:
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass

        # Load merged configuration and setup logging
        self.cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = self.cfg_loader.config
        self.logger = self._setup_logger()

    # ------------------------------------------------------------------
    def _setup_logger(self) -> logging.Logger:
        """Initialize process-wide logger based on config."""
        opts = self.cfg.get("global", {})
        level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
        logging.basicConfig(level=level, format="%(levelname)s | %(message)s")
        logger = logging.getLogger("MainOrchestrator")
        logger.info("Initialized main orchestrator")
        return logger

    # ------------------------------------------------------------------
    def run_phase(self, phase: str) -> None:
        """Dispatch to the selected pipeline phase."""
        self.logger.info(f"Starting phase: {phase.upper()}")

        # Ensure project src in sys.path
        base_dir = Path(self.cfg["global"]["base_dir"]).resolve()
        sys.path.append(str(base_dir / "src"))

        try:
            if phase == "ingestion":
                run_ingestion()

            elif phase == "embedding":
                run_embedding()

            elif phase == "retrieval":
                self._run_prompt_retrieval_chain()

            elif phase == "llm":
                self.logger.info("Launching LLM phase — interactive mode active.")
                orchestrator = LLMOrchestrator()
                orchestrator.run_interactive()
                orchestrator.close()

            elif phase == "evaluation":
                self._run_evaluation()

            elif phase == "all":
                self.logger.info("Running full pipeline (ingestion → embedding → prompt → retrieval → llm)")
                run_ingestion()
                run_embedding()
                self._run_prompt_retrieval_chain()
                orchestrator = LLMOrchestrator()
                orchestrator.run_interactive()
                orchestrator.close()

            else:
                self.logger.error(f"Unknown phase: {phase}")
                sys.exit(1)

            self.logger.info(f"Phase '{phase}' completed successfully.")

        except UnicodeDecodeError as ue:
            self.logger.error(f"Unicode decoding failed: {ue}. Retrying with UTF-8 replacement.")
            try:
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
            except Exception:
                pass
            raise

        except KeyboardInterrupt:
            self.logger.info("Execution manually interrupted by user.")
            sys.exit(0)

        except Exception as e:
            self.logger.exception(f"Phase '{phase}' failed: {e}")
            raise

    # ------------------------------------------------------------------
    def _run_prompt_retrieval_chain(self) -> None:
        """Execute prompt → retrieval phase."""
        self.logger.info("Executing prompt → retrieval phase")

        # Prompt phase
        prompt_orch = PromptOrchestrator()
        prompt_data = prompt_orch.get_prompt_object()
        if not prompt_data or "processed_query" not in prompt_data:
            self.logger.warning("Prompt phase returned no valid query. Aborting retrieval.")
            return

        query = prompt_data["processed_query"]
        intent = prompt_data["intent"]

        # Retrieval phase
        retrieval = RetrievalOrchestrator(config_path="configs/retrieval.yaml")
        self.logger.info(f"Query intent='{intent}' → executing retrieval flow")
        retrieved: List[Dict[str, Any]] = retrieval.retrieve(query, intent)
        retrieval.close()

        if not retrieved:
            self.logger.warning("No results retrieved.")
            return

        # Output results summary
        print("\n" + "=" * 80)
        print(f"Retrieved Top-{len(retrieved)} Chunks (intent={intent})")
        for i, r in enumerate(retrieved, start=1):
            meta = r.get("metadata", {}) or {}
            year = meta.get("year", "n/a")
            title = meta.get("source_file") or meta.get("title") or "Unknown"
            score = r.get("final_score", r.get("score", 0.0))
            print(f"[{i}] ({year}) {title} | score={float(score):.3f}")
        print("=" * 80 + "\n")

    # ------------------------------------------------------------------
    def _run_evaluation(self) -> None:
        """Batch evaluation over prior LLM logs using registered metrics."""
        if EvaluationOrchestrator is None:
            self.logger.error("Evaluation module not available. Please ensure src/core/evaluation exists.")
            return

        # Resolve evaluation config with sensible defaults
        eval_cfg = self.cfg.get("evaluation", {}) if isinstance(self.cfg, dict) else {}
        logs_dir = Path(eval_cfg.get("logs_dir", "data/logs")).resolve()
        out_dir = Path(eval_cfg.get("eval_logs_dir", "data/eval_logs")).resolve()
        k = int(eval_cfg.get("k", 5))
        glob_pat = eval_cfg.get("glob", "llm_*.json")
        gt_path = eval_cfg.get("ground_truth_path", "data/eval/ground_truth.json")

        # Create orchestrator and run batch evaluation
        self.logger.info(
            f"Evaluation settings → logs_dir={logs_dir} | out_dir={out_dir} | k={k} | glob={glob_pat}"
        )
        orch = EvaluationOrchestrator(
            output_dir=str(out_dir),
            k=k,
            ground_truth_path=gt_path
        )
        summary = orch.evaluate_batch_from_logs(logs_dir=str(logs_dir), pattern=glob_pat)

        # Human-readable console summary
        print("\n=== EVALUATION SUMMARY ===")
        print(f"files             : {summary.get('files', 0)}")
        print(f"mean NDCG@{k:>2}   : {summary.get('mean_ndcg@k', 0.0):.3f}")
        print(f"mean Faithfulness : {summary.get('mean_faithfulness', 0.0):.3f}")
        print("==========================\n")

    # ------------------------------------------------------------------
    def run(self, args: argparse.Namespace) -> None:
        """Entrypoint dispatcher using parsed CLI args."""
        if args.phase:
            self.run_phase(args.phase)
        else:
            self.logger.warning("No phase specified. Use --phase <name> or --phase all")


# ----------------------------------------------------------------------
def parse_args() -> argparse.Namespace:
    """CLI argument parser."""
    parser = argparse.ArgumentParser(description="Historical Drift Analyzer – Main Orchestrator")
    parser.add_argument(
        "--phase",
        type=str,
        required=True,
        choices=["ingestion", "embedding", "retrieval", "llm", "evaluation", "all"],
        help="Select which phase of the pipeline to execute",
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yaml",
        help="Path to the master configuration YAML file",
    )
    return parser.parse_args()


# ----------------------------------------------------------------------
if __name__ == "__main__":
    args = parse_args()
    orchestrator = MainOrchestrator(config_path=args.config)
    orchestrator.run(args)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\analysis_orchestrator.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\i_analyzer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\analysis\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\config\config_loader.py ==== 
from __future__ import annotations
import yaml
from pathlib import Path
from typing import Any, Dict


class ConfigLoader:
    """
    Universal YAML configuration loader for all pipeline phases.
    - Supports ${PROJECT_ROOT} / ${base_dir} placeholders
    - Can inherit from a master config (for global settings)
    - Provides safe defaults for missing sections
    """

    def __init__(self, path: str, master_path: str | None = "configs/config.yaml"):
        self.path = Path(path)
        if not self.path.exists():
            raise FileNotFoundError(f"Configuration file not found: {self.path}")

        # Load phase-specific YAML (e.g. ingestion.yaml)
        with open(self.path, "r", encoding="utf-8") as f:
            phase_cfg = yaml.safe_load(f) or {}

        # Load master config if available
        master_cfg: Dict[str, Any] = {}
        if master_path:
            master_file = Path(master_path)
            if master_file.exists():
                with open(master_file, "r", encoding="utf-8") as f:
                    master_cfg = yaml.safe_load(f) or {}

        # Merge configurations (phase overrides master)
        self._raw = self._merge_dicts(master_cfg, phase_cfg)

        # Detect project root
        self.project_root = self._detect_project_root()

        # Resolve all placeholders like ${base_dir}
        self.config = self._expand_vars(self._raw)

        # Ensure minimal safe defaults
        for section in ["paths", "options", "chunking"]:
            self.config.setdefault(section, {})

    # ------------------------------------------------------------------
    def _detect_project_root(self) -> Path:
        """Infer project root (the directory above 'configs')."""
        p = self.path.resolve()
        if "configs" in p.parts:
            idx = p.parts.index("configs")
            return Path(*p.parts[:idx])
        return p.parent

    # ------------------------------------------------------------------
    def _merge_dicts(self, base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
        """Recursively merge dicts, with override taking precedence."""
        merged = base.copy()
        for k, v in override.items():
            if isinstance(v, dict) and k in merged and isinstance(merged[k], dict):
                merged[k] = self._merge_dicts(merged[k], v)
            else:
                merged[k] = v
        return merged

    # ------------------------------------------------------------------
    def _expand_single_var(self, value: Any) -> Any:
        """Replace placeholders only when explicitly present."""
        if not isinstance(value, str) or "${" not in value:
            return value

        replacements = {
            "${PROJECT_ROOT}": str(self.project_root),
            "${project_root}": str(self.project_root),
            "${BASE_DIR}": str(self.project_root),
            "${base_dir}": str(self.project_root),
        }
        for ph, real in replacements.items():
            value = value.replace(ph, real)
        return str(Path(value).resolve())

    # ------------------------------------------------------------------
    def _expand_vars(self, data: Any) -> Any:
        """Recursively expand placeholders in nested structures."""
        if isinstance(data, dict):
            return {k: self._expand_vars(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._expand_vars(v) for v in data]
        elif isinstance(data, str):
            return self._expand_single_var(data)
        else:
            return data

    # ------------------------------------------------------------------
    def get(self, key: str, default: Any = None) -> Any:
        """Safely access top-level config sections."""
        return self.config.get(key, default)

    # ------------------------------------------------------------------
    @property
    def raw(self) -> Dict[str, Any]:
        """Return unexpanded raw YAML structure."""
        return self._raw


# ----------------------------------------------------------------------
def load_config(path: str, master_path: str | None = "configs/config.yaml") -> Dict[str, Any]:
    """Convenience function: directly load and expand a config dictionary."""
    return ConfigLoader(path, master_path).config
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedder_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict
import logging

from src.core.embedding.interfaces.i_embedder import IEmbedder


class SentenceTransformerEmbedder(IEmbedder):
    """Local embedder using sentence-transformers."""

    def __init__(self, model_name: str, dimension: int | None = None, normalize_embeddings: bool = True):
        # Lazy import to avoid hard dependency
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError("sentence-transformers is required for SentenceTransformerEmbedder. "
                              "Install via: poetry add sentence-transformers") from e

        self._model = SentenceTransformer(model_name)
        self._normalize = normalize_embeddings
        # If dimension not given, infer from model
        if dimension is None:
            test_vec = self._model.encode("test", normalize_embeddings=self._normalize)
            self._dimension = len(test_vec)
        else:
            self._dimension = dimension

    def embed_text(self, text: str) -> list[float]:
        # Encode single text
        return self._model.encode(text, normalize_embeddings=self._normalize).tolist()

    def embed_batch(self, texts, batch_size=None) -> list[list[float]]:
        # Encode multiple texts
        return self._model.encode(
            list(texts),
            batch_size=batch_size or 32,
            normalize_embeddings=self._normalize
        ).tolist()

    @property
    def dimension(self) -> int:
        # Return embedding dimension
        return self._dimension

    def close(self) -> None:
        # Nothing to close for sentence-transformers
        pass


class EmbedderFactory:
    """Factory for creating IEmbedder instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IEmbedder:
        opts: Dict[str, Any] = cfg.get("options", {})
        model_name = opts.get("embedding_model", "all-MiniLM-L6-v2")
        normalize = bool(opts.get("normalize_embeddings", True))
        dimension = opts.get("dimension", None)

        backend = opts.get("embedding_backend", "sentence-transformers").lower()

        if backend == "sentence-transformers":
            return SentenceTransformerEmbedder(
                model_name=model_name,
                dimension=dimension,
                normalize_embeddings=normalize,
            )
        else:
            raise ValueError(f"Unsupported embedding backend: {backend}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedding_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.embedding.embedder_factory import EmbedderFactory
from src.core.embedding.vector_store_factory import VectorStoreFactory

logger = logging.getLogger("EmbeddingOrchestrator")


def _load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _iter_chunk_files(chunks_dir: Path):
    for p in chunks_dir.glob("*.json"):
        if p.is_file():
            yield p


def _extract_chunks(chunk_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    if "chunks" in chunk_data and isinstance(chunk_data["chunks"], list):
        return chunk_data["chunks"]
    elif "text" in chunk_data:
        return [{"text": chunk_data["text"]}]
    return []


def _resolve_metadata_for_chunk(chunk_file: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Try to locate matching metadata JSON for a given chunk file."""
    base = chunk_file.stem.replace(".chunks", "")
    candidates = [
        metadata_dir / f"{chunk_file.name}",
        metadata_dir / f"{base}.json",
        metadata_dir / f"{base}.metadata.json",
    ]
    for candidate in candidates:
        if candidate.exists():
            return _load_json(candidate)
    logger.warning(f"No metadata found for {chunk_file.name}")
    return {}


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load merged configuration (embedding + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/embedding.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger.info("Starting embedding pipeline")

    paths: Dict[str, Any] = cfg.get("paths", {})
    chunks_dir = Path(paths.get("chunks_dir", "data/processed/chunks")).resolve()
    metadata_dir = Path(paths.get("metadata_dir", "data/processed/metadata")).resolve()

    if not chunks_dir.exists():
        raise FileNotFoundError(f"Chunks directory does not exist: {chunks_dir}")
    if not metadata_dir.exists():
        logger.warning(f"Metadata directory does not exist: {metadata_dir} (metadata will be empty)")

    # ------------------------------------------------------------------
    # 2. Initialize embedding backend + vector store
    # ------------------------------------------------------------------
    embedder = EmbedderFactory.from_config(cfg)
    logger.info(f"Initialized embedder with dimension={embedder.dimension}")

    vector_store = VectorStoreFactory.from_config(cfg, dimension=embedder.dimension)
    logger.info("Initialized vector store")

    batch_size = int(opts.get("batch_size", 16))
    texts_batch: List[str] = []
    metas_batch: List[Dict[str, Any]] = []

    try:
        for chunk_file in _iter_chunk_files(chunks_dir):
            chunk_json = _load_json(chunk_file)
            chunks = _extract_chunks(chunk_json)
            if not chunks:
                logger.warning(f"No chunks found in {chunk_file.name}")
                continue

            meta_data = _resolve_metadata_for_chunk(chunk_file, metadata_dir)

            for ch in chunks:
                text = ch.get("text", "").strip()
                if not text:
                    continue

                merged_meta = {
                    "source_file": meta_data.get("source_file", chunk_file.stem),
                    "title": meta_data.get("title"),
                    "authors": meta_data.get("authors"),
                    "year": meta_data.get("year"),
                    "detected_language": meta_data.get("detected_language"),
                    "page_count": meta_data.get("page_count"),
                    "origin_chunk_file": str(chunk_file.name),
                }

                texts_batch.append(text)
                metas_batch.append(merged_meta)

                if len(texts_batch) >= batch_size:
                    try:
                        vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
                        vector_store.add_vectors(vectors, texts_batch, metas_batch)
                        logger.info(f"Embedded and stored batch of size {len(texts_batch)}")
                    except Exception as e:
                        logger.error(f"Error during embedding or storing batch: {e}")
                    finally:
                        texts_batch.clear()
                        metas_batch.clear()

        if texts_batch:
            vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
            vector_store.add_vectors(vectors, texts_batch, metas_batch)
            logger.info(f"Embedded and stored final batch of size {len(texts_batch)}")

        vector_store.persist()
        logger.info("Embedding pipeline finished successfully.")

    finally:
        embedder.close()
        vector_store.close()


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\vector_store_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import logging
import os
import json
import sqlite3

from src.core.embedding.interfaces.i_vector_store import IVectorStore


class FAISSVectorStore(IVectorStore):
    """FAISS-based local vector store."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import faiss  # type: ignore
        except ImportError as e:
            raise ImportError("faiss-cpu is required for FAISSVectorStore. "
                              "Install via: poetry add faiss-cpu") from e

        self.faiss = faiss
        self.dimension = dimension
        self.persist_dir = persist_dir
        os.makedirs(persist_dir, exist_ok=True)
        self.index_path = os.path.join(persist_dir, "index.faiss")
        self.meta_path = os.path.join(persist_dir, "metadata.jsonl")

        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
        else:
            self.index = faiss.IndexFlatIP(self.dimension)

        # Metadata is stored separately as JSONL
        self._meta_fh = open(self.meta_path, "a", encoding="utf-8")

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import to avoid hard dependency at class load

        arr = np.array(vectors).astype("float32")
        self.index.add(arr)

        for i, doc in enumerate(documents):
            meta = metadatas[i] if metadatas and i < len(metadatas) else {}
            payload = {
                "text": doc,
                "metadata": meta,
            }
            self._meta_fh.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def persist(self) -> None:
        self.faiss.write_index(self.index, self.index_path)
        self._meta_fh.flush()

    def close(self) -> None:
        try:
            self.persist()
        finally:
            self._meta_fh.close()


class LanceDBVectorStore(IVectorStore):
    """LanceDB-based vector store (local file-based)."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import lancedb  # type: ignore
        except ImportError as e:
            raise ImportError("lancedb is required for LanceDBVectorStore. "
                              "Install via: poetry add lancedb") from e

        self.dimension = dimension
        os.makedirs(persist_dir, exist_ok=True)
        self.db = lancedb.connect(persist_dir)
        self.table = self.db.open_table("embeddings") if "embeddings" in self.db.table_names() else \
            self.db.create_table("embeddings", data=[
                {
                    "vector": [0.0] * dimension,
                    "text": "",
                    "metadata": {},
                }
            ])

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        rows = []
        for i, vec in enumerate(vectors):
            rows.append({
                "vector": vec,
                "text": documents[i],
                "metadata": metadatas[i] if metadatas and i < len(metadatas) else {},
            })
        self.table.add(rows)

    def persist(self) -> None:
        # LanceDB persists automatically
        pass

    def close(self) -> None:
        # Nothing to close
        pass


class SQLiteVectorStore(IVectorStore):
    """Very simple SQLite-based vector store (for debugging / small-scale)."""

    def __init__(self, persist_dir: str, dimension: int):
        os.makedirs(persist_dir, exist_ok=True)
        db_path = os.path.join(persist_dir, "vectors.sqlite3")
        self.conn = sqlite3.connect(db_path)
        self.dimension = dimension
        self._ensure_schema()

    def _ensure_schema(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                metadata TEXT,
                vector BLOB NOT NULL
            )
            """
        )
        self.conn.commit()

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import
        cur = self.conn.cursor()
        for i, vec in enumerate(vectors):
            meta_str = json.dumps(metadatas[i], ensure_ascii=False) if metadatas and i < len(metadatas) else "{}"
            arr = np.array(vec, dtype="float32").tobytes()
            cur.execute(
                "INSERT INTO embeddings (text, metadata, vector) VALUES (?, ?, ?)",
                (documents[i], meta_str, arr)
            )
        self.conn.commit()

    def persist(self) -> None:
        self.conn.commit()

    def close(self) -> None:
        self.conn.close()


class VectorStoreFactory:
    """Factory for creating IVectorStore instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any], dimension: int) -> IVectorStore:
        opts: Dict[str, Any] = cfg.get("options", {})
        store_name = opts.get("vector_store", "FAISS").upper()

        paths: Dict[str, Any] = cfg.get("paths", {})
        persist_dir = paths.get("vector_store_dir", "data/vector_store")

        if store_name == "FAISS":
            return FAISSVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "LANCEDB":
            return LanceDBVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "SQLITE":
            return SQLiteVectorStore(persist_dir=persist_dir, dimension=dimension)
        else:
            raise ValueError(f"Unsupported vector store: {store_name}")
 .
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_embedder.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Sequence, Optional


class IEmbedder(ABC):
    """Interface for all embedding backends."""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        # Return embedding vector for a single text
        pass

    @abstractmethod
    def embed_batch(self, texts: Sequence[str], batch_size: Optional[int] = None) -> List[List[float]]:
        # Return embedding vectors for a batch of texts
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        # Return embedding dimension
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_vector_store.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional


class IVectorStore(ABC):
    """Interface for all vector store backends."""

    @abstractmethod
    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: Optional[List[Dict[str, Any]]] = None) -> None:
        # Add vectors and associated payloads to the store
        pass

    @abstractmethod
    def persist(self) -> None:
        # Persist the store to disk
        pass

    @abstractmethod
    def close(self) -> None:
        # Close resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\auto_gt.py ==== 
# src/core/evaluation/auto_gt.py
from __future__ import annotations
import re
from typing import Dict, List, Any

from sentence_transformers import SentenceTransformer, util

from src.core.evaluation.settings import (
    EvaluationSettings,
    DEFAULT_EVAL_SETTINGS,
    SimilarityBands,
)


_CIT_PATTERN = re.compile(r"\[(\d+)\]")


class AutoGroundTruth:
    """
    Automatic graded relevance labelling for retrieval evaluation.
    Uses the same embedding model as the retrieval stack for consistency.
    """

    def __init__(
        self,
        settings: EvaluationSettings = DEFAULT_EVAL_SETTINGS,
        model_name: str | None = None,
        bands: SimilarityBands | None = None,
    ):
        # Select model name from explicit argument or global settings
        name = model_name or settings.auto_gt.model_name
        self.model = SentenceTransformer(name)

        # Inject thresholds from shared configuration
        b = bands or settings.auto_gt.similarity_bands
        self.high_thr = b.high
        self.mid_thr = b.mid
        self.low_thr = b.low

    def _extract_citations(self, output: str) -> List[int]:
        # Extract numeric citation markers [n] from model output
        if not output:
            return []
        return [int(m.group(1)) for m in _CIT_PATTERN.finditer(output)]

    def build(self, answer: str, retrieved_chunks: List[Dict[str, Any]]) -> Dict[str, int]:
        # Compute graded answer-conditioned relevance labels
        if not answer or not retrieved_chunks:
            return {}

        ans_emb = self.model.encode([answer], normalize_embeddings=True)

        labels: Dict[str, int] = {}
        cited = set(self._extract_citations(answer))

        for rank, ch in enumerate(retrieved_chunks, start=1):
            cid = ch.get("id") or f"auto::{rank}"
            text = ch.get("text", "") or ""
            chunk_emb = self.model.encode([text], normalize_embeddings=True)
            sim = float(util.cos_sim(ans_emb, chunk_emb)[0][0])

            cited_here = int(ch.get("rank", rank)) in cited

            if cited_here and sim >= self.high_thr:
                rel = 3
            elif sim >= self.high_thr:
                rel = 2
            elif sim >= self.mid_thr:
                rel = 1
            elif sim >= self.low_thr:
                rel = 0
            else:
                rel = 0

            labels[cid] = int(rel)

        return labels
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\convergence_plotter.py ==== 
# src/core/evaluation/convergence_plotter.py
from __future__ import annotations
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List

from src.core.evaluation.plot_style import apply_scientific_style, annotate_sample_info


class ConvergencePlotter:
    """Aggregates summary.json files across n-stages and visualizes mean ±95% CI convergence."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        # Store charts directory and apply unified plotting style
        self.charts_dir = Path(charts_dir)
        apply_scientific_style()

    def _load_stage_summaries(self) -> List[dict]:
        """Collect all summary_n*.json files (one per n-stage)."""
        summaries = []
        for fp in sorted(self.charts_dir.glob("summary_n*.json")):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                data["n"] = int("".join([c for c in fp.stem if c.isdigit()]))
                summaries.append(data)
            except Exception:
                continue
        return sorted(summaries, key=lambda x: x["n"])

    def plot(self) -> None:
        """Plot mean ±95% CI vs n for NDCG@k and Faithfulness."""
        data = self._load_stage_summaries()
        if not data:
            print("No stage summaries found in charts directory.")
            return

        ns = np.array([d["n"] for d in data])
        nd_mean = np.array([d["ndcg@k_mean"] for d in data])
        nd_lo = np.array([d["ndcg@k_ci95_lo"] for d in data])
        nd_hi = np.array([d["ndcg@k_ci95_hi"] for d in data])

        fa_mean = np.array([d["faith_mean"] for d in data])
        fa_lo = np.array([d["faith_ci95_lo"] for d in data])
        fa_hi = np.array([d["faith_ci95_hi"] for d in data])

        fig, ax = plt.subplots(figsize=(6.5, 4.5))

        ax.plot(ns, nd_mean, "-o", color="#1b9e77", label="NDCG@k mean")
        ax.fill_between(ns, nd_lo, nd_hi, color="#1b9e77", alpha=0.2)

        ax.plot(ns, fa_mean, "-o", color="#d95f02", label="Faithfulness mean")
        ax.fill_between(ns, fa_lo, fa_hi, color="#d95f02", alpha=0.2)

        ax.set_xlabel("Sample size n")
        ax.set_ylabel("Metric value")
        ax.set_title("Convergence of Evaluation Metrics (mean ±95% CI)")
        ax.set_ylim(0, 1.05)
        ax.legend(frameon=False)
        annotate_sample_info(ax, n=len(ns))

        fig.tight_layout()
        for ext in ("png", "svg"):
            fig.savefig(self.charts_dir / f"convergence_plot.{ext}", dpi=150, bbox_inches="tight")
        plt.close(fig)
        print(f"Convergence plot saved to {self.charts_dir}/convergence_plot.*")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import replace

from src.core.evaluation.interfaces.i_metric import IMetric
from src.core.evaluation.metrics.ndcg_metric import NDCGMetric
from src.core.evaluation.metrics.faithfulness_metric import FaithfulnessMetric
from src.core.evaluation.ground_truth_builder import GroundTruthBuilder
from src.core.evaluation.utils import make_chunk_id
from src.core.evaluation.settings import EvaluationSettings, DEFAULT_EVAL_SETTINGS

logger = logging.getLogger("EvaluationOrchestrator")


class EvaluationOrchestrator:
    """
    Deterministic evaluation orchestrator.

    Responsibilities:
    - Construct immutable settings (with safe overrides)
    - Evaluate single or batch LLM outputs
    - Persist per-query evaluation results and summary
    """

    def __init__(
        self,
        base_output_dir: str = "data/eval_logs",
        model_name: str = "default",
        settings: EvaluationSettings = DEFAULT_EVAL_SETTINGS,
        metrics: Dict[str, IMetric] | None = None,
        gt_builder: GroundTruthBuilder | None = None,
        k: int | None = None,
        bootstrap_iters: int | None = None,  # kept for compatibility, not in settings
    ):
        # Prepare output directory
        self.model_name = model_name
        self.out = Path(base_output_dir)
        self.out.mkdir(parents=True, exist_ok=True)

        # Override only fields that tatsächlich im Dataclass existieren
        settings_overrides: Dict[str, Any] = {}
        if k is not None:
            settings_overrides["ndcg_k"] = int(k)

        if settings_overrides:
            # Create a new immutable settings instance
            self.settings = replace(settings, **settings_overrides)
        else:
            self.settings = settings

        # Store bootstrap_iters separately (used by visualizers, not by settings)
        self.bootstrap_iters = bootstrap_iters

        # Extract effective k
        self.k = self.settings.ndcg_k

        # Metrics subsystem
        self.metrics = metrics or {
            "ndcg@k": NDCGMetric(k=self.k),
            "faithfulness": FaithfulnessMetric(settings=self.settings),
        }

        # Ground truth builder
        self.gt_builder = gt_builder or GroundTruthBuilder(settings=self.settings)

        logger.info(
            f"EvaluationOrchestrator ready | model={self.model_name} | k={self.k} | out={self.out}"
        )

    # -------------------------------------------------------------
    def _ensure_chunk_ids(self, items: List[Dict[str, Any]]) -> None:
        # Ensure each retrieved chunk has a stable ID
        for ch in items:
            if not ch.get("id"):
                ch["id"] = make_chunk_id(ch)

    # -------------------------------------------------------------
    def _safe_id(self, s: str | None) -> str:
        # Generate a filesystem-safe query identifier
        if not s:
            return "query"
        return "".join(ch if ch.isalnum() or ch in "-_" else "_" for ch in s)[:80] or "query"

    # -------------------------------------------------------------
    def _safe_year(self, meta: Dict[str, Any]) -> Optional[int]:
        # Extract a plausible publication year from metadata
        y = meta.get("year")
        try:
            yi = int(str(y))
            if 1900 <= yi <= 2100:
                return yi
        except Exception:
            pass
        return None

    # -------------------------------------------------------------
    def _dominant_decade(self, chunks: List[Dict[str, Any]]) -> Tuple[Optional[int], Dict[str, int]]:
        # Compute decade histogram and dominant decade
        counts: Dict[str, int] = {}
        for ch in chunks:
            y = self._safe_year(ch.get("metadata", {}) or {})
            d = f"{(y // 10) * 10}s" if y else "unknown"
            counts[d] = counts.get(d, 0) + 1

        if not counts:
            return None, {}

        dom_dec_str = sorted(counts.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]
        try:
            dom = int(dom_dec_str[:-1])
        except Exception:
            dom = None

        return dom, counts

    # -------------------------------------------------------------
    def _parse_citation_map_from_prompt(self, prompt_text: str) -> Dict[int, str]:
        # Parse "[n] filename (year)" style lines from the prompt preamble
        if not prompt_text:
            return {}
        import re

        pattern = re.compile(r"^\[(\d+)\]\s+(.+?)\s+\((?:\d{4}|n/a)\)$", re.MULTILINE)
        mapping: Dict[int, str] = {}
        for m in pattern.finditer(prompt_text):
            try:
                mapping[int(m.group(1))] = m.group(2).strip()
            except Exception:
                continue
        return mapping

    # -------------------------------------------------------------
    def _citation_hit_rate(
        self,
        model_output: str,
        retrieved_chunks: List[Dict[str, Any]],
        citation_map: Dict[int, str],
    ) -> float:
        # Compute proportion of citations that point to actually retrieved sources
        if not model_output or not citation_map:
            return 0.0

        import re

        nums = [int(x) for x in re.findall(r"\[(\d+)\]", model_output)]
        if not nums:
            return 0.0

        retrieved_sources = {
            (ch.get("metadata", {}) or {}).get("source_file", "").strip().lower()
            for ch in retrieved_chunks
        }

        hits = sum(
            1
            for n in nums
            if citation_map.get(n, "").strip().lower() in retrieved_sources
        )
        return hits / len(nums)

    # -------------------------------------------------------------
    def evaluate_single(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        model_output: str,
        prompt_text: str | None = None,
    ) -> Dict[str, float]:
        # Evaluate one query-answer pair and persist per-query metrics
        self._ensure_chunk_ids(retrieved_chunks)

        gt_map = self.gt_builder.build(query, retrieved_chunks)
        relevance_scores = [
            int(gt_map.get(ch["id"], ch.get("relevance", 0)))
            for ch in retrieved_chunks
        ]

        ndcg_val = self.metrics["ndcg@k"].compute(relevance_scores=relevance_scores)
        faith_val = self.metrics["faithfulness"].compute(
            context_chunks=[c.get("text", "") for c in retrieved_chunks],
            answer=model_output,
        )

        dom_dec, dec_counts = self._dominant_decade(retrieved_chunks)
        cit_map = self._parse_citation_map_from_prompt(prompt_text or "")
        cit_hit = self._citation_hit_rate(model_output, retrieved_chunks, cit_map)

        qid = self._safe_id(query)

        result = {
            "query_id": qid,
            "ndcg@k": float(ndcg_val),
            "faithfulness": float(faith_val),
            "dominant_decade": int(dom_dec) if dom_dec is not None else None,
            "decade_counts": dec_counts,
            "citation_hit_rate": float(cit_hit),
            "model_name": self.model_name,
        }

        out_f = self.out / f"{qid}_evaluation.json"
        out_f.write_text(json.dumps(result, indent=2), encoding="utf-8")

        logger.info(f"Evaluation complete → {out_f}")
        return result

    # -------------------------------------------------------------
    def evaluate_batch_from_logs(
        self,
        logs_dir: str = "data/logs",
        pattern: str = "llm_*.json",
    ) -> Dict[str, float]:
        # Evaluate all matching LLM logs and write a global summary
        logs_path = Path(logs_dir)
        if not logs_path.exists():
            alt = logs_path.parent / f"{logs_path.name}_{self.model_name}"
            if alt.exists():
                logs_path = alt
                logger.warning(f"Fallback logs directory used: {logs_path}")
            else:
                logger.error(f"Missing log directory: {logs_path}")
                return {
                    "model_name": self.model_name,
                    "files": 0,
                    "evaluated_files": 0,
                    "mean_ndcg@k": 0.0,
                    "mean_faithfulness": 0.0,
                }

        files = sorted(logs_path.glob(pattern))
        if not files:
            logger.error(f"No log files matching '{pattern}' in {logs_path}")
            return {
                "model_name": self.model_name,
                "files": 0,
                "evaluated_files": 0,
                "mean_ndcg@k": 0.0,
                "mean_faithfulness": 0.0,
            }

        nd_vals: List[float] = []
        fa_vals: List[float] = []
        evaluated = 0

        for fp in files:
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))

                query = (
                    data.get("query")
                    or data.get("user_query")
                    or data.get("prompt")
                    or data.get("query_refined")
                    or ""
                )

                model_output = data.get("model_output") or data.get("answer") or ""
                retrieved = data.get("retrieved_chunks") or data.get("context_snippets") or []
                prompt_text = data.get("prompt_final_to_llm") or ""

                for rank, ch in enumerate(retrieved, start=1):
                    ch.setdefault("rank", rank)
                    ch.setdefault("final_score", ch.get("score", 0.0))
                    if "text" not in ch and "snippet" in ch:
                        ch["text"] = ch["snippet"]

                if not query or not retrieved:
                    logger.warning(f"Skipped incomplete log: {fp.name}")
                    continue

                res = self.evaluate_single(query, retrieved, model_output, prompt_text)
                nd_vals.append(float(res["ndcg@k"]))
                fa_vals.append(float(res["faithfulness"]))
                evaluated += 1

            except Exception as e:
                err = self.out / f"{fp.stem}_eval_error.json"
                err.write_text(json.dumps({"error": str(e)}, indent=2), encoding="utf-8")
                logger.error(f"Evaluation failed for {fp.name}: {e}")

        mean_nd = float(sum(nd_vals) / len(nd_vals)) if nd_vals else 0.0
        mean_fa = float(sum(fa_vals) / len(fa_vals)) if fa_vals else 0.0

        summary = {
            "model_name": self.model_name,
            "files": len(files),
            "evaluated_files": evaluated,
            "mean_ndcg@k": mean_nd,
            "mean_faithfulness": mean_fa,
        }

        (self.out / "evaluation_summary.json").write_text(
            json.dumps(summary, indent=2), encoding="utf-8"
        )

        logger.info(
            f"Batch evaluation complete | model={self.model_name} | files={len(files)} | evaluated={evaluated}"
        )

        return summary
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_table_exporter.py ==== 
# src/core/evaluation/evaluation_table_exporter.py
from __future__ import annotations
import json
from pathlib import Path
from typing import Dict, Any, List

import pandas as pd


class EvaluationTableExporter:
    """Exports evaluation summary statistics as LaTeX, CSV, and Markdown tables."""

    def __init__(self, charts_dir: str = "data/eval_charts"):
        # Store paths for summary and table outputs
        self.charts_dir = Path(charts_dir)
        self.summary_path = self.charts_dir / "summary.json"
        self.out_tex = self.charts_dir / "evaluation_table.tex"
        self.out_csv = self.charts_dir / "evaluation_table.csv"
        self.out_md = self.charts_dir / "evaluation_table.md"

    # ------------------------------------------------------------------
    def _load_summary(self) -> Dict[str, Any]:
        if not self.summary_path.exists():
            raise FileNotFoundError(f"Missing summary.json in {self.charts_dir}")
        return json.loads(self.summary_path.read_text(encoding="utf-8"))

    # ------------------------------------------------------------------
    def _format_value(self, mean: float, lo: float, hi: float, digits: int = 3) -> str:
        """Format mean ± CI string."""
        return f"{mean:.{digits}f} ± {((hi - lo) / 2):.{digits}f}"

    # ------------------------------------------------------------------
    def export(self) -> Dict[str, Any]:
        """Generate LaTeX/CSV/Markdown tables from summary.json."""
        data = self._load_summary()
        if not data or "files" not in data:
            raise ValueError("Invalid or incomplete summary.json")

        rows: List[Dict[str, Any]] = [
            {
                "Metric": "NDCG@k",
                "Mean ± CI": self._format_value(
                    data["ndcg@k_mean"],
                    data["ndcg@k_ci95_lo"],
                    data["ndcg@k_ci95_hi"],
                ),
                "Median": f"{data['ndcg@k_median']:.3f}",
                "Std": f"{data['ndcg@k_std']:.3f}",
            },
            {
                "Metric": "Faithfulness",
                "Mean ± CI": self._format_value(
                    data["faith_mean"],
                    data["faith_ci95_lo"],
                    data["faith_ci95_hi"],
                ),
                "Median": f"{data['faith_median']:.3f}",
                "Std": f"{data['faith_std']:.3f}",
            },
        ]

        df = pd.DataFrame(rows)
        df.to_csv(self.out_csv, index=False, encoding="utf-8")

        md_lines = [
            "| Metric | Mean ± CI | Median | Std |",
            "|:--|--:|--:|--:|",
        ]
        for r in rows:
            md_lines.append(
                f"| {r['Metric']} | {r['Mean ± CI']} | {r['Median']} | {r['Std']} |"
            )
        self.out_md.write_text("\n".join(md_lines), encoding="utf-8")

        tex = [
            "\\begin{table}[h]",
            "\\centering",
            "\\caption{Evaluation metrics with 95\\% confidence intervals (bootstrap).}",
            "\\label{tab:evaluation_results}",
            "\\begin{tabular}{lccc}",
            "\\toprule",
            "Metric & Mean $\\pm$ CI & Median & Std \\\\",
            "\\midrule",
        ]
        for r in rows:
            tex.append(
                f"{r['Metric']} & {r['Mean ± CI']} & {r['Median']} & {r['Std']} \\\\"
            )
        tex.extend(
            [
                "\\bottomrule",
                "\\end{tabular}",
                "\\end{table}",
            ]
        )
        self.out_tex.write_text("\n".join(tex), encoding="utf-8")

        return {
            "csv": str(self.out_csv),
            "md": str(self.out_md),
            "tex": str(self.out_tex),
            "rows": len(rows),
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\evaluation_visualizer.py ==== 
# src/core/evaluation/evaluation_visualizer.py
from __future__ import annotations
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import spearmanr

from src.core.evaluation.plot_style import apply_scientific_style
from src.core.evaluation.settings import DEFAULT_EVAL_SETTINGS, EvaluationSettings

PRIMARY_COLOR = "#003359"
SECONDARY_COLOR = "#CC0000"
ACCENT_COLOR = "#FFB300"

FAITH_GOOD_COLOR = "#1a9850"
FAITH_MED_COLOR = "#fee08b"
FAITH_LOW_COLOR = "#d73027"


@dataclass
class VizConfig:
    # Configuration wrapper for visualizer
    logs_dir: str = "data/eval_logs"
    out_dir: str = "data/eval_charts"
    pattern: str = "*_evaluation.json"
    bootstrap_iters: int = DEFAULT_EVAL_SETTINGS.visualization.bootstrap_iters
    random_seed: int = DEFAULT_EVAL_SETTINGS.visualization.random_seed
    iqr_k: float = DEFAULT_EVAL_SETTINGS.visualization.iqr_k
    z_thresh: float = DEFAULT_EVAL_SETTINGS.visualization.z_thresh
    faith_high_thr: float = DEFAULT_EVAL_SETTINGS.visualization.faith_band_high
    faith_mid_thr: float = DEFAULT_EVAL_SETTINGS.visualization.faith_band_mid
    eval_settings: EvaluationSettings = DEFAULT_EVAL_SETTINGS


class EvaluationVisualizer:
    """
    Publication-oriented evaluation visualizer.
    """

    def __init__(self, cfg: VizConfig | None = None):
        self.cfg = cfg or VizConfig()
        self.logs_dir = Path(self.cfg.logs_dir)
        self.out_dir = Path(self.cfg.out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)
        np.random.seed(self.cfg.random_seed)
        apply_scientific_style()
        self._fig_no = 1

    # ------------------------------------------------------------------
    def _load_eval_rows(self) -> pd.DataFrame:
        rows: List[Dict[str, Any]] = []
        for fp in sorted(self.logs_dir.glob(self.cfg.pattern)):
            try:
                data = json.loads(fp.read_text(encoding="utf-8"))
                rows.append(
                    {
                        "query_id": data.get("query_id", fp.stem),
                        "ndcg@k": float(data.get("ndcg@k", np.nan)),
                        "faithfulness": float(data.get("faithfulness", np.nan)),
                        "citation_hit_rate": float(data.get("citation_hit_rate", np.nan)),
                        "dominant_decade": data.get("dominant_decade", None),
                        "model": data.get("model")
                        or data.get("llm_profile")
                        or data.get("model_name"),
                    }
                )
            except Exception:
                continue

        df = pd.DataFrame(rows)
        if df.empty:
            return pd.DataFrame(
                columns=[
                    "query_id",
                    "ndcg@k",
                    "faithfulness",
                    "citation_hit_rate",
                    "dominant_decade",
                    "model",
                ]
            )
        return df.dropna(how="all", subset=["ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def _bootstrap_ci(self, arr: np.ndarray, iters: int) -> Tuple[float, float, float]:
        arr = arr[~np.isnan(arr)]
        if len(arr) == 0:
            return float("nan"), float("nan"), float("nan")
        if len(arr) == 1:
            m = float(arr[0])
            return m, m, m
        n = len(arr)
        idx = np.random.randint(0, n, size=(iters, n))
        boot = np.mean(arr[idx], axis=1)
        return float(np.mean(boot)), float(np.percentile(boot, 2.5)), float(np.percentile(boot, 97.5))

    # ------------------------------------------------------------------
    def _outliers_iqr(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, float)
        clean = arr[~np.isnan(arr)]
        if len(clean) == 0:
            return np.zeros_like(arr, bool)
        q1, q3 = np.percentile(clean, [25, 75])
        iqr = q3 - q1
        if iqr == 0:
            return np.zeros_like(arr, bool)
        lo = q1 - self.cfg.iqr_k * iqr
        hi = q3 + self.cfg.iqr_k * iqr
        return (arr < lo) | (arr > hi)

    def _outliers_z(self, arr: np.ndarray) -> np.ndarray:
        arr = np.asarray(arr, float)
        mu, sd = np.nanmean(arr), np.nanstd(arr)
        if sd == 0 or np.isnan(sd):
            return np.zeros_like(arr, bool)
        z = (arr - mu) / sd
        return np.abs(z) > self.cfg.z_thresh

    # ------------------------------------------------------------------
    def _save_df(self, name: str, df: pd.DataFrame) -> None:
        df.to_csv(self.out_dir / f"{name}.csv", index=False, encoding="utf-8")

    def _save_fig(self, fig: plt.Figure, stem: str) -> None:
        fig.savefig(self.out_dir / f"{stem}.png", dpi=150, bbox_inches="tight")
        fig.savefig(self.out_dir / f"{stem}.svg", bbox_inches="tight")

    def _titled(self, base: str) -> str:
        title = f"Figure {self._fig_no}: {base}"
        self._fig_no += 1
        return title

    # ------------------------------------------------------------------
    def _faithfulness_band(self, val: float) -> str:
        if np.isnan(val):
            return "missing"
        if val >= self.cfg.faith_high_thr:
            return "high"
        if val >= self.cfg.faith_mid_thr:
            return "medium"
        return "low"

    # ------------------------------------------------------------------
    def plot_ndcg_histogram(self, df: pd.DataFrame) -> None:
        vals = df["ndcg@k"].astype(float).dropna().values
        if len(vals) == 0:
            return

        min_v, max_v = np.min(vals), np.max(vals)
        span = max_v - min_v

        if span < 0.05 and max_v > 0.8:
            xmin = max(0.8, min_v - 0.01)
            xmax = 1.0
            bins = np.linspace(xmin, xmax, 12)
        elif span < 0.2:
            xmin = max(0.0, min_v - 0.05)
            xmax = min(1.0, max_v + 0.05)
            bins = np.linspace(xmin, xmax, 15)
        else:
            xmin, xmax = 0.0, 1.0
            bins = 15

        fig = plt.figure(figsize=(6, 4))
        plt.hist(vals, bins=bins, color=PRIMARY_COLOR, edgecolor="black", alpha=0.8)
        plt.xlabel("NDCG@k")
        plt.ylabel("Count")
        plt.title(self._titled("Distribution of NDCG@k"))
        plt.xlim(xmin, xmax)
        ymin, ymax = plt.ylim()
        plt.ylim(0, max(ymax, 3))
        plt.tight_layout()
        self._save_fig(fig, "hist_ndcg")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_faithfulness_bands_global(self, df: pd.DataFrame) -> None:
        vals = df["faithfulness"].astype(float).dropna().values
        if len(vals) == 0:
            return

        bands = [self._faithfulness_band(v) for v in vals]
        series = pd.Series(bands)
        counts = series.value_counts().reindex(["high", "medium", "low"], fill_value=0)

        colors = [FAITH_GOOD_COLOR, FAITH_MED_COLOR, FAITH_LOW_COLOR]
        labels = ["High (≥ 0.70)", "Medium (0.40–0.69)", "Low (< 0.40)"]

        fig = plt.figure(figsize=(6, 4))
        x = np.arange(len(counts))
        plt.bar(x, counts.values, color=colors, edgecolor="black", alpha=0.85)
        plt.xticks(x, labels, rotation=10)
        plt.ylabel("Number of queries")
        plt.xlabel("Faithfulness band")
        plt.title(self._titled("Faithfulness bands across all queries"))
        for i, v in enumerate(counts.values):
            plt.text(i, v + 0.1, str(int(v)), ha="center", va="bottom", fontsize=9)
        plt.tight_layout()
        self._save_fig(fig, "faithfulness_bands_global")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_faithfulness_bands_by_model(self, df: pd.DataFrame) -> None:
        if "model" not in df.columns:
            return
        df_model = df.dropna(subset=["model"])
        if df_model.empty:
            return

        models = sorted(df_model["model"].unique())
        if len(models) <= 1:
            return

        df_model = df_model.copy()
        df_model["faith_band"] = df_model["faithfulness"].astype(float).apply(
            self._faithfulness_band
        )

        band_order = ["high", "medium", "low"]
        band_labels = ["High (≥ 0.70)", "Medium (0.40–0.69)", "Low (< 0.40)"]
        band_color_map = {
            "high": FAITH_GOOD_COLOR,
            "medium": FAITH_MED_COLOR,
            "low": FAITH_LOW_COLOR,
        }

        counts = (
            df_model.groupby(["model", "faith_band"])["query_id"]
            .count()
            .unstack(fill_value=0)
            .reindex(columns=band_order, fill_value=0)
        )

        x = np.arange(len(models))
        width = 0.22

        fig = plt.figure(figsize=(7, 4.5))
        for i, band in enumerate(band_order):
            offsets = x + (i - 1) * width
            plt.bar(
                offsets,
                counts[band].values,
                width=width,
                label=band_labels[i],
                color=band_color_map[band],
                edgecolor="black",
                alpha=0.9,
            )

        plt.xticks(x, models, rotation=10)
        plt.ylabel("Number of queries")
        plt.xlabel("LLM profile")
        plt.title(self._titled("Faithfulness band distribution by LLM"))
        plt.legend(frameon=False)
        plt.tight_layout()
        self._save_fig(fig, "faithfulness_bands_by_model")
        plt.close(fig)

    # ------------------------------------------------------------------
    def plot_scatter_correlation(self, df: pd.DataFrame) -> Tuple[float, float]:
        x = df["ndcg@k"].astype(float).values
        y = df["faithfulness"].astype(float).values
        mask = (~np.isnan(x)) & (~np.isnan(y))

        if mask.sum() < 3:
            rp = float("nan")
            rs = float("nan")
        else:
            rp = float(np.corrcoef(x[mask], y[mask])[0, 1])
            rs, _ = spearmanr(x[mask], y[mask])

        fig = plt.figure(figsize=(6, 4))
        plt.scatter(x[mask], y[mask], s=26, alpha=0.7, color=ACCENT_COLOR)
        plt.xlabel("NDCG@k")
        plt.ylabel("Faithfulness")
        plt.title(self._titled(f"Scatter NDCG@k vs Faithfulness (r={rp:.3f}, ρ={rs:.3f})"))
        plt.tight_layout()
        self._save_fig(fig, "scatter_ndcg_vs_faithfulness")
        plt.close(fig)
        return rp, rs

    # ------------------------------------------------------------------
    def plot_run_order_control(self, df: pd.DataFrame) -> None:
        df = df.reset_index(drop=True)
        df["idx"] = np.arange(len(df))

        for col, color in [("ndcg@k", PRIMARY_COLOR), ("faithfulness", SECONDARY_COLOR)]:
            vals = df[col].astype(float).values
            if len(vals) == 0:
                continue

            mu, sd = np.nanmean(vals), np.nanstd(vals)
            ucl, lcl = mu + 3 * sd, mu - 3 * sd

            fig = plt.figure(figsize=(8, 4))
            ax = plt.gca()
            ax.fill_between(df["idx"], mu - sd, mu + sd, color="gray", alpha=0.18)
            ax.plot(df["idx"], vals, marker="o", linestyle="-", color=color, linewidth=1.1)
            ax.axhline(mu, linestyle="--", color=color, linewidth=0.9)
            ax.axhline(ucl, linestyle=":", color="black", linewidth=0.8)
            ax.axhline(lcl, linestyle=":", color="black", linewidth=0.8)
            ax.set_xlabel("Run index")
            ax.set_ylabel(col)
            ax.set_title(self._titled(f"Run-order chart for {col}"))
            plt.tight_layout()
            self._save_fig(fig, f"run_order_{col}")
            plt.close(fig)

    # ------------------------------------------------------------------
    def detect_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        nd = df["ndcg@k"].astype(float).values
        fa = df["faithfulness"].astype(float).values

        mask = (
            self._outliers_iqr(nd)
            | self._outliers_z(nd)
            | self._outliers_iqr(fa)
            | self._outliers_z(fa)
        )
        return df[mask].copy()

    # ------------------------------------------------------------------
    def summarize(self, df: pd.DataFrame) -> Dict[str, Any]:
        nd = df["ndcg@k"].astype(float).values
        fa = df["faithfulness"].astype(float).values

        nd_m, nd_lo, nd_hi = self._bootstrap_ci(nd, self.cfg.bootstrap_iters)
        fa_m, fa_lo, fa_hi = self._bootstrap_ci(fa, self.cfg.bootstrap_iters)

        summary = {
            "files": int(df.shape[0]),
            "ndcg@k_mean": nd_m,
            "ndcg@k_ci95_lo": nd_lo,
            "ndcg@k_ci95_hi": nd_hi,
            "faith_mean": fa_m,
            "faith_ci95_lo": fa_lo,
            "faith_ci95_hi": fa_hi,
            "ndcg@k_median": float(np.nanmedian(nd)),
            "faith_median": float(np.nanmedian(fa)),
            "ndcg@k_std": float(np.nanstd(nd)),
            "faith_std": float(np.nanstd(fa)),
            "bootstrap_iters": self.cfg.bootstrap_iters,
            "iqr_k": self.cfg.iqr_k,
            "z_thresh": self.cfg.z_thresh,
            "faith_high_thr": self.cfg.faith_high_thr,
            "faith_mid_thr": self.cfg.faith_mid_thr,
        }

        (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
        return summary

    # ------------------------------------------------------------------
    def run_all(self) -> Dict[str, Any]:
        df = self._load_eval_rows()
        self._save_df("raw_eval", df)

        if df.empty:
            summary = {"files": 0}
            (self.out_dir / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
            return summary

        self.plot_ndcg_histogram(df)
        self.plot_faithfulness_bands_global(df)
        self.plot_faithfulness_bands_by_model(df)
        self.plot_scatter_correlation(df)
        self.plot_run_order_control(df)

        outliers = self.detect_outliers(df)
        self._save_df("outliers", outliers)

        summary = self.summarize(df)
        return summary
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\ground_truth_builder.py ==== 
# src/core/evaluation/ground_truth_builder.py
from __future__ import annotations
from typing import Dict, Any, List
import numpy as np
import logging

from sentence_transformers import SentenceTransformer, util

from src.core.config.config_loader import ConfigLoader
from src.core.evaluation.settings import (
    EvaluationSettings,
    DEFAULT_EVAL_SETTINGS,
    SimilarityBands,
)

logger = logging.getLogger("GroundTruthBuilder")


class GroundTruthBuilder:
    """
    Generates semantic ground truth labels for intrinsic retrieval evaluation (NDCG).
    The ground truth is computed by embedding the user query and measuring its similarity
    to each retrieved chunk. Higher similarity implies stronger relevance.
    """

    def __init__(
        self,
        config_path: str = "configs/embedding.yaml",
        settings: EvaluationSettings = DEFAULT_EVAL_SETTINGS,
        bands: SimilarityBands | None = None,
    ):
        # Load the same embedding model used in retrieval
        cfg = ConfigLoader(config_path).config
        model_name = cfg.get("options", {}).get("embedding_model", "multi-qa-mpnet-base-dot-v1")
        self.model = SentenceTransformer(model_name)

        # Inject similarity thresholds from global settings
        b = bands or settings.ground_truth.similarity_bands
        self.high_thr = b.high
        self.mid_thr = b.mid
        self.low_thr = b.low

    # ------------------------------------------------------------------
    def build(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, int]:
        # Construct graded relevance labels for each retrieved document
        if not query or not retrieved_docs:
            return {}

        q_emb = self.model.encode([query], normalize_embeddings=True)
        truth: Dict[str, int] = {}

        for d in retrieved_docs:
            text = d.get("text", "") or ""
            doc_id = d.get("id") or f"{d.get('metadata', {}).get('source_file')}"

            d_emb = self.model.encode([text], normalize_embeddings=True)
            sim = float(util.cos_sim(q_emb, d_emb)[0][0])

            if sim >= self.high_thr:
                rel = 3
            elif sim >= self.mid_thr:
                rel = 2
            elif sim >= self.low_thr:
                rel = 1
            else:
                rel = 0

            truth[doc_id] = rel

        logger.info(f"Semantic GT created (avg rel={np.mean(list(truth.values())):.2f})")
        return truth
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\multi_model_report_builder.py ==== 
from __future__ import annotations
import json
from pathlib import Path
from datetime import datetime
from typing import List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.platypus import (
    SimpleDocTemplate,
    Paragraph,
    Spacer,
    Image,
    Table,
    TableStyle,
    PageBreak,
)
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors

from src.core.evaluation.settings import DEFAULT_EVAL_SETTINGS

FAITH_COLORS = {
    "high": "#1a9850",
    "medium": "#fee08b",
    "low": "#d73027",
}

FAITH_LABELS = {
    "high": "High (≥0.70)",
    "medium": "Medium (0.40–0.69)",
    "low": "Low (<0.40)",
}


class MultiModelReportBuilder:
    """Aggregated PDF report comparing multiple LLMs on NDCG and faithfulness."""

    def __init__(self, base_dir: str = "data"):
        # Base directory scanned for eval_logs_* folders
        self.base = Path(base_dir)
        self.eval_dirs = sorted(self.base.glob("eval_logs_*"))
        self.out_dir = self.base / "model_comparison"
        self.out_dir.mkdir(parents=True, exist_ok=True)

        styles = getSampleStyleSheet()
        self.styleN = styles["Normal"]
        self.styleH = styles["Heading1"]
        self.styleH2 = styles["Heading2"]

    # ---------------------------------------------------------
    def _load_all_results(self) -> pd.DataFrame:
        rows: List[dict] = []
        for d in self.eval_dirs:
            model = d.name.replace("eval_logs_", "")
            for fp in d.glob("*_evaluation.json"):
                try:
                    x = json.loads(fp.read_text(encoding="utf-8"))
                    rows.append(
                        {
                            "query_id": x.get("query_id"),
                            "ndcg": float(x.get("ndcg@k", np.nan)),
                            "faith": float(x.get("faithfulness", np.nan)),
                            "model": model,
                        }
                    )
                except Exception:
                    continue
        return pd.DataFrame(rows)

    # ---------------------------------------------------------
    def _faith_band(self, v: float) -> str:
        if np.isnan(v):
            return "missing"
        high = DEFAULT_EVAL_SETTINGS.visualization.faith_band_high
        mid = DEFAULT_EVAL_SETTINGS.visualization.faith_band_mid
        if v >= high:
            return "high"
        if v >= mid:
            return "medium"
        return "low"

    # ---------------------------------------------------------
    def _plot_faithfulness_band_comparison(self, df: pd.DataFrame) -> Path:
        df = df.copy()
        df["band"] = df["faith"].apply(self._faith_band)
        bands = ["high", "medium", "low"]
        models = sorted(df["model"].unique())

        counts = (
            df.groupby(["model", "band"])["query_id"]
            .count()
            .unstack(fill_value=0)
            .reindex(columns=bands, fill_value=0)
        )

        x = np.arange(len(models))
        width = 0.22

        fig, ax = plt.subplots(figsize=(7, 4.5))

        for i, b in enumerate(bands):
            offsets = x + (i - 1) * width
            ax.bar(
                offsets,
                counts[b].values,
                width=width,
                color=FAITH_COLORS[b],
                edgecolor="black",
                alpha=0.9,
                label=FAITH_LABELS[b],
            )

        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=10)
        ax.set_ylabel("Number of queries")
        ax.set_title("Faithfulness band comparison across models")
        ax.legend(frameon=False)

        out = self.out_dir / "faithfulness_model_comparison.png"
        fig.tight_layout()
        fig.savefig(out, dpi=150, bbox_inches="tight")
        plt.close(fig)
        return out

    # ---------------------------------------------------------
    def _aggregate_stats(self, df: pd.DataFrame) -> list[list[str]]:
        rows = [
            [
                "Model",
                "Mean NDCG",
                "Mean Faith",
                "Median NDCG",
                "Median Faith",
                "Std NDCG",
                "Std Faith",
            ]
        ]

        for m in sorted(df["model"].unique()):
            d = df[df["model"] == m]
            rows.append(
                [
                    m,
                    f"{d['ndcg'].mean():.3f}",
                    f"{d['faith'].mean():.3f}",
                    f"{d['ndcg'].median():.3f}",
                    f"{d['faith'].median():.3f}",
                    f"{d['ndcg'].std():.3f}",
                    f"{d['faith'].std():.3f}",
                ]
            )
        return rows

    # ---------------------------------------------------------
    def build(self, name: str = "multi_model_benchmark_report.pdf") -> Path:
        pdf_path = self.out_dir / name
        df = self._load_all_results()

        if df.empty:
            raise ValueError("No evaluation files found for any model.")

        plot_path = self._plot_faithfulness_band_comparison(df)
        stats = self._aggregate_stats(df)

        doc = SimpleDocTemplate(
            str(pdf_path),
            pagesize=A4,
            leftMargin=2 * cm,
            rightMargin=2 * cm,
            topMargin=2 * cm,
            bottomMargin=2 * cm,
        )

        story = []

        story.append(Paragraph("<b>Multi-Model Benchmark Report</b>", self.styleH))
        story.append(Spacer(1, 0.3 * cm))
        story.append(
            Paragraph(
                f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                self.styleN,
            )
        )
        story.append(Spacer(1, 0.4 * cm))

        story.append(
            Paragraph(
                "This report compares multiple LLM profiles in terms of retrieval relevance "
                "(NDCG@k) and factual grounding (Faithfulness). All evaluations were performed "
                "using an identical prompt set, identical retrieval stack, and identical parameters.",
                self.styleN,
            )
        )
        story.append(PageBreak())

        story.append(Paragraph("1. Summary Statistics per Model", self.styleH))
        table = Table(stats, colWidths=[3.2 * cm] * 7)
        table.setStyle(
            TableStyle(
                [
                    ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                ]
            )
        )
        story.append(table)
        story.append(PageBreak())

        story.append(Paragraph("2. Faithfulness Band Comparison", self.styleH))
        story.append(Spacer(1, 0.2 * cm))
        story.append(Image(str(plot_path), width=15 * cm, height=9 * cm))
        story.append(PageBreak())

        story.append(Paragraph("End of Report", self.styleN))

        doc.build(story)
        return pdf_path
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\plot_style.py ==== 
"""
Unified visualization style for scientific evaluation plots.
Applies consistent layout, fonts, and color palette across all figures.
Follows principles from Tufte (1983), IEEE Vis Guidelines, and RSS Data Viz Guide (2023).
"""

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np


def apply_scientific_style() -> None:
    """Configure Matplotlib for publication-grade scientific figures."""
    mpl.rcParams.update(
        {
            "figure.dpi": 300,
            "savefig.dpi": 300,
            "savefig.format": "svg",
            "savefig.bbox": "tight",
            "font.family": "sans-serif",
            "font.sans-serif": ["Arial", "DejaVu Sans", "Liberation Sans"],
            "font.size": 10,
            "axes.labelsize": 10,
            "axes.titlesize": 11,
            "legend.fontsize": 9,
            "xtick.labelsize": 9,
            "ytick.labelsize": 9,
            "axes.linewidth": 0.8,
            "axes.grid": True,
            "grid.alpha": 0.25,
            "grid.linestyle": "--",
            "axes.spines.top": False,
            "axes.spines.right": False,
            "axes.prop_cycle": mpl.cycler(
                color=[
                    "#1b9e77",
                    "#d95f02",
                    "#7570b3",
                    "#e7298a",
                    "#66a61e",
                    "#e6ab02",
                ]
            ),
            "figure.figsize": (6, 4),
            "figure.autolayout": True,
            "savefig.transparent": False,
        }
    )


def annotate_sample_info(
    ax: plt.Axes,
    n: int | None = None,
    k: int | None = None,
    bootstrap_iters: int | None = None,
    show_conf_int: tuple[float, float] | None = None,
) -> None:
    """Add standardized annotation text (sample info, parameters) inside a figure."""
    txt_parts = []
    if n is not None:
        txt_parts.append(f"n={n}")
    if k is not None:
        txt_parts.append(f"k={k}")
    if bootstrap_iters is not None:
        txt_parts.append(f"boot={bootstrap_iters}")
    if show_conf_int is not None:
        lo, hi = show_conf_int
        txt_parts.append(f"95% CI=[{lo:.3f}, {hi:.3f}]")

    if not txt_parts:
        return

    ax.text(
        0.98,
        0.02,
        ", ".join(txt_parts),
        ha="right",
        va="bottom",
        fontsize=8,
        color="gray",
        alpha=0.85,
        transform=ax.transAxes,
        bbox=dict(
            facecolor="white",
            edgecolor="none",
            alpha=0.55,
            boxstyle="round,pad=0.2",
        ),
    )


def add_violin_overlay(ax: plt.Axes, data: np.ndarray, color: str = "#1b9e77") -> None:
    """
    Add a violin-style density overlay (no seaborn dependency).
    Uses kernel density estimation for smooth distribution visualization.
    """
    from scipy.stats import gaussian_kde

    if len(data) < 5:
        return
    data = np.asarray(data, dtype=float)
    data = data[~np.isnan(data)]
    if data.size == 0:
        return

    dmin, dmax = float(np.min(data)), float(np.max(data))
    if dmax - dmin < 1e-6:
        dmin -= 1e-3
        dmax += 1e-3

    kde = gaussian_kde(data)
    xs = np.linspace(dmin, dmax, 200)
    ys = kde(xs)
    ys = ys / ys.max() * 0.25

    ax.fill_betweenx(xs, -ys, ys, facecolor=color, alpha=0.18, linewidth=0)
    ax.plot(ys, xs, color=color, alpha=0.5, linewidth=0.6)
    ax.plot(-ys, xs, color=color, alpha=0.5, linewidth=0.6)

    cur_xlim = ax.get_xlim()
    pad = (cur_xlim[1] - cur_xlim[0]) * 0.05
    ax.set_xlim(cur_xlim[0] - pad, cur_xlim[1] + pad)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\report_builder.py ==== 
from __future__ import annotations
import logging
from pathlib import Path
from datetime import datetime

from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.platypus import (
    SimpleDocTemplate,
    Paragraph,
    Spacer,
    Image,
    Table,
    TableStyle,
    PageBreak,
)
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors

import json
import pandas as pd
import numpy as np
from scipy.stats import pearsonr, spearmanr

from src.core.evaluation.settings import DEFAULT_EVAL_SETTINGS

logger = logging.getLogger("ReportBuilder")


class ReportBuilder:
    """Statistical PDF report: summary stats, correlations, outliers, and charts."""

    def __init__(self, charts_dir: str = "data/eval_charts", eval_dir: str | None = None):
        # Base folder for figures and summary.json
        self.charts_dir = Path(charts_dir)

        # Evaluation-log directory (robust for eval_logs_*)
        if eval_dir:
            self.eval_dir = Path(eval_dir)
        else:
            parent = self.charts_dir.parent
            candidates = sorted(parent.glob("eval_logs_*"))
            if candidates:
                self.eval_dir = candidates[-1]
            else:
                self.eval_dir = parent / "eval_logs"

        styles = getSampleStyleSheet()
        self.styleN = styles["Normal"]
        self.styleH = styles["Heading1"]
        self.styleH2 = styles["Heading2"]

    # ------------------------------------------------------------------
    def _load_summary(self) -> dict:
        summary_path = self.charts_dir / "summary.json"
        if not summary_path.exists():
            raise FileNotFoundError(f"summary.json missing in {self.charts_dir}")
        return json.loads(summary_path.read_text(encoding="utf-8"))

    # ------------------------------------------------------------------
    def _load_detailed_results(self) -> pd.DataFrame:
        records = []
        for fp in sorted(self.eval_dir.glob("*_evaluation.json")):
            try:
                d = json.loads(fp.read_text(encoding="utf-8"))
                records.append(
                    {
                        "query_id": d.get("query_id", fp.stem),
                        "ndcg@k": float(d.get("ndcg@k", np.nan)),
                        "faithfulness": float(d.get("faithfulness", np.nan)),
                    }
                )
            except Exception:
                continue
        df = pd.DataFrame(records)
        return df.dropna(subset=["ndcg@k", "faithfulness"])

    # ------------------------------------------------------------------
    def _compute_correlations(self, df: pd.DataFrame) -> dict:
        if df.empty:
            return {}
        x = df["ndcg@k"].astype(float)
        y = df["faithfulness"].astype(float)
        pr, pp = pearsonr(x, y)
        sr, sp = spearmanr(x, y)
        return {
            "pearson_r": pr,
            "pearson_p": pp,
            "spearman_rho": sr,
            "spearman_p": sp,
        }

    # ------------------------------------------------------------------
    def _detect_outliers(self, df: pd.DataFrame, z_thresh: float | None = None) -> pd.DataFrame:
        if df.empty:
            return df
        eps = 1e-6
        thr = z_thresh or DEFAULT_EVAL_SETTINGS.visualization.z_thresh
        df = df.copy()
        df["z_ndcg"] = (df["ndcg@k"] - df["ndcg@k"].mean()) / (df["ndcg@k"].std() + eps)
        df["z_faith"] = (df["faithfulness"] - df["faithfulness"].mean()) / (
            df["faithfulness"].std() + eps
        )
        df["is_outlier"] = (df["z_ndcg"].abs() > thr) | (df["z_faith"].abs() > thr)
        return df[df["is_outlier"]]

    # ------------------------------------------------------------------
    def _find_charts(self) -> list[Path]:
        return sorted(self.charts_dir.glob("*.png"))

    # ------------------------------------------------------------------
    def build(self, custom_name: str | None = None) -> Path:
        summary = self._load_summary()
        n = summary.get("files", 0)

        pdf_name = custom_name or f"benchmark_report_n{n}.pdf"
        pdf_path = self.charts_dir / pdf_name

        doc = SimpleDocTemplate(
            str(pdf_path),
            pagesize=A4,
            rightMargin=2 * cm,
            leftMargin=2 * cm,
            topMargin=2 * cm,
            bottomMargin=2 * cm,
        )
        story = []

        story.append(Paragraph("<b>Statistical Benchmark Report</b>", self.styleH))
        story.append(Spacer(1, 0.4 * cm))
        story.append(
            Paragraph(
                f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                self.styleN,
            )
        )
        story.append(Spacer(1, 0.4 * cm))
        story.append(
            Paragraph(
                "This report provides quantitative evaluation results for NDCG@k and faithfulness, "
                "correlation diagnostics, outlier detection, and statistical visualizations.",
                self.styleN,
            )
        )
        story.append(PageBreak())

        story.append(Paragraph("1. Summary Statistics", self.styleH))
        story.append(Spacer(1, 0.2 * cm))

        rows = []
        for k, v in summary.items():
            label = k.replace("_", " ")
            if isinstance(v, (int, float)):
                rows.append([label, f"{v:.4f}"])
            else:
                rows.append([label, str(v)])

        table = Table(rows, colWidths=[8 * cm, 6 * cm])
        table.setStyle(
            TableStyle(
                [
                    ("BACKGROUND", (0, 0), (1, 0), colors.lightgrey),
                    ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                    ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                ]
            )
        )
        story.append(table)
        story.append(PageBreak())

        df = self._load_detailed_results()
        if not df.empty:
            story.append(Paragraph("2. Correlation and Outlier Analysis", self.styleH))
            story.append(Spacer(1, 0.2 * cm))

            corr = self._compute_correlations(df)
            if corr:
                crows = [
                    ["Metric Pair", "Correlation", "p-value"],
                    ["Pearson r", f"{corr['pearson_r']:.3f}", f"{corr['pearson_p']:.3e}"],
                    ["Spearman ρ", f"{corr['spearman_rho']:.3f}", f"{corr['spearman_p']:.3e}"],
                ]
                ct = Table(crows, colWidths=[6 * cm, 4 * cm, 4 * cm])
                ct.setStyle(
                    TableStyle(
                        [
                            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                            ("ALIGN", (0, 0), (-1, -1), "CENTER"),
                        ]
                    )
                )
                story.append(ct)
                story.append(Spacer(1, 0.3 * cm))

            out = self._detect_outliers(df)
            if not out.empty:
                story.append(Paragraph("Detected Outliers", self.styleH2))
                story.append(Spacer(1, 0.2 * cm))

                trows = [["Query", "NDCG@k", "Faithfulness", "z_ndcg", "z_faith"]]
                for _, r in out.iterrows():
                    qid = r["query_id"]
                    wrapped = "\n".join([qid[i : i + 45] for i in range(0, len(qid), 45)])
                    trows.append(
                        [
                            wrapped,
                            f"{r['ndcg@k']:.3f}",
                            f"{r['faithfulness']:.3f}",
                            f"{r['z_ndcg']:.2f}",
                            f"{r['z_faith']:.2f}",
                        ]
                    )

                ot = Table(
                    trows,
                    colWidths=[8 * cm, 2.4 * cm, 2.4 * cm, 2.0 * cm, 2.0 * cm],
                )
                ot.setStyle(
                    TableStyle(
                        [
                            ("BACKGROUND", (0, 0), (-1, 0), colors.lightgrey),
                            ("GRID", (0, 0), (-1, -1), 0.5, colors.grey),
                            ("ALIGN", (1, 1), (-1, -1), "CENTER"),
                            ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                            ("FONTSIZE", (0, 0), (-1, -1), 8),
                        ]
                    )
                )
                story.append(ot)
            else:
                story.append(Paragraph("No outliers detected.", self.styleN))

            story.append(PageBreak())

        images = self._find_charts()
        if images:
            story.append(Paragraph("3. Visual Analytics", self.styleH))
            story.append(Spacer(1, 0.2 * cm))
            for img in images:
                story.append(Paragraph(img.name.replace("_", " "), self.styleH2))
                story.append(Image(str(img), width=15 * cm, height=9 * cm))
                story.append(Spacer(1, 0.4 * cm))

        doc.build(story)
        logger.info(f"PDF generated → {pdf_path}")
        return pdf_path
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\settings.py ==== 
from __future__ import annotations
from dataclasses import dataclass, field


@dataclass(frozen=True)
class SimilarityBands:
    # Shared container for high/mid/low similarity thresholds
    high: float
    mid: float
    low: float


@dataclass(frozen=True)
class FaithfulnessSettings:
    # Configuration for the faithfulness metric
    ent_model: str = "cross-encoder/nli-deberta-base"
    emb_model: str = "multi-qa-mpnet-base-dot-v1"
    entailment_low_cutoff: float = 0.20
    emb_bands: SimilarityBands = field(
        default_factory=lambda: SimilarityBands(high=0.55, mid=0.35, low=0.0)
    )
    top_k: int = 3
    specificity_penalty: float = 0.15
    temporal_penalty: float = 0.15


@dataclass(frozen=True)
class GroundTruthSettings:
    # Configuration for semantic ground truth construction
    similarity_bands: SimilarityBands = field(
        default_factory=lambda: SimilarityBands(high=0.40, mid=0.25, low=0.10)
    )


@dataclass(frozen=True)
class AutoGroundTruthSettings:
    # Configuration for automatic answer-conditioned ground truth
    model_name: str = "multi-qa-mpnet-base-dot-v1"
    similarity_bands: SimilarityBands = field(
        default_factory=lambda: SimilarityBands(high=0.30, mid=0.15, low=0.07)
    )


@dataclass(frozen=True)
class VisualizationSettings:
    # Configuration for evaluation visualizations and band definitions
    faith_band_high: float = 0.70
    faith_band_mid: float = 0.40
    bootstrap_iters: int = 2000
    random_seed: int = 42
    iqr_k: float = 1.5
    z_thresh: float = 3.0


@dataclass(frozen=True)
class EvaluationSettings:
    # Global evaluation configuration passed into orchestrators and metrics
    ndcg_k: int = 10
    faithfulness: FaithfulnessSettings = field(default_factory=FaithfulnessSettings)
    ground_truth: GroundTruthSettings = field(default_factory=GroundTruthSettings)
    auto_gt: AutoGroundTruthSettings = field(default_factory=AutoGroundTruthSettings)
    visualization: VisualizationSettings = field(default_factory=VisualizationSettings)


# Default singleton style settings object
DEFAULT_EVAL_SETTINGS = EvaluationSettings()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\utils.py ==== 
from __future__ import annotations
from typing import Dict, Any
import hashlib


def make_chunk_id(chunk: Dict[str, Any]) -> str:
    # Create a deterministic chunk identifier based on metadata and text prefix
    meta = chunk.get("metadata", {}) or {}
    src = meta.get("source_file") or meta.get("title") or "unknown"
    year = meta.get("year", "na")
    text = (chunk.get("text") or "")[:200]
    h = hashlib.sha1(text.encode("utf-8", errors="ignore")).hexdigest()[:12]
    return f"{src}::{year}::{h}"
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\i_metric.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict


class IMetric(ABC):
    """Interface defining a unified metric contract for all evaluators."""

    @abstractmethod
    def compute(self, **kwargs: Any) -> float:
        """Compute the metric based on explicit input parameters."""
        pass

    @abstractmethod
    def describe(self) -> Dict[str, str]:
        """Return metadata about the metric (name, type, short description)."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\faithfulness_metric.py ==== 
# src/core/evaluation/metrics/faithfulness_metric.py
from __future__ import annotations
from typing import List, Dict, Any
import numpy as np
import re
import spacy

from sentence_transformers import SentenceTransformer, CrossEncoder
from sentence_transformers.util import cos_sim

from src.core.evaluation.settings import EvaluationSettings, DEFAULT_EVAL_SETTINGS


# Load spaCy model once (NER + sentence segmentation)
try:
    NLP = spacy.load("en_core_web_sm")
except Exception:
    NLP = None


class FaithfulnessMetric:
    """
    Claim-level, evidence-based faithfulness metric combining:
    - Cross-encoder NLI entailment for robust evidence validation
    - Embedding similarity fallback for borderline segments
    - Top-k evidence aggregation
    - Specificity penalty (excessive ungrounded detail)
    - Temporal consistency penalty (year/decade mismatch)
    """

    def __init__(self, settings: EvaluationSettings = DEFAULT_EVAL_SETTINGS):
        # Store settings locally for easier inspection
        self.settings = settings
        cfg = settings.faithfulness

        # Main entailment model (entails/neutral/contradiction)
        self.cross = CrossEncoder(cfg.ent_model)

        # Embedding model fallback
        self.emb = SentenceTransformer(cfg.emb_model)

        # Thresholds for embedding fallback
        self.high_thr = cfg.emb_bands.high
        self.mid_thr = cfg.emb_bands.mid
        self.low_thr = cfg.emb_bands.low

        # Evidence aggregation and penalties
        self.top_k = cfg.top_k
        self.entailment_low_cutoff = cfg.entailment_low_cutoff
        self.w_spec = cfg.specificity_penalty
        self.w_temp = cfg.temporal_penalty

    # ----------------------------------------------------------
    def _extract_claims(self, answer: str) -> List[str]:
        """Split answer into minimal evidence-checkable claims."""
        if not answer:
            return []
        if NLP:
            doc = NLP(answer)
            sents = [s.text.strip() for s in doc.sents]
        else:
            sents = re.split(r"[.!?]\s+", answer)

        claims = []
        for s in sents:
            s_clean = s.strip()
            if len(s_clean.split()) >= 3:
                claims.append(s_clean)
        return claims

    # ----------------------------------------------------------
    def _claim_date(self, claim: str) -> int | None:
        """Extract explicit years from claim (e.g. 1998, 2020)."""
        yrs = re.findall(r"\b(19\d{2}|20\d{2})\b", claim)
        return int(yrs[0]) if yrs else None

    # ----------------------------------------------------------
    def _specificity_score(self, claim: str) -> float:
        """Quantify factual specificity based on NER and numeric density."""
        if not NLP:
            nums = len(re.findall(r"\d+", claim))
            ents = len(re.findall(r"[A-Z][a-z]+", claim))
            return (nums + ents) / max(5, len(claim.split()))

        doc = NLP(claim)
        nums = sum(1 for t in doc if t.like_num)
        ents = len(doc.ents)
        return (nums + ents) / max(5, len(doc))

    # ----------------------------------------------------------
    def _entailment_score(self, claim: str, chunks: List[str]) -> float:
        """Compute aggregated entailment probability over all chunks."""
        if not chunks:
            return 0.0

        pairs = [(claim, c) for c in chunks]
        preds = self.cross.predict(pairs, apply_softmax=True)
        entail_probs = np.array([p[0] for p in preds])
        topk = np.sort(entail_probs)[-self.top_k :]
        return float(np.mean(topk))

    # ----------------------------------------------------------
    def _embedding_fallback(self, claim: str, chunks: List[str]) -> float:
        """Fallback evidence score using embedding similarity."""
        if not chunks:
            return 0.0

        c_emb = self.emb.encode([claim], normalize_embeddings=True)
        ch_emb = self.emb.encode(chunks, normalize_embeddings=True)

        sims = cos_sim(c_emb, ch_emb)[0].cpu().numpy()
        topk = np.sort(sims)[-self.top_k :]
        s = float(np.mean(topk))

        if s >= self.high_thr:
            return 1.0
        if s >= self.mid_thr:
            return 0.5
        if s >= self.low_thr:
            return 0.25
        return 0.0

    # ----------------------------------------------------------
    def _temporal_penalty(self, claim_year: int | None) -> float:
        """Temporal penalty hook for potential future decade-aware scoring."""
        if claim_year is None:
            return 0.0
        return 0.0

    # ----------------------------------------------------------
    def compute(self, context_chunks: List[str], answer: str) -> float:
        """Main faithfulness routine combining entailment, penalties and fallback."""
        if not context_chunks or not answer:
            return 0.0

        claims = self._extract_claims(answer)
        if not claims:
            return 0.0

        scores: List[float] = []

        for claim in claims:
            claim_year = self._claim_date(claim)
            specificity = self._specificity_score(claim)

            ent = self._entailment_score(claim, context_chunks)
            if ent < self.entailment_low_cutoff:
                ent = self._embedding_fallback(claim, context_chunks)

            # Specificity penalty scales with factual density
            spec_pen = min(1.0, specificity) * self.w_spec

            # Temporal penalty currently a no-op but kept for extension
            temp_pen = self._temporal_penalty(claim_year)

            raw_score = max(0.0, ent - spec_pen - temp_pen)
            scores.append(raw_score)

        return float(np.mean(scores))

    # ----------------------------------------------------------
    def describe(self) -> Dict[str, str]:
        return {
            "name": "FaithfulnessV2",
            "type": "extrinsic",
            "description": (
                "Claim-level factual evaluation using cross-encoder entailment, "
                "top-k evidence aggregation, specificity and temporal penalties."
            ),
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\ndcg_metric.py ==== 
from __future__ import annotations
import math
from typing import List, Dict
from src.core.evaluation.interfaces.i_metric import IMetric


class NDCGMetric(IMetric):
    """Normalized Discounted Cumulative Gain (intrinsic retrieval metric)."""

    def __init__(self, k: int):
        # Store cut-off rank for NDCG computation
        self.k = k

    def compute(self, relevance_scores: List[int]) -> float:
        """Compute NDCG@k from graded relevance list."""

        def dcg(scores: List[int]) -> float:
            # Compute discounted cumulative gain up to rank k
            return sum(s / math.log2(i + 2) for i, s in enumerate(scores[: self.k]))

        if not relevance_scores:
            return 0.0

        ideal = sorted(relevance_scores, reverse=True)
        idcg = dcg(ideal)
        return (dcg(relevance_scores) / idcg) if idcg > 0 else 0.0

    def describe(self) -> Dict[str, str]:
        return {
            "name": "NDCG@k",
            "type": "intrinsic",
            "description": "Measures retrieval ranking quality using graded relevance with logarithmic discount.",
        }
.
==== C:\Users\katha\historical-drift-analyzer\src\core\evaluation\metrics\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestion_orchestrator.py ==== 
from __future__ import annotations
import logging
import json
from pathlib import Path
from typing import Any, Dict

from src.core.config.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.parser.parser_factory import ParserFactory
from src.core.ingestion.metadata.metadata_extractor_factory import MetadataExtractorFactory
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner
from src.core.ingestion.chunking.chunking_orchestrator import ChunkingOrchestrator


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration (merge phase + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/ingestion.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("IngestionOrchestrator")
    logger.info("Starting ingestion pipeline")

    # ------------------------------------------------------------------
    # 2. Initialize core components
    # ------------------------------------------------------------------
    parser_factory = ParserFactory(cfg, logger=logger)
    metadata_factory = MetadataExtractorFactory.from_config(cfg)
    cleaner = RagTextCleaner.default()
    chunking_orchestrator = ChunkingOrchestrator(config=cfg)

    active_metadata_fields = opts.get("metadata_fields", [])
    parallelism = opts.get("parallelism", "auto")

    # ------------------------------------------------------------------
    # 3. Resolve all paths
    # ------------------------------------------------------------------
    paths = cfg.get("paths", {})
    raw_dir = Path(paths.get("raw_pdfs", "data/raw_pdfs")).resolve()
    parsed_dir = Path(paths.get("parsed", "data/processed/parsed")).resolve()
    cleaned_dir = Path(paths.get("cleaned", "data/processed/cleaned")).resolve()
    metadata_dir = Path(paths.get("metadata", "data/processed/metadata")).resolve()
    chunks_dir = Path(paths.get("chunks", "data/processed/chunks")).resolve()

    for d in [parsed_dir, cleaned_dir, metadata_dir, chunks_dir]:
        ensure_dir(d)

    pdf_files = sorted(raw_dir.glob("*.pdf"))
    if not pdf_files:
        logger.warning(f"No PDF files found in {raw_dir}")
        return
    logger.info(f"Found {len(pdf_files)} PDF(s) in {raw_dir}")

    # ------------------------------------------------------------------
    # 4. Parallel or sequential mode
    # ------------------------------------------------------------------
    use_parallel = (
        isinstance(parallelism, int) and parallelism > 1
    ) or (isinstance(parallelism, str) and parallelism.lower() == "auto")

    if use_parallel:
        try:
            parallel_parser = parser_factory.create_parallel_parser()
            logger.info("Running ingestion in parallel mode ...")
            results = parallel_parser.parse_all(raw_dir, parsed_dir)

            for res in results:
                if "text" not in res:
                    continue
                res["text"] = cleaner.clean(res["text"])
                res["chunks"] = chunking_orchestrator.process(res["text"], metadata=res.get("metadata", {}))

                pdf_name = Path(res["metadata"]["source_file"]).stem
                pdf_path = raw_dir / f"{pdf_name}.pdf"
                all_meta = metadata_factory.extract_all(str(pdf_path))
                filtered_meta = {k: v for k, v in all_meta.items()
                                 if not active_metadata_fields or k in active_metadata_fields}
                res["metadata"].update(filtered_meta)

                # Save outputs
                (parsed_dir / f"{pdf_name}.parsed.json").write_text(
                    json.dumps({"text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (cleaned_dir / f"{pdf_name}.cleaned.json").write_text(
                    json.dumps({"cleaned_text": res["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (metadata_dir / f"{pdf_name}.metadata.json").write_text(
                    json.dumps(res["metadata"], ensure_ascii=False, indent=2), encoding="utf-8"
                )
                (chunks_dir / f"{pdf_name}.chunks.json").write_text(
                    json.dumps({"chunks": res["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
                )

            logger.info(f"Parallel ingestion completed ({len(results)} file(s)).")
            return
        except Exception as e:
            logger.error(f"Parallel ingestion failed → fallback to sequential: {e}")

    # ------------------------------------------------------------------
    # 5. Sequential fallback mode
    # ------------------------------------------------------------------
    parser = parser_factory.create_parser()
    for pdf in pdf_files:
        logger.info(f"Parsing {pdf.name} ...")
        try:
            parsed_result = parser.parse(str(pdf))
            if "text" not in parsed_result:
                continue

            parsed_result["text"] = cleaner.clean(parsed_result["text"])
            parsed_result["chunks"] = chunking_orchestrator.process(
                parsed_result["text"], metadata=parsed_result.get("metadata", {})
            )

            base_metadata = parsed_result.get("metadata", {})
            all_metadata = metadata_factory.extract_all(str(pdf))
            if active_metadata_fields:
                all_metadata = {k: v for k, v in all_metadata.items() if k in active_metadata_fields}
            base_metadata.update(all_metadata)
            parsed_result["metadata"] = base_metadata

            # Write outputs
            (parsed_dir / f"{pdf.stem}.parsed.json").write_text(
                json.dumps({"text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (cleaned_dir / f"{pdf.stem}.cleaned.json").write_text(
                json.dumps({"cleaned_text": parsed_result["text"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (metadata_dir / f"{pdf.stem}.metadata.json").write_text(
                json.dumps(base_metadata, ensure_ascii=False, indent=2), encoding="utf-8"
            )
            (chunks_dir / f"{pdf.stem}.chunks.json").write_text(
                json.dumps({"chunks": parsed_result["chunks"]}, ensure_ascii=False, indent=2), encoding="utf-8"
            )

            logger.info(f"Completed {pdf.name}")
        except Exception as e:
            logger.error(f"Failed to parse {pdf.name}: {e}")

    # ------------------------------------------------------------------
    # 6. Finish
    # ------------------------------------------------------------------
    logger.info("Ingestion complete.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\ingestor.py ==== 
# src/core/ingestion/ingestor.py
from __future__ import annotations

from pathlib import Path
from typing import Dict, Any, List, Optional

from docling.document_converter import DocumentConverter
from src.core.ingestion.interfaces.i_ingestor import IIngestor


class Ingestor(IIngestor):
    # Handles PDF ingestion using Docling (no chunking)
    def __init__(self, enable_ocr: bool = True):
        self.enable_ocr = enable_ocr
        self.converter = DocumentConverter()

    def ingest(self, source_path: str) -> Dict[str, Any]:
        pdf_path = Path(source_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        # Perform robust conversion using Docling
        result = self.converter.convert(str(pdf_path))
        document = getattr(result, "document", None)

        # Extract plain text (fallback if needed)
        text = ""
        if document:
            if hasattr(document, "export_to_markdown"):
                text = document.export_to_markdown()
            elif hasattr(document, "text"):
                text = document.text

        # Build metadata
        metadata = {
            "source_file": pdf_path.name,
            "source_path": str(pdf_path),
            "page_count": self._get_page_count(document),
            "has_ocr": self._has_ocr(document),
            "toc": self._extract_toc(document),
        }

        # Single-chunk representation (no splitting yet)
        chunks = [{"text": text, "page": None, "bbox": None}] if text else []

        return {"metadata": metadata, "chunks": chunks}

    # -------------------------------------------------------
    def _get_page_count(self, document: Any) -> Optional[int]:
        if document is None:
            return None
        return len(getattr(document, "pages", []))

    def _has_ocr(self, document: Any) -> bool:
        if document is None:
            return False
        return bool(getattr(document, "ocr_content", None))

    def _extract_toc(self, document: Any) -> List[Dict[str, Any]]:
        toc: List[Dict[str, Any]] = []
        if document and hasattr(document, "outline_items") and document.outline_items:
            for item in document.outline_items:
                toc.append({
                    "title": getattr(item, "title", ""),
                    "page": getattr(item, "page_number", None),
                })
        return toc
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata_merger.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\adaptive_chunker.py ==== 
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor
import spacy
import logging

class AdaptiveChunker(IChunker):
    """Chunker that adapts to the content structure by splitting at semantic breaks."""

    def __init__(self,
                 chunk_size: int = 500,
                 overlap: int = 200,
                 min_chunk_length: int = 400,
                 pdf_path: Optional[str] = None,
                 page_number_extractor: Optional[IPageNumberExtractor] = None,
                 text_length: Optional[int] = None):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length
        self.pdf_path = pdf_path
        self.page_number_extractor = page_number_extractor
        self.text_length = text_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

        # Extract page count if available
        if pdf_path and page_number_extractor:
            try:
                self.page_count = self.page_number_extractor.extract_page_number(pdf_path)
                self.adjust_chunking_based_on_page_count()
            except Exception as e:
                logging.warning(f"Error extracting page count from PDF: {e}")
                self.page_count = None
                self.chunk_size = 1000  # Fallback

        if text_length:
            self.adjust_chunking_based_on_text_length(text_length)

    def adjust_chunking_based_on_page_count(self):
        """Adjust chunk size and overlap based on the page count."""
        if self.page_count:
            if self.page_count > 50:
                self.chunk_size = 1000
                self.overlap = 700
            elif self.page_count > 30:
                self.chunk_size = 1500
                self.overlap = 500
            elif self.page_count > 10:
                self.chunk_size = 2000
                self.overlap = 300
            else:
                self.chunk_size = 2500
                self.overlap = 200
        else:
            self.chunk_size = 1000
            self.overlap = 200

    def adjust_chunking_based_on_text_length(self, text_length: int):
        """Adjust chunk size and overlap based on the length of the text."""
        if text_length:
            if text_length > 5000:
                self.chunk_size = 1000
                self.overlap = 600
            elif text_length > 2000:
                self.chunk_size = 1500
                self.overlap = 400
            else:
                self.chunk_size = 2000
                self.overlap = 200

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Chunk the text into adaptive chunks based on content size and overlap."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)  # Use spaCy to process the text
        sentences = [sent.text.strip() for sent in doc.sents]  # Extract sentences from the spaCy doc

        for sentence in sentences:
            # If the current chunk plus sentence exceeds chunk size, store the chunk and start a new one
            if len(current_chunk) + len(sentence) > self.chunk_size:
                if current_chunk:  # Prevent empty chunks
                    chunks.append({
                        "text": current_chunk.strip(),
                        # "chunk_size": len(current_chunk.strip()),  # Removed from output
                        # "overlap": self.overlap  # Removed from output
                    })
                current_chunk = sentence
            else:
                current_chunk += " " + sentence

            # Merge chunks if they are too small
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                # "chunk_size": len(current_chunk.strip()),  # Removed from output
                # "overlap": self.overlap  # Removed from output
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\chunking_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Any, Dict, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
from src.core.ingestion.chunking.adaptive_chunker import AdaptiveChunker
from src.core.ingestion.chunking.static_chunker import StaticChunker
from src.core.ingestion.metadata.implementations.page_number_extractor import PageNumberExtractor


class ChunkingOrchestrator:
    """Handles the selection and execution of chunking strategies based on YAML config."""

    def __init__(self, config: Dict[str, Any], pdf_path: Optional[str] = None):
        """Initialize with YAML configuration and an optional PDF path."""
        self.config = config
        self.pdf_path = pdf_path
        
        # Überprüfen, ob der 'chunking' Abschnitt in der Konfiguration vorhanden ist
        if "chunking" not in self.config:
            raise KeyError("The 'chunking' section is missing in the configuration file")

        # Debugging: Gibt den 'chunking' Abschnitt aus
        logging.debug(f"Chunking config: {self.config['chunking']}")

        # Wählt die passende Chunking-Strategie aus der Konfiguration
        self.chunker = self.select_chunker()  

    def select_chunker(self) -> IChunker:
        """Select chunking strategy based on the configuration."""
        chunking_config = self.config["chunking"]  # Zugriff auf den 'chunking'-Abschnitt der Konfiguration
        chunking_mode = chunking_config["mode"]
        chunk_size = chunking_config["chunk_size"]  # Get the chunk size from config
        overlap = chunking_config["overlap"]
        enable_overlap = chunking_config["enable_overlap"]
        min_chunk_length = chunking_config["min_chunk_length"]
        sentence_boundary_detection = chunking_config["sentence_boundary_detection"]
        merge_short_chunks = chunking_config["merge_short_chunks"]

        # Erstelle die Chunker-Instanz basierend auf dem gewählten Modus
        if chunking_mode == "adaptive":
            # Instanziiere AdaptiveChunker mit den relevanten Konfigurationswerten
            page_number_extractor = PageNumberExtractor() if self.pdf_path else None
            return AdaptiveChunker(
                chunk_size=chunk_size,
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
                pdf_path=self.pdf_path,
                page_number_extractor=page_number_extractor,
            )
        elif chunking_mode == "static":
            # Instanziiere StaticChunker mit den relevanten Konfigurationswerten
            return StaticChunker(
                chunk_size=chunk_size,  # Pass the chunk_size from config
                overlap=overlap if enable_overlap else 0,
                min_chunk_length=min_chunk_length,
            )
        else:
            raise ValueError(f"Unknown chunking strategy: {chunking_mode}")

    def process(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Process the text and return chunks with metadata using the selected chunking strategy."""
        return self.chunker.chunk(text, metadata)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\i_chunker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IChunker(ABC):
    """Interface for all chunking strategies."""

    @abstractmethod
    def chunk(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Split the cleaned text into smaller chunks.
        
        Each returned item should be a dictionary with:
        - "text": the chunked text as a string.
        - "metadata": additional information (e.g., document info, chunking context).
        
        Args:
            text: The cleaned text to be chunked.
            metadata: Optional metadata related to the chunking process. Default is an empty dictionary.
        
        Returns:
            A list of dictionaries, each containing:
            - "text": A chunk of the input text.
            - "metadata": The metadata associated with the chunk.
        """
        if metadata is None:
            metadata = {}
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\static_chunker.py ==== 
# src/core/ingestion/chunking/static_chunker.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from src.core.ingestion.chunking.i_chunker import IChunker
import spacy

class StaticChunker(IChunker):
    """Chunker that uses fixed chunk size and overlap for chunking."""

    def __init__(self, 
                 chunk_size: int,  # Configuration-based chunk size
                 overlap: int, 
                 min_chunk_length: int):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_length = min_chunk_length

        # Initialize spaCy NLP model for sentence segmentation
        self.nlp = spacy.load("en_core_web_sm")

    def chunk(self, text: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Split text into overlapping fixed-size chunks with sentence boundaries."""
        chunks = []
        current_chunk = ""

        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]

        for sentence in sentences:
            if len(current_chunk) + len(sentence) + 1 <= self.chunk_size:
                current_chunk += ". " + sentence
            else:
                chunks.append({
                    "text": current_chunk.strip(),
                    "chunk_size": len(current_chunk.strip()),  # Display actual chunk length
                    "configured_chunk_size": self.chunk_size,   # Config value for comparison
                    "overlap": self.overlap                    # Configured overlap
                })
                current_chunk = sentence

            # Merge small chunks if necessary
            if len(current_chunk) < self.min_chunk_length and chunks:
                last_chunk = chunks[-1]
                last_chunk["text"] += " " + current_chunk
                last_chunk["chunk_size"] = len(last_chunk["text"])
                current_chunk = ""

        if current_chunk:
            chunks.append({
                "text": current_chunk.strip(),
                "chunk_size": len(current_chunk.strip()),
                "configured_chunk_size": self.chunk_size,
                "overlap": self.overlap
            })

        return chunks
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\utils_chunking.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\chunking\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\base_cleaner.py ==== 
from __future__ import annotations
from typing import final
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner


class BaseTextCleaner(ITextCleaner):
    """Base class providing a stable clean(...) interface."""

    @final
    def clean(self, text: str) -> str:
        # Guarantee non-null string
        if not text:
            return ""
        return self._clean_impl(text)

    def _clean_impl(self, text: str) -> str:
        # Must be implemented by subclasses
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\cleaner_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict

from src.core.ingestion.config_loader import ConfigLoader
from src.core.ingestion.utils.file_utils import ensure_dir
from src.core.ingestion.cleaner.rag_text_cleaner import RagTextCleaner


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load configuration
    # ------------------------------------------------------------------
    cfg = ConfigLoader("configs/ingestion.yaml").config
    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)

    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger = logging.getLogger("CleanerOrchestrator")
    logger.info("Starting cleaning phase")

    # ------------------------------------------------------------------
    # 2. Resolve paths
    # ------------------------------------------------------------------
    parsed_dir = Path(cfg["paths"]["parsed"]).resolve()
    cleaned_dir = Path(cfg["paths"].get("cleaned", "data/processed/cleaned")).resolve()
    ensure_dir(cleaned_dir)

    parsed_files = sorted(parsed_dir.glob("*.parsed.json"))
    if not parsed_files:
        logger.warning(f"No parsed files found in {parsed_dir}")
        return

    logger.info(f"Found {len(parsed_files)} parsed file(s) for cleaning")

    # ------------------------------------------------------------------
    # 3. Initialize deterministic multi-stage cleaner
    # ------------------------------------------------------------------
    cleaner = RagTextCleaner.default()

    # ------------------------------------------------------------------
    # 4. Iterate over parsed JSONs
    # ------------------------------------------------------------------
    for idx, parsed_path in enumerate(parsed_files, start=1):
        try:
            with open(parsed_path, "r", encoding="utf-8") as f:
                parsed_data = json.load(f)

            raw_text = parsed_data.get("text", "")
            if not raw_text:
                logger.warning(f"Skipping {parsed_path.name}: no text field")
                continue

            # --- Step 1: Clean text ---
            cleaned_text = cleaner.clean(raw_text)

            # --- Step 2: Write cleaned output as JSON ---
            cleaned_filename = parsed_path.stem.replace(".parsed", "") + ".cleaned.json"
            cleaned_path = cleaned_dir / cleaned_filename

            cleaned_data = {
                "cleaned_text": cleaned_text,
            }

            with open(cleaned_path, "w", encoding="utf-8") as cf:
                json.dump(cleaned_data, cf, ensure_ascii=False, indent=2)

            logger.info(f"✓ Cleaned {parsed_path.name} ({idx}/{len(parsed_files)})")

        except Exception as e:
            logger.error(f"✗ Failed to clean {parsed_path.name}: {e}")

    # ------------------------------------------------------------------
    # 5. Finish
    # ------------------------------------------------------------------
    logger.info("Cleaning phase completed successfully.")


if __name__ == "__main__":
    main()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\deep_text_cleaner.py ==== 
from __future__ import annotations
import re
from cleantext import clean
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner

class DeepTextCleaner(BaseTextCleaner):
    """
    Advanced text cleaner using heuristic and statistical patterns
    to remove non-content sections like references, tables, and equations.
    """

    def _clean_impl(self, text: str) -> str:
        # Step 1: Global clean using clean-text
        text = clean(
            text,
            fix_unicode=True,
            to_ascii=False,
            lower=False,
            no_urls=True,
            no_emails=True,
            no_phone_numbers=True,
            no_numbers=False,
            no_currency_symbols=True,
            no_punct=False,
        )

        # Step 2: Remove reference-like or appendix sections
        text = re.sub(
            r"(?is)(references|bibliography|literaturverzeichnis|acknowledg(e)?ments|appendix).*",
            "",
            text,
        )

        # Step 3: Drop DOI/arXiv/URL lines
        text = re.sub(r"(?im)^\s*(doi|arxiv|http|https)[:\s].*$", "", text)

        # Step 4: Drop lines that are mostly numbers, formulas, or citation lists
        filtered_lines = []
        for line in text.splitlines():
            clean_line = line.strip()
            if not clean_line:
                continue
            # Discard lines with >30% digits or symbols
            digit_ratio = sum(ch.isdigit() for ch in clean_line) / max(len(clean_line), 1)
            symbol_ratio = sum(not ch.isalnum() and not ch.isspace() for ch in clean_line) / max(len(clean_line), 1)
            if digit_ratio > 0.3 or symbol_ratio > 0.4:
                continue
            # Drop if line looks like citation or table
            if re.match(r"^\[\d+\]\s*[A-Z][a-z]+", clean_line):
                continue
            if re.match(r"(?i)^table\s+\d+", clean_line):
                continue
            filtered_lines.append(clean_line)

        text = "\n".join(filtered_lines)

        # Step 5: Collapse whitespace and blank lines
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\i_text_cleaner.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod


class ITextCleaner(ABC):
    """Interface for all text cleaners in the ingestion pipeline."""

    @abstractmethod
    def clean(self, text: str) -> str:
        """Return a cleaned version of the given text."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rag_text_cleaner.py ==== 
from __future__ import annotations
from typing import List
from src.core.ingestion.cleaner.i_text_cleaner import ITextCleaner

from src.core.ingestion.cleaner.simple_cleaners import (
    UnicodeNormalizer,
    SoftHyphenCleaner,
    HeaderFooterCleaner,
    LayoutLineJoinCleaner,
    TrailingWhitespaceCleaner,
    HTMLCleaner,                  # removes HTML tags and entities
    ScientificNotationCleaner,    # removes scientific notation terms like "Eq. 1"
    ReferencePatternCleaner,      # NEW – removes APA / IEEE / arXiv / ACM / MLA style refs
    ReferencesCleaner,            # removes everything after 'References' or 'Bibliography'
)

from src.core.ingestion.cleaner.deep_text_cleaner import DeepTextCleaner  # advanced cleaning


class RagTextCleaner(ITextCleaner):
    """
    Composite text cleaner for RAG ingestion.
    Executes a deterministic sequence of cleaners to normalize and denoise scientific text.
    """

    def __init__(self, cleaners: List[ITextCleaner]):
        self.cleaners = cleaners

    # ------------------------------------------------------------------
    @classmethod
    def default(cls) -> "RagTextCleaner":
        """
        Build a deterministic chain of text cleaners combining rule-based and deep cleaning.
        """
        cleaners: List[ITextCleaner] = [
            UnicodeNormalizer(),          # normalize spaces and zero-width chars
            SoftHyphenCleaner(),          # remove soft hyphens and join split words
            HeaderFooterCleaner(),        # drop headers, footers, funding info
            LayoutLineJoinCleaner(),      # repair layout-induced line breaks
            TrailingWhitespaceCleaner(),  # trim redundant spaces and newlines
            HTMLCleaner(),                # remove HTML tags and entities
            ScientificNotationCleaner(),  # remove equations, theorem/lemma markers
            ReferencePatternCleaner(),    # remove inline citation patterns, DOIs, arXiv etc.
            ReferencesCleaner(),          # cut after "References"/"Bibliography"
            DeepTextCleaner(),            # deep filter for noise, refs, tables, math etc.
        ]
        return cls(cleaners)

    # ------------------------------------------------------------------
    def clean(self, text: str) -> str:
        """
        Run all sub-cleaners consecutively in deterministic order.
        """
        for cleaner in self.cleaners:
            text = cleaner.clean(text)
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\rules.py ==== 
# src/core/ingestion/cleaner/rules.py
from __future__ import annotations
import re

# Common residual headings we want to drop only if they appear alone in a line
SINGLE_LINE_HEADER_PATTERNS = [
    r"(?i)^\s*table\s+of\s+contents\s*$",
    r"(?i)^\s*inhaltsverzeichnis\s*$",
    r"(?i)^\s*contents\s*$",
]

# Footer / page indicator
FOOTER_PATTERNS = [
    r"(?i)^\s*page\s+\d+\s*$",
    r"(?i)^\s*p\.?\s*\d+\s*$",
    r"^\s*\d+\s*$",
]

# Funding and preprint noise – but only if line is short
FUNDING_PATTERNS = [
    r"(?i)^\s*funded\s+by.*$",
    r"(?i)^\s*supported\s+by.*$",
    r"(?i)^\s*this\s+preprint\s+.*$",
    r"(?i)^\s*arxiv:\s*\d{4}\.\d{4,5}.*$",
    r"(?i)^\s*doi:\s*10\.\d{4,9}/[-._;()/:A-Z0-9]+$",
]

# Lines we consider layout trash if they are too short
MAX_LEN_FOR_STRICT_DROP = 80

SOFT_HYPHEN = "\xad"
SOFT_HYPHEN_RE = re.compile(SOFT_HYPHEN)

# Hyphenated line break like "prob-\nabilistic"
HYPHEN_LINEBREAK_RE = re.compile(r"(\w+)-\n(\w+)")

# Linebreak in the middle of sentence like "proba-\nbilistic"
INLINE_LINEBREAK_RE = re.compile(r"(\S)\n(\S)")

# Multiple blank lines
MULTI_NEWLINE_RE = re.compile(r"\n{3,}")

# Unicode spaces
UNICODE_SPACES_RE = re.compile(r"[\u00a0\u2000-\u200b]+")


def is_short_line(line: str) -> bool:
    # Heuristic: many layout lines are short
    return len(line.strip()) <= MAX_LEN_FOR_STRICT_DROP


def match_any(patterns: list[str], line: str) -> bool:
    for pat in patterns:
        if re.match(pat, line):
            return True
    return False
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\simple_cleaners.py ==== 
from __future__ import annotations
import re
from typing import List
from src.core.ingestion.cleaner.base_cleaner import BaseTextCleaner
from src.core.ingestion.cleaner import rules


class HTMLCleaner(BaseTextCleaner):
    """Removes HTML tags and entities like &nbsp;."""

    def _clean_impl(self, text: str) -> str:
        # Remove all HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove HTML entities like &nbsp;
        text = re.sub(r'&[a-zA-Z]+;', '', text)
        return text


class ScientificNotationCleaner(BaseTextCleaner):
    """Removes scientific notations and references like 'Eq. 1', 'Theorem 3'."""

    def _clean_impl(self, text: str) -> str:
        # Remove scientific expressions such as Eq. 1 or Lemma 3
        text = re.sub(r'\b(Eq|Theorem|Lemma)\s+\d+\b', '', text)
        return text


class UnicodeNormalizer(BaseTextCleaner):
    """Normalize unicode spaces and zero-width chars."""

    def _clean_impl(self, text: str) -> str:
        # Replace unicode spaces with a normal space
        text = rules.UNICODE_SPACES_RE.sub(" ", text)
        return text


class SoftHyphenCleaner(BaseTextCleaner):
    """Remove soft hyphens and join line-broken words."""

    def _clean_impl(self, text: str) -> str:
        # Remove soft hyphen character
        text = text.replace(rules.SOFT_HYPHEN, "")
        # Join words split by hyphen and linebreak
        text = rules.HYPHEN_LINEBREAK_RE.sub(r"\1\2", text)
        return text


class LayoutLineJoinCleaner(BaseTextCleaner):
    """
    Join lines that were broken by the PDF layout but belong together.
    """

    def _clean_impl(self, text: str) -> str:
        # Join inline linebreaks like "probabilistic\nreasoning" -> "probabilistic reasoning"
        text = rules.INLINE_LINEBREAK_RE.sub(r"\1 \2", text)
        # Collapse too many blank lines
        text = rules.MULTI_NEWLINE_RE.sub("\n\n", text)
        return text.strip()


class HeaderFooterCleaner(BaseTextCleaner):
    """Remove obvious header/footer/single-line noise."""

    def _clean_impl(self, text: str) -> str:
        cleaned_lines: List[str] = []
        for line in text.splitlines():
            raw = line.rstrip()

            # Skip headers
            if rules.match_any(rules.SINGLE_LINE_HEADER_PATTERNS, raw):
                continue

            # Skip short footers
            if rules.match_any(rules.FOOTER_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            # Skip short funding or preprint notices
            if rules.match_any(rules.FUNDING_PATTERNS, raw) and rules.is_short_line(raw):
                continue

            cleaned_lines.append(raw)

        return "\n".join(cleaned_lines).strip()


class TrailingWhitespaceCleaner(BaseTextCleaner):
    """Normalize whitespace globally."""

    def _clean_impl(self, text: str) -> str:
        # Strip trailing spaces per line
        lines = [ln.rstrip() for ln in text.splitlines()]
        text = "\n".join(lines)
        # Collapse 3+ newlines into 2
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()


# ----------------------------------------------------------------------
# NEW CLEANER: removes APA / IEEE / arXiv / ACM / MLA style in-text citations
# ----------------------------------------------------------------------
class ReferencePatternCleaner(BaseTextCleaner):
    """
    Removes inline reference patterns such as (Smith, 2020), [12], [1–5], (Smith et al., 2021),
    DOIs, arXiv IDs, and conference citation snippets (Proc. IEEE, JMLR, NeurIPS, etc.).
    """

    def _clean_impl(self, text: str) -> str:
        # APA/Harvard-style: (Smith, 2020), (Doe & Roe, 2021)
        text = re.sub(
            r"\([A-Z][A-Za-z\-]+(?:,?\s(?:[A-Z]\.)+)*(?:\s&\s[A-Z][A-Za-z\-]+)*,\s?\d{4}[a-z]?\)",
            "",
            text,
        )

        # et al. style: (Smith et al., 2020)
        text = re.sub(
            r"\([A-Z][A-Za-z\-]+\s+et\s+al\.,\s*\d{4}[a-z]?\)",
            "",
            text,
        )

        # IEEE numeric: [1], [12], [1,2], [1–5], [1-3]
        text = re.sub(
            r"\[\s?\d+(?:[\-,–]\s?\d+)*(?:\s*,\s*\d+)*\s?\]",
            "",
            text,
        )

        # Year-only: (2020), (1999a)
        text = re.sub(
            r"\(\s?\d{4}[a-z]?\s?\)",
            "",
            text,
        )

        # Inline DOIs and arXiv references
        text = re.sub(r"\bdoi:\s*\S+", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\barXiv:\s*\S+", "", text, flags=re.IGNORECASE)

        # Conference/journal abbreviations with year numbers
        text = re.sub(
            r"\b(Proc\.|Proceedings|Conf\.|Conference|JMLR|ICML|NeurIPS|NIPS|AAAI|IJCAI|ACL|EMNLP|COLING|IEEE|ACM)\b.*?\d{4}",
            "",
            text,
            flags=re.IGNORECASE,
        )

        # Author-name style in brackets: [Smith 2020]
        text = re.sub(
            r"\[[A-Z][A-Za-z\-]+(?:\s+et\s+al\.)?,?\s*\d{4}[a-z]?\]",
            "",
            text,
        )

        # “In Proceedings of …” phrases
        text = re.sub(
            r"(?i)\bin\s+proceedings\s+of\b.*?(?=[\.\n])",
            "",
            text,
        )

        # Normalize multiple spaces and punctuation spacing
        text = re.sub(r"\s{2,}", " ", text)
        text = re.sub(r"\s+([\.,;:])", r"\1", text)

        return text.strip()


class ReferencesCleaner(BaseTextCleaner):
    """Removes everything starting from 'References', 'Bibliography', or 'Literaturverzeichnis'."""

    def _clean_impl(self, text: str) -> str:
        pattern = re.compile(
            r"(?im)^\s*(references|bibliography|literaturverzeichnis)\s*$"
        )
        match = pattern.search(text)
        if match:
            cutoff_index = match.start()
            # Only cut if reference section is beyond 20% of text
            if cutoff_index > len(text) * 0.2:
                text = text[:cutoff_index]
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\cleaner\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\i_ingestor.py ==== 
from abc import ABC, abstractmethod
from typing import Any

class IIngestor(ABC):
    """Interface for all ingestion components."""

    @abstractmethod
    def ingest(self, source_path: str) -> Any:
        """Convert one document into structured JSON chunks and metadata."""
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\metadata_extractor_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import importlib
import logging
from pathlib import Path


class MetadataExtractorFactory:
    """
    Dynamically loads and executes PDF-based metadata extractors.
    Each extractor implementation must expose a class named <Name>Extractor in
    src.core.ingestion.metadata.implementations.<name>_extractor.
    """

    def __init__(self, active_fields: List[str], pdf_root: str | Path):
        self.logger = logging.getLogger(__name__)
        self.active_fields = active_fields
        self.pdf_root = Path(pdf_root).resolve()
        self.extractors: Dict[str, Any] = {}
        self._load_extractors()

    # ------------------------------------------------------------------
    @classmethod
    def from_config(cls, cfg: Dict[str, Any]) -> MetadataExtractorFactory:
        """Initialize factory from ingestion.yaml configuration."""
        opts = cfg.get("options", {})
        active = opts.get("metadata_fields", [])
        pdf_root = cfg.get("paths", {}).get("raw_pdfs", "./data/raw_pdfs")
        return cls(active_fields=active, pdf_root=pdf_root)

    # ------------------------------------------------------------------
    def _load_extractors(self) -> None:
        """Dynamically import extractor implementations for requested fields."""
        base_pkg = "src.core.ingestion.metadata.implementations"

        mapping = {
            "title": "title_extractor",
            "authors": "author_extractor",
            "year": "year_extractor",
            # "abstract": "abstract_extractor",  # Deactivated abstract extraction
            "detected_language": "language_detector",
            "file_size": "file_size_extractor",
            "toc": "toc_extractor",
            "page_number": "page_number_extractor",  # Page number extractor
        }

        for field in self.active_fields:
            module_name = mapping.get(field)
            if not module_name:
                self.logger.warning(f"No extractor mapping found for field '{field}' → skipped.")
                continue

            try:
                # Dynamically import the module
                module = importlib.import_module(f"{base_pkg}.{module_name}")
                # Construct the extractor class name from the field (e.g., "PageNumberExtractor" from "page_number")
                class_name = "".join([p.capitalize() for p in field.split("_")]) + "Extractor"
                # Get the extractor class from the module
                extractor_class = getattr(module, class_name)
                # Instantiate the extractor class and store it in the dictionary
                self.extractors[field] = extractor_class(base_dir=self.pdf_root)
                self.logger.debug(f"Loaded extractor: {extractor_class.__name__} for '{field}'")
            except ModuleNotFoundError:
                self.logger.warning(f"Implementation missing for '{field}' ({module_name}.py not found).")
            except Exception as e:
                self.logger.error(f"Failed to load extractor for '{field}': {e}")

    # ------------------------------------------------------------------
    def extract_all(self, pdf_path: str) -> Dict[str, Any]:
        """Run all configured extractors directly on a given PDF file path."""
        pdf_path = str(Path(pdf_path).resolve())
        results: Dict[str, Any] = {}

        for field, extractor in self.extractors.items():
            try:
                value = extractor.extract(pdf_path)
                results[field] = value
            except Exception as e:
                self.logger.warning(f"Extractor '{field}' failed for {pdf_path}: {e}")
                results[field] = None

        return results
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\abstract_extractor.py ==== 
from __future__ import annotations
from typing import Optional
from pathlib import Path
import fitz


# class AbstractExtractor:
#     """Extracts abstract text directly from GROBID TEI XML or PDF XMP metadata."""

#     def __init__(self, base_dir: Path | str | None = None):
#         self.base_dir = Path(base_dir).resolve() if base_dir else None

#     # ------------------------------------------------------------------
#     def extract(self, pdf_path: str) -> Optional[str]:
#         pdf_file = Path(pdf_path)

#         # 1. Try GROBID XML
#         xml_path = self._find_grobid_xml(pdf_file)
#         if xml_path and xml_path.exists():
#             abstract = self._extract_from_grobid(xml_path)
#             if abstract:
#                 return abstract

#         # 2. Try PDF metadata (some PDFs include "subject"/"keywords"/"description")
#         abstract = self._extract_from_pdf_metadata(pdf_file)
#         if abstract:
#             return abstract

#         # 3. Fallback: None (no text heuristics)
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
#         """Read abstract-like information from PDF metadata fields."""
#         try:
#             with fitz.open(pdf_file) as doc:
#                 meta = doc.metadata or {}
#                 for key in ("subject", "Subject", "description", "Description", "keywords", "Keywords"):
#                     val = meta.get(key)
#                     if isinstance(val, str) and len(val.strip()) > 20:
#                         return re.sub(r"\s+", " ", val.strip())
#         except Exception:
#             return None
#         return None

#     # ------------------------------------------------------------------
#     def _find_grobid_xml(self, pdf_file: Path) -> Path | None:
#         xml_candidate = pdf_file.with_suffix(".tei.xml")
#         if xml_candidate.exists():
#             return xml_candidate
#         if self.base_dir:
#             alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
#             if alt.exists():
#                 return alt
#         return None

#     # ------------------------------------------------------------------
#     def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
#         """Parse TEI XML to extract the abstract section."""
#         try:
#             with open(xml_path, "r", encoding="utf-8") as f:
#                 xml = f.read()
#             root = etree.fromstring(xml.encode("utf-8"))
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             abs_text = root.xpath("string(//tei:abstract)", namespaces=ns)
#             if abs_text and len(abs_text.strip()) > 10:
#                 return re.sub(r"\s+", " ", abs_text.strip())
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\author_extractor.py ==== 
# src/core/ingestion/metadata/implementations/author_extractor.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
from pathlib import Path
import fitz
import logging
from lxml import etree
import re

# optional imports
try:
    import spacy
    from spacy.language import Language
    from spacy.pipeline import EntityRuler
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

try:
    from nameparser import HumanName
    NAMEPARSER_AVAILABLE = True
except ImportError:
    NAMEPARSER_AVAILABLE = False


class LocalNameVerifier:
    """Deterministic local fallback for name validation without ML models."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def __init__(self):
        # extended bad keyword set for academic false positives
        self.bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "school", "machine", "learning", "arxiv", "neural", "transformer",
            "appendix", "algorithm", "keywords", "vol", "doi", "intelligence",
            "markov", "chain", "computing", "zentrum", "sciences",
            "centre", "center", "data", "model", "probability", "network"
        }

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 80:
            return False
        if any(ch.isdigit() for ch in text):
            return False
        if any(tok.lower() in self.bad_keywords for tok in text.split()):
            return False

        if NAMEPARSER_AVAILABLE:
            try:
                hn = HumanName(text)
                if hn.first or hn.last:
                    return True
            except Exception:
                pass

        if self.TITLE_PATTERN.search(text):
            return True
        return bool(self.NAME_PATTERN.search(text))


class AuthorsExtractor:
    """Deterministic, language-agnostic author extractor with silent mode (no info logs)."""

    def __init__(self, base_dir: Path | str | None = None):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.name_verifier = LocalNameVerifier()
        self.nlp = None

        if SPACY_AVAILABLE:
            self.nlp = self._init_spacy_pipeline()
        else:
            self.logger.warning("spaCy not installed; PERSON detection disabled.")

    def _init_spacy_pipeline(self):
        """Try to load spaCy model, else build local EntityRuler fallback."""
        try:
            return spacy.load("en_core_web_sm", disable=["tagger", "lemmatizer", "parser"])
        except Exception:
            try:
                return spacy.load("de_core_news_sm", disable=["tagger", "lemmatizer", "parser"])
            except Exception:
                self.logger.warning("No spaCy model available; using local EntityRuler fallback.")
                return self._build_local_person_ruler()

    def _build_local_person_ruler(self) -> "Language":
        """Constructs a blank English spaCy pipeline with a rule-based PERSON detector."""
        nlp = spacy.blank("en")
        ruler = nlp.add_pipe("entity_ruler")
        patterns = [
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_TITLE": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"IS_ALPHA": True}, {"IS_ALPHA": True}]},
            {"label": "PERSON", "pattern": [{"IS_TITLE": True}, {"TEXT": {"REGEX": "^[A-Z][a-z]+\\.$"}}]},
        ]
        ruler.add_patterns(patterns)
        return nlp

    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[str]:
        candidates: List[str] = []

        if parsed_document and parsed_document.get("grobid_xml"):
            candidates.extend(self._extract_from_grobid_xml(parsed_document["grobid_xml"]))
        else:
            xml_path = self._find_grobid_sidecar(Path(pdf_path))
            if xml_path:
                candidates.extend(self._extract_from_grobid_file(xml_path))

        if not candidates:
            candidates.extend(self._extract_from_pdf_metadata(Path(pdf_path)))

        if not candidates:
            candidates.extend(self._extract_from_first_page(Path(pdf_path)))

        candidates = [self._normalize(c) for c in candidates if c.strip()]
        verified = [n for n in candidates if self.name_verifier.is_person_name(n)]

        cleaned = [
            re.sub(
                r"\b(Google|Inc\.?|University|Institute|Lab|Labs|Dept\.?|Center|Centre|Zentrum|Sciences?)\b",
                "",
                n,
                flags=re.IGNORECASE
            ).strip()
            for n in verified
        ]

        seen: set[str] = set()
        return [a for a in cleaned if not (a in seen or seen.add(a))]

    def _find_grobid_sidecar(self, pdf_file: Path) -> Optional[Path]:
        local = pdf_file.with_suffix(".tei.xml")
        if local.exists():
            return local
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid_file(self, xml_path: Path) -> List[str]:
        try:
            return self._extract_from_grobid_xml(xml_path.read_text(encoding="utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID XML read error: {e}")
            return []

    def _extract_from_grobid_xml(self, xml_text: str) -> List[str]:
        ns = {"tei": "http://www.tei-c.org/ns/1.0"}
        try:
            root = etree.fromstring(xml_text.encode("utf-8"))
        except Exception as e:
            self.logger.warning(f"GROBID parse error: {e}")
            return []

        authors: List[str] = []
        for xp in [
            "//tei:analytic//tei:author",
            "//tei:monogr//tei:author",
            "//tei:respStmt//tei:author",
            "//tei:editor",
        ]:
            for node in root.xpath(xp, namespaces=ns):
                fn = node.xpath("string(.//tei:forename)", namespaces=ns).strip()
                ln = node.xpath("string(.//tei:surname)", namespaces=ns).strip()
                if fn or ln:
                    authors.append(f"{fn} {ln}".strip())
                else:
                    txt = " ".join(node.xpath(".//text()", namespaces=ns)).strip()
                    if txt:
                        authors.extend(self._extract_persons_ner(txt))
        return authors

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
        except Exception as e:
            self.logger.warning(f"PDF metadata read error: {e}")
            return []
        author_field = meta.get("author") or meta.get("Author") or meta.get("authors")
        return self._extract_persons_ner(str(author_field)) if author_field else []

    def _extract_from_first_page(self, pdf_file: Path) -> List[str]:
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return []
                text = doc.load_page(0).get_text("text")
        except Exception as e:
            self.logger.warning(f"First-page read error: {e}")
            return []
        text = text.split("Abstract")[0] if "Abstract" in text else text
        return self._extract_persons_ner(text)

    def _extract_persons_ner(self, text: str) -> List[str]:
        if not self.nlp or not text.strip():
            return []
        doc = self.nlp(text)
        persons = []
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if any(e.start <= ent.start < e.end for e in doc.ents if e.label_ in {"ORG", "GPE", "LOC"}):
                    continue
                persons.append(ent.text.strip())
        return [p for p in persons if self._is_name_like(p)]

    def _is_name_like(self, text: str) -> bool:
        bad_keywords = {
            "abstract", "introduction", "university", "department", "institute",
            "arxiv", "model", "learning", "probability", "appendix", "keywords",
            "ai", "neural", "transformer", "algorithm", "vol",
            "intelligence", "computing", "zentrum", "sciences", "centre", "center"
        }
        if any(tok.lower() in bad_keywords for tok in text.split()):
            return False
        if any(ch.isdigit() for ch in text):
            return False
        return 1 <= len(text.split()) <= 5 and text[0].isupper()

    def _normalize(self, s: str) -> str:
        """Normalize spacing and special characters."""
        return " ".join(s.replace("•", "").replace("†", "").split()).strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\file_size_extractor.py ==== 
from __future__ import annotations
import os
from pathlib import Path


class FileSizeExtractor:
    """Extracts the exact file size (in bytes) directly from the PDF file."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> int | None:
        """Return the file size of the given PDF file in bytes."""
        pdf_file = Path(pdf_path)
        try:
            if not pdf_file.is_file():
                return None
            return pdf_file.stat().st_size
        except Exception:
            return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\language_detector.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import re
import fitz

try:
    from langdetect import detect
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False


class DetectedLanguageExtractor:
    """Detects the dominant document language from PDF text or XMP metadata."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Identify document language via PDF content or metadata."""
        pdf_file = Path(pdf_path)

        # 1. Try PDF XMP metadata
        lang = self._extract_from_metadata(pdf_file)
        if lang:
            return lang

        # 2. Try text-based detection (without parser)
        lang = self._detect_from_text(pdf_file)
        if lang:
            return lang

        return None

    # ------------------------------------------------------------------
    def _extract_from_metadata(self, pdf_file: Path) -> Optional[str]:
        """Check embedded language fields in PDF metadata."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                for key in ("language", "Language", "lang", "Lang"):
                    val = meta.get(key)
                    if val and isinstance(val, str):
                        val_norm = val.lower().strip()
                        if val_norm.startswith("en"):
                            return "en"
                        if val_norm.startswith("de"):
                            return "de"
        except Exception:
            return None
        return None

    # ------------------------------------------------------------------
    def _detect_from_text(self, pdf_file: Path) -> Optional[str]:
        """Extract small text sample from first pages and apply language detection."""
        sample_text = ""
        try:
            with fitz.open(pdf_file) as doc:
                max_pages = min(3, len(doc))
                for i in range(max_pages):
                    sample_text += doc.load_page(i).get_text("text") + "\n"
        except Exception:
            return None

        if not sample_text or len(sample_text.strip()) < 30:
            return None

        # Use langdetect if installed
        if LANGDETECT_AVAILABLE:
            try:
                lang = detect(sample_text)
                if lang in ("en", "de"):
                    return lang
            except Exception:
                pass

        # Simple regex-based fallback
        text_lower = sample_text.lower()
        if re.search(r"\b(und|nicht|sein|dass|werden|ein|eine|mit|auf|für|dem|den|das|ist|sind|der|die|das)\b", text_lower):
            return "de"
        if re.search(r"\b(the|and|of|for|with|from|that|this|by|is|are|we|deep|learning|language|model)\b", text_lower):
            return "en"
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\page_number_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz  # PyMuPDF
from src.core.ingestion.metadata.interfaces.i_page_number_extractor import IPageNumberExtractor


class PageNumberExtractor(IPageNumberExtractor):
    """Extracts the page number from the first page of the PDF document."""
    
    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the first page of the PDF.
        
        :param pdf_path: Path to the PDF file.
        :return: The page number (if available), otherwise None.
        """
        pdf_file = Path(pdf_path)

        try:
            with fitz.open(pdf_file) as doc:
                # Getting the total page count
                total_pages = len(doc)
                if total_pages > 0:
                    # For this example, we're assuming the page number is extracted from the first page
                    return 1  # For example, we return the first page number; adapt as needed
        except Exception as e:
            print(f"Error extracting page number from {pdf_file}: {e}")
        
        return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\title_extractor.py ==== 
from __future__ import annotations
from pathlib import Path
from typing import Optional
import fitz
import re
from lxml import etree

class TitleExtractor:
    """
    Robust, deterministic extractor for scientific paper titles.
    Filename fallback *is not used* (explicitly excluded).
    """

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    def extract(self, pdf_path: str) -> Optional[str]:
        """
        Extract the most probable title of a given PDF.
        Priority:
          1. PDF metadata
          2. GROBID TEI XML
          3. Layout + heuristic on first page
        Returns None if no reliable title is found.
        """
        pdf_file = Path(pdf_path)

        # 1. Extract from PDF metadata
        title = self._extract_from_pdf_metadata(pdf_file)
        if self._is_valid(title):
            return title

        # 2. Extract from GROBID TEI XML
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            grobid_title = self._extract_from_grobid(xml_path)
            if self._is_valid(grobid_title):
                return grobid_title

        # 3. Layout-based extraction (heuristic on first page)
        title_from_layout = self._extract_from_first_page_layout(pdf_file)
        if self._is_valid(title_from_layout):
            return title_from_layout

        # No valid title found after all methods
        return None

    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Optional[str]:
        """Extract title from embedded PDF metadata fields."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                title_meta = meta.get("title") or meta.get("Title")
                if title_meta:
                    cleaned = self._normalize(title_meta)
                    if self._is_valid(cleaned):
                        return cleaned
        except Exception as e:
            print(f"Error extracting from PDF metadata: {e}")
        return None

    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Locate associated GROBID TEI XML file (same stem .tei.xml)."""
        candidate1 = pdf_file.with_suffix(".tei.xml")
        if candidate1.exists():
            return candidate1
        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt
        return None

    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Extract title from structured GROBID TEI XML."""
        try:
            xml_content = xml_path.read_text(encoding="utf-8")
            root = etree.fromstring(xml_content.encode("utf-8"))
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}
            xpaths = [
                "//tei:analytic/tei:title[@type='main']",
                "//tei:monogr/tei:title[@type='main']",
                "//tei:titleStmt/tei:title[@type='main']",
                "//tei:titleStmt/tei:title"
            ]
            for xp in xpaths:
                title = root.xpath(f"string({xp})", namespaces=ns)
                cleaned = self._normalize(title)
                if self._is_valid(cleaned):
                    return cleaned
        except Exception as e:
            print(f"Error extracting from GROBID XML: {e}")
        return None

    def _extract_from_first_page_layout(self, pdf_file: Path) -> Optional[str]:
        """
        Layout + heuristic extraction: choose candidate block(s) from first page based on
        font size, vertical position, then filter out author/affiliation lines.
        """
        try:
            with fitz.open(pdf_file) as doc:
                if doc.page_count == 0:
                    return None
                page = doc.load_page(0)
                blocks = page.get_text("dict")["blocks"]
        except Exception as e:
            print(f"Error extracting layout from first page: {e}")
            return None

        # Collect text segments with font size & position
        candidates: list[tuple[float, float, str]] = []  # (fontsize, y0, text)
        for block in blocks:
            if "lines" not in block:
                continue
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue
                    fontsize = span.get("size", 0)
                    y0 = span.get("origin", (0, 0))[1]
                    candidates.append((fontsize, y0, text))

        if not candidates:
            return None

        # Sort by font size descending, then vertical position ascending (top of page)
        candidates.sort(key=lambda x: (-x[0], x[1]))

        # Choose top-N spans (e.g., top 3) as potential title block
        top_spans = candidates[:3]
        combined = " ".join(span[2] for span in top_spans)
        cleaned = self._normalize(combined)

        # Filter out if it looks like author/affiliation block
        if self._looks_like_author_block(cleaned):
            # Try next spans if possible
            if len(candidates) > 3:
                alt_spans = candidates[3:6]
                combined2 = " ".join(span[2] for span in alt_spans)
                cleaned2 = self._normalize(combined2)
                if self._is_valid(cleaned2) and not self._looks_like_author_block(cleaned2):
                    return cleaned2
            return None

        if self._is_valid(cleaned):
            return cleaned

        return None

    def _looks_like_author_block(self, text: str) -> bool:
        """Heuristic to detect if a block is likely authors/affiliations rather than title."""
        if not text:
            return False
        # Frequent keywords in affiliations/authors
        if re.search(r"\b(university|dept|department|institute|school|college|laboratory|lab|affiliat|author|authors?)\b", text, re.I):
            return True
        # Many names separated by commas/and before a newline
        if re.match(r"[A-Z][a-z]+(\s[A-Z][a-z]+)+,?\s(and|&)\s[A-Z][a-z]+", text):
            return True
        return False

    def _is_valid(self, text: Optional[str]) -> bool:
        """Check if a title candidate is semantically plausible."""
        if not text:
            return False
        t = text.strip()

        # Title should not be too short
        if len(t) < 10:
            return False
        # Title should have a reasonable word count
        if len(t.split()) < 3 or len(t.split()) > 30:
            return False

        # Exclude if it's just numbers or version identifiers
        if re.match(r"^[0-9._-]+$", t):
            return False

        # Exclude arXiv IDs and DOIs
        if re.search(r"arXiv:\d+\.\d+", t) or re.search(r"doi:\S+", t, re.IGNORECASE):
            return False

        # Exclude non-title fragments like "ENTREE, P2 Pl"
        if re.search(r"(P2\sPl|ENTREE|v\d+)", t, re.IGNORECASE):
            return False

        return True

    def _normalize(self, text: Optional[str]) -> str:
        """Normalize whitespace, remove soft-hyphens and ligatures."""
        if not text:
            return ""
        # Merge hyphenated line breaks
        text = re.sub(r"(\w)-\s+(\w)", r"\1\2", text)
        text = text.replace("ﬁ", "fi").replace("ﬂ", "fl")
        text = text.replace("_", " ")
        text = re.sub(r"\s+", " ", text)
        # Remove arXiv IDs and version tags
        text = re.sub(r"arXiv:\d+\.\d+", "", text)  # Remove arXiv ID
        text = re.sub(r"v\d+", "", text)  # Remove version tags
        return text.strip()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\toc_extractor.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
import fitz
import re
from pathlib import Path


class TocExtractor:
    """Extracts table of contents (TOC) entries directly from a PDF using PyMuPDF."""

    def __init__(self, base_dir: Path | str | None = None):
        self.base_dir = Path(base_dir).resolve() if base_dir else None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> List[Dict[str, Any]]:
        """Return structured TOC entries, purely PDF-based."""
        pdf_file = Path(pdf_path)
        toc_entries: List[Dict[str, Any]] = []
        try:
            with fitz.open(pdf_file) as doc:
                toc = doc.get_toc(simple=True)
                if toc:
                    for level, title, page in toc:
                        title_clean = re.sub(r"\s+", " ", title.strip())
                        if title_clean:
                            toc_entries.append(
                                {"level": int(level), "title": title_clean, "page": int(page)}
                            )
                    return toc_entries

                # fallback to heuristic textual TOC
                toc_entries = self._extract_textual_toc(doc)
                return toc_entries

        except Exception as e:
            print(f"[WARN] Failed to extract TOC from {pdf_file}: {e}")
            return []

    # ------------------------------------------------------------------
    def _extract_textual_toc(self, doc: fitz.Document) -> List[Dict[str, Any]]:
        """Detect textual TOCs on the first pages when outline metadata is missing."""
        toc_entries: List[Dict[str, Any]] = []
        try:
            max_pages = min(5, len(doc))
            pattern = re.compile(r"^\s*(\d{0,2}\.?)+\s*[A-ZÄÖÜa-zäöü].{3,}\s+(\d{1,3})\s*$")

            for i in range(max_pages):
                text = doc.load_page(i).get_text("text")
                if not re.search(r"(Inhalt|Contents|Table of Contents)", text, re.I):
                    continue
                for line in text.splitlines():
                    if pattern.match(line.strip()):
                        parts = re.split(r"\s{2,}", line.strip())
                        if len(parts) >= 2:
                            title = re.sub(r"^\d+(\.\d+)*\s*", "", parts[0])
                            page = re.findall(r"\d+", parts[-1])
                            page_num = int(page[0]) if page else None
                            toc_entries.append(
                                {"level": 1, "title": title.strip(), "page": page_num}
                            )
            return toc_entries

        except Exception:
            return []
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\year_extractor.py ==== 
from __future__ import annotations
import re
import datetime
import time
from typing import Iterable, Tuple, Optional, Dict, List
from pathlib import Path
import fitz  # PyMuPDF
from lxml import etree

# optional normalizer
try:
    from unidecode import unidecode
except Exception:
    unidecode = None

# optional Crossref client
try:
    from habanero import Crossref
except Exception:
    Crossref = None  # type: ignore

CURRENT_YEAR = datetime.datetime.now().year
FUTURE_GRACE = 1  # allow slight future offset

# strict year patterns
YEAR_STRICT_RE = re.compile(r"(?<!\d)(19|20)\d{2}(?!\d)")
YEAR_CONTEXT_RE = re.compile(
    r"(?:published\s+online[:\s]*|accepted[:\s]*|received[:\s]*|©|copyright)\s*((19|20)\d{2})",
    re.I
)


class YearExtractor:
    """Robust publication year extractor using multi-source strategies."""

    def __init__(self, base_dir: Path | str | None = None,
                 max_text_pages: int = 3,
                 enable_crossref: bool = True):

        self.base_dir = Path(base_dir).resolve() if base_dir else None
        self.max_text_pages = max(1, int(max_text_pages))
        self.crossref = None

        if enable_crossref and Crossref is not None:
            try:
                self.crossref = Crossref(mailto="contact@example.com")
            except Exception:
                self.crossref = None

    # ------------------------------------------------------------------
    def extract(self, pdf_path: str) -> Optional[str]:
        """Main orchestrator for multi-source year extraction."""
        pdf_file = Path(pdf_path)
        candidates: List[Tuple[int, int, str]] = []  # (priority, score, year_str)

        # 1) GROBID TEI
        xml_path = self._find_grobid_xml(pdf_file)
        if xml_path and xml_path.exists():
            year = self._extract_from_grobid(xml_path)
            if year:
                candidates.append((1, 100, year))

        # 2) PDF metadata (with consistency refinement)
        year_meta, meta_score = self._extract_from_pdf_metadata(pdf_file)
        if year_meta:
            refined = self._refine_with_text_consistency(pdf_file, int(year_meta))
            if refined:
                candidates.append((2, meta_score, refined))

        # 3) Early-page visible text
        year_text, text_score = self._extract_from_page_text(pdf_file, self.max_text_pages)
        if year_text:
            candidates.append((3, text_score, year_text))

        # 4) Filename heuristics
        year_fn, fn_score = self._extract_from_filename(pdf_file.name)
        if year_fn:
            candidates.append((4, fn_score, year_fn))

        # 5) DOI/Title lookup
        if not candidates:
            title, doi, arxiv = self._extract_title_doi_arxiv(pdf_file)
            if arxiv:
                y = self._year_from_arxiv_id(arxiv)
                if y:
                    candidates.append((5, 60, str(y)))

            if (doi or title) and self.crossref:
                y = self._lookup_via_crossref(title, doi)
                if y:
                    candidates.append((6, 70, y))

        if not candidates:
            return None

        # sort by priority, score, then by recency
        candidates.sort(key=lambda t: (t[0], -t[1], -int(t[2])))
        return candidates[0][2]

    # ------------------------------------------------------------------
    def _lookup_via_crossref(self, title: Optional[str], doi: Optional[str]) -> Optional[str]:
        """Crossref lookup as fallback (no hardcoded year rejection)."""
        if not self.crossref:
            return None

        for attempt in range(2):
            try:
                if doi:
                    msg = self.crossref.works(ids=doi).get("message", {})
                elif title:
                    msg = self.crossref.works(query=title, limit=1).get("message", {}).get("items", [{}])[0]
                else:
                    return None

                for key in ("published-print", "published-online", "issued"):
                    info = msg.get(key)
                    if info and "date-parts" in info:
                        y = info["date-parts"][0][0]
                        if self._valid_year(y):
                            return str(y)

            except Exception:
                time.sleep(1)

        return None

    # ------------------------------------------------------------------
    def _extract_title_doi_arxiv(self, pdf_file: Path) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """Extract DOI, arXiv ID, title from first pages."""
        doi_re = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b", re.I)
        arxiv_re = re.compile(
            r"\b(?:arxiv[:/ ]?)?(\d{4}\.\d{4,5}|[a-z\-]+/\d{7}|[0-9]{7,8})(v\d+)?\b",
            re.I
        )

        title = doi = arxiv = None

        try:
            with fitz.open(pdf_file) as doc:
                meta_title = (doc.metadata or {}).get("title")
                if meta_title:
                    title = meta_title.strip()

                txt = ""
                for i in range(min(3, len(doc))):
                    txt += (doc.load_page(i).get_text("text") or "") + "\n"
        except Exception:
            txt = ""

        if unidecode and txt:
            txt = unidecode(txt)

        m = doi_re.search(txt)
        if m:
            doi = m.group(0).rstrip(".,)")

        m = arxiv_re.search(txt)
        if m:
            arxiv = m.group(1)

        if not title and txt:
            for line in txt.splitlines():
                s = line.strip()
                if len(s) > 10 and not re.search(r"(abstract|introduction|contents)", s, re.I):
                    title = s
                    break

        return title, doi, arxiv

    # ------------------------------------------------------------------
    def _year_from_arxiv_id(self, arxiv_id: str) -> Optional[int]:
        """Interpret arXiv ID year component."""
        try:
            m = re.match(r"^(\d{2})(\d{2})\.\d{4,5}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                y = 2000 + yy if yy < 25 else 1900 + yy
                return y if self._valid_year(y) else None

            m = re.match(r"^[a-z\-]+/(\d{2})(\d{2})\d{3,4}$", arxiv_id, re.I)
            if m:
                yy = int(m.group(1))
                y = 2000 + yy if yy < 25 else 1900 + yy
                return y if self._valid_year(y) else None

            m = re.match(r"^(\d{2})(\d{2})\d{3,4}$", arxiv_id)
            if m:
                yy = int(m.group(1))
                y = 2000 + yy if yy < 25 else 1900 + yy
                return y if self._valid_year(y) else None

        except Exception:
            pass

        return None

    # ------------------------------------------------------------------
    def _valid_year(self, y: int) -> bool:
        """Reject only unrealistic values."""
        return 1800 <= y <= CURRENT_YEAR + FUTURE_GRACE

    # ------------------------------------------------------------------
    def _find_grobid_xml(self, pdf_file: Path) -> Optional[Path]:
        """Locate TEI XML file."""
        cand = pdf_file.with_suffix(".tei.xml")
        if cand.exists():
            return cand

        if self.base_dir:
            alt = self.base_dir / "grobid_xml" / f"{pdf_file.stem}.tei.xml"
            if alt.exists():
                return alt

        return None

    # ------------------------------------------------------------------
    def _extract_from_grobid(self, xml_path: Path) -> Optional[str]:
        """Extract year from TEI XML."""
        try:
            xml = etree.fromstring(xml_path.read_bytes())
            ns = {"tei": "http://www.tei-c.org/ns/1.0"}

            xps = [
                "//tei:sourceDesc//tei:imprint/tei:date",
                "//tei:biblStruct//tei:imprint/tei:date",
                "//tei:profileDesc//tei:creation/tei:date"
            ]

            for xp in xps:
                for node in xml.xpath(xp, namespaces=ns):

                    for key in ("when", "when-iso", "notBefore", "notAfter"):
                        val = node.get(key)
                        if val:
                            y = self._year_from_date_string(val)
                            if y:
                                return str(y)

                    txt = (node.text or "").strip()
                    for y in self._years_from_string(txt):
                        return str(y)

        except Exception:
            pass

        return None

    # ------------------------------------------------------------------
    def _extract_from_pdf_metadata(self, pdf_file: Path) -> Tuple[Optional[str], int]:
        """Parse metadata and extract plausible year."""
        try:
            with fitz.open(pdf_file) as doc:
                meta = doc.metadata or {}
                kv = {k.lower(): v for k, v in meta.items() if isinstance(v, str)}

                best = (None, -1)

                for key, score in [
                    ("creationdate", 80),
                    ("moddate", 60),
                    ("date", 50)
                ]:
                    val = kv.get(key)
                    if not val:
                        continue

                    y = self._year_from_date_string(val)
                    if y and self._valid_year(y):
                        if score > best[1]:
                            best = (y, score)

                if best[0]:
                    return str(best[0]), best[1]

        except Exception:
            pass

        return None, -1

    # ------------------------------------------------------------------
    def _refine_with_text_consistency(self, pdf_file: Path, year_meta: int) -> Optional[str]:
        """Check consistency of metadata with surrounding text."""
        try:
            with fitz.open(pdf_file) as doc:
                pages = []
                for i in range(min(3, len(doc))):
                    pages.append(doc.load_page(i).get_text("text") or "")
                for i in range(max(0, len(doc) - 2), len(doc)):
                    pages.append(doc.load_page(i).get_text("text") or "")
                txt = "\n".join(pages)

        except Exception:
            return str(year_meta)

        if unidecode:
            txt = unidecode(txt)

        # strong context match
        ctx = YEAR_CONTEXT_RE.findall(txt)
        if ctx:
            yrs = [int(x[0]) for x in ctx if self._valid_year(int(x[0]))]
            if yrs:
                return str(max(yrs))

        # fallback: largest strict year in text
        yrs = [int(m.group()) for m in YEAR_STRICT_RE.finditer(txt)]
        yrs = [y for y in yrs if self._valid_year(y)]

        if not yrs:
            return str(year_meta)

        y_txt = max(yrs)

        # if metadata and text differ too much, trust text
        if abs(y_txt - year_meta) >= 10:
            return str(y_txt)

        return str(year_meta)

    # ------------------------------------------------------------------
    def _extract_from_page_text(self, pdf_file: Path, pages: int) -> Tuple[Optional[str], int]:
        """Extract year from early page content."""
        try:
            with fitz.open(pdf_file) as doc:
                best = (None, -1)

                for i in range(min(len(doc), pages)):
                    txt = doc.load_page(i).get_text("text") or ""

                    if unidecode:
                        txt = unidecode(txt)

                    # direct contextual indicators
                    ctx = YEAR_CONTEXT_RE.findall(txt)
                    if ctx:
                        yrs = [int(x[0]) for x in ctx if self._valid_year(int(x[0]))]
                        if yrs:
                            y = max(yrs)
                            score = 95 - i
                            return str(y), score

                    # strict year pattern
                    yrs = [int(m.group()) for m in YEAR_STRICT_RE.finditer(txt)]
                    yrs = [y for y in yrs if self._valid_year(y)]

                    if yrs:
                        y = max(yrs)
                        score = 45 + (pages - i)
                        if score > best[1]:
                            best = (y, score)

                if best[0]:
                    return str(best[0]), best[1]

        except Exception:
            pass

        return None, -1

    # ------------------------------------------------------------------
    def _extract_from_filename(self, name: str) -> Tuple[Optional[str], int]:
        """Extract year from filename."""
        base = unidecode(name) if unidecode else name
        base = re.sub(r"v\d+\b", "", base, flags=re.I)

        yrs = [y for y in self._years_from_string(base) if self._valid_year(y)]
        if yrs:
            return str(max(yrs)), 30

        return None, -1

    # ------------------------------------------------------------------
    def _year_from_date_string(self, s: str) -> Optional[int]:
        """Interpret year in date strings."""
        if not s:
            return None

        s = s.strip()

        m = re.match(r"^D:(\d{4})", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None

        m = re.match(r"^(\d{4})(?:[-/]\d{2}(?:[-/]\d{2})?)?$", s)
        if m:
            y = int(m.group(1))
            return y if self._valid_year(y) else None

        for y in self._years_from_string(s):
            return y

        return None

    # ------------------------------------------------------------------
    def _years_from_string(self, s: str) -> Iterable[int]:
        """Find all strict isolated 4-digit years."""
        seen = set()
        for m in YEAR_STRICT_RE.finditer(s):
            y = int(m.group(0))
            if y not in seen and self._valid_year(y):
                seen.add(y)
                yield y
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\implementations\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_abstract_extractor.py ==== 
from __future__ import annotations
from typing import Any, Dict
from lxml import etree
import re
# from src.core.ingestion.metadata.interfaces.i_abstract_extractor import IAbstractExtractor


# class AbstractExtractor(IAbstractExtractor):
#     """Extracts abstract section using GROBID TEI XML."""

#     def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
#         xml_data = parsed_document.get("grobid_xml")
#         if not xml_data or not xml_data.strip().startswith("<"):
#             return None

#         try:
#             ns = {"tei": "http://www.tei-c.org/ns/1.0"}
#             root = etree.fromstring(xml_data.encode("utf8"))
#             # collect all text under <abstract> or <div type='abstract'>
#             xpath_candidates = [
#                 "//tei:abstract",
#                 "//tei:div[@type='abstract']",
#                 "//tei:profileDesc/tei:abstract",
#             ]
#             for path in xpath_candidates:
#                 text = root.xpath(f"string({path})", namespaces=ns)
#                 if text and len(text.strip()) > 20:
#                     cleaned = re.sub(r"\s+", " ", text.strip())
#                     return cleaned
#         except Exception:
#             return None
#         return None
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_author_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict, List

class IAuthorExtractor(ABC):
    """Interface for extracting document authors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> List[str]:
        """Extract author names as a list of strings."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_file_size_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IFileSizeExtractor(ABC):
    """Interface for extracting the file size of a PDF."""

    @abstractmethod
    def extract(self, pdf_path: str) -> int | None:
        """Return file size in bytes."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_language_detector.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ILanguageDetector(ABC):
    """Interface for detecting the main document language."""

    @abstractmethod
    def detect(self, text: str, metadata: Dict[str, Any] | None = None) -> str | None:
        """Detect the dominant language code (e.g. 'en', 'de')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_page_number_extractor.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Optional


class IPageNumberExtractor(ABC):
    """Interface for extracting page numbers from a document."""

    @abstractmethod
    def extract_page_number(self, pdf_path: str) -> Optional[int]:
        """Extract the page number from the given PDF path.
        
        :param pdf_path: Path to the PDF file
        :return: The page number (if available), otherwise None.
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_title_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class ITitleExtractor(ABC):
    """Interface for extracting the main document title."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the document title."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\i_year_extractor.py ==== 
from abc import ABC, abstractmethod
from typing import Any, Dict

class IYearExtractor(ABC):
    """Interface for extracting publication year."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any]) -> str | None:
        """Extract the publication year as string (e.g. '2023')."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\toc_extractor_interface.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
from abc import ABC, abstractmethod


class TocExtractorInterface(ABC):
    """Interface for table-of-contents extractors."""

    @abstractmethod
    def extract(self, pdf_path: str, parsed_document: Dict[str, Any] | None = None) -> List[Dict[str, Any]]:
        """
        Extracts the table of contents (TOC) from a given PDF file.

        Returns:
            A list of dictionaries with keys:
                - level: int
                - title: str
                - page: int
        """
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\interfaces\__init__.py ==== 
# Expose all interfaces for clean import
from .i_title_extractor import ITitleExtractor
from .i_author_extractor import IAuthorExtractor
from .i_year_extractor import IYearExtractor
#from .i_abstract_extractor import IAbstractExtractor
from .i_language_detector import ILanguageDetector
from .i_file_size_extractor import IFileSizeExtractor
#from .i_has_ocr_extractor import IHasOcrExtractor

__all__ = [
    "ITitleExtractor",
    "IAuthorExtractor",
    "IYearExtractor",
    #"IAbstractExtractor",
    "ILanguageDetector",
    "IFileSizeExtractor",
    #"IHasOcrExtractor",
]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\metadata\utils\name_verifier.py ==== 
import re
from nameparser import HumanName

class LocalNameVerifier:
    """Local deterministic fallback for name validation."""

    TITLE_PATTERN = re.compile(r"^(dr|prof|mr|ms|mrs|herr|frau)\.?\s", re.IGNORECASE)
    NAME_PATTERN = re.compile(r"^[A-Z][a-z]+(?:[-'\s][A-Z][a-z]+)*$")

    def is_person_name(self, text: str) -> bool:
        if not text or len(text) > 60:
            return False
        if any(ch.isdigit() for ch in text):
            return False

        hn = HumanName(text)
        # requires at least one capitalized component
        valid_structure = bool(hn.first or hn.last)
        valid_chars = bool(self.NAME_PATTERN.search(text)) or bool(self.TITLE_PATTERN.search(text))
        invalid_tokens = any(tok.lower() in {
            "university", "institute", "department", "school",
            "machine", "learning", "arxiv", "neural", "transformer",
            "abstract", "introduction", "appendix"
        } for tok in text.split())

        return valid_structure and valid_chars and not invalid_tokens
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json
import time

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        # Konfiguriere die maximale Anzahl an Workern, basierend auf der Anzahl der CPU-Kerne
        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            # Maximal 4 Worker verwenden, wenn die CPU nicht so viele Kerne hat
            self.num_workers = min(max(1, multiprocessing.cpu_count() - 1), 4)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        """
        Parse all PDFs in the given directory and output the parsed results to the output directory.
        
        :param pdf_dir: Directory containing the PDF files.
        :param output_dir: Directory where the parsed results should be saved.
        :return: List of dictionaries containing parsed content from the PDFs.
        """
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    # Set timeout for each future (e.g., 300 seconds = 5 minutes)
                    result = future.result(timeout=300)  # Timeout set to 5 minutes
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"Parsed {pdf.name}")
                except concurrent.futures.TimeoutError:
                    self.logger.error(f"Timeout occurred while parsing {pdf.name}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        """
        Parse a single PDF file using the appropriate parser and return the extracted data.
        
        :param pdf_path: Path to the PDF file to parse.
        :return: A dictionary containing parsed content from the PDF.
        """
        parser = self.parser_factory.create_parser()
        try:
            result = parser.parse(pdf_path)
            return result
        except Exception as e:
            self.logger.error(f"Error parsing {pdf_path}: {e}")
            return {"text": "", "metadata": {"source_file": pdf_path}}

.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser
from src.core.ingestion.parser.pdfplumber_parser import PdfPlumberParser  # Import PdfPlumberParser

class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()  # Get parser mode (fitz, pdfplumber, auto)
        self.parallelism = opts.get("parallelism", "auto")  # Get parallelism setting
        self.language = opts.get("language", "auto")  # Get language setting
        self.exclude_toc = True  # Enforce exclusion of table of contents globally
        self.max_pages = opts.get("max_pages", None)  # Max pages to parse

        # Determine optimal CPU usage based on parallelism
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    def create_parser(self) -> IPdfParser:
        """Return configured parser instance based on configuration (fitz, pdfplumber, etc.)."""
        if self.parser_mode == "fitz":
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PyMuPDFParser
        elif self.parser_mode == "pdfplumber":
            return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Use PdfPlumberParser
        elif self.parser_mode == "auto":
            # Automatically choose the best parser
            try:
                return PdfPlumberParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
            except Exception:
                return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)  # Fallback to PyMuPDFParser
        else:
            raise ValueError(f"Unsupported parser mode: {self.parser_mode}")  # Error if parser mode is unsupported

    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)  # Return parallel parser for distribution
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pdfplumber_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
import pdfplumber  # Import pdfplumber for better multi-column text extraction
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PdfPlumberParser(IPdfParser):
    """Robust PDF parser using pdfplumber to handle multi-column text extraction and OCR fallback."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(pdf_path) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page using pdfplumber
        with pdfplumber.open(pdf_path) as pdf:
            for page_index, page in enumerate(pdf.pages):
                if self.max_pages and page_index >= self.max_pages:
                    break

                # Extract text from the page considering multi-column layout
                page_body = self._extract_text_from_page(page)
                if page_body:
                    text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(pdf.pages),
        }

        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_text_from_page(self, page) -> str:
        """
        Extract text from a page considering the multi-column layout using pdfplumber.
        
        :param page: The pdfplumber page object.
        :return: Extracted text from the page.
        """
        # Split the page into columns using pdfplumber's layout extraction
        width = page.width
        left_column = page.within_bbox((0, 0, width / 3, page.height))  # Left column
        middle_column = page.within_bbox((width / 3, 0, 2 * width / 3, page.height))  # Middle column
        right_column = page.within_bbox((2 * width / 3, 0, width, page.height))  # Right column

        # Extract text from each column
        left_text = left_column.extract_text()
        middle_text = middle_column.extract_text()
        right_text = right_column.extract_text()

        # Combine the text from all columns
        combined_text = f"{left_text}\n{middle_text}\n{right_text}"

        return combined_text

    def _extract_toc_titles(self, pdf_path: Path) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param pdf_path: The path to the PDF file.
        :return: A list of TOC titles.
        """
        toc_titles = []
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                # Try to get TOC titles from the page (if any)
                toc = page.extract_text()
                if toc and re.search(r"(?i)\btable\s+of\s+contents\b", toc):
                    toc_titles.append(toc)
        return toc_titles

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
import pytesseract
import fitz  # PyMuPDF
from PIL import Image, ImageEnhance, ImageFilter
import os
import re
from typing import Dict, Any, List
from pathlib import Path
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser

class PyMuPDFParser(IPdfParser):
    """Robust PDF parser using PyMuPDF with layout-based filtering of non-body text and OCR fallback for scanned PDFs."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None, use_ocr: bool = True):
        """
        Initialize the parser with options for excluding the table of contents and limiting the number of pages.
        
        :param exclude_toc: Flag to exclude table of contents pages from extraction.
        :param max_pages: Maximum number of pages to extract text from.
        :param use_ocr: Flag to use OCR if no text is found.
        """
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages
        self.use_ocr = use_ocr  # Flag to use OCR if no text is found

    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """
        Extract clean body text (including abstract) and minimal metadata from a PDF file.

        :param pdf_path: Path to the PDF file to extract text from.
        :return: A dictionary with 'text' (extracted body text) and 'metadata' (file metadata).
        """
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []  # To hold the extracted text from each page
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []  # Extract TOC titles to filter out

        # Extract body text from each page
        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            blocks = page.get_text("blocks")

            # If no text is found, use OCR
            if not blocks and self.use_ocr:
                blocks = self._apply_ocr_to_page(page)

            page_body = self._extract_body_from_blocks(blocks)
            if not page_body:
                continue  # Skip empty pages or pages without meaningful content
            # Skip ToC-like pages entirely
            if self.exclude_toc and self._looks_like_toc_page(page_body, toc_titles):
                continue
            text_blocks.append(page_body)

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text.strip(), "metadata": metadata}

    def _extract_toc_titles(self, doc) -> List[str]:
        """
        Extract table of contents (TOC) titles from the PDF document to filter them out from the main text.
        
        :param doc: The PyMuPDF document object.
        :return: A list of TOC titles.
        """
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """
        Heuristic to detect table of contents pages by checking if the text contains typical TOC structure.

        :param text: The text from a page.
        :param toc_titles: The list of TOC titles to match against.
        :return: True if the page looks like a TOC, False otherwise.
        """
        if not text or len(text) < 100:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    def _extract_body_from_blocks(self, blocks: list) -> str:
        """
        Optimized extraction to ensure only meaningful body text (e.g., abstract, sections) is included.
        
        :param blocks: The list of text blocks extracted from the page.
        :return: A string of extracted body text.
        """
        # Initialize variables for abstract detection and block storage
        abstract_found = False
        abstract_block = []
        candidate_blocks = []

        for (x0, y0, x1, y1, text, *_ ) in blocks:
            if not text or len(text.strip()) < 30:
                continue  # Skip empty or short text blocks

            # If the abstract hasn't been found, try to capture it
            if not abstract_found and re.search(r"(?i)\babstract\b", text):
                abstract_found = True
                abstract_block.append(text.strip())
                continue  # Skip content after abstract until the main body starts

            # Filter out metadata and irrelevant blocks
            if re.search(r"(?i)(title|author|doi|keywords|arxiv|university|faculty|institute|version)", text):
                continue  # Skip metadata blocks
            if len(text.strip().split()) < 20:
                continue  # Skip short blocks

            # Add the remaining relevant blocks
            candidate_blocks.append(text.strip())

        # Add the abstract at the beginning of the body if it was found
        if abstract_found:
            candidate_blocks.insert(0, "\n".join(abstract_block))

        return "\n".join(candidate_blocks)

    def _remove_residual_metadata(self, text: str) -> str:
        """
        Remove residual header/footer and metadata-like fragments, excluding abstract.
        
        :param text: The extracted text to clean up.
        :return: Cleaned text with metadata removed.
        """
        # Extended patterns for filtering out metadata
        patterns = [
            r"(?im)^table\s+of\s+contents.*$",
            r"(?im)^inhaltsverzeichnis.*$",
            r"(?im)^\s*(title|author|doi|keywords|arxiv|university|faculty|institute|version).*?$",
            r"(?im)\bpage\s+\d+\b",  # Remove page numbers
            r"(?m)^\s*\d+\s*$",  # Remove single digit page numbers like "1", "2", "3"
            r"(?i)\bhttps?://\S+",  # Remove URLs
            r"(?i)\b\w+@\w+\.\w+",  # Remove email addresses
            r"(?i)(RSISE|NICTA|university|faculty|institute)\b.*",  # Remove institution names
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.DOTALL)

        # Optionally, remove excessive line breaks or extra spaces
        text = re.sub(r"\n{2,}", "\n\n", text)  # Merge consecutive line breaks
        return text.strip()

    def _apply_ocr_to_page(self, page) -> List[str]:
        """
        Apply OCR to a page if the page does not contain text. Uses Tesseract to extract text from scanned pages.
        
        :param page: The page to apply OCR on.
        :return: A list with OCR extracted text.
        """
        # Convert the page to an image
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        
        # Preprocess the image (optional, if necessary)
        img = self._preprocess_image_for_ocr(img)

        # Apply OCR
        ocr_text = pytesseract.image_to_string(img)
        return [(0, 0, pix.width, pix.height, ocr_text)]  # Return OCR'd text as a block

    def _preprocess_image_for_ocr(self, img: Image) -> Image:
        """
        Preprocess the image to improve OCR accuracy by enhancing contrast and sharpness.
        
        :param img: The image to preprocess.
        :return: The preprocessed image ready for OCR.
        """
        # Sharpen the image
        img = img.filter(ImageFilter.SHARPEN)
        
        # Increase the contrast of the image
        enhancer = ImageEnhance.Contrast(img)
        img = enhancer.enhance(2)  # Enhance the contrast by a factor of 2
        
        return img
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\file_utils.py ==== 
import os
from PyPDF2 import PdfFileReader
from pathlib import Path

def ensure_dir(path: Path):
    """Create directory if it doesn't exist."""
    path.mkdir(parents=True, exist_ok=True)

def get_file_size(file_path: Path) -> int:
    """Returns the file size in bytes."""
    return os.path.getsize(file_path)

def get_pdf_page_count(file_path: Path) -> int:
    """Returns the number of pages in a PDF."""
    with open(file_path, "rb") as file:
        reader = PdfFileReader(file)
        return reader.getNumPages()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\language_detector.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\performance_timer.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\utils\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\llm_orchestrator.py ==== 
# src/core/llm/llm_orchestrator.py
from __future__ import annotations
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
import json
import time
import numpy as np
from sentence_transformers import SentenceTransformer, util

from src.core.config.config_loader import ConfigLoader
from src.core.retrieval.orchestrator.retrieval_orchestrator import RetrievalOrchestrator
from src.core.llm.ollama_llm import OllamaLLM
from src.core.prompt.prompt_orchestrator import PromptOrchestrator
from src.core.prompt.query.prompt_builder import PromptBuilder


class LLMOrchestrator:
    """Coherent LLM orchestration pipeline with validated logging and deterministic retrieval."""

    def __init__(self, config_path: str = "configs/llm.yaml", logs_dir: str = "data/logs"):
        self.cfg = ConfigLoader(config_path).config

        self.logger = logging.getLogger("LLMOrchestrator")
        self._setup_logging()

        self.logs_dir = Path(logs_dir)
        self.logs_dir.mkdir(parents=True, exist_ok=True)

        self.prompt_phase = PromptOrchestrator()

        self.retriever = self._init_retriever()

        citation_style = (
            self.cfg.get("generation", {})
            .get("citations", {})
            .get("style", "ieee")
            .lower()
        )
        self.prompt_builder = PromptBuilder(citation_style=citation_style)

        self.llm = self._init_llm()

        # Embedding model for factual consistency revision
        self._ff_model = SentenceTransformer(
            self.cfg.get("generation", {}).get(
                "faith_embed_model", "multi-qa-mpnet-base-dot-v1"
            )
        )

        self.logger.info(
            f"LLMOrchestrator initialized (citations={citation_style}, logs_dir={self.logs_dir})."
        )

    # --------------------------------------------------------------
    def _setup_logging(self) -> None:
        # Configure logging level from YAML
        lvl = self.cfg.get("global", {}).get("log_level", "INFO").upper()
        logging.basicConfig(
            level=getattr(logging, lvl, logging.INFO),
            format="%(levelname)s | %(message)s",
        )
        self.logger.info("Logging configured.")

    # --------------------------------------------------------------
    def _init_retriever(self) -> RetrievalOrchestrator:
        # Initialize retrieval subsystem
        r = RetrievalOrchestrator()
        self.logger.info("Retriever initialized.")
        return r

    # --------------------------------------------------------------
    def _init_llm(self) -> OllamaLLM:
        # Initialize LLM backend
        profile = (
            self.cfg.get("generation", {})
            .get("llm", {})
            .get("profile", "mistral_7b")
        )
        llm = OllamaLLM(config_path="configs/llm.yaml", profile=profile)
        self.logger.info(f"LLM backend ready (profile={profile}).")
        return llm

    # --------------------------------------------------------------
    def _split_into_sentences(self, text: str) -> list[str]:
        # Very simple sentence splitter
        import re
        s = re.split(r"(?<=[.!?])\s+", text.strip())
        return [t for t in s if t]

    # --------------------------------------------------------------
    def _fact_check_and_revise(
        self,
        answer: str,
        evidence_texts: List[str],
        min_sim: float = 0.22,
        drop_below: float = 0.15,
    ) -> str:
        # Embedding based factual consistency refinement
        if not answer or not evidence_texts:
            return answer

        sents = self._split_into_sentences(answer)
        if not sents:
            return answer

        ans_emb = self._ff_model.encode(
            sents, normalize_embeddings=True, convert_to_tensor=True
        )
        ev_emb = self._ff_model.encode(
            evidence_texts, normalize_embeddings=True, convert_to_tensor=True
        )

        sims = util.cos_sim(ans_emb, ev_emb).cpu().numpy()

        keep, revise = [], []
        for i, sent in enumerate(sents):
            max_sim = float(np.max(sims[i])) if sims.shape[1] > 0 else 0.0
            if max_sim >= min_sim:
                keep.append(sent)
            elif max_sim >= drop_below:
                revise.append(sent)

        if not revise:
            return " ".join(keep) if keep else answer

        instruction = (
            "Revise the following sentences to align strictly with the evidence snippets. "
            "If support is insufficient write 'insufficient evidence'. Preserve numeric citations."
        )

        ev_join = "\n\n".join(f"[EVID{i+1}] {e}" for i, e in enumerate(evidence_texts[:8]))
        task = instruction + "\n\n" + ev_join + "\n\nSentences:\n" + "\n".join(f"- {s}" for s in revise)

        try:
            revised = self.llm.generate(task)
            return " ".join(keep + [revised])
        except Exception:
            return " ".join(keep) if keep else answer

    # --------------------------------------------------------------
    def _safe_atomic_write(self, path: Path, payload: Dict[str, Any]) -> None:
        # Atomic write of JSON files to prevent partial logs
        tmp = path.with_suffix(".tmp")
        tmp.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        tmp.replace(path)

    # --------------------------------------------------------------
    def _compose_full_prompt(
        self,
        refined_query: str,
        intent: str,
        retrieved_chunks: List[Dict[str, Any]],
    ) -> str:
        # Assemble system prompt with context and citation constraints
        sys_p = self.prompt_builder.build_prompt(refined_query, intent)

        if intent == "chronological":
            def sy(meta: Dict[str, Any]) -> int:
                try:
                    return int(meta.get("year", 9999))
                except Exception:
                    return 9999

            retrieved_chunks = sorted(retrieved_chunks, key=lambda c: sy(c.get("metadata", {})))
            self.logger.info("Chunks sorted chronologically.")

        unique_sources: Dict[str, int] = {}
        ref_data: Dict[int, Dict[str, Any]] = {}

        for chunk in retrieved_chunks:
            meta = chunk.get("metadata", {}) or {}
            src = meta.get("source_file", "Unknown.pdf")
            year = meta.get("year", "n/a")
            if src not in unique_sources:
                ref_id = len(unique_sources) + 1
                unique_sources[src] = ref_id
                ref_data[ref_id] = {"source_file": src, "year": year}

        lines = []
        lines.append(sys_p.strip())
        lines.append("")
        lines.append(f"Refined query:\n{refined_query.strip()}\n")
        lines.append("Context snippets:")

        for chunk in retrieved_chunks:
            meta = chunk.get("metadata", {}) or {}
            src = meta.get("source_file", "Unknown.pdf")
            year = meta.get("year", "n/a")
            ref_id = unique_sources.get(src, 0)

            header = f"[{ref_id}] {src} ({year})"
            text = chunk.get("text", "").strip() or "[Empty chunk]"

            lines.append(header)
            lines.append(text)
            lines.append("")

        lines.append(
            "Answer the refined query using only the context above. "
            "Use numeric citations. If a claim lacks evidence write 'insufficient evidence'."
        )
        lines.append("")
        lines.append("Reference index:")

        for rid, meta in ref_data.items():
            lines.append(f"[{rid}] {meta['source_file']} ({meta['year']})")

        lines.append("")
        lines.append("IMPORTANT OUTPUT REQUIREMENTS:")
        lines.append("Your final answer must end with a section titled 'References'.")
        lines.append("List all unique PDFs exactly once in the format:")
        lines.append("[n] FILENAME.pdf (YEAR)")
        lines.append("This section must be at the end of your output.")

        return "\n".join(lines)

    # --------------------------------------------------------------
    def _log_llm_input(self, query: str, intent: str, prompt: str, chunks: List[Dict[str, Any]]) -> None:
        # Log input with atomic write
        ts = time.strftime("%Y-%m-%dT%H-%M-%S")
        path = self.logs_dir / f"llm_input_{ts}.json"

        payload = {
            "timestamp": ts,
            "query_refined": query,
            "intent": intent,
            "prompt_final_to_llm": prompt,
            "chunks_final_to_llm": chunks,
        }

        self._safe_atomic_write(path, payload)
        self.logger.info(f"Logged refined LLM input → {path}")

    # --------------------------------------------------------------
    def _log_llm_run(
        self,
        query: str,
        intent: str,
        retrieved: List[Dict[str, Any]],
        output: str,
        prompt: str,
    ) -> str:
        # Log completed LLM output
        ts = time.strftime("%Y-%m-%dT%H-%M-%S")
        qid = "".join(ch if ch.isalnum() or ch in "-_" else "_" for ch in query)[:80] or "query"

        path = self.logs_dir / f"llm_{ts}.json"
        payload = {
            "timestamp": ts,
            "query_id": qid,
            "query": query,
            "query_refined": query,
            "intent": intent,
            "prompt_final_to_llm": prompt,
            "retrieved_chunks": retrieved,
            "model_output": output,
        }

        self._safe_atomic_write(path, payload)
        return qid

    # --------------------------------------------------------------
    def process_query(self, query_obj: Optional[Dict[str, Any]]) -> str:
        # Full pipeline execution
        if not query_obj:
            self.logger.warning("Empty query object received.")
            return ""

        query = query_obj.get("refined_query") or query_obj.get("processed_query")
        intent = query_obj.get("intent", "conceptual")

        if not query or not query.strip():
            self.logger.warning("Query text missing or empty.")
            return ""

        query = query.strip()

        self.logger.info(f"Starting pipeline | intent={intent} | query='{query}'")

        try:
            retrieved = self.retriever.retrieve(query, intent)
        except Exception as e:
            self.logger.exception(f"Retrieval failed: {e}")
            return ""

        try:
            prompt = self._compose_full_prompt(query, intent, retrieved)
        except Exception as e:
            self.logger.exception(f"Prompt construction failed: {e}")
            return ""

        self._log_llm_input(query, intent, prompt, retrieved)

        try:
            raw_ans = self.llm.generate(prompt.strip())
            evidence = [c.get("text", "") for c in retrieved if c.get("text")]
            final_ans = self._fact_check_and_revise(raw_ans, evidence)

            qid = self._log_llm_run(query, intent, retrieved, final_ans, prompt)
            self.logger.info(f"LLM generation successful (id={qid}).")

            return final_ans.strip()
        except Exception as e:
            self.logger.exception(f"LLM generation failed: {e}")
            return ""

    # --------------------------------------------------------------
    def close(self) -> None:
        # Safe close of all subcomponents
        self.logger.info("Closing LLMOrchestrator.")
        try:
            if hasattr(self.retriever, "close"):
                self.retriever.close()
        except Exception:
            pass

        try:
            if hasattr(self.llm, "close"):
                self.llm.close()
        except Exception:
            pass

        try:
            if hasattr(self.prompt_phase, "close"):
                self.prompt_phase.close()
        except Exception:
            pass

        self.logger.info("LLMOrchestrator closed.")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\ollama_llm.py ==== 
# src/core/llm/ollama_llm.py
from __future__ import annotations
import subprocess
import logging
from typing import Any
from src.core.llm.interfaces.i_llm import ILLM
from src.core.config.config_loader import ConfigLoader

logger = logging.getLogger(__name__)


class OllamaLLM(ILLM):
    """
    Lightweight local Ollama backend.

    - Model selection is controlled via `profiles[profile].model`.
    - Sampling hyperparameters (temperature, max_tokens) are shared globally
      via the `sampling` section in configs/llm.yaml for fair comparisons.
    """

    def __init__(self, config_path: str = "configs/llm.yaml", profile: str | None = None):
        # Load full configuration
        cfg = ConfigLoader(config_path).config
        global_cfg = cfg.get("global", {})
        profiles = cfg.get("profiles", {})
        sampling_cfg = cfg.get("sampling", {})

        # Resolve active profile for this instance
        self.profile = profile or "mistral_7b"
        profile_cfg = profiles.get(self.profile, {})

        # Load model configuration from profile
        self.model = profile_cfg.get("model", "mistral:7b-instruct")
        self.auto_pull = bool(profile_cfg.get("auto_pull", True))

        # Shared sampling configuration for all models
        self.temperature = float(sampling_cfg.get("temperature", 0.1))
        self.max_tokens = int(sampling_cfg.get("max_tokens", 2048))

        # Configure logging
        log_level = global_cfg.get("log_level", "INFO").upper()
        logging.basicConfig(
            level=getattr(logging, log_level, logging.INFO),
            format="%(levelname)s | %(message)s",
        )

        logger.info(
            f"OllamaLLM ready (profile={self.profile}, model={self.model}, "
            f"temperature={self.temperature}, max_tokens={self.max_tokens})"
        )
        self._ensure_model_available()

    # ------------------------------------------------------------------
    def _ensure_model_available(self) -> None:
        # Ensure the configured Ollama model is available locally
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
            )
            if result.returncode != 0:
                raise RuntimeError(result.stderr.strip())

            if self.model.lower() not in result.stdout.lower():
                if not self.auto_pull:
                    logger.warning(
                        f"Model '{self.model}' missing (auto_pull disabled)."
                    )
                    return
                logger.info(f"Pulling model '{self.model}' ...")
                pull_result = subprocess.run(
                    ["ollama", "pull", self.model],
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    errors="replace",
                )
                if pull_result.returncode != 0:
                    raise RuntimeError(pull_result.stderr.strip())
                logger.info(f"Successfully pulled '{self.model}'.")
            else:
                logger.info(f"Model '{self.model}' available locally.")
        except Exception as e:
            logger.error(f"Model check failed: {e}")

    # ------------------------------------------------------------------
    def generate(self, prompt: str) -> str:
        """
        Run the Ollama model and return its output.

        Expectation:
        - `prompt` is the complete composed prompt
          (instruction + query + all context chunks).
        - Sampling hyperparameters are configured globally in YAML.
        """
        try:
            # Note: the basic CLI form does not expose all sampling options.
            # Here we rely on Ollama's defaults plus model-level configuration.
            cmd = ["ollama", "run", self.model, prompt]
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
            )

            if result.returncode != 0:
                raise RuntimeError(result.stderr.strip())

            output = result.stdout.strip()
            logger.info(
                f"Ollama generation successful | model={self.model} | len={len(output)}"
            )
            return output

        except Exception as e:
            logger.exception(f"Ollama model run failed: {e}")
            raise

    # ------------------------------------------------------------------
    def close(self) -> None:
        """No persistent connections or open streams."""
        logger.info("OllamaLLM closed cleanly.")
        return
.
==== C:\Users\katha\historical-drift-analyzer\src\core\llm\interfaces\i_llm.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod


class ILLM(ABC):
    """Interface for any local or remote LLM backend."""

    @abstractmethod
    def generate(self, prompt: str) -> str:
        """
        Generate an answer given a COMPLETE prompt.

        Der Prompt enthält:
        - Systeminstruktion,
        - User-Query,
        - eingebettete Kontextsnippets (falls RAG).
        """
        raise NotImplementedError

    @abstractmethod
    def close(self) -> None:
        """Gracefully close model connection or session."""
        raise NotImplementedError
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\formatters.py ==== 
from __future__ import annotations
import logging
import json
from datetime import datetime

try:
    # Use colorama for colored console output if available
    from colorama import Fore, Style
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class PlainFormatter(logging.Formatter):
    # Simple, deterministic log format without colors
    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%Y-%m-%d %H:%M:%S")
        return f"{ts} | {record.levelname:<8} | {record.getMessage()}"


class ColorFormatter(logging.Formatter):
    # Colored console formatter for better readability
    COLORS = {
        "DEBUG": Fore.BLUE,
        "INFO": Fore.GREEN,
        "WARNING": Fore.YELLOW,
        "ERROR": Fore.RED,
        "CRITICAL": Fore.MAGENTA,
    }

    def format(self, record: logging.LogRecord) -> str:
        ts = datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        color = self.COLORS.get(record.levelname, "")
        reset = Style.RESET_ALL if COLORAMA_AVAILABLE else ""
        return f"{color}{ts} | {record.levelname:<8} | {record.getMessage()}{reset}"


class JSONFormatter(logging.Formatter):
    # JSON-based formatter for structured, machine-readable logs
    def format(self, record: logging.LogRecord) -> str:
        entry = {
            "timestamp": datetime.utcfromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }
        return json.dumps(entry, ensure_ascii=False)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\logger.py ==== 
from __future__ import annotations
import logging
import os
import json
from pathlib import Path
from logging.handlers import RotatingFileHandler
from datetime import datetime
from typing import Any, Dict
from src.core.logging.formatters import PlainFormatter, ColorFormatter, JSONFormatter

try:
    # Initialize colorama if available for colored console output
    from colorama import init as colorama_init
    colorama_init()
    COLORAMA_AVAILABLE = True
except ImportError:
    COLORAMA_AVAILABLE = False


class StructuredLogger:
    # Singleton-based, configurable logger for all pipeline phases
    _instance: logging.Logger | None = None

    @classmethod
    def get_logger(cls, name: str = "HDA", config: Dict[str, Any] | None = None) -> logging.Logger:
        # Reuse existing logger if already created
        if cls._instance:
            return cls._instance

        cfg = config or {}
        level = getattr(logging, cfg.get("level", "INFO").upper(), logging.INFO)
        log_to_file = cfg.get("log_to_file", True)
        log_dir = Path(cfg.get("log_dir", "logs"))
        file_name = cfg.get("file_name", "hda.log")
        rotate = cfg.get("rotate_logs", True)
        json_log = cfg.get("json_log", False)
        console_color = cfg.get("console_color", True)

        # Create and configure logger
        logger = logging.getLogger(name)
        logger.setLevel(level)
        logger.propagate = False

        # Console handler setup
        console_handler = logging.StreamHandler()
        if console_color and COLORAMA_AVAILABLE:
            console_handler.setFormatter(ColorFormatter())
        else:
            console_handler.setFormatter(PlainFormatter())
        logger.addHandler(console_handler)

        # File handler setup
        if log_to_file:
            log_dir.mkdir(parents=True, exist_ok=True)
            file_path = log_dir / file_name
            if rotate:
                handler = RotatingFileHandler(file_path, maxBytes=5_000_000, backupCount=5, encoding="utf-8")
            else:
                handler = logging.FileHandler(file_path, encoding="utf-8")
            handler.setFormatter(JSONFormatter() if json_log else PlainFormatter())
            logger.addHandler(handler)

        # Store singleton instance
        cls._instance = logger
        return logger
.
==== C:\Users\katha\historical-drift-analyzer\src\core\logging\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\prompt_orchestrator.py ==== 
from __future__ import annotations
import logging
import re
from typing import Dict, Any, Optional

from src.core.prompt.query.query_input import QueryInput
from src.core.prompt.query.query_preprocessor import QueryPreprocessor
from src.core.prompt.query.prompt_builder import PromptBuilder


class PromptOrchestrator:
    """
    Coordinates the full prompt understanding pipeline:
    (1) user input → (2) preprocessing → (3) intent classification → (4) intent-guided query reformulation.
    Returns a refined query that aligns with the detected analytical intent.
    """

    def __init__(self, log_level: str = "INFO"):
        self.logger = logging.getLogger("PromptOrchestrator")
        if not self.logger.handlers:
            logging.basicConfig(
                level=getattr(logging, log_level.upper(), logging.INFO),
                format="%(levelname)s | %(message)s"
            )

        self.query_input = QueryInput()
        self.preprocessor = QueryPreprocessor()
        self.prompt_builder = PromptBuilder()
        self.logger.info("PromptOrchestrator initialized successfully.")

    # ------------------------------------------------------------------
    def get_prompt_object(self) -> Dict[str, Any]:
        """
        Execute the complete prompt phase: read, clean, classify, reformulate.

        Returns
        -------
        dict
            {
                "refined_query": str,
                "intent": str
            }
        """
        try:
            raw_query: Optional[str] = self.query_input.read_interactive()
            if not raw_query or not raw_query.strip():
                self.logger.warning("No input provided. Skipping prompt phase.")
                return {}

            # Preprocess & classify
            result: Dict[str, Any] = self.preprocessor.process(raw_query)
            if not result or "processed_query" not in result:
                self.logger.warning("Preprocessing failed or returned empty result.")
                return {}

            processed_query = result["processed_query"]
            intent = result["intent"]

            # Intent-guided reformulation
            refined_query = self.prompt_builder.reformulate_query(processed_query, intent)

            # Cosmetic cleanup: remove duplicated “over time” patterns
            refined_query = re.sub(r"over time\?\s*over time", "over time", refined_query, flags=re.IGNORECASE)
            refined_query = re.sub(r"over time\s+over time", "over time", refined_query, flags=re.IGNORECASE)

            self.logger.info(f"Prompt phase complete → intent='{intent}', refined query='{refined_query}'")
            return {"refined_query": refined_query, "intent": intent}

        except KeyboardInterrupt:
            self.logger.info("Prompt input cancelled by user (Ctrl+C). Exiting interactive mode.")
            raise

        except EOFError:
            self.logger.info("Input stream closed (EOF). Terminating prompt phase.")
            raise KeyboardInterrupt

        except Exception as e:
            self.logger.exception(f"Unexpected error in prompt phase: {e}")
            return {}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\prompt_builder.py ==== 
from __future__ import annotations
import logging
from typing import Literal

logger = logging.getLogger(__name__)
Intent = Literal["chronological", "conceptual", "analytical", "comparative"]
CitationStyle = Literal["ieee"]


class PromptBuilder:
    """
    Builds intent-specific system prompts for RAG/LLM orchestration.

    Fully prevents event-year hallucinations by enforcing that the model
    may use ONLY explicitly stated event years from the snippets.
    """

    def __init__(self, citation_style: CitationStyle = "ieee"):
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
        self.citation_style = citation_style

    # ------------------------------------------------------------------
    def _citation_instruction(self) -> str:
        """IEEE-style citation policy with strict anti-hallucination rules."""
        return (
            "Use numeric IEEE-style citations [1], [2], etc., for statements supported by the provided snippets. "
            "Each number corresponds to one unique PDF listed below. Multiple snippets originating from the same PDF share the same number. "
            "Never assign multiple citation numbers to the same source.\n\n"

            "**Your final answer MUST end with a separate section titled 'References'.**\n"
            "This section MUST list all unique PDFs exactly once, in the following strict format:\n"
            "[n] FILENAME.pdf (YEAR)\n\n"

            "Do not fabricate author names, journals, or article titles — only use the given filename and metadata year.\n\n"

            "Temporal Attribution Rules:\n"
            "1. You may ONLY use event years that appear explicitly in the snippet text.\n"
            "2. If the snippet text explicitly contains a year (e.g., 'In the 1950s', 'In 1976'), treat that as the factual historical reference.\n"
            "3. If a snippet DOES NOT contain an explicit event year, you MUST NOT guess, infer, approximate, or estimate any year.\n"
            "   Instead, write exactly: '(event year not stated; described in YEAR PDF [n])'.\n"
            "4. The metadata publication year indicates only when the PDF was published, not when the events occurred.\n"
            "5. Never replace or override an explicit event year with a metadata year.\n"
            "6. Never deduce approximate historical periods from textual content (e.g., never infer '1990s' unless explicitly stated).\n\n"

            "Output Structuring Guidelines:\n"
            "- For every key historical or conceptual point:\n"
            "  • If an explicit event year exists in the snippet → include it.\n"
            "  • If no explicit event year exists → write '(event year not stated; described in YEAR PDF [n])'.\n"
            "- Recommended dual-year structure:\n"
            "  • (1950s; described in 2025 PDF [7]) The Turing Test was proposed as a benchmark.\n"
            "This dual timestamping ensures full temporal grounding without hallucination."
        )

    # ------------------------------------------------------------------
    def _system_prompt_for(self, intent: Intent) -> str:
        """Return intent-specific system instruction with hallucination-proof constraints."""
        cite_rule = self._citation_instruction()

        end_rule = (
            "\n\nIMPORTANT:\n"
            "**Your output MUST end with a final section titled 'References'.**\n"
            "This section must list all unique PDFs exactly once in IEEE numeric format.\n"
        )

        if intent == "chronological":
            return (
                "You are an analytical historian of Artificial Intelligence. "
                "Describe how the concept evolved across time, highlighting paradigm shifts, milestones, and key theoretical transformations. "
                "Present findings in a coherent historical narrative ordered strictly by explicit *event years* found in the snippets. "
                "If a snippet provides no explicit event year, you MUST write '(event year not stated; described in YEAR PDF [n])'. "
                "Never guess or estimate historical periods under any circumstances. "
                "Avoid enumeration; emphasize causal relations and conceptual transitions. "
                f"{cite_rule}"
                f"{end_rule}"
            )

        if intent == "conceptual":
            return (
                "You are a domain expert in Artificial Intelligence. Provide a precise definition, clarify theoretical foundations, "
                "and explain how interpretations evolved across time and publications. "
                "Use event years ONLY if explicitly stated in the snippets. "
                f"{cite_rule}"
                f"{end_rule}"
            )

        if intent == "analytical":
            return (
                "You are a rigorous AI researcher. Analyze mechanisms, methodologies, and implications over time. "
                "Event years may only be used if explicitly present in the snippet text. "
                f"{cite_rule}"
                f"{end_rule}"
            )

        # comparative
        return (
            "You are a comparative analyst. Compare major frameworks or schools of thought, "
            "specifying explicit historical information only when stated in the provided snippets. "
            "Never infer missing event years. "
            f"{cite_rule}"
            f"{end_rule}"
        )

    # ------------------------------------------------------------------
    def reformulate_query(self, query: str, intent: Intent) -> str:
        """Intent-guided canonical reformulation."""
        if not query or not query.strip():
            raise ValueError("Empty query cannot be reformulated")

        q = query.strip()

        if intent == "chronological":
            return (
                f"Trace the historical development and evolution of {q} strictly through the explicit event years present in the snippets. "
                "If no explicit event year is present for a point, note that the event year is not stated."
            )

        if intent == "conceptual":
            return (
                f"Define {q}, describe its theoretical foundations, and explain how definitions evolved historically across publications."
            )

        if intent == "analytical":
            return (
                f"Analyze the mechanisms, strengths, and limitations of {q}, noting origins only when explicitly stated."
            )

        if intent == "comparative":
            return (
                f"Compare and contrast the main theoretical perspectives on {q}, grounding historical claims only in explicit snippet content."
            )

        return q

    # ------------------------------------------------------------------
    def build_prompt(self, query: str, intent: Intent) -> str:
        """Return the complete system prompt including citation + temporal grounding."""
        if not query or not query.strip():
            raise ValueError("Empty query passed to PromptBuilder")
        return self._system_prompt_for(intent)
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_classifier.py ==== 
# src/core/prompt/query/query_classifier.py 
from __future__ import annotations
import logging
import numpy as np
from typing import Literal, Dict, List, Optional
from sentence_transformers import SentenceTransformer, util

Intent = Literal["chronological", "conceptual", "analytical", "comparative"]

class QueryClassifier:
    """
    Embedding-based intent classifier.
    No heuristics, no language dependency, fully model-driven.
    """
    def __init__(
        self,
        model_name: str = "all-MiniLM-L6-v2",
        label_texts: Optional[Dict[str, str]] = None
    ):
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

        self.model = SentenceTransformer(model_name)
        # Canonical label representations (configurable)
        self.label_texts = label_texts or {
            "chronological": "questions about historical development or changes over time",
            "conceptual": "questions asking for definition, explanation or theoretical meaning",
            "analytical": "questions asking for comparison, evaluation or analysis",
            "comparative": "questions asking for contrast or difference between ideas",
        }

        self.labels = list(self.label_texts.keys())
        # Precompute label embeddings for efficient similarity calculation
        self.label_embeddings = self.model.encode(
            list(self.label_texts.values()), normalize_embeddings=True
        )
        self.logger.info(f"Initialized semantic intent classifier with {len(self.labels)} labels")

    # ------------------------------------------------------------------
    def classify(self, query: str) -> Intent:
        """Compute embedding similarity to predefined intent prototypes."""
        if not query or not query.strip():
            return "conceptual"
        # Encode user query
        q_emb = self.model.encode(query, normalize_embeddings=True)
        # Compute cosine similarity between query and label embeddings
        sims = util.cos_sim(q_emb, self.label_embeddings)[0].cpu().numpy()
        # Select intent with highest similarity
        idx = int(np.argmax(sims))
        intent = self.labels[idx]
        self.logger.info(f"Predicted semantic intent='{intent}' (sim={sims[idx]:.3f})")
        return intent  # type: ignore
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_input.py ==== 
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional

logger = logging.getLogger(__name__)

@dataclass
class QueryInputConfig:
    prompt_text: str = "> "
    strip_input: bool = True
    allow_empty: bool = False

class QueryInput:
    """Handles raw query intake (interactive or programmatic)."""
    def __init__(self, cfg: Optional[QueryInputConfig] = None):
        self.cfg = cfg or QueryInputConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")

    def read_interactive(self) -> Optional[str]:
        """Read query from stdin (interactive mode)."""
        try:
            raw = input(self.cfg.prompt_text)
        except (KeyboardInterrupt, EOFError):
            # Raise further to allow graceful termination at orchestrator level
            print()  # newline for clean CLI exit
            raise
        q = raw.strip() if self.cfg.strip_input else raw
        if not q and not self.cfg.allow_empty:
            logger.warning("Empty query ignored.")
            return None
        return q

    def from_program(self, query: Optional[str]) -> Optional[str]:
        """Receive query from another process or script."""
        if query is None:
            return None
        return query.strip() if self.cfg.strip_input else query
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\query_preprocessor.py ==== 
from __future__ import annotations
import logging
from dataclasses import dataclass
from typing import Optional
from src.core.prompt.query.query_classifier import QueryClassifier

logger = logging.getLogger(__name__)

@dataclass
class QueryPreprocessorConfig:
    lowercase: bool = True
    remove_double_spaces: bool = True
    embedding_model: str = "all-MiniLM-L6-v2"

class QueryPreprocessor:
    """Clean, normalize, and classify queries without heuristics."""
    def __init__(self, cfg: Optional[QueryPreprocessorConfig] = None):
        self.cfg = cfg or QueryPreprocessorConfig()
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
        # Initialize classifier for semantic intent detection
        self.classifier = QueryClassifier(model_name=self.cfg.embedding_model)

    def validate(self, query: Optional[str]) -> str:
        """Ensure query is not empty or invalid."""
        if not query or not query.strip():
            raise ValueError("Empty query is not allowed")
        return query.strip()

    def clean(self, query: str) -> str:
        """Normalize casing and spacing according to configuration."""
        q = query.lower() if self.cfg.lowercase else query
        if self.cfg.remove_double_spaces:
            q = " ".join(q.split())
        return q.strip()

    def process(self, raw_query: Optional[str]) -> dict:
        """Return cleaned text + semantic intent."""
        q = self.validate(raw_query)
        clean_q = self.clean(q)
        intent = self.classifier.classify(clean_q)
        logger.info(f"Processed query='{clean_q}' intent='{intent}'")
        return {"raw_query": q, "processed_query": clean_q, "intent": intent}
.
==== C:\Users\katha\historical-drift-analyzer\src\core\prompt\query\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\faiss_retriever.py ==== 
# src/core/retrieval/faiss_retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import re
import numpy as np
import logging
from math import exp
from src.core.retrieval.interfaces.i_retriever import IRetriever


class FAISSRetriever(IRetriever):
    """FAISS-based semantic retriever with optional temporal and source diversification."""

    def __init__(
        self,
        vector_store_dir: str,
        model_name: str,
        top_k_retrieve: int = 50,
        normalize_embeddings: bool = True,
        use_gpu: bool = False,
        similarity_metric: str = "cosine",
        temporal_awareness: bool = True,
        temporal_tau: float = 8.0,
        temporal_weight: float = 0.30,
        valid_year_range: Tuple[int, int] = (1900, 2100),
        diversify_sources: bool = True,  # enable balanced source selection
    ):
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError(
                "faiss-cpu and sentence-transformers are required. "
                "Install via: poetry add faiss-cpu sentence-transformers"
            ) from e

        self.faiss = faiss
        self.vector_store_dir = Path(vector_store_dir).resolve()
        self.index_path = self.vector_store_dir / "index.faiss"
        self.meta_path = self.vector_store_dir / "metadata.jsonl"

        if not self.index_path.exists() or not self.meta_path.exists():
            raise FileNotFoundError(f"Incomplete vector store: {self.vector_store_dir}")

        self.model = SentenceTransformer(model_name)
        self.top_k_retrieve = int(top_k_retrieve)
        self.normalize_embeddings = bool(normalize_embeddings)
        self.use_gpu = bool(use_gpu)
        self.similarity_metric = similarity_metric.lower().strip()
        self.temporal_awareness = bool(temporal_awareness)
        self.temporal_tau = float(temporal_tau)
        self.temporal_weight = float(temporal_weight)
        self.valid_year_range = valid_year_range
        self.diversify_sources = bool(diversify_sources)

        if self.similarity_metric not in {"cosine", "dot"}:
            raise ValueError(f"Unsupported similarity metric: {self.similarity_metric}")

        # Load FAISS index
        self.logger.info(f"Loading FAISS index: {self.index_path}")
        self.index = faiss.read_index(str(self.index_path))
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                self.logger.info("FAISS GPU acceleration enabled")
            except Exception as e:
                self.logger.warning(f"GPU mode failed, falling back to CPU: {e}")

        # Load metadata
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.metadata = [json.loads(line) for line in f]

        self.logger.info(
            f"FAISSRetriever ready | entries={len(self.metadata)} | metric={self.similarity_metric.upper()} "
            f"| temporal_awareness={self.temporal_awareness} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def _encode_query(self, query: str) -> np.ndarray:
        # Encode query with normalization
        vec = self.model.encode([query], normalize_embeddings=self.normalize_embeddings)
        return np.asarray(vec, dtype="float32")

    def _normalize_scores(self, distances: np.ndarray) -> np.ndarray:
        # Normalize distances depending on metric
        if self.similarity_metric in {"cosine", "dot"}:
            return distances
        return 1 - distances

    # ------------------------------------------------------------------
    def _extract_years_from_query(self, query: str) -> List[int]:
        """
        Extract explicit years (e.g. 2021), decade mentions (e.g. 'in the 2020s'),
        or century references (e.g. '21st century') from a text query.
        Returns a sorted list of representative years.
        """
        if not query:
            return []

        text = query.lower()
        years: set[int] = set()
        lo, hi = self.valid_year_range

        # 1) Explicit years (e.g. 1999, 2023)
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", text):
            try:
                y = int(m)
                if lo <= y <= hi:
                    years.add(y)
            except ValueError:
                continue

        # 2) Decades (e.g. 1980s, 2020s, early 1990s)
        for m in re.findall(r"\b(19|20)\d0s\b", text):
            try:
                decade = int(m + "0")
                if lo <= decade <= hi:
                    years.update(range(decade, decade + 10))
            except ValueError:
                continue

        # 3) Centuries (e.g. "20th century", "21st century")
        if "20th century" in text:
            years.update(range(1900, 2000))
        if "21st century" in text:
            years.update(range(2000, 2100))

        # Optional compact logging: only start years of detected decades
        unique_decades = sorted({(y // 10) * 10 for y in years})
        return unique_decades

    # ------------------------------------------------------------------
    def _temporal_modulate(self, base_score: float, doc_year: Optional[int], query_years: List[int]) -> float:
        # Exponential weighting for temporal alignment
        if doc_year is None or not query_years:
            return base_score
        nearest = min(abs(doc_year - y) for y in query_years)
        w = exp(-nearest / max(self.temporal_tau, 1e-6))
        return base_score * (1.0 + self.temporal_weight * w)

    def _safe_doc_year(self, entry: Dict[str, Any]) -> Optional[int]:
        # Extract publication year safely from metadata
        meta = entry.get("metadata", {}) or {}
        y = meta.get("year")
        try:
            yi = int(str(y))
            lo, hi = self.valid_year_range
            if lo <= yi <= hi:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _apply_source_diversity(self, results: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """Ensure chunks from different PDFs dominate early ranks."""
        if not self.diversify_sources or not results:
            return results[:top_k]

        diversified, seen = [], set()
        for r in results:
            src = r["metadata"].get("source_file", "unknown")
            if src not in seen:
                diversified.append(r)
                seen.add(src)
            if len(diversified) >= top_k:
                break

        # Fill up with remaining if fewer than top_k unique
        if len(diversified) < top_k:
            for r in results:
                if r not in diversified:
                    diversified.append(r)
                if len(diversified) >= top_k:
                    break
        return diversified

    # ------------------------------------------------------------------
    def search(self, query: str, top_k: Optional[int] = None, temporal_mode: bool = True) -> List[Dict[str, Any]]:
        """Similarity search with optional temporal and source diversification."""
        self.temporal_awareness = bool(temporal_mode)
        k = int(top_k or self.top_k_retrieve)
        k = max(1, min(k, self.index.ntotal))

        q_vec = self._encode_query(query)
        D, I = self.index.search(q_vec, k * 5 if self.diversify_sources else k)
        scores = self._normalize_scores(D[0])
        query_years = self._extract_years_from_query(query) if self.temporal_awareness else []

        results: List[Dict[str, Any]] = []
        for score, idx in zip(scores, I[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue
            entry = self.metadata[idx]
            doc_year = self._safe_doc_year(entry)
            mod_score = (
                self._temporal_modulate(float(score), doc_year, query_years)
                if self.temporal_awareness else float(score)
            )
            results.append({
                "score": float(mod_score),
                "text": (entry.get("text", "") or "")[:500],
                "metadata": entry.get("metadata", {}) or {},
            })

        results.sort(key=lambda r: r["score"], reverse=True)
        diversified = self._apply_source_diversity(results, top_k=k)

        self.logger.info(
            f"Retrieved {len(diversified)} candidates | temporal_mode={self.temporal_awareness} "
            f"| diversify_sources={self.diversify_sources} | years_in_query={query_years or 'none'}"
        )
        return diversified

    # ------------------------------------------------------------------
    def close(self) -> None:
        self.logger.info("FAISS retriever closed")
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\reranker_factory.py ==== 
# src/core/retrieval/reranker_factory.py
from __future__ import annotations
import logging
from typing import Any, Dict, Type

from src.core.retrieval.temporal_reranker import TemporalReranker
from src.core.retrieval.semantic_reranker import SemanticReranker
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)


class RerankerFactory:
    """Factory for deterministic and clean reranker construction."""

    _registry: Dict[str, Type[IReranker]] = {
        "temporal": TemporalReranker,
        "semantic": SemanticReranker,
    }

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IReranker:
        """Instantiate reranker from configuration dictionary with clean parameter mapping."""
        opts = cfg.get("options", {})
        rtype = str(opts.get("reranker", "semantic")).lower()

        if rtype not in RerankerFactory._registry:
            raise ValueError(
                f"Unsupported reranker type: {rtype}. "
                f"Available: {list(RerankerFactory._registry.keys())}"
            )

        cls = RerankerFactory._registry[rtype]
        logger.info(f"Initializing reranker of type='{rtype}'")

        # -------------------------------------------------------------
        # Temporal Reranker (Golden Middle Version)
        # -------------------------------------------------------------
        if cls is TemporalReranker:
            return TemporalReranker(
                semantic_threshold=float(opts.get("semantic_threshold", 0.40)),
                min_year=int(opts.get("min_year", 1900)),
                must_include=list(opts.get("must_include", [])),
                blacklist_sources=list(opts.get("blacklist_sources", [])),
            )

        # -------------------------------------------------------------
        # Semantic Reranker
        # -------------------------------------------------------------
        if cls is SemanticReranker:
            return SemanticReranker(
                model_name=str(
                    opts.get(
                        "semantic_model",
                        "cross-encoder/ms-marco-MiniLM-L-6-v2"
                    )
                ),
                top_k=int(opts.get("top_k_rerank", 25)),
                semantic_weight=float(opts.get("semantic_weight", 0.75)),
                use_gpu=bool(opts.get("use_gpu", False)),
            )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retriever_factory.py ==== 
# src/core/retrieval/retriever_factory.py
from __future__ import annotations
import logging
from typing import Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

logger = logging.getLogger(__name__)

class RetrieverFactory:
    """Factory für Intent-spezifische Retriever-Instanzen (vollständig entkoppelte Datenflüsse)."""

    @staticmethod
    def build(intent: str, cfg: Dict[str, Any]) -> FAISSRetriever:
        """Erzeuge deterministischen Retriever für den angegebenen Intent."""
        paths = cfg.get("paths", {})
        opts = cfg.get("options", {})

        # Basisparameter, deterministisch
        common_args = dict(
            model_name=opts.get("embedding_model", "all-MiniLM-L6-v2"),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_tau=float(opts.get("temporal_tau", 8.0)),
            temporal_weight=float(opts.get("temporal_weight", 0.30)),
            top_k_retrieve=int(opts.get("top_k_retrieve", 80)),
        )

        # Intent → Index-Ordner Mapping
        index_map = {
            "conceptual": paths.get("conceptual_vector_dir", "data/vector_store/conceptual"),
            "chronological": paths.get("chronological_vector_dir", "data/vector_store/chronological"),
            "analytical": paths.get("analytical_vector_dir", "data/vector_store/analytical"),
            "comparative": paths.get("comparative_vector_dir", "data/vector_store/comparative"),
        }

        # Fallback auf globales Standardverzeichnis
        vdir = index_map.get(intent, paths.get("vector_store_dir", "data/vector_store/default"))
        temporal_flag = (intent == "chronological")

        logger.info(f"Building FAISSRetriever for intent='{intent}' | dir={vdir} | temporal={temporal_flag}")

        return FAISSRetriever(
            vector_store_dir=vdir,
            temporal_awareness=temporal_flag,
            **common_args,
        )
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\semantic_reranker.py ==== 
# src/core/retrieval/semantic_reranker.py
from __future__ import annotations
import logging
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class SemanticReranker(IReranker):
    """Cross-Encoder-basiertes Re-Ranking nach semantischer Relevanz."""

    def __init__(
        self,
        model_name: str,
        top_k: int = 25,
        semantic_weight: float = 0.75,
        use_gpu: bool = False,
    ):
        self.model_name = model_name
        self.top_k = top_k
        self.semantic_weight = semantic_weight
        self.device = "cuda" if use_gpu else "cpu"
        self.model = CrossEncoder(model_name, device=self.device)
        logger.info(f"Semantic Cross-Encoder loaded: {model_name} ({self.device})")

    def rerank(self, docs: List[Dict[str, Any]], top_k: int | None = None) -> List[Dict[str, Any]]:
        """Score documents via Cross-Encoder similarity."""
        if not docs:
            return []

        k = top_k or self.top_k
        pairs = [(d.get("query", ""), d.get("text", "")) for d in docs]
        scores = self.model.predict(pairs)

        for d, score in zip(docs, scores):
            base = float(d.get("score", 0.0))
            d["final_score"] = self.semantic_weight * float(score) + (1 - self.semantic_weight) * base

        docs.sort(key=lambda x: x["final_score"], reverse=True)
        return docs[:k]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\temporal_reranker.py ==== 
from __future__ import annotations
import logging
from typing import List, Dict, Any, Tuple, Dict, Optional
from collections import defaultdict
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class TemporalReranker(IReranker):
    """
    Semantic-first temporal diversification.
    Ensures that each decade contributes at most one document,
    but only if semantic relevance is sufficiently high.
    No score manipulation. No age penalties or boosts.
    """

    def __init__(
        self,
        semantic_threshold: float = 0.40,  # minimal score required for decade coverage
        min_year: int = 1900,
        must_include: List[str] | None = None,
        blacklist_sources: List[str] | None = None
    ):
        self.semantic_threshold = semantic_threshold
        self.min_year = min_year
        self.must_include = must_include or []
        self.blacklist_sources = blacklist_sources or []

    # ------------------------------------------------------------------
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        if not results:
            return []

        # Extract valid years
        for r in results:
            r["year"] = self._extract_year(r)

        # Apply blacklists
        results = [r for r in results if not self._is_blacklisted(r)]
        if not results:
            return []

        # ---- 1. Pure semantic sort ----
        results_sorted = sorted(results, key=lambda r: r.get("score", 0.0), reverse=True)

        # ---- 2. Group by decade ----
        decade_groups = self._group_by_decade(results_sorted)

        # ---- 3. Pick top semantic document per decade (if strong) ----
        selected = []
        seen = set()

        for dec in sorted(decade_groups.keys()):
            best = decade_groups[dec][0]
            if best["score"] >= self.semantic_threshold:
                key = self._src_key(best)
                if key not in seen:
                    selected.append(best)
                    seen.add(key)

        # ---- 4. Fill remaining purely by semantic score ----
        for r in results_sorted:
            key = self._src_key(r)
            if key not in seen:
                selected.append(r)
                seen.add(key)
            if len(selected) >= top_k:
                break

        # ---- 5. Inject must-include sources at front ----
        selected = self._inject_must_include(selected, results, top_k)

        logger.info(f"Temporal diversification complete | decades={len(decade_groups)} | threshold={self.semantic_threshold}")
        return selected[:top_k]

    # ------------------------------------------------------------------
    def _extract_year(self, r):
        meta = r.get("metadata", {})
        y = meta.get("year") or r.get("year")
        try:
            y = int(str(y))
            if y < self.min_year or y > 2100:
                return self.min_year
            return y
        except Exception:
            return self.min_year

    def _group_by_decade(self, results):
        groups = defaultdict(list)
        for r in results:
            decade = (r["year"] // 10) * 10
            groups[decade].append(r)
        return groups

    def _src_key(self, r):
        meta = r.get("metadata", {})
        return (meta.get("source_file") or meta.get("title") or "unknown").lower()

    def _is_blacklisted(self, r):
        key = self._src_key(r)
        return any(b.lower() in key for b in self.blacklist_sources)

    def _inject_must_include(self, ranked, all_results, top_k):
        must = [r for r in all_results if any(m in self._src_key(r) for m in self.must_include)]
        if not must:
            return ranked
        merged, seen = [], set()
        for r in must + ranked:
            key = self._src_key(r)
            if key not in seen:
                merged.append(r)
                seen.add(key)
            if len(merged) >= top_k:
                break
        return merged
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_reranker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IReranker(ABC):
    """Interface for reranker strategies (temporal, semantic, hybrid, etc.)."""
    @abstractmethod
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        # Rerank a list of retrieval results based on a custom strategy
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_retriever.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IRetriever(ABC):
    """Interface for all retriever implementations."""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # Return top-k relevant documents/chunks for a query
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\__init__.py ==== 
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\diversity_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer, util

class DiversityPipeline:
    """Applies semantic and temporal diversity rules."""

    def __init__(self, embed_model: SentenceTransformer):
        self.embed_model = embed_model

    def _clean(self, t: str) -> str:
        return " ".join(t.replace("\n", " ").replace("\r", " ").split())

    def apply(self, ranked: List[Dict[str, Any]], k: int, historical: bool) -> List[Dict[str, Any]]:
        # Non-historical: simple dedupe
        if not historical:
            seen, out = set(), []
            for r in ranked:
                tx = (r.get("text") or "").strip()
                h = hash(tx)
                if h not in seen:
                    seen.add(h)
                    out.append(r)
                if len(out) >= k:
                    break
            return out

        # Historical: semantic & temporal diversity
        texts = [self._clean(r.get("text", "")) for r in ranked]
        idxs = [i for i, t in enumerate(texts) if t]
        embs = self.embed_model.encode([texts[i] for i in idxs], normalize_embeddings=True)

        selected, kept_embs = [], []
        for j, idx in enumerate(idxs):
            if len(selected) >= k:
                break
            cand = ranked[idx]
            emb = embs[j]
            if kept_embs:
                sims = util.cos_sim(emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue
            selected.append(cand)
            kept_embs.append(emb)
        return selected
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\final_selector.py ==== 
from __future__ import annotations
from typing import List, Dict

class FinalSelector:
    """Ensure exact final_k chunk selection."""

    def select(self, items: List[Dict], k: int) -> List[Dict]:
        if len(items) >= k:
            return items[:k]
        if not items:
            return []
        pad = items[-1].copy()
        return items + [pad.copy() for _ in range(k - len(items))]
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\relevance_annotator.py ==== 
from __future__ import annotations
from typing import List, Dict
import numpy as np

class RelevanceAnnotator:
    """Assigns 0-3 relevance labels based on score distribution."""

    def apply(self, items: List[Dict], population: List[Dict]) -> List[Dict]:
        scores = np.array([float(x.get("final_score", 0.0)) for x in population])
        if scores.size == 0 or np.allclose(scores.std(), 0.0):
            for x in items:
                x["relevance"] = 1
            return items
        q1, q2, q3 = np.quantile(scores, [0.25, 0.5, 0.75])
        for x in items:
            s = float(x.get("final_score", 0.0))
            x["relevance"] = int(0 if s <= q1 else 1 if s <= q2 else 2 if s <= q3 else 3)
        return items
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\reranking_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from src.core.retrieval.reranker_factory import RerankerFactory

class RerankingPipeline:
    """Unified reranking abstraction."""

    def __init__(self, cfg: Dict[str, Any]):
        self.cfg = cfg
        self.cached_type = None
        self.cached_reranker = None

    def _get(self, intent: str):
        rtype = "temporal" if intent == "chronological" else "semantic"
        if rtype != self.cached_type:
            self.cached_reranker = RerankerFactory.from_config({"options": {"reranker": rtype}})
            self.cached_type = rtype
        return self.cached_reranker

    def run(self, docs: List[Dict[str, Any]], intent: str) -> List[Dict[str, Any]]:
        reranker = self._get(intent)
        ranked = reranker.rerank(docs, top_k=len(docs))
        for d in ranked:
            d["final_score"] = float(d.get("final_score", d.get("score", 0.0)))
        ranked.sort(key=lambda x: x["final_score"], reverse=True)
        return ranked
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\retrieval_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import Dict, Any, List
from src.core.config.config_loader import ConfigLoader
from sentence_transformers import SentenceTransformer
from src.core.retrieval.faiss_retriever import FAISSRetriever

from src.core.retrieval.orchestrator.retrieval_pipeline import RetrievalPipeline
from src.core.retrieval.orchestrator.reranking_pipeline import RerankingPipeline
from src.core.retrieval.orchestrator.diversity_pipeline import DiversityPipeline
from src.core.retrieval.orchestrator.relevance_annotator import RelevanceAnnotator
from src.core.retrieval.orchestrator.final_selector import FinalSelector

from src.core.evaluation.utils import make_chunk_id


class RetrievalOrchestrator:
    """Clean multi-stage retrieval orchestrator for RAG."""

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg = cfg_loader.config

        opts = self.cfg["options"]
        paths = self.cfg["paths"]

        # Multi-stage parameters
        self.final_k = int(opts.get("final_k", 10))
        oversample = int(opts.get("oversample_factor", 15))
        self.initial_k = max(self.final_k * oversample, self.final_k * 8)

        # Embedding model
        self.embed_model = SentenceTransformer(opts["embedding_model"])

        # Retriever instance
        self.retriever = FAISSRetriever(
            vector_store_dir=paths["vector_store_dir"],
            model_name=opts["embedding_model"],
            top_k_retrieve=self.initial_k,
            normalize_embeddings=True,
            use_gpu=opts.get("use_gpu", False),
            similarity_metric=opts.get("similarity_metric", "cosine"),
            temporal_awareness=False,
            diversify_sources=opts.get("diversify_sources", True),
        )

        # Pipeline modules
        self.stage_retrieve = RetrievalPipeline(self.retriever, self.initial_k)
        self.stage_rerank = RerankingPipeline(self.cfg)
        self.stage_diversity = DiversityPipeline(self.embed_model)
        self.stage_label = RelevanceAnnotator()
        self.stage_select = FinalSelector()

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        if not query.strip():
            return []

        historical = intent == "chronological"

        # Stage 1: Broad retrieval
        raw = self.stage_retrieve.run(query, historical)

        # Stage 2: Full reranking
        ranked = self.stage_rerank.run(raw, intent)

        # Stage 3: Diversity
        diversified = self.stage_diversity.apply(ranked, self.final_k, historical)

        # Stage 4: Relevance annotation
        annotated = self.stage_label.apply(diversified, ranked)

        # Stage 5: Final-k selection
        final = self.stage_select.select(annotated, self.final_k)

        # Assign stable IDs and ranks
        for i, x in enumerate(final, start=1):
            x["rank"] = i
            if not x.get("id"):
                x["id"] = make_chunk_id(x)

        return final

    # ------------------------------------------------------------------
    def close(self):
        self.retriever.close()
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\orchestrator\retrieval_pipeline.py ==== 
from __future__ import annotations
from typing import List, Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

class RetrievalPipeline:
    """Handles broad FAISS retrieval only."""

    def __init__(self, retriever: FAISSRetriever, initial_k: int):
        self.retriever = retriever
        self.initial_k = initial_k

    def run(self, query: str, historical: bool) -> List[Dict[str, Any]]:
        # Perform broad retrieval without post-processing
        raw = self.retriever.search(
            query,
            top_k=self.initial_k,
            temporal_mode=historical,
        )
        for r in raw:
            r["query"] = query
        return raw
.
==== C:\Users\katha\historical-drift-analyzer\src\core\utils\__init__.py ==== 
.
