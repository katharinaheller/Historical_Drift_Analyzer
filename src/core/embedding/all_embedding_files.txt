==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\all_embedding_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\all_embedding_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedder_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict
import logging

from src.core.embedding.interfaces.i_embedder import IEmbedder


class SentenceTransformerEmbedder(IEmbedder):
    """Local embedder using sentence-transformers."""

    def __init__(self, model_name: str, dimension: int | None = None, normalize_embeddings: bool = True):
        # Lazy import to avoid hard dependency
        try:
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError("sentence-transformers is required for SentenceTransformerEmbedder. "
                              "Install via: poetry add sentence-transformers") from e

        self._model = SentenceTransformer(model_name)
        self._normalize = normalize_embeddings
        # If dimension not given, infer from model
        if dimension is None:
            test_vec = self._model.encode("test", normalize_embeddings=self._normalize)
            self._dimension = len(test_vec)
        else:
            self._dimension = dimension

    def embed_text(self, text: str) -> list[float]:
        # Encode single text
        return self._model.encode(text, normalize_embeddings=self._normalize).tolist()

    def embed_batch(self, texts, batch_size=None) -> list[list[float]]:
        # Encode multiple texts
        return self._model.encode(
            list(texts),
            batch_size=batch_size or 32,
            normalize_embeddings=self._normalize
        ).tolist()

    @property
    def dimension(self) -> int:
        # Return embedding dimension
        return self._dimension

    def close(self) -> None:
        # Nothing to close for sentence-transformers
        pass


class EmbedderFactory:
    """Factory for creating IEmbedder instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IEmbedder:
        opts: Dict[str, Any] = cfg.get("options", {})
        model_name = opts.get("embedding_model", "all-MiniLM-L6-v2")
        normalize = bool(opts.get("normalize_embeddings", True))
        dimension = opts.get("dimension", None)

        backend = opts.get("embedding_backend", "sentence-transformers").lower()

        if backend == "sentence-transformers":
            return SentenceTransformerEmbedder(
                model_name=model_name,
                dimension=dimension,
                normalize_embeddings=normalize,
            )
        else:
            raise ValueError(f"Unsupported embedding backend: {backend}")
 
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\embedding_orchestrator.py ==== 
from __future__ import annotations
import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from src.core.config.config_loader import ConfigLoader
from src.core.embedding.embedder_factory import EmbedderFactory
from src.core.embedding.vector_store_factory import VectorStoreFactory

logger = logging.getLogger("EmbeddingOrchestrator")


def _load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def _iter_chunk_files(chunks_dir: Path):
    for p in chunks_dir.glob("*.json"):
        if p.is_file():
            yield p


def _extract_chunks(chunk_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    if "chunks" in chunk_data and isinstance(chunk_data["chunks"], list):
        return chunk_data["chunks"]
    elif "text" in chunk_data:
        return [{"text": chunk_data["text"]}]
    return []


def _resolve_metadata_for_chunk(chunk_file: Path, metadata_dir: Path) -> Dict[str, Any]:
    """Try to locate matching metadata JSON for a given chunk file."""
    base = chunk_file.stem.replace(".chunks", "")
    candidates = [
        metadata_dir / f"{chunk_file.name}",
        metadata_dir / f"{base}.json",
        metadata_dir / f"{base}.metadata.json",
    ]
    for candidate in candidates:
        if candidate.exists():
            return _load_json(candidate)
    logger.warning(f"No metadata found for {chunk_file.name}")
    return {}


def main() -> None:
    # ------------------------------------------------------------------
    # 1. Load merged configuration (embedding + master)
    # ------------------------------------------------------------------
    cfg_loader = ConfigLoader("configs/embedding.yaml", master_path="configs/config.yaml")
    cfg = cfg_loader.config

    opts: Dict[str, Any] = cfg.get("options", {})
    log_level = getattr(logging, opts.get("log_level", "INFO").upper(), logging.INFO)
    logging.basicConfig(level=log_level, format="%(levelname)s | %(message)s")
    logger.info("Starting embedding pipeline")

    paths: Dict[str, Any] = cfg.get("paths", {})
    chunks_dir = Path(paths.get("chunks_dir", "data/processed/chunks")).resolve()
    metadata_dir = Path(paths.get("metadata_dir", "data/processed/metadata")).resolve()

    if not chunks_dir.exists():
        raise FileNotFoundError(f"Chunks directory does not exist: {chunks_dir}")
    if not metadata_dir.exists():
        logger.warning(f"Metadata directory does not exist: {metadata_dir} (metadata will be empty)")

    # ------------------------------------------------------------------
    # 2. Initialize embedding backend + vector store
    # ------------------------------------------------------------------
    embedder = EmbedderFactory.from_config(cfg)
    logger.info(f"Initialized embedder with dimension={embedder.dimension}")

    vector_store = VectorStoreFactory.from_config(cfg, dimension=embedder.dimension)
    logger.info("Initialized vector store")

    batch_size = int(opts.get("batch_size", 16))
    texts_batch: List[str] = []
    metas_batch: List[Dict[str, Any]] = []

    try:
        for chunk_file in _iter_chunk_files(chunks_dir):
            chunk_json = _load_json(chunk_file)
            chunks = _extract_chunks(chunk_json)
            if not chunks:
                logger.warning(f"No chunks found in {chunk_file.name}")
                continue

            meta_data = _resolve_metadata_for_chunk(chunk_file, metadata_dir)

            for ch in chunks:
                text = ch.get("text", "").strip()
                if not text:
                    continue

                merged_meta = {
                    "source_file": meta_data.get("source_file", chunk_file.stem),
                    "title": meta_data.get("title"),
                    "authors": meta_data.get("authors"),
                    "year": meta_data.get("year"),
                    "detected_language": meta_data.get("detected_language"),
                    "page_count": meta_data.get("page_count"),
                    "origin_chunk_file": str(chunk_file.name),
                }

                texts_batch.append(text)
                metas_batch.append(merged_meta)

                if len(texts_batch) >= batch_size:
                    try:
                        vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
                        vector_store.add_vectors(vectors, texts_batch, metas_batch)
                        logger.info(f"Embedded and stored batch of size {len(texts_batch)}")
                    except Exception as e:
                        logger.error(f"Error during embedding or storing batch: {e}")
                    finally:
                        texts_batch.clear()
                        metas_batch.clear()

        if texts_batch:
            vectors = embedder.embed_batch(texts_batch, batch_size=batch_size)
            vector_store.add_vectors(vectors, texts_batch, metas_batch)
            logger.info(f"Embedded and stored final batch of size {len(texts_batch)}")

        vector_store.persist()
        logger.info("Embedding pipeline finished successfully.")

    finally:
        embedder.close()
        vector_store.close()


if __name__ == "__main__":
    main()

==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\vector_store_factory.py ==== 
from __future__ import annotations
from typing import Any, Dict, List
import logging
import os
import json
import sqlite3

from src.core.embedding.interfaces.i_vector_store import IVectorStore


class FAISSVectorStore(IVectorStore):
    """FAISS-based local vector store."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import faiss  # type: ignore
        except ImportError as e:
            raise ImportError("faiss-cpu is required for FAISSVectorStore. "
                              "Install via: poetry add faiss-cpu") from e

        self.faiss = faiss
        self.dimension = dimension
        self.persist_dir = persist_dir
        os.makedirs(persist_dir, exist_ok=True)
        self.index_path = os.path.join(persist_dir, "index.faiss")
        self.meta_path = os.path.join(persist_dir, "metadata.jsonl")

        if os.path.exists(self.index_path):
            self.index = faiss.read_index(self.index_path)
        else:
            self.index = faiss.IndexFlatIP(self.dimension)

        # Metadata is stored separately as JSONL
        self._meta_fh = open(self.meta_path, "a", encoding="utf-8")

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import to avoid hard dependency at class load

        arr = np.array(vectors).astype("float32")
        self.index.add(arr)

        for i, doc in enumerate(documents):
            meta = metadatas[i] if metadatas and i < len(metadatas) else {}
            payload = {
                "text": doc,
                "metadata": meta,
            }
            self._meta_fh.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def persist(self) -> None:
        self.faiss.write_index(self.index, self.index_path)
        self._meta_fh.flush()

    def close(self) -> None:
        try:
            self.persist()
        finally:
            self._meta_fh.close()


class LanceDBVectorStore(IVectorStore):
    """LanceDB-based vector store (local file-based)."""

    def __init__(self, persist_dir: str, dimension: int):
        try:
            import lancedb  # type: ignore
        except ImportError as e:
            raise ImportError("lancedb is required for LanceDBVectorStore. "
                              "Install via: poetry add lancedb") from e

        self.dimension = dimension
        os.makedirs(persist_dir, exist_ok=True)
        self.db = lancedb.connect(persist_dir)
        self.table = self.db.open_table("embeddings") if "embeddings" in self.db.table_names() else \
            self.db.create_table("embeddings", data=[
                {
                    "vector": [0.0] * dimension,
                    "text": "",
                    "metadata": {},
                }
            ])

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        rows = []
        for i, vec in enumerate(vectors):
            rows.append({
                "vector": vec,
                "text": documents[i],
                "metadata": metadatas[i] if metadatas and i < len(metadatas) else {},
            })
        self.table.add(rows)

    def persist(self) -> None:
        # LanceDB persists automatically
        pass

    def close(self) -> None:
        # Nothing to close
        pass


class SQLiteVectorStore(IVectorStore):
    """Very simple SQLite-based vector store (for debugging / small-scale)."""

    def __init__(self, persist_dir: str, dimension: int):
        os.makedirs(persist_dir, exist_ok=True)
        db_path = os.path.join(persist_dir, "vectors.sqlite3")
        self.conn = sqlite3.connect(db_path)
        self.dimension = dimension
        self._ensure_schema()

    def _ensure_schema(self) -> None:
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                text TEXT NOT NULL,
                metadata TEXT,
                vector BLOB NOT NULL
            )
            """
        )
        self.conn.commit()

    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: List[Dict[str, Any]] | None = None) -> None:
        import numpy as np  # local import
        cur = self.conn.cursor()
        for i, vec in enumerate(vectors):
            meta_str = json.dumps(metadatas[i], ensure_ascii=False) if metadatas and i < len(metadatas) else "{}"
            arr = np.array(vec, dtype="float32").tobytes()
            cur.execute(
                "INSERT INTO embeddings (text, metadata, vector) VALUES (?, ?, ?)",
                (documents[i], meta_str, arr)
            )
        self.conn.commit()

    def persist(self) -> None:
        self.conn.commit()

    def close(self) -> None:
        self.conn.close()


class VectorStoreFactory:
    """Factory for creating IVectorStore instances from config."""

    @staticmethod
    def from_config(cfg: Dict[str, Any], dimension: int) -> IVectorStore:
        opts: Dict[str, Any] = cfg.get("options", {})
        store_name = opts.get("vector_store", "FAISS").upper()

        paths: Dict[str, Any] = cfg.get("paths", {})
        persist_dir = paths.get("vector_store_dir", "data/vector_store")

        if store_name == "FAISS":
            return FAISSVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "LANCEDB":
            return LanceDBVectorStore(persist_dir=persist_dir, dimension=dimension)
        elif store_name == "SQLITE":
            return SQLiteVectorStore(persist_dir=persist_dir, dimension=dimension)
        else:
            raise ValueError(f"Unsupported vector store: {store_name}")
 
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_embedder.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import List, Sequence, Optional


class IEmbedder(ABC):
    """Interface for all embedding backends."""

    @abstractmethod
    def embed_text(self, text: str) -> List[float]:
        # Return embedding vector for a single text
        pass

    @abstractmethod
    def embed_batch(self, texts: Sequence[str], batch_size: Optional[int] = None) -> List[List[float]]:
        # Return embedding vectors for a batch of texts
        pass

    @property
    @abstractmethod
    def dimension(self) -> int:
        # Return embedding dimension
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\i_vector_store.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional


class IVectorStore(ABC):
    """Interface for all vector store backends."""

    @abstractmethod
    def add_vectors(self,
                    vectors: List[List[float]],
                    documents: List[str],
                    metadatas: Optional[List[Dict[str, Any]]] = None) -> None:
        # Add vectors and associated payloads to the store
        pass

    @abstractmethod
    def persist(self) -> None:
        # Persist the store to disk
        pass

    @abstractmethod
    def close(self) -> None:
        # Close resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\embedding\interfaces\__init__.py ==== 
.
