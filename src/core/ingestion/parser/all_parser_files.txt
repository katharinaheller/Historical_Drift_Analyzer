==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\all_parser_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parallel_pdf_parser.py ==== 
# src/core/ingestion/parser/parallel_pdf_parser.py
from __future__ import annotations
import concurrent.futures
import logging
import multiprocessing
from pathlib import Path
from typing import Any, Dict, List, Optional
import json

from src.core.ingestion.parser.parser_factory import ParserFactory


class ParallelPdfParser:
    """CPU-based parallel parser orchestrator using multiple processes."""

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        self.parser_factory = ParserFactory(config, logger=self.logger)

        parallel_cfg = config.get("options", {}).get("parallelism", "auto")
        if isinstance(parallel_cfg, int) and parallel_cfg > 0:
            self.num_workers = parallel_cfg
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(f"Initialized ParallelPdfParser with {self.num_workers} worker(s)")

    # ------------------------------------------------------------------
    def parse_all(self, pdf_dir: str | Path, output_dir: str | Path) -> List[Dict[str, Any]]:
        pdf_dir, output_dir = Path(pdf_dir), Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        pdf_files = sorted(pdf_dir.glob("*.pdf"))
        if not pdf_files:
            self.logger.warning(f"No PDFs found in {pdf_dir}")
            return []

        results: List[Dict[str, Any]] = []
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            future_to_pdf = {executor.submit(self._parse_single, str(pdf)): pdf for pdf in pdf_files}
            for future in concurrent.futures.as_completed(future_to_pdf):
                pdf = future_to_pdf[future]
                try:
                    result = future.result()
                    results.append(result)
                    out_path = output_dir / f"{pdf.stem}.parsed.json"
                    with open(out_path, "w", encoding="utf-8") as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    self.logger.info(f"✓ Parsed {pdf.name}")
                except Exception as e:
                    self.logger.error(f"✗ Failed to parse {pdf.name}: {e}")

        return results

    # ------------------------------------------------------------------
    def _parse_single(self, pdf_path: str) -> Dict[str, Any]:
        parser = self.parser_factory.create_parser()
        return parser.parse(pdf_path)

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\parser_factory.py ==== 
from __future__ import annotations
from typing import Dict, Any, Optional
import logging
import multiprocessing

from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser
from src.core.ingestion.parser.pymupdf_parser import PyMuPDFParser


class ParserFactory:
    """
    Factory for creating local PDF parsers and optional parallel orchestrators.
    Handles YAML parameters like 'parallelism' and parser configuration.
    """

    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        opts = config.get("options", {})

        self.parser_mode = opts.get("pdf_parser", "auto").lower()
        self.parallelism = opts.get("parallelism", "auto")
        self.language = opts.get("language", "auto")
        self.exclude_toc = True  # enforced globally
        self.max_pages = opts.get("max_pages", None)

        # determine optimal CPU usage
        if isinstance(self.parallelism, int) and self.parallelism > 0:
            self.num_workers = self.parallelism
        else:
            self.num_workers = max(1, multiprocessing.cpu_count() - 1)

        self.logger.info(
            f"ParserFactory initialized | mode={self.parser_mode}, "
            f"workers={self.num_workers}, exclude_toc={self.exclude_toc}"
        )

    # ------------------------------------------------------------------
    def create_parser(self) -> IPdfParser:
        """Return configured parser instance (currently only PyMuPDF)."""
        if self.parser_mode in ("fitz", "auto"):
            return PyMuPDFParser(exclude_toc=self.exclude_toc, max_pages=self.max_pages)
        raise ValueError(f"Unsupported parser mode: {self.parser_mode}")

    # ------------------------------------------------------------------
    def create_parallel_parser(self):
        """Return a parallel orchestrator that distributes parsing tasks."""
        from src.core.ingestion.parser.parallel_pdf_parser import ParallelPdfParser
        return ParallelPdfParser(self.config, logger=self.logger)

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\pymupdf_parser.py ==== 
from __future__ import annotations
from typing import Dict, Any, List
from pathlib import Path
import fitz  # PyMuPDF
import re
from src.core.ingestion.parser.interfaces.i_pdf_parser import IPdfParser


class PyMuPDFParser(IPdfParser):
    """Lightweight, deterministic PDF parser using PyMuPDF (fitz)."""

    def __init__(self, exclude_toc: bool = True, max_pages: int | None = None):
        self.exclude_toc = exclude_toc
        self.max_pages = max_pages

    # ------------------------------------------------------------------
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Extract plain text and minimal metadata from the given PDF."""
        pdf_path = Path(pdf_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"File not found: {pdf_path}")

        doc = fitz.open(pdf_path)
        text_blocks: List[str] = []

        # Optionally detect and skip ToC pages
        toc_titles = self._extract_toc_titles(doc) if self.exclude_toc else []

        for page_index, page in enumerate(doc):
            if self.max_pages and page_index >= self.max_pages:
                break
            page_text = page.get_text("text", flags=0)
            # Skip pages that are likely table of contents
            if self.exclude_toc and self._looks_like_toc_page(page_text, toc_titles):
                continue
            text_blocks.append(page_text.strip())

        clean_text = "\n".join(t for t in text_blocks if t)
        clean_text = self._remove_residual_metadata(clean_text)

        metadata = {
            "source_file": str(pdf_path.name),
            "page_count": len(doc),
            "has_toc": bool(doc.get_toc()),
        }

        doc.close()
        return {"text": clean_text, "metadata": metadata}

    # ------------------------------------------------------------------
    def _extract_toc_titles(self, doc) -> List[str]:
        """Extract TOC titles to later filter out from main text."""
        toc = doc.get_toc()
        return [entry[1].strip() for entry in toc if len(entry) >= 2]

    # ------------------------------------------------------------------
    def _looks_like_toc_page(self, text: str, toc_titles: List[str]) -> bool:
        """Heuristic to detect table of contents pages."""
        if not text or len(text) < 50:
            return False
        if re.search(r"(?i)\btable\s+of\s+contents\b|\binhaltsverzeichnis\b", text):
            return True
        match_count = sum(1 for t in toc_titles if t and t in text)
        return match_count > 5

    # ------------------------------------------------------------------
    def _remove_residual_metadata(self, text: str) -> str:
        """Remove common metadata fragments from extracted text."""
        # Common document headers, page numbers, or TOC remnants
        patterns = [
            r"(?i)^table\s+of\s+contents.*$",
            r"(?i)^inhaltsverzeichnis.*$",
            r"(?i)^author:.*$",
            r"(?i)^title:.*$",
            r"(?i)^abstract:.*$",
            r"\bpage\s+\d+\b",
            r"^\s*\d+\s*$",
        ]
        for p in patterns:
            text = re.sub(p, "", text, flags=re.MULTILINE)
        return re.sub(r"\n{2,}", "\n\n", text).strip()

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\ingestion\parser\interfaces\i_pdf_parser.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Dict, Any


class IPdfParser(ABC):
    """Interface for all local PDF parsers."""

    @abstractmethod
    def parse(self, pdf_path: str) -> Dict[str, Any]:
        """Parse a PDF file and return structured output with text and metadata."""
        raise NotImplementedError
.
