==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\all_retrieval_files.txt ==== 
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\all_retrieval_files.txt ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\faiss_retriever.py ==== 
# src/core/retrieval/faiss_retriever.py
from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path
import json
import re
import numpy as np
import logging
from math import exp
from src.core.retrieval.interfaces.i_retriever import IRetriever


class FAISSRetriever(IRetriever):
    """FAISS-based semantic retriever with optional temporal and source diversification."""

    def __init__(
        self,
        vector_store_dir: str,
        model_name: str,
        top_k_retrieve: int = 50,
        normalize_embeddings: bool = True,
        use_gpu: bool = False,
        similarity_metric: str = "cosine",
        temporal_awareness: bool = True,
        temporal_tau: float = 8.0,
        temporal_weight: float = 0.30,
        valid_year_range: Tuple[int, int] = (1900, 2100),
        diversify_sources: bool = True,  # enable balanced source selection
    ):
        self.logger = logging.getLogger(self.__class__.__name__)

        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer
        except ImportError as e:
            raise ImportError(
                "faiss-cpu and sentence-transformers are required. "
                "Install via: poetry add faiss-cpu sentence-transformers"
            ) from e

        self.faiss = faiss
        self.vector_store_dir = Path(vector_store_dir).resolve()
        self.index_path = self.vector_store_dir / "index.faiss"
        self.meta_path = self.vector_store_dir / "metadata.jsonl"

        if not self.index_path.exists() or not self.meta_path.exists():
            raise FileNotFoundError(f"Incomplete vector store: {self.vector_store_dir}")

        self.model = SentenceTransformer(model_name)
        self.top_k_retrieve = int(top_k_retrieve)
        self.normalize_embeddings = bool(normalize_embeddings)
        self.use_gpu = bool(use_gpu)
        self.similarity_metric = similarity_metric.lower().strip()
        self.temporal_awareness = bool(temporal_awareness)
        self.temporal_tau = float(temporal_tau)
        self.temporal_weight = float(temporal_weight)
        self.valid_year_range = valid_year_range
        self.diversify_sources = bool(diversify_sources)

        if self.similarity_metric not in {"cosine", "dot"}:
            raise ValueError(f"Unsupported similarity metric: {self.similarity_metric}")

        # Load FAISS index
        self.logger.info(f"Loading FAISS index: {self.index_path}")
        self.index = faiss.read_index(str(self.index_path))
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
                self.logger.info("FAISS GPU acceleration enabled")
            except Exception as e:
                self.logger.warning(f"GPU mode failed, falling back to CPU: {e}")

        # Load metadata
        with open(self.meta_path, "r", encoding="utf-8") as f:
            self.metadata = [json.loads(line) for line in f]

        self.logger.info(
            f"FAISSRetriever ready | entries={len(self.metadata)} | metric={self.similarity_metric.upper()} "
            f"| temporal_awareness={self.temporal_awareness} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def _encode_query(self, query: str) -> np.ndarray:
        # Encode query with normalization
        vec = self.model.encode([query], normalize_embeddings=self.normalize_embeddings)
        return np.asarray(vec, dtype="float32")

    def _normalize_scores(self, distances: np.ndarray) -> np.ndarray:
        # Normalize distances depending on metric
        if self.similarity_metric in {"cosine", "dot"}:
            return distances
        return 1 - distances

    # ------------------------------------------------------------------
    def _extract_years_from_query(self, query: str) -> List[int]:
        """
        Extract explicit years (e.g. 2021), decade mentions (e.g. 'in the 2020s'),
        or century references (e.g. '21st century') from a text query.
        Returns a sorted list of representative years.
        """
        if not query:
            return []

        text = query.lower()
        years: set[int] = set()
        lo, hi = self.valid_year_range

        # 1) Explicit years (e.g. 1999, 2023)
        for m in re.findall(r"\b(19\d{2}|20\d{2})\b", text):
            try:
                y = int(m)
                if lo <= y <= hi:
                    years.add(y)
            except ValueError:
                continue

        # 2) Decades (e.g. 1980s, 2020s, early 1990s)
        for m in re.findall(r"\b(19|20)\d0s\b", text):
            try:
                decade = int(m + "0")
                if lo <= decade <= hi:
                    years.update(range(decade, decade + 10))
            except ValueError:
                continue

        # 3) Centuries (e.g. "20th century", "21st century")
        if "20th century" in text:
            years.update(range(1900, 2000))
        if "21st century" in text:
            years.update(range(2000, 2100))

        # Optional compact logging: only start years of detected decades
        unique_decades = sorted({(y // 10) * 10 for y in years})
        return unique_decades

    # ------------------------------------------------------------------
    def _temporal_modulate(self, base_score: float, doc_year: Optional[int], query_years: List[int]) -> float:
        # Exponential weighting for temporal alignment
        if doc_year is None or not query_years:
            return base_score
        nearest = min(abs(doc_year - y) for y in query_years)
        w = exp(-nearest / max(self.temporal_tau, 1e-6))
        return base_score * (1.0 + self.temporal_weight * w)

    def _safe_doc_year(self, entry: Dict[str, Any]) -> Optional[int]:
        # Extract publication year safely from metadata
        meta = entry.get("metadata", {}) or {}
        y = meta.get("year")
        try:
            yi = int(str(y))
            lo, hi = self.valid_year_range
            if lo <= yi <= hi:
                return yi
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _apply_source_diversity(self, results: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """Ensure chunks from different PDFs dominate early ranks."""
        if not self.diversify_sources or not results:
            return results[:top_k]

        diversified, seen = [], set()
        for r in results:
            src = r["metadata"].get("source_file", "unknown")
            if src not in seen:
                diversified.append(r)
                seen.add(src)
            if len(diversified) >= top_k:
                break

        # Fill up with remaining if fewer than top_k unique
        if len(diversified) < top_k:
            for r in results:
                if r not in diversified:
                    diversified.append(r)
                if len(diversified) >= top_k:
                    break
        return diversified

    # ------------------------------------------------------------------
    def search(self, query: str, top_k: Optional[int] = None, temporal_mode: bool = True) -> List[Dict[str, Any]]:
        """Similarity search with optional temporal and source diversification."""
        self.temporal_awareness = bool(temporal_mode)
        k = int(top_k or self.top_k_retrieve)
        k = max(1, min(k, self.index.ntotal))

        q_vec = self._encode_query(query)
        D, I = self.index.search(q_vec, k * 5 if self.diversify_sources else k)
        scores = self._normalize_scores(D[0])
        query_years = self._extract_years_from_query(query) if self.temporal_awareness else []

        results: List[Dict[str, Any]] = []
        for score, idx in zip(scores, I[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue
            entry = self.metadata[idx]
            doc_year = self._safe_doc_year(entry)
            mod_score = (
                self._temporal_modulate(float(score), doc_year, query_years)
                if self.temporal_awareness else float(score)
            )
            results.append({
                "score": float(mod_score),
                "text": (entry.get("text", "") or "")[:500],
                "metadata": entry.get("metadata", {}) or {},
            })

        results.sort(key=lambda r: r["score"], reverse=True)
        diversified = self._apply_source_diversity(results, top_k=k)

        self.logger.info(
            f"Retrieved {len(diversified)} candidates | temporal_mode={self.temporal_awareness} "
            f"| diversify_sources={self.diversify_sources} | years_in_query={query_years or 'none'}"
        )
        return diversified

    # ------------------------------------------------------------------
    def close(self) -> None:
        self.logger.info("FAISS retriever closed")

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\reranker_factory.py ==== 
# src/core/retrieval/reranker_factory.py
from __future__ import annotations
import logging
from typing import Any, Dict, Type
from src.core.retrieval.temporal_reranker import TemporalReranker
from src.core.retrieval.semantic_reranker import SemanticReranker
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class RerankerFactory:
    """Factory for deterministic reranker construction."""

    _registry: Dict[str, Type[IReranker]] = {
        "temporal": TemporalReranker,
        "semantic": SemanticReranker,
    }

    @staticmethod
    def from_config(cfg: Dict[str, Any]) -> IReranker:
        """Instantiate reranker from configuration dictionary."""
        opts = cfg.get("options", {})
        rtype = str(opts.get("reranker", "semantic")).lower()

        if rtype not in RerankerFactory._registry:
            raise ValueError(
                f"Unsupported reranker type: {rtype}. "
                f"Available: {list(RerankerFactory._registry.keys())}"
            )

        cls = RerankerFactory._registry[rtype]
        logger.info(f"Initializing reranker of type='{rtype}'")

        if cls is TemporalReranker:
            return TemporalReranker(
                lambda_weight=float(opts.get("lambda_weight", 0.55)),
                min_year=int(opts.get("min_year", 1900)),
                enforce_decade_balance=bool(opts.get("enforce_decade_balance", True)),
                age_score_boost=float(opts.get("age_score_boost", 0.25)),
                min_decade_threshold=int(opts.get("min_decade_threshold", 3)),
                nonlinear_boost=str(opts.get("nonlinear_boost", "sigmoid")),
                ignore_years=list(opts.get("ignore_years", [])),
                recency_cutoff_year=opts.get("recency_cutoff_year"),
                allow_legacy_backfill=bool(opts.get("allow_legacy_backfill", True)),
                legacy_backfill_max_ratio=float(opts.get("legacy_backfill_max_ratio", 0.3)),
                must_include=list(opts.get("must_include", [])),
                blacklist_sources=list(opts.get("blacklist_sources", [])),
                zscore_normalization=bool(opts.get("zscore_normalization", False)),
            )

        if cls is SemanticReranker:
            return SemanticReranker(
                model_name=str(
                    opts.get("semantic_model", "cross-encoder/ms-marco-MiniLM-L-6-v2")
                ),
                top_k=int(opts.get("top_k_rerank", 25)),
                semantic_weight=float(opts.get("semantic_weight", 0.75)),
                use_gpu=bool(opts.get("use_gpu", False)),
            )

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retrieval_orchestrator.py ==== 
from __future__ import annotations
import logging
from typing import List, Dict, Any, Optional
from collections import defaultdict
import numpy as np
from sentence_transformers import SentenceTransformer, util

from src.core.retrieval.faiss_retriever import FAISSRetriever
from src.core.retrieval.reranker_factory import RerankerFactory
from src.core.config.config_loader import ConfigLoader
from src.core.evaluation.utils import make_chunk_id  # # stable id builder for evaluation


class RetrievalOrchestrator:
    """
    Unified retrieval orchestrator — controlled externally by prompt intent.
    Pipeline:
      1. Receive (query, intent)
      2. Retrieve chunks from FAISS
      3. Apply reranker (semantic / temporal)
      4. Enforce optional diversity
      5. Calibrate graded relevance (0..3) via score distribution (proxy)
      6. Assign stable id + rank and return exactly top_k results
    """

    def __init__(self, config_path: str = "configs/retrieval.yaml"):
        self.logger = logging.getLogger("RetrievalOrchestrator")
        cfg_loader = ConfigLoader(config_path)
        self.cfg: Dict[str, Any] = cfg_loader.config

        opts = self.cfg.get("options", {})
        paths = self.cfg.get("paths", {})

        self.top_k = int(opts.get("top_k", 10))
        self.vector_store_dir = str(paths.get("vector_store_dir", "data/vector_store"))
        self.embedding_model = opts.get("embedding_model", "all-MiniLM-L6-v2")

        # # Feature flags
        self.diversify_sources = bool(opts.get("diversify_sources", True))
        self.max_initial = max(self.top_k * 8, int(opts.get("oversample_factor", 8)) * self.top_k)

        # # Initialize FAISS retriever
        self.retriever = FAISSRetriever(
            vector_store_dir=self.vector_store_dir,
            model_name=self.embedding_model,
            top_k_retrieve=self.max_initial,
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_awareness=False,
            diversify_sources=self.diversify_sources,
        )

        # # Embedding model for diversity computations
        self.embed_model = SentenceTransformer(self.embedding_model)

        # # Cache for reranker to avoid re-instantiation overhead
        self._cached_reranker_type: Optional[str] = None
        self._cached_reranker = None

        self.logger.info(
            f"RetrievalOrchestrator initialized | top_k={self.top_k} | diversify_sources={self.diversify_sources}"
        )

    # ------------------------------------------------------------------
    def retrieve(self, query: str, intent: str) -> List[Dict[str, Any]]:
        # # retrieve relevant chunks according to user intent
        if not query or not query.strip():
            self.logger.warning("Empty query ignored.")
            return []

        is_historical = intent == "chronological"
        self.logger.info(f"Retrieval started | intent={intent} | top_k={self.top_k}")

        try:
            raw_results = self.retriever.search(query, top_k=self.max_initial, temporal_mode=is_historical)
        except Exception as e:
            self.logger.exception(f"FAISS retrieval failed: {e}")
            return []

        if not raw_results:
            self.logger.warning("No retrieval results found.")
            return []

        # # Step 2: reranking with cached instance
        reranker_type = "temporal" if is_historical else "semantic"
        if reranker_type != self._cached_reranker_type or self._cached_reranker is None:
            self._cached_reranker = RerankerFactory.from_config({"options": {"reranker": reranker_type}})
            self._cached_reranker_type = reranker_type

        try:
            reranked = self._cached_reranker.rerank(raw_results, top_k=len(raw_results))
        except Exception as e:
            self.logger.exception(f"Reranking failed ({reranker_type}): {e}")
            reranked = raw_results

        # # normalize score field and sort deterministically
        for x in reranked:
            x["final_score"] = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
        reranked.sort(key=lambda x: (x["final_score"], x.get("id", "")), reverse=True)

        # # Step 3: optional historical diversity enforcement
        diversified = self._enforce_diversity(reranked, self.top_k, is_historical)

        # # Step 4: graded relevance (0..3) via score quantiles as white-box proxy
        diversified = self._attach_graded_relevance(diversified, ref_population=reranked)

        # # Step 5: ensure exact k and assign stable ids + ranks
        final = self._ensure_exact_k(diversified, self.top_k)
        for i, x in enumerate(final, start=1):
            if not x.get("id"):
                x["id"] = make_chunk_id(x)  # # stable id for evaluation mapping
            x["rank"] = i                  # # deterministic rank for citation mapping

        self._log_decade_distribution(final)
        self.logger.info(f"Retrieval finished | returned={len(final)} | mode={intent}")
        return final

    # ------------------------------------------------------------------
    def _enforce_diversity(self, results: List[Dict[str, Any]], k: int, historical: bool) -> List[Dict[str, Any]]:
        # # diversify by decade and source for chronological intent
        if not results:
            return []

        if not historical or not self.diversify_sources:
            # # simple deduplication on text hash to avoid near-duplicates
            seen, out = set(), []
            for r in results:
                t = (r.get("text") or "").strip()
                if not t:
                    continue
                h = hash(t)
                if h in seen:
                    continue
                seen.add(h)
                out.append(r)
                if len(out) >= k:
                    break
            return out

        # # historical: add semantic diversity + decade/source spread
        selected, used_sources, used_decades = [], set(), set()
        pool_texts = [self._clean_text(r.get("text", "")) for r in results]
        pool_idxs = [i for i, t in enumerate(pool_texts) if t]
        if not pool_idxs:
            return results[:k]

        # # batch-encode to reduce latency for diversity check
        embs = self.embed_model.encode([pool_texts[i] for i in pool_idxs], normalize_embeddings=True)
        kept_embs = []

        for j, idx in enumerate(pool_idxs):
            r = results[idx]
            if len(selected) >= k:
                break
            meta = r.get("metadata", {}) or {}
            src = (meta.get("source_file") or "unknown").lower()
            year = self._safe_year(r)
            decade = (year // 10) * 10 if year else None

            if src in used_sources and decade in used_decades and len(selected) < int(k * 0.8):
                continue

            cand_emb = embs[j]
            if kept_embs:
                sims = util.cos_sim(cand_emb, kept_embs)[0]
                if float(sims.max()) > 0.95:
                    continue

            selected.append(r)
            kept_embs.append(cand_emb)
            used_sources.add(src)
            if decade:
                used_decades.add(decade)

        # # if not enough, fill up with remaining best scores
        if len(selected) < k:
            used_ids = {id(x) for x in selected}
            for r in results:
                if id(r) in used_ids:
                    continue
                selected.append(r)
                if len(selected) >= k:
                    break

        return selected

    # ------------------------------------------------------------------
    def _attach_graded_relevance(self, items: List[Dict[str, Any]], ref_population: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # # attach graded relevance labels (0..3) derived from score quantiles
        if not items:
            return items

        scores = np.array([float(x.get("final_score", x.get("score", 0.0)) or 0.0) for x in ref_population], dtype=float)
        if scores.size == 0 or np.allclose(scores.std(), 0.0):
            for x in items:
                x["relevance"] = 1
            return items

        try:
            q1, q2, q3 = np.quantile(scores, [0.25, 0.5, 0.75])
        except Exception:
            smin, smax = float(scores.min()), float(scores.max())
            step = (smax - smin) / 4.0 if smax > smin else 1.0
            q1, q2, q3 = smin + step, smin + 2 * step, smin + 3 * step

        for x in items:
            s = float(x.get("final_score", x.get("score", 0.0)) or 0.0)
            if s <= q1:
                rel = 0
            elif s <= q2:
                rel = 1
            elif s <= q3:
                rel = 2
            else:
                rel = 3
            x["relevance"] = int(rel)

        return items

    # ------------------------------------------------------------------
    def _ensure_exact_k(self, results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:
        # # ensure deterministic top-k output length
        if not results:
            return []
        if len(results) > k:
            return results[:k]
        if 0 < len(results) < k:
            pad = results[-1].copy()
            padding = [pad.copy() for _ in range(k - len(results))]
            return results + padding
        return results

    # ------------------------------------------------------------------
    def _safe_year(self, r: Dict[str, Any]) -> Optional[int]:
        # # safely extract valid publication year
        meta = r.get("metadata", {}) or {}
        y = meta.get("year", r.get("year"))
        try:
            y = int(y)
            if 1900 <= y <= 2100:
                return y
        except Exception:
            pass
        return None

    # ------------------------------------------------------------------
    def _clean_text(self, t: str) -> str:
        # # conservative cleanup to stabilize hashing/embeddings
        if not t:
            return ""
        s = t.replace("\n", " ").replace("\r", " ")
        return " ".join(s.split())

    # ------------------------------------------------------------------
    def _log_decade_distribution(self, items: List[Dict[str, Any]]) -> None:
        # # log decade distribution for diagnostics
        hist: Dict[str, int] = defaultdict(int)
        for r in items:
            y = self._safe_year(r)
            decade = f"{(y // 10) * 10}s" if y else "unknown"
            hist[decade] += 1
        msg = ", ".join(f"{k}:{v}" for k, v in sorted(hist.items()))
        self.logger.info(f"Decade distribution: {msg}")

    # ------------------------------------------------------------------
    def close(self) -> None:
        # # close retriever resources gracefully
        try:
            self.retriever.close()
        except Exception as e:
            self.logger.warning(f"Failed to close retriever: {e}")
        self.logger.info("RetrievalOrchestrator closed cleanly.")

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\retriever_factory.py ==== 
# src/core/retrieval/retriever_factory.py
from __future__ import annotations
import logging
from typing import Dict, Any
from src.core.retrieval.faiss_retriever import FAISSRetriever

logger = logging.getLogger(__name__)

class RetrieverFactory:
    """Factory für Intent-spezifische Retriever-Instanzen (vollständig entkoppelte Datenflüsse)."""

    @staticmethod
    def build(intent: str, cfg: Dict[str, Any]) -> FAISSRetriever:
        """Erzeuge deterministischen Retriever für den angegebenen Intent."""
        paths = cfg.get("paths", {})
        opts = cfg.get("options", {})

        # Basisparameter, deterministisch
        common_args = dict(
            model_name=opts.get("embedding_model", "all-MiniLM-L6-v2"),
            normalize_embeddings=True,
            use_gpu=False,
            similarity_metric="cosine",
            temporal_tau=float(opts.get("temporal_tau", 8.0)),
            temporal_weight=float(opts.get("temporal_weight", 0.30)),
            top_k_retrieve=int(opts.get("top_k_retrieve", 80)),
        )

        # Intent → Index-Ordner Mapping
        index_map = {
            "conceptual": paths.get("conceptual_vector_dir", "data/vector_store/conceptual"),
            "chronological": paths.get("chronological_vector_dir", "data/vector_store/chronological"),
            "analytical": paths.get("analytical_vector_dir", "data/vector_store/analytical"),
            "comparative": paths.get("comparative_vector_dir", "data/vector_store/comparative"),
        }

        # Fallback auf globales Standardverzeichnis
        vdir = index_map.get(intent, paths.get("vector_store_dir", "data/vector_store/default"))
        temporal_flag = (intent == "chronological")

        logger.info(f"Building FAISSRetriever for intent='{intent}' | dir={vdir} | temporal={temporal_flag}")

        return FAISSRetriever(
            vector_store_dir=vdir,
            temporal_awareness=temporal_flag,
            **common_args,
        )

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\semantic_reranker.py ==== 
# src/core/retrieval/semantic_reranker.py
from __future__ import annotations
import logging
from typing import List, Dict, Any
from sentence_transformers import CrossEncoder
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class SemanticReranker(IReranker):
    """Cross-Encoder-basiertes Re-Ranking nach semantischer Relevanz."""

    def __init__(
        self,
        model_name: str,
        top_k: int = 25,
        semantic_weight: float = 0.75,
        use_gpu: bool = False,
    ):
        self.model_name = model_name
        self.top_k = top_k
        self.semantic_weight = semantic_weight
        self.device = "cuda" if use_gpu else "cpu"
        self.model = CrossEncoder(model_name, device=self.device)
        logger.info(f"Semantic Cross-Encoder loaded: {model_name} ({self.device})")

    def rerank(self, docs: List[Dict[str, Any]], top_k: int | None = None) -> List[Dict[str, Any]]:
        """Score documents via Cross-Encoder similarity."""
        if not docs:
            return []

        k = top_k or self.top_k
        pairs = [(d.get("query", ""), d.get("text", "")) for d in docs]
        scores = self.model.predict(pairs)

        for d, score in zip(docs, scores):
            base = float(d.get("score", 0.0))
            d["final_score"] = self.semantic_weight * float(score) + (1 - self.semantic_weight) * base

        docs.sort(key=lambda x: x["final_score"], reverse=True)
        return docs[:k]

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\temporal_reranker.py ==== 
# src/core/retrieval/temporal_reranker.py
from __future__ import annotations
import logging, math
from statistics import mean, pstdev, median
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Any, Set
from src.core.retrieval.interfaces.i_reranker import IReranker

logger = logging.getLogger(__name__)

class TemporalReranker(IReranker):
    """Combines semantic scores with temporal diversity balancing."""

    def __init__(
        self,
        lambda_weight: float = 0.55,
        min_year: int = 1900,
        enforce_decade_balance: bool = True,
        age_score_boost: float = 0.25,
        min_decade_threshold: int = 3,
        nonlinear_boost: str = "sigmoid",
        ignore_years: List[int] | None = None,
        recency_cutoff_year: int | None = None,
        allow_legacy_backfill: bool = True,
        legacy_backfill_max_ratio: float = 0.3,
        must_include: List[str] | None = None,
        blacklist_sources: List[str] | None = None,
        zscore_normalization: bool = False,
    ):
        self.lambda_weight = lambda_weight
        self.age_score_boost = age_score_boost
        self.enforce_decade_balance = enforce_decade_balance
        self.min_decade_threshold = min_decade_threshold
        self.nonlinear_boost = nonlinear_boost.lower()
        self.min_year = min_year
        self.ignore_years: Set[int] = set(ignore_years or [])
        self.recency_cutoff_year = recency_cutoff_year
        self.allow_legacy_backfill = allow_legacy_backfill
        self.legacy_backfill_max_ratio = legacy_backfill_max_ratio
        self.must_include = must_include or []
        self.blacklist_sources = blacklist_sources or []
        self.zscore_normalization = zscore_normalization

    # ------------------------------------------------------------------
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 10) -> List[Dict[str, Any]]:
        if not results:
            return []

        # Jahr extrahieren + Filter anwenden
        for r in results:
            r["year"] = self._extract_year(r)
        results = [r for r in results if r["year"] not in self.ignore_years]
        results = [r for r in results if not self._is_blacklisted(r)]
        if not results:
            return []

        # Z-Score-Normalisierung optional
        if self.zscore_normalization:
            self._normalize_scores(results)

        # Zeitliche Gewichtung
        now = datetime.utcnow().year
        for r in results:
            y = r["year"]
            r["adjusted_score"] = self._apply_age_boost(r.get("score", 0.0), y, now)

        results.sort(key=lambda x: x["adjusted_score"], reverse=True)
        decade_groups = self._group_by_decade(results)
        decades = sorted(decade_groups.keys())

        if not decades:
            return results[:top_k]

        λ = self._adaptive_lambda(len(decades))
        selected = (
            self._balanced_decade_selection(decade_groups, decades, top_k)
            if self.enforce_decade_balance
            else results[:top_k]
        )

        median_dec = int(median(decades))
        for r in selected:
            base = r.get("adjusted_score", 0.0)
            dec_diff = abs((r["year"] // 10) * 10 - median_dec)
            temporal_div = 1 / (1 + dec_diff / 10)
            r["final_score"] = λ * base + (1 - λ) * temporal_div

        ranked = sorted(selected, key=lambda x: x["final_score"], reverse=True)
        ranked = self._apply_recency_cutoff(ranked, results, top_k)
        ranked = self._inject_must_include(ranked, results, top_k)

        logger.info(
            f"Temporal reranking complete | decades={len(decades)} | λ={λ:.2f} | age_boost={self.age_score_boost:.2f}"
        )
        return ranked[:top_k]

    # ------------------------------------------------------------------
    def _extract_year(self, r: Dict[str, Any]) -> int:
        meta = r.get("metadata", {})
        y = meta.get("year") or r.get("year")
        try:
            val = int(str(y))
            if val < self.min_year or val > 2100:
                raise ValueError
            return val
        except Exception:
            return self.min_year

    def _group_by_decade(self, results: List[Dict[str, Any]]) -> Dict[int, List[Dict[str, Any]]]:
        out: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
        for r in results:
            d = (r["year"] // 10) * 10
            out[d].append(r)
        for d in out:
            out[d].sort(key=lambda x: x.get("adjusted_score", 0.0), reverse=True)
        return out

    def _apply_age_boost(self, base: float, year: int, now: int) -> float:
        age = max(0, now - year)
        if self.nonlinear_boost == "sigmoid":
            s = 1 / (1 + math.exp((age - 6) / 4.0))
        elif self.nonlinear_boost == "sqrt":
            s = math.sqrt(max(0, 1 - min(age / 40, 1)))
        else:
            s = max(0, 1 - min(age / 30, 1))
        return base + self.age_score_boost * s

    def _adaptive_lambda(self, n_decades: int) -> float:
        if n_decades < self.min_decade_threshold:
            return max(0.3, self.lambda_weight * (n_decades / self.min_decade_threshold))
        return min(1.0, self.lambda_weight + 0.05 * math.log1p(n_decades))

    def _balanced_decade_selection(self, groups: Dict[int, List[Dict[str, Any]]], decades: List[int], top_k: int):
        selected: List[Dict[str, Any]] = []
        for d in decades:
            if groups[d]:
                selected.append(groups[d].pop(0))
                if len(selected) >= top_k:
                    return selected
        i = 0
        while len(selected) < top_k and any(groups.values()):
            d = decades[i % len(decades)]
            if groups[d]:
                selected.append(groups[d].pop(0))
            i += 1
        return selected

    def _normalize_scores(self, results: List[Dict[str, Any]]):
        vals = [r.get("score", 0.0) for r in results]
        if len(vals) < 2:
            return
        μ, σ = mean(vals), pstdev(vals)
        if σ < 1e-8:
            return
        for r in results:
            r["score"] = (r["score"] - μ) / σ

    def _apply_recency_cutoff(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        if not self.recency_cutoff_year:
            return ranked[:top_k]
        recent = [r for r in ranked if r["year"] >= self.recency_cutoff_year]
        legacy = [r for r in ranked if r["year"] < self.recency_cutoff_year]
        if len(recent) >= top_k:
            return recent[:top_k]
        if not self.allow_legacy_backfill:
            return recent
        max_legacy = int(top_k * self.legacy_backfill_max_ratio)
        return (recent + legacy[:max_legacy])[:top_k]

    def _inject_must_include(self, ranked: List[Dict[str, Any]], all_results: List[Dict[str, Any]], top_k: int):
        must = [r for r in all_results if self._matches_must_include(r)]
        if not must:
            return ranked
        merged, seen = [], set()
        for r in must + ranked:
            key = self._src_key(r)
            if key not in seen:
                seen.add(key)
                merged.append(r)
            if len(merged) >= top_k:
                break
        return merged

    def _src_key(self, r: Dict[str, Any]) -> str:
        meta = r.get("metadata", {})
        return (meta.get("source_file") or meta.get("title") or "unknown").lower()

    def _matches_must_include(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(m.lower() in key for m in self.must_include)

    def _is_blacklisted(self, r: Dict[str, Any]) -> bool:
        key = self._src_key(r)
        return any(b.lower() in key for b in self.blacklist_sources)

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\__init__.py ==== 

==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_reranker.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IReranker(ABC):
    """Interface for reranker strategies (temporal, semantic, hybrid, etc.)."""
    @abstractmethod
    def rerank(self, results: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
        # Rerank a list of retrieval results based on a custom strategy
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\i_retriever.py ==== 
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any, Dict, List


class IRetriever(ABC):
    """Interface for all retriever implementations."""
    @abstractmethod
    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # Return top-k relevant documents/chunks for a query
        pass

    @abstractmethod
    def close(self) -> None:
        # Release resources if necessary
        pass
.
==== C:\Users\katha\historical-drift-analyzer\src\core\retrieval\interfaces\__init__.py ==== 
.
